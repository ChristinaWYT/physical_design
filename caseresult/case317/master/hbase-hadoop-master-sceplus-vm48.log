Wed Aug  6 15:10:00 PDT 2014 Starting master on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-08-06 15:10:00,984 INFO  [main] util.VersionInfo: HBase 2.0.0-SNAPSHOT
2014-08-06 15:10:00,984 INFO  [main] util.VersionInfo: Subversion git://sceplus-vm48/home/hadoop/hbase-2.0.0-SNAPSHOT -r 50ac59fa8530bbd35c21cd61cfd64d2bd7d3eb57
2014-08-06 15:10:00,985 INFO  [main] util.VersionInfo: Compiled by hadoop on Sun Aug  3 14:37:33 PDT 2014
2014-08-06 15:10:01,254 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/
2014-08-06 15:10:01,254 INFO  [main] util.ServerCommandLine: env:SHLVL=4
2014-08-06 15:10:01,254 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-master-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Dhbase.security.logger=INFO,RFAS
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.53 41269 22
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=12288
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-master.znode
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-08-06 15:10:01,255 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-08-06 15:10:01,256 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-08-06 15:10:01,256 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.53 41269 9.1.143.58 22
2014-08-06 15:10:01,256 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-08-06 15:10:01,256 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-08-06 15:10:01,256 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-08-06 15:10:01,259 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.7.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/../hbase-server/target:/home/hadoop/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/.m2/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/home/hadoop/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/home/hadoop/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/home/hadoop/.m2/repository/com/lmax/disruptor/3.2.0/disruptor-3.2.0.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/hadoop/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/.m2/repository/commons-codec/commons-codec/1.7/commons-codec-1.7.jar:/home/hadoop/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/hadoop/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/hadoop/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/.m2/repository/io/netty/netty-all/4.0.19.Final/netty-all-4.0.19.Final.jar:/home/hadoop/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/hadoop/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/hadoop/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/../lib/tools.jar:/home/hadoop/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/hadoop/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/hadoop/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/home/hadoop/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-hs/2.4.0/hadoop-mapreduce-client-hs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.4.0/hadoop-minicluster-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.4.0/hadoop-yarn-server-applicationhistoryservice-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.4.0/hadoop-yarn-server-nodemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.4.0/hadoop-yarn-server-resourcemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/2.4.0/hadoop-yarn-server-tests-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.4.0/hadoop-yarn-server-web-proxy-2.4.0.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-client/target/hbase-client-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-it/target/hbase-it-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-prefix-tree/target/hbase-prefix-tree-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-protocol/target/hbase-protocol-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-shell/target/hbase-shell-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-testing-util/target/hbase-testing-util-2.0.0-SNAPSHOT.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/home/hadoop/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jettison/jettison/1.3.1/jettison-1.3.1.jar:/home/hadoop/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/hadoop/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/hadoop/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/.m2/repository/org/jboss/netty/netty/3.2.4.Final/netty-3.2.4.Final.jar:/home/hadoop/.m2/repository/org/jruby/jruby-complete/1.6.8/jruby-complete-1.6.8.jar:/home/hadoop/.m2/repository/org/mockito/mockito-all/1.9.0/mockito-all-1.9.0.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/home/hadoop/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/hadoop/.m2/repository/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar:/home/hadoop/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/*.jar:::/home/hadoop/hbase/selfdapql.jar
2014-08-06 15:10:01,260 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-08-06 15:10:01,260 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-08-06 15:10:01,260 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-08-06 15:10:01,260 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_CLASSPATH=:/home/hadoop/hbase/selfdapql.jar
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-master.autorestart
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=3788
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-master-sceplus-vm48.log
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-master-sceplus-vm48
2014-08-06 15:10:01,261 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-08-06 15:10:01,264 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Oracle Corporation, vmVersion=24.51-b03
2014-08-06 15:10:01,265 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_master, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx12288m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-master-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Dhbase.security.logger=INFO,RFAS]
2014-08-06 15:10:01,681 INFO  [main] regionserver.RSRpcServices: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020 server-side HConnection retries=350
2014-08-06 15:10:01,890 INFO  [main] ipc.SimpleRpcScheduler: Using deadline as user call queue, count=5
2014-08-06 15:10:01,917 INFO  [main] ipc.RpcServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020: started 10 reader(s).
2014-08-06 15:10:02,058 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-08-06 15:10:02,075 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-08-06 15:10:02,169 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-08-06 15:10:02,169 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-08-06 15:10:02,418 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-08-06 15:10:02,423 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache size=4.76 GB, blockSize=64 KB
2014-08-06 15:10:02,441 INFO  [main] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:10:03,147 INFO  [main] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-08-06 15:10:03,162 WARN  [main] trace.SpanReceiverHost: Class org.cloudera.htrace.impl.LocalFileSpanReceiver cannot be found. org.cloudera.htrace.impl.LocalFileSpanReceiver
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
2014-08-06 15:10:03,186 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.7.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/../hbase-server/target:/home/hadoop/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/.m2/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/home/hadoop/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/home/hadoop/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/home/hadoop/.m2/repository/com/lmax/disruptor/3.2.0/disruptor-3.2.0.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/hadoop/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/.m2/repository/commons-codec/commons-codec/1.7/commons-codec-1.7.jar:/home/hadoop/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/hadoop/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/hadoop/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/.m2/repository/io/netty/netty-all/4.0.19.Final/netty-all-4.0.19.Final.jar:/home/hadoop/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/hadoop/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/hadoop/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/../lib/tools.jar:/home/hadoop/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/hadoop/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/hadoop/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/home/hadoop/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-hs/2.4.0/hadoop-mapreduce-client-hs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.4.0/hadoop-minicluster-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.4.0/hadoop-yarn-server-applicationhistoryservice-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.4.0/hadoop-yarn-server-nodemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.4.0/hadoop-yarn-server-resourcemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/2.4.0/hadoop-yarn-server-tests-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.4.0/hadoop-yarn-server-web-proxy-2.4.0.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-client/target/hbase-client-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-it/target/hbase-it-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-prefix-tree/target/hbase-prefix-tree-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-protocol/target/hbase-protocol-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-shell/target/hbase-shell-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-testing-util/target/hbase-testing-util-2.0.0-SNAPSHOT.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/home/hadoop/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jettison/jettison/1.3.1/jettison-1.3.1.jar:/home/hadoop/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/hadoop/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/hadoop/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/.m2/repository/org/jboss/netty/netty/3.2.4.Final/netty-3.2.4.Final.jar:/home/hadoop/.m2/repository/org/jruby/jruby-complete/1.6.8/jruby-complete-1.6.8.jar:/home/hadoop/.m2/repository/org/mockito/mockito-all/1.9.0/mockito-all-1.9.0.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/home/hadoop/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/hadoop/.m2/repository/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar:/home/hadoop/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/*.jar:::/home/hadoop/hbase/selfdapql.jar
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-08-06 15:10:03,187 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop
2014-08-06 15:10:03,188 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=master:16020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:03,207 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=master:16020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:03,208 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:03,213 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:10:03,239 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120000, negotiated timeout = 90000
2014-08-06 15:10:03,309 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-08-06 15:10:03,310 INFO  [RpcServer.listener,port=16020] ipc.RpcServer: RpcServer.listener,port=16020: starting
2014-08-06 15:10:03,392 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-08-06 15:10:03,397 INFO  [main] http.HttpRequestLog: Http request log for http.requests.master is not defined
2014-08-06 15:10:03,412 INFO  [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter)
2014-08-06 15:10:03,415 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
2014-08-06 15:10:03,415 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2014-08-06 15:10:03,415 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2014-08-06 15:10:03,437 INFO  [main] http.HttpServer: Jetty bound to port 16030
2014-08-06 15:10:03,437 INFO  [main] mortbay.log: jetty-6.1.26
2014-08-06 15:10:03,809 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16030
2014-08-06 15:10:03,810 INFO  [main] master.HMaster: hbase.rootdir=hdfs://master:54310/hbase, hbase.cluster.distributed=true
2014-08-06 15:10:03,824 INFO  [main] master.HMaster: Adding ZNode for /hbase/backup-masters/sceplus-vm48.almaden.ibm.com,16020,1407363002241 in backup master directory
2014-08-06 15:10:03,947 INFO  [main] mortbay.log: jetty-6.1.26
2014-08-06 15:10:03,949 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010
2014-08-06 15:10:03,970 INFO  [ActiveMasterManager] master.ActiveMasterManager: Deleting ZNode for /hbase/backup-masters/sceplus-vm48.almaden.ibm.com,16020,1407363002241 from backup master directory
2014-08-06 15:10:03,983 INFO  [ActiveMasterManager] master.ActiveMasterManager: Registered Active Master=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:04,146 INFO  [ActiveMasterManager] util.FSUtils: Waiting for dfs to exit safe mode...
2014-08-06 15:10:14,151 INFO  [ActiveMasterManager] util.FSUtils: Waiting for dfs to exit safe mode...
2014-08-06 15:10:24,724 INFO  [ActiveMasterManager] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-08-06 15:10:24,747 INFO  [ActiveMasterManager] master.SplitLogManager: Timeout=120000, unassigned timeout=180000, distributedLogReplay=true
2014-08-06 15:10:24,750 INFO  [ActiveMasterManager] master.SplitLogManager: Found 0 orphan tasks and 0 rescan nodes
2014-08-06 15:10:24,831 INFO  [ActiveMasterManager] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x563eae13, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:24,832 INFO  [ActiveMasterManager] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x563eae13 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:24,833 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:24,835 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-06 15:10:24,840 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147ad5f2b100001, negotiated timeout = 90000
2014-08-06 15:10:25,017 INFO  [ActiveMasterManager] master.HMaster: Server active/primary master=sceplus-vm48.almaden.ibm.com,16020,1407363002241, sessionid=0x47ad5f2e120000, setting cluster-up flag (Was=false)
2014-08-06 15:10:25,018 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: ClusterId : 78c9dafc-f226-435b-aae6-e472cfc131f9
2014-08-06 15:10:25,041 INFO  [ActiveMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort
2014-08-06 15:10:25,046 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.8 G, globalMemStoreLimitLowMark=4.5 G, maxHeap=11.9 G
2014-08-06 15:10:25,051 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-08-06 15:10:25,052 INFO  [ActiveMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/flush-table-proc/acquired /hbase/flush-table-proc/reached /hbase/flush-table-proc/abort
2014-08-06 15:10:25,068 INFO  [ActiveMasterManager] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=replicationLogCleaner, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:25,069 INFO  [ActiveMasterManager] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:25,070 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:25,071 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:10:25,074 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,16020,1407363002241 with port=16020, startcode=1407363002241
2014-08-06 15:10:25,079 WARN  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.
2014-08-06 15:10:25,081 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120003, negotiated timeout = 90000
2014-08-06 15:10:25,103 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn false
2014-08-06 15:10:25,103 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,16020,1407363002241 with port=16020, startcode=1407363002241
2014-08-06 15:10:25,116 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] master.ServerManager: Registering server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:25,154 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 51 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-06 15:10:25,156 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: WAL configuration: blocksize=128 MB, rollsize=121.60 MB, enabled=true, prefix=sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241, logDir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241, oldLogDir=hdfs://master:54310/hbase/oldWALs
2014-08-06 15:10:25,485 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: Slow sync cost: 262 ms, current pipeline: []
2014-08-06 15:10:25,486 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407363025164
2014-08-06 15:10:25,531 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-08-06 15:10:25,545 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,16020,1407363002241] other RSs: [sceplus-vm48.almaden.ibm.com,16020,1407363002241, sceplus-vm49.almaden.ibm.com,16020,1407363003212]
2014-08-06 15:10:25,565 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.ServerManager: Registering server=slave1,16020,1407363003212
2014-08-06 15:10:25,582 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x361e0c23, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:25,582 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x361e0c23 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:25,583 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:25,584 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-06 15:10:25,589 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147ad5f2b100003, negotiated timeout = 90000
2014-08-06 15:10:25,605 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 502 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-06 15:10:25,642 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,16020,1407363002241 starting
2014-08-06 15:10:25,646 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HeapMemoryManager: Starting HeapMemoryTuner chore.
2014-08-06 15:10:25,651 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x592b22fe, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:25,652 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x592b22fe connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:25,653 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:25,653 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,16020,1407363002241, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:16020, sessionid=0x47ad5f2e120000
2014-08-06 15:10:25,654 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:10:25,657 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120004, negotiated timeout = 90000
2014-08-06 15:10:27,111 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 2008 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-06 15:10:28,616 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 3513 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-06 15:10:29,620 INFO  [ActiveMasterManager] master.ServerManager: Finished waiting for region servers count to settle; checked in 2, slept for 4517 ms, expecting minimum of 2, maximum of 2147483647, master is running, selfCheckedIn true
2014-08-06 15:10:29,621 INFO  [ActiveMasterManager] master.ServerManager: Registering server=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:29,621 INFO  [ActiveMasterManager] master.HMaster: Registered server found up in zk but who has not yet reported in: sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:29,627 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146 doesn't belong to a known region server, splitting
2014-08-06 15:10:29,627 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241 belongs to an existing region server
2014-08-06 15:10:29,628 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting doesn't belong to a known region server, splitting
2014-08-06 15:10:29,628 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929 doesn't belong to a known region server, splitting
2014-08-06 15:10:29,628 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/slave1,16020,1407363003212 belongs to an existing region server
2014-08-06 15:10:29,725 INFO  [ActiveMasterManager] zookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address=sceplus-vm48.almaden.ibm.com,16020,1407361669146, exception=org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on sceplus-vm48.almaden.ibm.com,16020,1407363002241
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2605)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:795)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1067)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:20158)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2013)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
	at java.lang.Thread.run(Thread.java:744)

2014-08-06 15:10:29,739 INFO  [ActiveMasterManager] zookeeper.MetaTableLocator: Unsetting hbase:meta region location in ZooKeeper
2014-08-06 15:10:29,785 INFO  [ActiveMasterManager] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:29,786 INFO  [ActiveMasterManager] master.RegionStates: Transition {1588230740 state=OFFLINE, ts=1407363029663, server=null} to {1588230740 state=PENDING_OPEN, ts=1407363029785, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:10:29,798 INFO  [ActiveMasterManager] regionserver.RSRpcServices: Open hbase:meta,,1.1588230740
2014-08-06 15:10:29,804 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: WAL configuration: blocksize=128 MB, rollsize=121.60 MB, enabled=true, prefix=sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241, logDir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241, oldLogDir=hdfs://master:54310/hbase/oldWALs
2014-08-06 15:10:29,838 INFO  [ActiveMasterManager] master.SplitLogManager: dead splitlog workers [sceplus-vm48.almaden.ibm.com,16020,1407361669146, slave1,16020,1407361709205, slave1,16020,1407362699929]
2014-08-06 15:10:29,856 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: Slow sync cost: 27 ms, current pipeline: []
2014-08-06 15:10:29,856 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407363029810.meta
2014-08-06 15:10:29,869 INFO  [ActiveMasterManager] master.SplitLogManager: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting is empty dir, no logs to split
2014-08-06 15:10:29,872 INFO  [ActiveMasterManager] master.SplitLogManager: hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting is empty dir, no logs to split
2014-08-06 15:10:29,872 INFO  [ActiveMasterManager] master.SplitLogManager: started splitting 1 logs in [hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting, hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting, hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting]
2014-08-06 15:10:29,902 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
2014-08-06 15:10:29,903 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta
2014-08-06 15:10:29,904 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:29,966 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta, length=9
2014-08-06 15:10:29,966 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:10:29,972 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta
2014-08-06 15:10:29,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=false, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta after 17ms
2014-08-06 15:10:29,999 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:10:30,011 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:10:30,119 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=80012096
2014-08-06 15:10:30,121 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for hbase:meta,,1.1588230740
2014-08-06 15:10:30,137 INFO  [PostOpenDeployTasks:1588230740] zookeeper.MetaTableLocator: Setting hbase:meta region location in ZooKeeper as sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:30,164 INFO  [PostOpenDeployTasks:1588230740] master.RegionStates: Transition {1588230740 state=PENDING_OPEN, ts=1407363029785, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {1588230740 state=OPEN, ts=1407363030164, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:10:30,164 INFO  [PostOpenDeployTasks:1588230740] master.RegionStates: Onlined 1588230740 on sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:30,752 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 1 unassigned = 0 tasks={/hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta=last_update = 1407363029971 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0}
2014-08-06 15:10:33,992 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=1 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta after 4020ms
2014-08-06 15:10:34,071 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x5ec1d56b, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:34,074 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5ec1d56b connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:34,074 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:34,075 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:10:34,082 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120006, negotiated timeout = 90000
2014-08-06 15:10:34,705 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:10:34,707 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47ad5f2e120006
2014-08-06 15:10:34,710 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47ad5f2e120006 closed
2014-08-06 15:10:34,710 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-06 15:10:34,812 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 118 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta is corrupted = false progress failed = false
2014-08-06 15:10:34,820 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:34,820 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta in 4911ms
2014-08-06 15:10:34,822 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:34,859 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta to hdfs://master:54310/hbase/oldWALs/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361697833.meta
2014-08-06 15:10:34,864 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361697833.meta
2014-08-06 15:10:34,885 INFO  [main-EventThread] zookeeper.RecoveringRegionWatcher: /hbase/recovering-regions/1588230740 znode deleted. Region: 1588230740 completes recovery.
2014-08-06 15:10:34,906 WARN  [ActiveMasterManager] master.SplitLogManager: returning success without actually splitting and deleting all the log files in path hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting
2014-08-06 15:10:34,927 WARN  [ActiveMasterManager] master.SplitLogManager: returning success without actually splitting and deleting all the log files in path hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting
2014-08-06 15:10:34,934 WARN  [ActiveMasterManager] master.SplitLogManager: returning success without actually splitting and deleting all the log files in path hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting
2014-08-06 15:10:34,935 INFO  [ActiveMasterManager] master.SplitLogManager: finished splitting (more than or equal to) 9 bytes in 1 log files in [hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting, hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting, hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting] in 5062ms
2014-08-06 15:10:34,935 INFO  [ActiveMasterManager] master.ServerManager: AssignmentManager hasn't finished failover cleanup; waiting
2014-08-06 15:10:34,939 INFO  [ActiveMasterManager] master.HMaster: hbase:meta assigned=1, rit=false, location=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,061 INFO  [ActiveMasterManager] hbase.MetaMigrationConvertingToPB: META already up-to date with PB serialization
2014-08-06 15:10:35,124 INFO  [ActiveMasterManager] master.AssignmentManager: Found regions out on cluster or in RIT; presuming failover
2014-08-06 15:10:35,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407361709205 before assignment.
2014-08-06 15:10:35,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-06 15:10:35,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Reassigning 0 region(s) that slave1,16020,1407361709205 was carrying (and 0 regions(s) that were opening on this server)
2014-08-06 15:10:35,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Splitting logs for sceplus-vm48.almaden.ibm.com,16020,1407361669146 before assignment.
2014-08-06 15:10:35,135 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-06 15:10:35,136 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407362699929 before assignment.
2014-08-06 15:10:35,136 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-06 15:10:35,141 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: dead splitlog workers [slave1,16020,1407361709205]
2014-08-06 15:10:35,147 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363035116, server=sceplus-vm48.almaden.ibm.com,16020,1407361669146} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OFFLINE, ts=1407363035147, server=sceplus-vm48.almaden.ibm.com,16020,1407361669146}
2014-08-06 15:10:35,148 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OFFLINE
2014-08-06 15:10:35,149 INFO  [ActiveMasterManager] master.AssignmentManager: Joined the cluster in 88ms, failover=true
2014-08-06 15:10:35,154 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Reassigning 1 region(s) that sceplus-vm48.almaden.ibm.com,16020,1407361669146 was carrying (and 0 regions(s) that were opening on this server)
2014-08-06 15:10:35,162 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: started splitting 42 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting]
2014-08-06 15:10:35,184 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Bulk assigning 1 region(s) across 3 server(s), round-robin=true
2014-08-06 15:10:35,187 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.AssignmentManager: Assigning 1 region(s) to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,191 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OFFLINE, ts=1407363035147, server=sceplus-vm48.almaden.ibm.com,16020,1407361669146} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363035191, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:10:35,191 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN&sn=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,196 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:10:35,218 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834 acquired by slave1,16020,1407363003212
2014-08-06 15:10:35,220 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Bulk assigning done
2014-08-06 15:10:35,220 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for fe1c441ecb6e3789e0fdd62314d7b06a to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:10:35,225 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
2014-08-06 15:10:35,226 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,228 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:10:35,228 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:10:35,258 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=40000008
2014-08-06 15:10:35,259 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:10:35,261 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153, length=131546616
2014-08-06 15:10:35,262 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:10:35,266 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153
2014-08-06 15:10:35,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 after 2ms
2014-08-06 15:10:35,273 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363035191, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363035273, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:10:35,274 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=40000008&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,278 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Onlined fe1c441ecb6e3789e0fdd62314d7b06a on sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,284 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: dead splitlog workers [sceplus-vm48.almaden.ibm.com,16020,1407361669146]
2014-08-06 15:10:35,287 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: started splitting 1 logs in [hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting]
2014-08-06 15:10:35,320 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=OPEN, ts=1407363035117, server=slave1,16020,1407362699929} to {9a384662d9ff6cc90600a0f3504ef123 state=OFFLINE, ts=1407363035320, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,320 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=OFFLINE
2014-08-06 15:10:35,324 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=OPEN, ts=1407363035120, server=slave1,16020,1407362699929} to {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363035324, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,324 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=OFFLINE
2014-08-06 15:10:35,327 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=OPEN, ts=1407363035118, server=slave1,16020,1407362699929} to {2baa40eaf58caba1c3e44d5edf1ac795 state=OFFLINE, ts=1407363035327, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,327 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=OFFLINE
2014-08-06 15:10:35,331 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=OPEN, ts=1407363035122, server=slave1,16020,1407362699929} to {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363035331, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,331 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=OFFLINE
2014-08-06 15:10:35,333 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=OPEN, ts=1407363035119, server=slave1,16020,1407362699929} to {0c3669f1b658ec2cfea4662920345d3f state=OFFLINE, ts=1407363035333, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,334 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=OFFLINE
2014-08-06 15:10:35,336 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=OPEN, ts=1407363035118, server=slave1,16020,1407362699929} to {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363035336, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,336 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=OFFLINE
2014-08-06 15:10:35,338 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=OPEN, ts=1407363035119, server=slave1,16020,1407362699929} to {2a16f0086072207fedd5efb5f679bde4 state=OFFLINE, ts=1407363035338, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,338 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=OFFLINE
2014-08-06 15:10:35,341 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=OPEN, ts=1407363035121, server=slave1,16020,1407362699929} to {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363035341, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,341 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=OFFLINE
2014-08-06 15:10:35,343 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=OPEN, ts=1407363035122, server=slave1,16020,1407362699929} to {941f3daa3189b04c7d5d94a986175384 state=OFFLINE, ts=1407363035343, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,343 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=OFFLINE
2014-08-06 15:10:35,345 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=OPEN, ts=1407363035121, server=slave1,16020,1407362699929} to {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363035345, server=slave1,16020,1407362699929}
2014-08-06 15:10:35,345 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=OFFLINE
2014-08-06 15:10:35,348 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Reassigning 10 region(s) that slave1,16020,1407362699929 was carrying (and 0 regions(s) that were opening on this server)
2014-08-06 15:10:35,416 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x204c84ff, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:10:35,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x204c84ff connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:10:35,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:10:35,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:10:35,422 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120007, negotiated timeout = 90000
2014-08-06 15:10:35,524 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Bulk assigning 10 region(s) across 3 server(s), round-robin=true
2014-08-06 15:10:35,525 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.AssignmentManager: Assigning 5 region(s) to sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,525 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363035324, server=slave1,16020,1407362699929} to {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363035525, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:35,525 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.AssignmentManager: Assigning 5 region(s) to slave1,16020,1407363003212
2014-08-06 15:10:35,525 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,526 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=OFFLINE, ts=1407363035320, server=slave1,16020,1407362699929} to {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_OPEN, ts=1407363035526, server=slave1,16020,1407363003212}
2014-08-06 15:10:35,526 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:35,530 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363035331, server=slave1,16020,1407362699929} to {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363035530, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:35,530 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=OFFLINE, ts=1407363035327, server=slave1,16020,1407362699929} to {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_OPEN, ts=1407363035530, server=slave1,16020,1407363003212}
2014-08-06 15:10:35,530 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,530 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:35,533 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=OFFLINE, ts=1407363035333, server=slave1,16020,1407362699929} to {0c3669f1b658ec2cfea4662920345d3f state=PENDING_OPEN, ts=1407363035533, server=slave1,16020,1407363003212}
2014-08-06 15:10:35,533 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363035336, server=slave1,16020,1407362699929} to {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363035533, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:35,533 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:35,533 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,536 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=OFFLINE, ts=1407363035338, server=slave1,16020,1407362699929} to {2a16f0086072207fedd5efb5f679bde4 state=PENDING_OPEN, ts=1407363035536, server=slave1,16020,1407363003212}
2014-08-06 15:10:35,536 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:35,537 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363035341, server=slave1,16020,1407362699929} to {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363035537, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:35,537 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,538 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=OFFLINE, ts=1407363035343, server=slave1,16020,1407362699929} to {941f3daa3189b04c7d5d94a986175384 state=PENDING_OPEN, ts=1407363035538, server=slave1,16020,1407363003212}
2014-08-06 15:10:35,538 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:35,539 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363035345, server=slave1,16020,1407362699929} to {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363035539, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:35,539 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:35,892 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Bulk assigning done
2014-08-06 15:10:35,893 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 9a384662d9ff6cc90600a0f3504ef123 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:10:35,899 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
2014-08-06 15:10:35,900 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:10:35,936 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024, length=131664368
2014-08-06 15:10:35,936 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:10:35,941 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024
2014-08-06 15:10:35,943 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 after 2ms
2014-08-06 15:10:36,081 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_OPEN, ts=1407363035526, server=slave1,16020,1407363003212} to {9a384662d9ff6cc90600a0f3504ef123 state=OPEN, ts=1407363036081, server=slave1,16020,1407363003212}
2014-08-06 15:10:36,081 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407363003212
2014-08-06 15:10:36,085 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Onlined 9a384662d9ff6cc90600a0f3504ef123 on slave1,16020,1407363003212
2014-08-06 15:10:36,085 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 97e653821cd22644b2780dc096d62059 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:10:36,167 ERROR [defaultRpcServer.handler=8,queue=3,port=16020] master.AssignmentManager: Failed to transtion region from {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363035530, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to OPENED by slave1,16020,1407363003212: 3ce788492d2889f8070653d1a524657a is not pending open on slave1,16020,1407363003212
2014-08-06 15:10:36,171 ERROR [defaultRpcServer.handler=40,queue=0,port=16020] master.AssignmentManager: Failed to transtion region from {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363035525, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to OPENED by slave1,16020,1407363003212: 97e653821cd22644b2780dc096d62059 is not pending open on slave1,16020,1407363003212
2014-08-06 15:10:36,172 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640 acquired by slave1,16020,1407363003212
2014-08-06 15:10:36,176 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_OPEN, ts=1407363035530, server=slave1,16020,1407363003212} to {2baa40eaf58caba1c3e44d5edf1ac795 state=OPEN, ts=1407363036176, server=slave1,16020,1407363003212}
2014-08-06 15:10:36,177 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=OPEN&openSeqNum=40002928&server=slave1,16020,1407363003212
2014-08-06 15:10:36,182 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Onlined 2baa40eaf58caba1c3e44d5edf1ac795 on slave1,16020,1407363003212
2014-08-06 15:10:36,191 ERROR [defaultRpcServer.handler=17,queue=2,port=16020] master.MasterRpcServices: Region server slave1,16020,1407363003212 reported a fatal error:
ABORTING region server slave1,16020,1407363003212: Exception running postOpenDeployTasks; region=3ce788492d2889f8070653d1a524657a
Cause:
java.io.IOException: Failed to report opened region to master: usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1730)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:321)

2014-08-06 15:10:36,191 ERROR [defaultRpcServer.handler=22,queue=2,port=16020] master.MasterRpcServices: Region server slave1,16020,1407363003212 reported a fatal error:
ABORTING region server slave1,16020,1407363003212: Exception running postOpenDeployTasks; region=97e653821cd22644b2780dc096d62059
Cause:
java.io.IOException: Failed to report opened region to master: usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1730)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:321)

2014-08-06 15:10:36,753 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:10:42,751 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:10:46,075 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sceplus-vm49.almaden.ibm.com,16020,1407363003212]
2014-08-06 15:10:46,080 INFO  [main-EventThread] replication.ReplicationTrackerZKImpl: /hbase/rs/sceplus-vm49.almaden.ibm.com,16020,1407363003212 znode expired, triggering replicatorRemoved event
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Splitting logs for sceplus-vm49.almaden.ibm.com,16020,1407363003212 before assignment.
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363035537, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363035539, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363035533, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363035525, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:46,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363035530, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:10:46,084 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/e9de6146bf9f987041eeac083bc7b535 already deleted, retry=false
2014-08-06 15:10:46,084 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363035537, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363046084, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:46,084 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=OFFLINE
2014-08-06 15:10:46,091 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/d39bb1863f3baceabd838c633cba9f20 already deleted, retry=false
2014-08-06 15:10:46,091 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363035539, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363046091, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:46,091 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=OFFLINE
2014-08-06 15:10:46,096 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/a390894508c5f49c438ee5a3585e0f78 already deleted, retry=false
2014-08-06 15:10:46,096 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363035533, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363046096, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:46,096 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=OFFLINE
2014-08-06 15:10:46,109 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/97e653821cd22644b2780dc096d62059 already deleted, retry=false
2014-08-06 15:10:46,109 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363035525, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363046109, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:46,109 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=OFFLINE
2014-08-06 15:10:46,121 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/3ce788492d2889f8070653d1a524657a already deleted, retry=false
2014-08-06 15:10:46,121 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363035530, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363046121, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212}
2014-08-06 15:10:46,121 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=OFFLINE
2014-08-06 15:10:46,124 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Reassigning 0 region(s) that sceplus-vm49.almaden.ibm.com,16020,1407363003212 was carrying (and 5 regions(s) that were opening on this server)
2014-08-06 15:10:46,124 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Assigning 5 region(s) to slave1,16020,1407363003212
2014-08-06 15:10:46,125 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363046084, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363046124, server=slave1,16020,1407363003212}
2014-08-06 15:10:46,125 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:46,127 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363046091, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363046127, server=slave1,16020,1407363003212}
2014-08-06 15:10:46,127 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:46,129 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363046096, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363046129, server=slave1,16020,1407363003212}
2014-08-06 15:10:46,129 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:46,131 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363046109, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363046131, server=slave1,16020,1407363003212}
2014-08-06 15:10:46,132 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:46,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363046121, server=sceplus-vm49.almaden.ibm.com,16020,1407363003212} to {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363046134, server=slave1,16020,1407363003212}
2014-08-06 15:10:46,134 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=PENDING_OPEN&sn=slave1,16020,1407363003212
2014-08-06 15:10:46,139 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Unable to communicate with slave1,16020,1407363003212 in order to assign regions, 
java.io.IOException: Call to slave1/9.1.143.59:16020 failed on local exception: java.io.IOException: Connection to slave1/9.1.143.59:16020 is closing. Call id=18, waitTime=1
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1571)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1542)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.openRegion(AdminProtos.java:20964)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:766)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1679)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2653)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2634)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:291)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Connection to slave1/9.1.143.59:16020 is closing. Call id=18, waitTime=1
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.cleanupCalls(RpcClient.java:1265)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.close(RpcClient.java:1059)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.run(RpcClient.java:792)
2014-08-06 15:10:46,143 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for e9de6146bf9f987041eeac083bc7b535 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:10:46,147 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=1 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:46,147 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:46,147 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:46,147 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:46,147 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,149 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,150 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,150 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,152 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=3 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,152 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,152 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=3 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,152 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:48,752 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:10:49,036 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm49.almaden.ibm.com,16020,1407363003212's hlogs to my queue
2014-08-06 15:10:49,047 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm49.almaden.ibm.com,16020,1407363003212/lock
2014-08-06 15:10:50,154 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,202 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,202 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,201 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,154 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,204 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=5 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,204 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=4 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,204 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=4 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:50,204 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=5 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:51,144 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for 97e653821cd22644b2780dc096d62059 to be assigned.
2014-08-06 15:10:51,145 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region 97e653821cd22644b2780dc096d62059 didn't complete assignment in time
2014-08-06 15:10:51,145 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 3ce788492d2889f8070653d1a524657a to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:10:52,206 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,207 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,208 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,210 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=5 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,207 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,210 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=6 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:52,208 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,211 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,212 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,214 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,214 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=7 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,214 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=8 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,215 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,214 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:54,753 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:10:56,215 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,216 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,217 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=9 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,262 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,263 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=9 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,262 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,264 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=9 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,264 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059., try=10 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,266 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.io.IOException: This connection is closing for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=9 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:56,267 WARN  [AM.-pool1-t4] master.RegionStates: Failed to open/close 97e653821cd22644b2780dc096d62059 on slave1,16020,1407363003212, set to FAILED_CLOSE
2014-08-06 15:10:56,268 INFO  [AM.-pool1-t4] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363046131, server=slave1,16020,1407363003212} to {97e653821cd22644b2780dc096d62059 state=FAILED_CLOSE, ts=1407363056268, server=slave1,16020,1407363003212}
2014-08-06 15:10:56,268 INFO  [AM.-pool1-t4] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=FAILED_CLOSE
2014-08-06 15:10:56,279 INFO  [AM.-pool1-t4] master.AssignmentManager: Skip assigning {ENCODED => 97e653821cd22644b2780dc096d62059, NAME => 'usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059.', STARTKEY => 'user5', ENDKEY => 'user6'}, we couldn't close it: {97e653821cd22644b2780dc096d62059 state=FAILED_CLOSE, ts=1407363056268, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,265 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:58,267 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:58,268 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned java.net.ConnectException: Connection refused for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=9 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:58,268 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:58,271 WARN  [AM.-pool1-t5] master.RegionStates: Failed to open/close 3ce788492d2889f8070653d1a524657a on slave1,16020,1407363003212, set to FAILED_CLOSE
2014-08-06 15:10:58,268 WARN  [AM.-pool1-t2] master.RegionStates: Failed to open/close d39bb1863f3baceabd838c633cba9f20 on slave1,16020,1407363003212, set to FAILED_CLOSE
2014-08-06 15:10:58,272 INFO  [AM.-pool1-t5] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363046134, server=slave1,16020,1407363003212} to {3ce788492d2889f8070653d1a524657a state=FAILED_CLOSE, ts=1407363058271, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,269 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:10:58,273 WARN  [AM.-pool1-t3] master.RegionStates: Failed to open/close a390894508c5f49c438ee5a3585e0f78 on slave1,16020,1407363003212, set to FAILED_CLOSE
2014-08-06 15:10:58,272 INFO  [AM.-pool1-t5] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=FAILED_CLOSE
2014-08-06 15:10:58,272 INFO  [AM.-pool1-t2] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363046127, server=slave1,16020,1407363003212} to {d39bb1863f3baceabd838c633cba9f20 state=FAILED_CLOSE, ts=1407363058272, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,273 INFO  [AM.-pool1-t3] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363046129, server=slave1,16020,1407363003212} to {a390894508c5f49c438ee5a3585e0f78 state=FAILED_CLOSE, ts=1407363058273, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,274 INFO  [AM.-pool1-t2] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=FAILED_CLOSE
2014-08-06 15:10:58,275 INFO  [AM.-pool1-t3] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=FAILED_CLOSE
2014-08-06 15:10:58,282 INFO  [AM.-pool1-t5] master.AssignmentManager: Skip assigning {ENCODED => 3ce788492d2889f8070653d1a524657a, NAME => 'usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a.', STARTKEY => 'user9', ENDKEY => ''}, we couldn't close it: {3ce788492d2889f8070653d1a524657a state=FAILED_CLOSE, ts=1407363058271, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,282 INFO  [AM.-pool1-t2] master.AssignmentManager: Skip assigning {ENCODED => d39bb1863f3baceabd838c633cba9f20, NAME => 'usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20.', STARTKEY => 'user7', ENDKEY => 'user8'}, we couldn't close it: {d39bb1863f3baceabd838c633cba9f20 state=FAILED_CLOSE, ts=1407363058272, server=slave1,16020,1407363003212}
2014-08-06 15:10:58,282 INFO  [AM.-pool1-t3] master.AssignmentManager: Skip assigning {ENCODED => a390894508c5f49c438ee5a3585e0f78, NAME => 'usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78.', STARTKEY => 'user2', ENDKEY => 'user3'}, we couldn't close it: {a390894508c5f49c438ee5a3585e0f78 state=FAILED_CLOSE, ts=1407363058273, server=slave1,16020,1407363003212}
2014-08-06 15:10:59,757 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:00,272 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407363003212 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:11:00,273 WARN  [AM.-pool1-t1] master.RegionStates: Failed to open/close e9de6146bf9f987041eeac083bc7b535 on slave1,16020,1407363003212, set to FAILED_CLOSE
2014-08-06 15:11:00,273 INFO  [AM.-pool1-t1] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363046124, server=slave1,16020,1407363003212} to {e9de6146bf9f987041eeac083bc7b535 state=FAILED_CLOSE, ts=1407363060273, server=slave1,16020,1407363003212}
2014-08-06 15:11:00,273 INFO  [AM.-pool1-t1] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=FAILED_CLOSE
2014-08-06 15:11:00,281 INFO  [AM.-pool1-t1] master.AssignmentManager: Skip assigning {ENCODED => e9de6146bf9f987041eeac083bc7b535, NAME => 'usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535.', STARTKEY => 'user6', ENDKEY => 'user7'}, we couldn't close it: {e9de6146bf9f987041eeac083bc7b535 state=FAILED_CLOSE, ts=1407363060273, server=slave1,16020,1407363003212}
2014-08-06 15:11:01,182 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for e9de6146bf9f987041eeac083bc7b535 to be assigned.
2014-08-06 15:11:01,183 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region e9de6146bf9f987041eeac083bc7b535 didn't complete assignment in time
2014-08-06 15:11:01,184 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for d39bb1863f3baceabd838c633cba9f20 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:05,757 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:06,240 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for 3ce788492d2889f8070653d1a524657a to be assigned.
2014-08-06 15:11:06,240 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region 3ce788492d2889f8070653d1a524657a didn't complete assignment in time
2014-08-06 15:11:06,240 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 0c3669f1b658ec2cfea4662920345d3f to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:11,757 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:14,011 INFO  [PriorityRpcServer.handler=0,queue=0,port=16020] regionserver.RSRpcServices: Compacting hbase:meta,,1.1588230740
2014-08-06 15:11:14,018 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HRegion: Starting compaction on info in region hbase:meta,,1.1588230740
2014-08-06 15:11:14,020 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HStore: Starting compaction of 1 file(s) in info of hbase:meta,,1.1588230740 into tmpdir=hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp, totalSize=28.3 K
2014-08-06 15:11:14,023 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:11:14,215 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HStore: Completed major compaction of 1 (all) file(s) in info of hbase:meta,,1.1588230740 into 59a5e287d15a481a98e19833a8d71bcd(size=28.3 K), total size for store is 28.3 K. This selection was in queue for 0sec, and took 0sec to execute.
2014-08-06 15:11:14,218 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.CompactSplitThread: Completed compaction: Request = regionName=hbase:meta,,1.1588230740, storeName=info, fileCount=1, fileSize=28.3 K, priority=1, time=1021481379218215; duration=0sec
2014-08-06 15:11:16,255 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for d39bb1863f3baceabd838c633cba9f20 to be assigned.
2014-08-06 15:11:16,255 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region d39bb1863f3baceabd838c633cba9f20 didn't complete assignment in time
2014-08-06 15:11:16,255 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for a390894508c5f49c438ee5a3585e0f78 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:17,758 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:21,261 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for 0c3669f1b658ec2cfea4662920345d3f to be assigned.
2014-08-06 15:11:21,261 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region 0c3669f1b658ec2cfea4662920345d3f didn't complete assignment in time
2014-08-06 15:11:21,261 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for a390894508c5f49c438ee5a3585e0f78 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:22,759 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:27,759 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:31,277 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for a390894508c5f49c438ee5a3585e0f78 to be assigned.
2014-08-06 15:11:31,278 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region a390894508c5f49c438ee5a3585e0f78 didn't complete assignment in time
2014-08-06 15:11:31,278 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 97e653821cd22644b2780dc096d62059 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:33,761 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:36,282 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for a390894508c5f49c438ee5a3585e0f78 to be assigned.
2014-08-06 15:11:36,283 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region a390894508c5f49c438ee5a3585e0f78 didn't complete assignment in time
2014-08-06 15:11:36,283 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 2a16f0086072207fedd5efb5f679bde4 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:38,761 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:44,762 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:46,300 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for 97e653821cd22644b2780dc096d62059 to be assigned.
2014-08-06 15:11:46,301 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region 97e653821cd22644b2780dc096d62059 didn't complete assignment in time
2014-08-06 15:11:46,302 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 3ce788492d2889f8070653d1a524657a to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:49,763 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:51,305 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for 2a16f0086072207fedd5efb5f679bde4 to be assigned.
2014-08-06 15:11:51,305 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region 2a16f0086072207fedd5efb5f679bde4 didn't complete assignment in time
2014-08-06 15:11:51,305 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for e9de6146bf9f987041eeac083bc7b535 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:11:54,764 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:11:59,765 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:01,322 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for 3ce788492d2889f8070653d1a524657a to be assigned.
2014-08-06 15:12:01,323 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region 3ce788492d2889f8070653d1a524657a didn't complete assignment in time
2014-08-06 15:12:01,332 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.MasterFileSystem: Log dir for server sceplus-vm49.almaden.ibm.com,16020,1407363003212 does not exist
2014-08-06 15:12:01,333 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: dead splitlog workers [sceplus-vm49.almaden.ibm.com,16020,1407363003212]
2014-08-06 15:12:01,333 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: started splitting 0 logs in []
2014-08-06 15:12:01,347 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: finished splitting (more than or equal to) 0 bytes in 0 log files in [] in 14ms
2014-08-06 15:12:01,347 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] handler.LogReplayHandler: Finished processing shutdown of sceplus-vm49.almaden.ibm.com,16020,1407363003212
2014-08-06 15:12:05,766 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:06,327 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for e9de6146bf9f987041eeac083bc7b535 to be assigned.
2014-08-06 15:12:06,327 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region e9de6146bf9f987041eeac083bc7b535 didn't complete assignment in time
2014-08-06 15:12:06,327 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 941f3daa3189b04c7d5d94a986175384 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:10,767 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:16,768 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:21,349 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Timed out on waiting for 941f3daa3189b04c7d5d94a986175384 to be assigned.
2014-08-06 15:12:21,355 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Region 941f3daa3189b04c7d5d94a986175384 didn't complete assignment in time
2014-08-06 15:12:21,355 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for d39bb1863f3baceabd838c633cba9f20 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:21,769 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:27,774 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363036178 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363035260 last_version = 2 cur_worker_name = slave1,16020,1407363003212 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:28,989 INFO  [defaultRpcServer.handler=0,queue=0,port=16020] master.ServerManager: Registering server=slave1,16020,1407363145796
2014-08-06 15:12:28,990 INFO  [defaultRpcServer.handler=0,queue=0,port=16020] master.ServerManager: Triggering server recovery; existingServer slave1,16020,1407363003212 looks stale, new server:slave1,16020,1407363145796
2014-08-06 15:12:28,996 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407363003212 before assignment.
2014-08-06 15:12:28,997 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-06 15:12:29,009 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=OPEN, ts=1407363036081, server=slave1,16020,1407363003212} to {9a384662d9ff6cc90600a0f3504ef123 state=OFFLINE, ts=1407363149009, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,010 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=OFFLINE
2014-08-06 15:12:29,016 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=OPEN, ts=1407363036176, server=slave1,16020,1407363003212} to {2baa40eaf58caba1c3e44d5edf1ac795 state=OFFLINE, ts=1407363149016, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,016 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=OFFLINE
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {941f3daa3189b04c7d5d94a986175384 state=PENDING_OPEN, ts=1407363035538, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {e9de6146bf9f987041eeac083bc7b535 state=FAILED_CLOSE, ts=1407363060273, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {d39bb1863f3baceabd838c633cba9f20 state=FAILED_CLOSE, ts=1407363058272, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {a390894508c5f49c438ee5a3585e0f78 state=FAILED_CLOSE, ts=1407363058273, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {0c3669f1b658ec2cfea4662920345d3f state=PENDING_OPEN, ts=1407363035533, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,019 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {97e653821cd22644b2780dc096d62059 state=FAILED_CLOSE, ts=1407363056268, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,020 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {3ce788492d2889f8070653d1a524657a state=FAILED_CLOSE, ts=1407363058271, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,020 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {2a16f0086072207fedd5efb5f679bde4 state=PENDING_OPEN, ts=1407363035536, server=slave1,16020,1407363003212} to be reassigned by SSH for slave1,16020,1407363003212
2014-08-06 15:12:29,023 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/941f3daa3189b04c7d5d94a986175384 already deleted, retry=false
2014-08-06 15:12:29,023 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=PENDING_OPEN, ts=1407363035538, server=slave1,16020,1407363003212} to {941f3daa3189b04c7d5d94a986175384 state=OFFLINE, ts=1407363149023, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,023 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=OFFLINE
2014-08-06 15:12:29,029 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/e9de6146bf9f987041eeac083bc7b535 already deleted, retry=false
2014-08-06 15:12:29,029 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=FAILED_CLOSE, ts=1407363060273, server=slave1,16020,1407363003212} to {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363149029, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,029 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=OFFLINE
2014-08-06 15:12:29,035 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/d39bb1863f3baceabd838c633cba9f20 already deleted, retry=false
2014-08-06 15:12:29,035 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=FAILED_CLOSE, ts=1407363058272, server=slave1,16020,1407363003212} to {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363149035, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,036 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=OFFLINE
2014-08-06 15:12:29,044 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/a390894508c5f49c438ee5a3585e0f78 already deleted, retry=false
2014-08-06 15:12:29,044 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=FAILED_CLOSE, ts=1407363058273, server=slave1,16020,1407363003212} to {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363149044, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,044 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=OFFLINE
2014-08-06 15:12:29,050 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/0c3669f1b658ec2cfea4662920345d3f already deleted, retry=false
2014-08-06 15:12:29,051 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=PENDING_OPEN, ts=1407363035533, server=slave1,16020,1407363003212} to {0c3669f1b658ec2cfea4662920345d3f state=OFFLINE, ts=1407363149051, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,051 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=OFFLINE
2014-08-06 15:12:29,056 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/97e653821cd22644b2780dc096d62059 already deleted, retry=false
2014-08-06 15:12:29,056 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=FAILED_CLOSE, ts=1407363056268, server=slave1,16020,1407363003212} to {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363149056, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,057 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=OFFLINE
2014-08-06 15:12:29,062 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/3ce788492d2889f8070653d1a524657a already deleted, retry=false
2014-08-06 15:12:29,062 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=FAILED_CLOSE, ts=1407363058271, server=slave1,16020,1407363003212} to {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363149062, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,062 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=OFFLINE
2014-08-06 15:12:29,068 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/2a16f0086072207fedd5efb5f679bde4 already deleted, retry=false
2014-08-06 15:12:29,068 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=PENDING_OPEN, ts=1407363035536, server=slave1,16020,1407363003212} to {2a16f0086072207fedd5efb5f679bde4 state=OFFLINE, ts=1407363149068, server=slave1,16020,1407363003212}
2014-08-06 15:12:29,068 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=OFFLINE
2014-08-06 15:12:29,071 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Reassigning 2 region(s) that slave1,16020,1407363003212 was carrying (and 8 regions(s) that were opening on this server)
2014-08-06 15:12:29,073 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407363145796
2014-08-06 15:12:29,073 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=OFFLINE, ts=1407363149023, server=slave1,16020,1407363003212} to {941f3daa3189b04c7d5d94a986175384 state=PENDING_OPEN, ts=1407363149073, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,073 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,076 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407363149029, server=slave1,16020,1407363003212} to {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363149076, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,076 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,079 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407363149035, server=slave1,16020,1407363003212} to {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363149079, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,079 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407363149044, server=slave1,16020,1407363003212} to {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363149081, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,081 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,084 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=OFFLINE, ts=1407363149051, server=slave1,16020,1407363003212} to {0c3669f1b658ec2cfea4662920345d3f state=PENDING_OPEN, ts=1407363149084, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,084 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,086 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407363149056, server=slave1,16020,1407363003212} to {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363149086, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,086 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,089 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407363149062, server=slave1,16020,1407363003212} to {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363149089, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,089 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,091 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=OFFLINE, ts=1407363149068, server=slave1,16020,1407363003212} to {2a16f0086072207fedd5efb5f679bde4 state=PENDING_OPEN, ts=1407363149091, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,091 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,094 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=OFFLINE, ts=1407363149009, server=slave1,16020,1407363003212} to {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_OPEN, ts=1407363149094, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,094 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,096 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=OFFLINE, ts=1407363149016, server=slave1,16020,1407363003212} to {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_OPEN, ts=1407363149096, server=slave1,16020,1407363145796}
2014-08-06 15:12:29,096 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:12:29,647 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638 acquired by slave1,16020,1407363145796
2014-08-06 15:12:29,770 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitting task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640
2014-08-06 15:12:29,778 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitting task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834
2014-08-06 15:12:29,790 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitted 2 out of 43 tasks
2014-08-06 15:12:29,790 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/RESCAN0000000501 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:29,793 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/RESCAN0000000502 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:29,946 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 941f3daa3189b04c7d5d94a986175384 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,477 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=PENDING_OPEN, ts=1407363149079, server=slave1,16020,1407363145796} to {d39bb1863f3baceabd838c633cba9f20 state=OPEN, ts=1407363150477, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,477 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=OPEN&openSeqNum=40002710&server=slave1,16020,1407363145796
2014-08-06 15:12:30,478 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=PENDING_OPEN, ts=1407363149073, server=slave1,16020,1407363145796} to {941f3daa3189b04c7d5d94a986175384 state=OPEN, ts=1407363150478, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,479 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=OPEN&openSeqNum=40002856&server=slave1,16020,1407363145796
2014-08-06 15:12:30,482 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Onlined d39bb1863f3baceabd838c633cba9f20 on slave1,16020,1407363145796
2014-08-06 15:12:30,482 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStates: Onlined 941f3daa3189b04c7d5d94a986175384 on slave1,16020,1407363145796
2014-08-06 15:12:30,483 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for e9de6146bf9f987041eeac083bc7b535 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,489 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: dead splitlog workers [slave1,16020,1407362699929]
2014-08-06 15:12:30,493 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=PENDING_OPEN, ts=1407363149076, server=slave1,16020,1407363145796} to {e9de6146bf9f987041eeac083bc7b535 state=OPEN, ts=1407363150493, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,493 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=OPEN&openSeqNum=40002606&server=slave1,16020,1407363145796
2014-08-06 15:12:30,493 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: started splitting 1 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting]
2014-08-06 15:12:30,499 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Onlined e9de6146bf9f987041eeac083bc7b535 on slave1,16020,1407363145796
2014-08-06 15:12:30,499 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for a390894508c5f49c438ee5a3585e0f78 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,560 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=PENDING_OPEN, ts=1407363149081, server=slave1,16020,1407363145796} to {a390894508c5f49c438ee5a3585e0f78 state=OPEN, ts=1407363150560, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,560 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=OPEN&openSeqNum=40002052&server=slave1,16020,1407363145796
2014-08-06 15:12:30,565 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Onlined a390894508c5f49c438ee5a3585e0f78 on slave1,16020,1407363145796
2014-08-06 15:12:30,565 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 0c3669f1b658ec2cfea4662920345d3f to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,701 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939 acquired by slave1,16020,1407363145796
2014-08-06 15:12:30,707 INFO  [defaultRpcServer.handler=10,queue=0,port=16020] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=PENDING_OPEN, ts=1407363149084, server=slave1,16020,1407363145796} to {0c3669f1b658ec2cfea4662920345d3f state=OPEN, ts=1407363150707, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,707 INFO  [defaultRpcServer.handler=10,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=OPEN&openSeqNum=40002376&server=slave1,16020,1407363145796
2014-08-06 15:12:30,712 INFO  [defaultRpcServer.handler=10,queue=0,port=16020] master.RegionStates: Onlined 0c3669f1b658ec2cfea4662920345d3f on slave1,16020,1407363145796
2014-08-06 15:12:30,712 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 97e653821cd22644b2780dc096d62059 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,712 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=PENDING_OPEN, ts=1407363149086, server=slave1,16020,1407363145796} to {97e653821cd22644b2780dc096d62059 state=OPEN, ts=1407363150712, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,713 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=OPEN&openSeqNum=40002490&server=slave1,16020,1407363145796
2014-08-06 15:12:30,716 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Onlined 97e653821cd22644b2780dc096d62059 on slave1,16020,1407363145796
2014-08-06 15:12:30,716 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 3ce788492d2889f8070653d1a524657a to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,794 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=PENDING_OPEN, ts=1407363149089, server=slave1,16020,1407363145796} to {3ce788492d2889f8070653d1a524657a state=OPEN, ts=1407363150794, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,794 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=OPEN&openSeqNum=40001654&server=slave1,16020,1407363145796
2014-08-06 15:12:30,799 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_OPEN, ts=1407363149094, server=slave1,16020,1407363145796} to {9a384662d9ff6cc90600a0f3504ef123 state=OPEN, ts=1407363150799, server=slave1,16020,1407363145796}
2014-08-06 15:12:30,799 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407363145796
2014-08-06 15:12:30,800 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStates: Onlined 3ce788492d2889f8070653d1a524657a on slave1,16020,1407363145796
2014-08-06 15:12:30,800 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 2a16f0086072207fedd5efb5f679bde4 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:30,803 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Onlined 9a384662d9ff6cc90600a0f3504ef123 on slave1,16020,1407363145796
2014-08-06 15:12:31,193 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=PENDING_OPEN, ts=1407363149091, server=slave1,16020,1407363145796} to {2a16f0086072207fedd5efb5f679bde4 state=OPEN, ts=1407363151193, server=slave1,16020,1407363145796}
2014-08-06 15:12:31,194 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=OPEN&openSeqNum=40002224&server=slave1,16020,1407363145796
2014-08-06 15:12:31,198 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStates: Onlined 2a16f0086072207fedd5efb5f679bde4 on slave1,16020,1407363145796
2014-08-06 15:12:31,199 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.AssignmentManager: Waiting for 2baa40eaf58caba1c3e44d5edf1ac795 to leave regions-in-transition, timeOut=15000 ms.
2014-08-06 15:12:31,208 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_OPEN, ts=1407363149096, server=slave1,16020,1407363145796} to {2baa40eaf58caba1c3e44d5edf1ac795 state=OPEN, ts=1407363151208, server=slave1,16020,1407363145796}
2014-08-06 15:12:31,208 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=OPEN&openSeqNum=40002928&server=slave1,16020,1407363145796
2014-08-06 15:12:31,212 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Onlined 2baa40eaf58caba1c3e44d5edf1ac795 on slave1,16020,1407363145796
2014-08-06 15:12:31,222 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: dead splitlog workers [slave1,16020,1407363003212]
2014-08-06 15:12:31,227 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: started splitting 1 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407363003212-splitting]
2014-08-06 15:12:33,772 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 45 unassigned = 41 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363035267 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638=last_update = 1407363149707 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939=last_update = 1407363150761 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363035942 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 0 error = 0}
2014-08-06 15:12:35,772 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitting task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
2014-08-06 15:12:35,778 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitted 1 out of 45 tasks
2014-08-06 15:12:35,782 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/RESCAN0000000505 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:36,772 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitting task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
2014-08-06 15:12:36,780 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: resubmitted 1 out of 45 tasks
2014-08-06 15:12:36,787 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/RESCAN0000000506 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:37,835 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:37,851 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362260638 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362260638
2014-08-06 15:12:37,853 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362260638
2014-08-06 15:12:37,880 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665 acquired by slave1,16020,1407363145796
2014-08-06 15:12:38,033 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:38,051 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362386939 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362386939
2014-08-06 15:12:38,054 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362386939
2014-08-06 15:12:38,691 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658 acquired by slave1,16020,1407363145796
2014-08-06 15:12:38,772 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 41 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363155778 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = 1407363158691 last_version = 1 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = 1407363157967 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0}
2014-08-06 15:12:44,357 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region d39bb1863f3baceabd838c633cba9f20 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:44,365 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] regionserver.SplitLogWorker: BADVERSION failed to assert ownership for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:407)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.attemptToOwnTask(SplitLogWorker.java:392)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$2.progress(SplitLogWorker.java:460)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:972)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1728)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:12:44,366 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] regionserver.SplitLogWorker: Failed to heartbeat the task/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
2014-08-06 15:12:44,375 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region e9de6146bf9f987041eeac083bc7b535 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:44,606 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region 2a16f0086072207fedd5efb5f679bde4 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:44,607 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 110 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 is corrupted = false progress failed = true
2014-08-06 15:12:44,607 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] regionserver.SplitLogWorker: log splitting of WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 failed, returning error
java.io.IOException: java.io.IOException: Timeout when waiting region d39bb1863f3baceabd838c633cba9f20 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:650)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:126)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:983)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1728)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Timeout when waiting region d39bb1863f3baceabd838c633cba9f20 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:44,612 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: transisition task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 to ERR sceplus-vm48.almaden.ibm.com,16020,1407363002241 failed because of version mismatch
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:407)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:878)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.endTask(HLogSplitterHandler.java:125)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:94)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:12:44,613 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 in 129387ms
2014-08-06 15:12:44,773 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 41 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363155778 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658=last_update = 1407363158800 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665=last_update = 1407363157967 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 2 error = 0}
2014-08-06 15:12:44,949 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region e9de6146bf9f987041eeac083bc7b535 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:44,963 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] regionserver.SplitLogWorker: BADVERSION failed to assert ownership for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:407)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker.attemptToOwnTask(SplitLogWorker.java:392)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$2.progress(SplitLogWorker.java:460)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:972)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1728)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:12:44,964 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] regionserver.SplitLogWorker: Failed to heartbeat the task/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
2014-08-06 15:12:45,143 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region 0c3669f1b658ec2cfea4662920345d3f online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:45,186 ERROR [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Exiting thread
java.io.IOException: Timeout when waiting region 2baa40eaf58caba1c3e44d5edf1ac795 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:45,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47ad5f2e120007
2014-08-06 15:12:45,199 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47ad5f2e120007 closed
2014-08-06 15:12:45,200 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-06 15:12:45,300 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 115 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 is corrupted = false progress failed = true
2014-08-06 15:12:45,300 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] regionserver.SplitLogWorker: log splitting of WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 failed, returning error
java.io.IOException: java.io.IOException: Timeout when waiting region e9de6146bf9f987041eeac083bc7b535 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:650)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:126)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:983)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1728)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Timeout when waiting region e9de6146bf9f987041eeac083bc7b535 online for 120000 milliseconds.
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.waitUntilRegionOnline(HLogSplitter.java:1690)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.locateRegionAndRefreshLastFlushedSequenceId(HLogSplitter.java:1580)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.groupEditsByServer(HLogSplitter.java:1484)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.append(HLogSplitter.java:1397)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:870)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:862)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:832)
2014-08-06 15:12:45,306 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: transisition task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 to ERR sceplus-vm48.almaden.ibm.com,16020,1407363002241 failed because of version mismatch
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:115)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1270)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:407)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:878)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.endTask(HLogSplitterHandler.java:125)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:94)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-06 15:12:45,306 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 in 129407ms
2014-08-06 15:12:46,649 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:46,666 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362225658 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362225658
2014-08-06 15:12:46,668 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362225658
2014-08-06 15:12:46,688 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579
2014-08-06 15:12:46,689 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:46,721 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362216579, length=129440229
2014-08-06 15:12:46,721 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:12:46,728 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362216579
2014-08-06 15:12:46,730 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362216579 after 2ms
2014-08-06 15:12:46,741 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941 acquired by slave1,16020,1407363145796
2014-08-06 15:12:46,807 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x37df8f67, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:12:46,808 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x37df8f67 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:12:46,809 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:12:46,810 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:12:46,819 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e12000d, negotiated timeout = 90000
2014-08-06 15:12:46,833 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:46,845 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407363003212-splitting/slave1%2C16020%2C1407363003212.1407363024941 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407363003212.1407363024941
2014-08-06 15:12:46,847 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407363003212-splitting%2Fslave1%252C16020%252C1407363003212.1407363024941
2014-08-06 15:12:46,883 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: finished splitting (more than or equal to) 17 bytes in 1 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407363003212-splitting] in 15656ms
2014-08-06 15:12:46,883 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407363003212
2014-08-06 15:12:47,225 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:47,238 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362365665 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362365665
2014-08-06 15:12:47,240 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362365665
2014-08-06 15:12:47,412 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448 acquired by slave1,16020,1407363145796
2014-08-06 15:12:47,577 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
2014-08-06 15:12:47,578 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:47,609 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024, length=131664368
2014-08-06 15:12:47,609 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:12:47,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024
2014-08-06 15:12:47,622 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 after 5ms
2014-08-06 15:12:48,177 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208 acquired by slave1,16020,1407363145796
2014-08-06 15:12:49,776 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 40 unassigned = 36 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = 1407363166729 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363155778 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208=last_update = 1407363168236 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = 1407363167496 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363167620 last_version = 5 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0}
2014-08-06 15:12:52,270 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:12:52,286 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361693208 to hdfs://master:54310/hbase/oldWALs/sceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146.1407361693208
2014-08-06 15:12:52,288 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C16020%2C1407361669146-splitting%2Fsceplus-vm48.almaden.ibm.com%252C16020%252C1407361669146.1407361693208
2014-08-06 15:12:52,325 INFO  [main-EventThread] zookeeper.RecoveringRegionWatcher: /hbase/recovering-regions/fe1c441ecb6e3789e0fdd62314d7b06a znode deleted. Region: fe1c441ecb6e3789e0fdd62314d7b06a completes recovery.
2014-08-06 15:12:52,337 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: finished splitting (more than or equal to) 9 bytes in 1 log files in [hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407361669146-splitting] in 137049ms
2014-08-06 15:12:52,338 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.LogReplayHandler: Finished processing shutdown of sceplus-vm48.almaden.ibm.com,16020,1407361669146
2014-08-06 15:12:52,354 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834 acquired by slave1,16020,1407363145796
2014-08-06 15:12:54,778 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 39 unassigned = 35 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579=last_update = 1407363166729 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363155778 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363172475 last_version = 5 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 4 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = 1407363167496 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024=last_update = 1407363167620 last_version = 5 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 4 error = 0}
2014-08-06 15:12:56,473 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:12:56,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 158 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362216579 is corrupted = false progress failed = false
2014-08-06 15:12:56,483 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:56,483 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579 in 9795ms
2014-08-06 15:12:56,485 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:56,504 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362216579 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362216579
2014-08-06 15:12:56,507 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362216579
2014-08-06 15:12:56,526 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444
2014-08-06 15:12:56,527 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:56,563 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362205444, length=129025809
2014-08-06 15:12:56,563 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:12:56,571 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362205444
2014-08-06 15:12:56,574 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362205444 after 3ms
2014-08-06 15:12:58,698 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:12:58,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 159 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 is corrupted = false progress failed = false
2014-08-06 15:12:58,703 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:58,703 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 in 11125ms
2014-08-06 15:12:58,704 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:58,717 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362010024 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362010024
2014-08-06 15:12:58,719 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362010024
2014-08-06 15:12:58,737 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
2014-08-06 15:12:58,737 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:12:58,772 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153, length=131546616
2014-08-06 15:12:58,773 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:12:58,780 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153
2014-08-06 15:12:58,784 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 after 4ms
2014-08-06 15:13:00,778 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 37 unassigned = 33 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363178781 last_version = 5 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834=last_update = 1407363172475 last_version = 5 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = 1407363176572 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448=last_update = 1407363167496 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0}
2014-08-06 15:13:03,773 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:03,882 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting/slave1%2C16020%2C1407362699929.1407362702448 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407362699929.1407362702448
2014-08-06 15:13:03,884 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407362699929-splitting%2Fslave1%252C16020%252C1407362699929.1407362702448
2014-08-06 15:13:03,925 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955 acquired by slave1,16020,1407363145796
2014-08-06 15:13:04,011 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: finished splitting (more than or equal to) 9 bytes in 1 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407362699929-splitting] in 33518ms
2014-08-06 15:13:04,013 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407362699929
2014-08-06 15:13:04,244 INFO  [ActiveMasterManager] master.HMaster: Master has completed initialization
2014-08-06 15:13:04,255 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:04,269 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362024834 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362024834
2014-08-06 15:13:04,284 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362024834
2014-08-06 15:13:04,574 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665 acquired by slave1,16020,1407363145796
2014-08-06 15:13:05,781 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 35 unassigned = 31 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153=last_update = 1407363178781 last_version = 5 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665=last_update = 1407363184678 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444=last_update = 1407363176572 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955=last_update = 1407363183977 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 7 error = 0}
2014-08-06 15:13:06,226 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:06,238 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362076955 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362076955
2014-08-06 15:13:06,240 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362076955
2014-08-06 15:13:06,273 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962 acquired by slave1,16020,1407363145796
2014-08-06 15:13:06,766 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:06,785 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362184665 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362184665
2014-08-06 15:13:06,788 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362184665
2014-08-06 15:13:06,836 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970 acquired by slave1,16020,1407363145796
2014-08-06 15:13:08,183 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:08,200 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362170962 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362170962
2014-08-06 15:13:08,203 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362170962
2014-08-06 15:13:08,217 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355 acquired by slave1,16020,1407363145796
2014-08-06 15:13:08,612 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:08,614 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 158 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362205444 is corrupted = false progress failed = false
2014-08-06 15:13:08,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:08,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444 in 12093ms
2014-08-06 15:13:08,621 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:08,642 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362205444 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362205444
2014-08-06 15:13:08,644 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362205444
2014-08-06 15:13:08,660 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653
2014-08-06 15:13:08,661 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:08,695 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362212653, length=129449181
2014-08-06 15:13:08,695 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:08,700 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362212653
2014-08-06 15:13:08,702 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362212653 after 2ms
2014-08-06 15:13:08,759 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:08,918 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:08,934 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:08,934 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:09,093 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362058970 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362058970
2014-08-06 15:13:09,096 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362058970
2014-08-06 15:13:09,119 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371 acquired by slave1,16020,1407363145796
2014-08-06 15:13:09,131 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:09,131 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:09,139 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:09,194 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:09,194 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:09,194 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:10,184 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:10,185 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 161 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 is corrupted = false progress failed = false
2014-08-06 15:13:10,192 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,192 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 in 11455ms
2014-08-06 15:13:10,193 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,205 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362001153 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362001153
2014-08-06 15:13:10,206 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362001153
2014-08-06 15:13:10,217 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976
2014-08-06 15:13:10,217 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,251 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362230976, length=137664887
2014-08-06 15:13:10,251 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:10,257 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362230976
2014-08-06 15:13:10,259 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362230976 after 2ms
2014-08-06 15:13:10,311 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:10,323 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:10,330 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:10,347 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:10,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:10,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:10,382 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:10,403 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:10,421 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:10,432 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362152355 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362152355
2014-08-06 15:13:10,434 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362152355
2014-08-06 15:13:10,476 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312 acquired by slave1,16020,1407363145796
2014-08-06 15:13:10,498 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:10,609 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:10,633 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362368371 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362368371
2014-08-06 15:13:10,635 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362368371
2014-08-06 15:13:10,783 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:10,783 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 27 unassigned = 24 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976=last_update = 1407363190258 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653=last_update = 1407363188701 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312=last_update = 1407363190522 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 15 error = 0}
2014-08-06 15:13:10,783 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362212653 is corrupted = false progress failed = false
2014-08-06 15:13:10,789 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,789 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653 in 2128ms
2014-08-06 15:13:10,790 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,799 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314
2014-08-06 15:13:10,802 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362212653 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362212653
2014-08-06 15:13:10,804 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362212653
2014-08-06 15:13:10,815 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:10,869 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:10,881 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362381314, length=128013502
2014-08-06 15:13:10,881 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:10,890 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362505312 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362505312
2014-08-06 15:13:10,891 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362381314
2014-08-06 15:13:10,892 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362505312
2014-08-06 15:13:10,893 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362381314 after 2ms
2014-08-06 15:13:10,927 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:10,931 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:10,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:10,965 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:10,970 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:10,979 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:10,984 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:10,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:11,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:11,299 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473 acquired by slave1,16020,1407363145796
2014-08-06 15:13:11,555 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:11,555 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362381314 is corrupted = false progress failed = false
2014-08-06 15:13:11,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:11,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314 in 760ms
2014-08-06 15:13:11,561 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:11,573 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362381314 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362381314
2014-08-06 15:13:11,575 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362381314
2014-08-06 15:13:11,588 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071
2014-08-06 15:13:11,589 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:11,618 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362266071, length=130613479
2014-08-06 15:13:11,618 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:11,622 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362266071
2014-08-06 15:13:11,624 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362266071 after 2ms
2014-08-06 15:13:11,687 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:11,808 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:11,832 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:11,855 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:11,867 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:11,876 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:11,882 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:11,896 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:11,904 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:12,226 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456 acquired by slave1,16020,1407363145796
2014-08-06 15:13:13,026 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:13,027 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362230976 is corrupted = false progress failed = false
2014-08-06 15:13:13,044 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,044 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976 in 2827ms
2014-08-06 15:13:13,045 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,057 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362230976 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362230976
2014-08-06 15:13:13,059 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362230976
2014-08-06 15:13:13,068 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796
2014-08-06 15:13:13,069 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,102 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362220796, length=160794864
2014-08-06 15:13:13,102 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:13,106 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362220796
2014-08-06 15:13:13,109 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362220796 after 3ms
2014-08-06 15:13:13,141 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:13,153 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:13,161 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:13,164 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:13,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:13,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:13,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:13,211 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:13,220 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:13,335 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:13,348 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362035473 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362035473
2014-08-06 15:13:13,349 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362035473
2014-08-06 15:13:13,361 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102 acquired by slave1,16020,1407363145796
2014-08-06 15:13:13,577 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:13,589 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362371456 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362371456
2014-08-06 15:13:13,591 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362371456
2014-08-06 15:13:13,847 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:13,847 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362266071 is corrupted = false progress failed = false
2014-08-06 15:13:13,851 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,851 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071 in 2262ms
2014-08-06 15:13:13,853 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,866 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362266071 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362266071
2014-08-06 15:13:13,867 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362266071
2014-08-06 15:13:13,876 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807
2014-08-06 15:13:13,877 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:13,905 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362072807, length=132377605
2014-08-06 15:13:13,905 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:13,910 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362072807
2014-08-06 15:13:13,912 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362072807 after 2ms
2014-08-06 15:13:14,011 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:14,228 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173 acquired by slave1,16020,1407363145796
2014-08-06 15:13:14,236 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:14,296 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:14,426 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:14,574 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:14,701 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:14,839 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:15,177 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:15,245 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:15,255 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:15,255 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362220796 is corrupted = false progress failed = false
2014-08-06 15:13:15,260 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,260 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796 in 2191ms
2014-08-06 15:13:15,261 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,271 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362220796 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362220796
2014-08-06 15:13:15,272 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362220796
2014-08-06 15:13:15,298 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517
2014-08-06 15:13:15,299 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,330 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362248517, length=131287718
2014-08-06 15:13:15,330 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:15,341 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362248517
2014-08-06 15:13:15,343 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362248517 after 2ms
2014-08-06 15:13:15,392 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:15,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:15,456 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:15,457 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362072807 is corrupted = false progress failed = false
2014-08-06 15:13:15,461 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,461 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807 in 1584ms
2014-08-06 15:13:15,462 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,464 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:15,472 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362072807 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362072807
2014-08-06 15:13:15,474 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362072807
2014-08-06 15:13:15,481 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:15,487 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:15,497 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:15,520 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:15,632 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:15,729 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:15,784 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 18 unassigned = 15 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517=last_update = 1407363195342 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = -1 last_version = 2 cur_worker_name = null status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173=last_update = 1407363194260 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102=last_update = 1407363193403 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 24 error = 0}
2014-08-06 15:13:15,897 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:15,908 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407361986102 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407361986102
2014-08-06 15:13:15,909 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407361986102
2014-08-06 15:13:15,926 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113 acquired by slave1,16020,1407363145796
2014-08-06 15:13:15,937 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683
2014-08-06 15:13:15,938 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:15,964 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362207683, length=127716903
2014-08-06 15:13:15,964 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:15,974 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362207683
2014-08-06 15:13:15,977 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362207683 after 3ms
2014-08-06 15:13:16,028 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:16,038 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:16,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:16,070 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:16,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:16,081 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:16,119 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:16,120 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:16,159 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:16,245 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:16,260 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362253173 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362253173
2014-08-06 15:13:16,262 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362253173
2014-08-06 15:13:16,919 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466 acquired by slave1,16020,1407363145796
2014-08-06 15:13:16,959 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:16,972 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362166113 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362166113
2014-08-06 15:13:16,974 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362166113
2014-08-06 15:13:17,413 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:17,414 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362248517 is corrupted = false progress failed = false
2014-08-06 15:13:17,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:17,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517 in 2119ms
2014-08-06 15:13:17,419 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:17,431 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362248517 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362248517
2014-08-06 15:13:17,433 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362248517
2014-08-06 15:13:17,444 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371
2014-08-06 15:13:17,444 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:17,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362245371, length=133616529
2014-08-06 15:13:17,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:17,480 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362245371
2014-08-06 15:13:17,481 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362245371 after 1ms
2014-08-06 15:13:17,542 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:17,576 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:17,641 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:17,648 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:17,654 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:17,654 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362207683 is corrupted = false progress failed = false
2014-08-06 15:13:17,660 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:17,660 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683 in 1722ms
2014-08-06 15:13:17,661 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:17,674 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362207683 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362207683
2014-08-06 15:13:17,676 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362207683
2014-08-06 15:13:17,713 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:17,799 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:17,887 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941 acquired by slave1,16020,1407363145796
2014-08-06 15:13:17,894 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:17,957 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:18,016 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:18,167 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966
2014-08-06 15:13:18,168 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:18,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362029966, length=131315219
2014-08-06 15:13:18,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:18,206 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362029966
2014-08-06 15:13:18,207 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362029966 after 1ms
2014-08-06 15:13:18,237 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:18,240 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:18,245 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:18,252 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:18,253 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:18,258 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:18,263 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:18,264 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:18,275 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:18,928 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:18,929 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 12 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362245371 is corrupted = false progress failed = false
2014-08-06 15:13:18,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:18,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371 in 1491ms
2014-08-06 15:13:18,935 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:18,951 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362192466 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362192466
2014-08-06 15:13:18,953 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362192466
2014-08-06 15:13:18,954 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:18,971 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362245371 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362245371
2014-08-06 15:13:18,973 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362245371
2014-08-06 15:13:18,978 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077 acquired by slave1,16020,1407363145796
2014-08-06 15:13:19,024 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640
2014-08-06 15:13:19,025 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:19,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362019640, length=131302439
2014-08-06 15:13:19,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:19,102 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362019640
2014-08-06 15:13:19,105 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362019640 after 3ms
2014-08-06 15:13:19,237 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:19,242 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:19,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:19,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362029966 is corrupted = false progress failed = false
2014-08-06 15:13:19,424 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:19,425 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966 in 1257ms
2014-08-06 15:13:19,461 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:19,469 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:19,470 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:19,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:19,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:19,635 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362029966 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362029966
2014-08-06 15:13:19,636 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362029966
2014-08-06 15:13:19,637 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:19,699 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362376941 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362376941
2014-08-06 15:13:19,700 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362376941
2014-08-06 15:13:19,725 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305
2014-08-06 15:13:19,726 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:19,752 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362014305, length=136075233
2014-08-06 15:13:19,752 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:19,759 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362014305
2014-08-06 15:13:19,761 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362014305 after 1ms
2014-08-06 15:13:19,781 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:19,781 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:19,781 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:19,795 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431 acquired by slave1,16020,1407363145796
2014-08-06 15:13:19,810 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:19,814 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:19,827 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:19,842 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:19,845 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:19,856 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:19,862 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:19,951 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:19,951 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:20,206 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:20,223 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362276431 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362276431
2014-08-06 15:13:20,225 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362276431
2014-08-06 15:13:20,525 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008 acquired by slave1,16020,1407363145796
2014-08-06 15:13:20,786 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 8 unassigned = 4 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077=last_update = 1407363199018 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305=last_update = 1407363199759 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640=last_update = 1407363199103 last_version = 5 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407363002241 status = in_progress incarnation = 1 resubmits = 1 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008=last_update = 1407363200678 last_version = 2 cur_worker_name = slave1,16020,1407363145796 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 42 done = 34 error = 0}
2014-08-06 15:13:21,572 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:21,587 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362053077 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362053077
2014-08-06 15:13:21,589 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362053077
2014-08-06 15:13:21,604 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458 acquired by slave1,16020,1407363145796
2014-08-06 15:13:21,612 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:21,613 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 10 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362019640 is corrupted = false progress failed = false
2014-08-06 15:13:21,624 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:21,624 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640 in 2599ms
2014-08-06 15:13:21,625 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:21,636 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362019640 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362019640
2014-08-06 15:13:21,638 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362019640
2014-08-06 15:13:21,671 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233
2014-08-06 15:13:21,671 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:21,730 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:21,733 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362049233, length=133335551
2014-08-06 15:13:21,733 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:21,742 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362049233
2014-08-06 15:13:21,756 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362049233 after 14ms
2014-08-06 15:13:21,766 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47ad5f2e12000d
2014-08-06 15:13:21,772 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] zookeeper.ZooKeeper: Session: 0x47ad5f2e12000d closed
2014-08-06 15:13:21,772 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-06 15:13:21,874 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362014305 is corrupted = false progress failed = false
2014-08-06 15:13:21,879 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x3febb28, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-06 15:13:21,880 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:21,880 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305 in 2155ms
2014-08-06 15:13:21,880 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x3febb28 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-06 15:13:21,880 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-06 15:13:21,883 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-06 15:13:21,884 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:21,892 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47ad5f2e120011, negotiated timeout = 90000
2014-08-06 15:13:21,897 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362014305 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362014305
2014-08-06 15:13:21,898 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362014305
2014-08-06 15:13:21,912 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:21,917 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:21,917 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:21,918 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:21,942 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:22,159 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:22,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:22,202 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:22,305 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:22,375 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951
2014-08-06 15:13:22,376 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:22,402 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362196951, length=135321254
2014-08-06 15:13:22,402 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:22,408 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362196951
2014-08-06 15:13:22,409 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362196951 after 1ms
2014-08-06 15:13:22,534 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:22,534 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:22,534 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:22,541 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:22,543 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:22,550 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:22,551 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:22,558 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:22,652 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:22,926 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:22,927 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 30 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362049233 is corrupted = false progress failed = false
2014-08-06 15:13:22,932 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:22,932 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233 in 1261ms
2014-08-06 15:13:22,933 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:22,943 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362049233 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362049233
2014-08-06 15:13:22,944 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362049233
2014-08-06 15:13:23,311 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407363002241] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968
2014-08-06 15:13:23,313 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968 acquired by sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:23,382 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362271968, length=132259708
2014-08-06 15:13:23,382 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-06 15:13:23,391 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362271968
2014-08-06 15:13:23,392 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362271968 after 1ms
2014-08-06 15:13:23,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: d39bb1863f3baceabd838c633cba9f20 because it's not in recovering.
2014-08-06 15:13:23,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2baa40eaf58caba1c3e44d5edf1ac795 because it's not in recovering.
2014-08-06 15:13:23,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: a390894508c5f49c438ee5a3585e0f78 because it's not in recovering.
2014-08-06 15:13:23,467 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 97e653821cd22644b2780dc096d62059 because it's not in recovering.
2014-08-06 15:13:23,477 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: e9de6146bf9f987041eeac083bc7b535 because it's not in recovering.
2014-08-06 15:13:23,586 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 941f3daa3189b04c7d5d94a986175384 because it's not in recovering.
2014-08-06 15:13:23,586 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: logReplay skip region: 2a16f0086072207fedd5efb5f679bde4 because it's not in recovering.
2014-08-06 15:13:23,586 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: logReplay skip region: 3ce788492d2889f8070653d1a524657a because it's not in recovering.
2014-08-06 15:13:23,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: logReplay skip region: 0c3669f1b658ec2cfea4662920345d3f because it's not in recovering.
2014-08-06 15:13:23,601 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:23,611 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362083008 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362083008
2014-08-06 15:13:23,613 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362083008
2014-08-06 15:13:24,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:24,059 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 9 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362196951 is corrupted = false progress failed = false
2014-08-06 15:13:24,076 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:24,076 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951 in 1700ms
2014-08-06 15:13:24,077 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:24,087 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362196951 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362196951
2014-08-06 15:13:24,088 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362196951
2014-08-06 15:13:24,255 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458 entered state: DONE slave1,16020,1407363145796
2014-08-06 15:13:24,265 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362004458 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362004458
2014-08-06 15:13:24,266 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362004458
2014-08-06 15:13:24,410 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-06 15:13:24,423 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47ad5f2e120011
2014-08-06 15:13:24,427 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47ad5f2e120011 closed
2014-08-06 15:13:24,427 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-06 15:13:24,527 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 17 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362271968 is corrupted = false progress failed = false
2014-08-06 15:13:24,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:24,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407363002241 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968 in 1226ms
2014-08-06 15:13:24,538 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:13:24,548 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting/slave1%2C16020%2C1407361709205.1407362271968 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407361709205.1407362271968
2014-08-06 15:13:24,549 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407361709205-splitting%2Fslave1%252C16020%252C1407361709205.1407362271968
2014-08-06 15:13:24,558 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: finished splitting (more than or equal to) 5304284078 bytes in 42 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407361709205-splitting] in 169396ms
2014-08-06 15:13:24,558 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407361709205
2014-08-06 15:13:27,100 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 5747186176 and memstore heap size to 4470033408
2014-08-06 15:15:02,443 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.35 GB, max=5.35 GB, accesses=1085, hits=1077, hitRatio=99.26%, , cachingAccesses=1082, cachingHits=1074, cachingHitsRatio=99.26%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-08-06 15:15:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:15:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363035273, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363335162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:15:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:15:35,167 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:15:35,178 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:15:35,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:15:35,182 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:15:35,183 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363335162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363335183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:15:35,183 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:15:35,187 INFO  [AM.-pool1-t6] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:15:35,187 INFO  [AM.-pool1-t6] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363335183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363335187, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:15:35,187 INFO  [AM.-pool1-t6] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:15:35,190 INFO  [AM.-pool1-t6] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:15:35,201 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:15:35,201 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:15:35,222 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:15:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:15:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363335187, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363335223, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:15:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:16:26,098 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 6385762304 and memstore heap size to 3831457280
2014-08-06 15:20:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1088, hits=1080, hitRatio=99.26%, , cachingAccesses=1085, cachingHits=1077, cachingHitsRatio=99.26%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-08-06 15:20:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:20:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363335223, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363635161, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:20:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:20:35,176 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:20:35,182 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:20:35,183 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:20:35,183 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:20:35,184 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363635161, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363635184, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:20:35,184 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:20:35,191 INFO  [AM.-pool1-t7] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:20:35,192 INFO  [AM.-pool1-t7] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363635184, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363635192, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:20:35,192 INFO  [AM.-pool1-t7] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:20:35,194 INFO  [AM.-pool1-t7] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:20:35,217 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:20:35,218 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:20:35,253 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:20:35,253 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:20:35,254 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363635192, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363635254, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:20:35,254 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:25:02,442 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1091, hits=1083, hitRatio=99.27%, , cachingAccesses=1088, cachingHits=1080, cachingHitsRatio=99.26%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-08-06 15:25:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:25:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363635254, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363935162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:25:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:25:35,188 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:25:35,196 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:25:35,196 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:25:35,196 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:25:35,197 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407363935162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363935197, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:25:35,197 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:25:35,201 INFO  [AM.-pool1-t8] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:25:35,201 INFO  [AM.-pool1-t8] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407363935197, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363935201, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:25:35,201 INFO  [AM.-pool1-t8] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:25:35,204 INFO  [AM.-pool1-t8] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:25:35,216 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:25:35,217 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:25:35,246 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:25:35,247 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:25:35,248 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407363935201, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363935248, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:25:35,248 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:30:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1094, hits=1086, hitRatio=99.27%, , cachingAccesses=1091, cachingHits=1083, cachingHitsRatio=99.27%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-08-06 15:30:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:30:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407363935248, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364235162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:30:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:30:35,183 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:30:35,186 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:30:35,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:30:35,187 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:30:35,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364235162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364235187, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:30:35,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:30:35,191 INFO  [AM.-pool1-t9] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:30:35,191 INFO  [AM.-pool1-t9] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364235187, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364235191, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:30:35,191 INFO  [AM.-pool1-t9] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:30:35,193 INFO  [AM.-pool1-t9] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:30:35,206 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:30:35,207 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:30:35,235 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:30:35,236 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:30:35,236 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364235191, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364235236, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:30:35,237 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:35:02,442 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1097, hits=1089, hitRatio=99.27%, , cachingAccesses=1094, cachingHits=1086, cachingHitsRatio=99.27%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-08-06 15:35:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:35:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364235236, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364535162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:35:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:35:35,193 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:35:35,198 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:35:35,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:35:35,199 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:35:35,200 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364535162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364535200, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:35:35,200 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:35:35,204 INFO  [AM.-pool1-t10] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:35:35,204 INFO  [AM.-pool1-t10] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364535200, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364535204, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:35:35,204 INFO  [AM.-pool1-t10] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:35:35,207 INFO  [AM.-pool1-t10] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:35:35,220 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:35:35,222 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:35:35,248 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:35:35,249 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:35:35,249 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364535204, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364535249, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:35:35,250 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:40:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1100, hits=1092, hitRatio=99.27%, , cachingAccesses=1097, cachingHits=1089, cachingHitsRatio=99.27%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-08-06 15:40:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:40:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364535249, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364835162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:40:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:40:35,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:40:35,181 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:40:35,181 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:40:35,181 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:40:35,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407364835162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364835182, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:40:35,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:40:35,195 INFO  [AM.-pool1-t11] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:40:35,195 INFO  [AM.-pool1-t11] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407364835182, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364835195, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:40:35,195 INFO  [AM.-pool1-t11] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:40:35,198 INFO  [AM.-pool1-t11] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:40:35,210 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:40:35,211 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:40:35,237 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:40:35,238 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:40:35,238 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407364835195, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364835238, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:40:35,239 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:44:33,070 INFO  [defaultRpcServer.handler=8,queue=3,port=16020] master.HMaster: Client=hadoop//9.1.143.58 disable usertable
2014-08-06 15:44:33,105 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Attempting to disable table usertable
2014-08-06 15:44:33,105 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from DISABLING to DISABLING
2014-08-06 15:44:33,114 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Offlining 10 regions.
2014-08-06 15:44:33,120 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=OPEN, ts=1407363150799, server=slave1,16020,1407363145796} to {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_CLOSE, ts=1407365073120, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,120 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=PENDING_CLOSE
2014-08-06 15:44:33,121 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=OPEN, ts=1407363151208, server=slave1,16020,1407363145796} to {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,121 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=PENDING_CLOSE
2014-08-06 15:44:33,121 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-2] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=OPEN, ts=1407363150560, server=slave1,16020,1407363145796} to {a390894508c5f49c438ee5a3585e0f78 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,121 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-2] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=PENDING_CLOSE
2014-08-06 15:44:33,121 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-3] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=OPEN, ts=1407363151193, server=slave1,16020,1407363145796} to {2a16f0086072207fedd5efb5f679bde4 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-3] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=PENDING_CLOSE
2014-08-06 15:44:33,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-4] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=OPEN, ts=1407363150707, server=slave1,16020,1407363145796} to {0c3669f1b658ec2cfea4662920345d3f state=PENDING_CLOSE, ts=1407365073122, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-4] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=PENDING_CLOSE
2014-08-06 15:44:33,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-5] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=OPEN, ts=1407363150712, server=slave1,16020,1407363145796} to {97e653821cd22644b2780dc096d62059 state=PENDING_CLOSE, ts=1407365073122, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,123 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-5] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=PENDING_CLOSE
2014-08-06 15:44:33,123 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-6] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=OPEN, ts=1407363150493, server=slave1,16020,1407363145796} to {e9de6146bf9f987041eeac083bc7b535 state=PENDING_CLOSE, ts=1407365073123, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,123 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-6] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=PENDING_CLOSE
2014-08-06 15:44:33,129 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-7] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=OPEN, ts=1407363150477, server=slave1,16020,1407363145796} to {d39bb1863f3baceabd838c633cba9f20 state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,129 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-8] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=OPEN, ts=1407363150478, server=slave1,16020,1407363145796} to {941f3daa3189b04c7d5d94a986175384 state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,129 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-7] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=PENDING_CLOSE
2014-08-06 15:44:33,129 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-9] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=OPEN, ts=1407363150794, server=slave1,16020,1407363145796} to {3ce788492d2889f8070653d1a524657a state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,129 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-8] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=PENDING_CLOSE
2014-08-06 15:44:33,130 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-9] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=PENDING_CLOSE
2014-08-06 15:44:33,267 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Transition {9a384662d9ff6cc90600a0f3504ef123 state=PENDING_CLOSE, ts=1407365073120, server=slave1,16020,1407363145796} to {9a384662d9ff6cc90600a0f3504ef123 state=OFFLINE, ts=1407365073267, server=slave1,16020,1407363145796}
2014-08-06 15:44:33,267 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStateStore: Updating row usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123. with state=OFFLINE
2014-08-06 15:44:33,271 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Offlined 9a384662d9ff6cc90600a0f3504ef123 from slave1,16020,1407363145796
2014-08-06 15:44:34,614 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Transition {a390894508c5f49c438ee5a3585e0f78 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796} to {a390894508c5f49c438ee5a3585e0f78 state=OFFLINE, ts=1407365074614, server=slave1,16020,1407363145796}
2014-08-06 15:44:34,615 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78. with state=OFFLINE
2014-08-06 15:44:34,618 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Offlined a390894508c5f49c438ee5a3585e0f78 from slave1,16020,1407363145796
2014-08-06 15:44:35,493 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Transition {2baa40eaf58caba1c3e44d5edf1ac795 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796} to {2baa40eaf58caba1c3e44d5edf1ac795 state=OFFLINE, ts=1407365075493, server=slave1,16020,1407363145796}
2014-08-06 15:44:35,494 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795. with state=OFFLINE
2014-08-06 15:44:35,496 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Offlined 2baa40eaf58caba1c3e44d5edf1ac795 from slave1,16020,1407363145796
2014-08-06 15:44:38,636 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStates: Transition {e9de6146bf9f987041eeac083bc7b535 state=PENDING_CLOSE, ts=1407365073123, server=slave1,16020,1407363145796} to {e9de6146bf9f987041eeac083bc7b535 state=OFFLINE, ts=1407365078636, server=slave1,16020,1407363145796}
2014-08-06 15:44:38,636 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535. with state=OFFLINE
2014-08-06 15:44:38,639 INFO  [defaultRpcServer.handler=17,queue=2,port=16020] master.RegionStates: Offlined e9de6146bf9f987041eeac083bc7b535 from slave1,16020,1407363145796
2014-08-06 15:44:39,063 INFO  [defaultRpcServer.handler=27,queue=2,port=16020] master.RegionStates: Transition {3ce788492d2889f8070653d1a524657a state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796} to {3ce788492d2889f8070653d1a524657a state=OFFLINE, ts=1407365079063, server=slave1,16020,1407363145796}
2014-08-06 15:44:39,063 INFO  [defaultRpcServer.handler=27,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a. with state=OFFLINE
2014-08-06 15:44:39,065 INFO  [defaultRpcServer.handler=27,queue=2,port=16020] master.RegionStates: Offlined 3ce788492d2889f8070653d1a524657a from slave1,16020,1407363145796
2014-08-06 15:44:41,515 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.RegionStates: Transition {2a16f0086072207fedd5efb5f679bde4 state=PENDING_CLOSE, ts=1407365073121, server=slave1,16020,1407363145796} to {2a16f0086072207fedd5efb5f679bde4 state=OFFLINE, ts=1407365081515, server=slave1,16020,1407363145796}
2014-08-06 15:44:41,515 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4. with state=OFFLINE
2014-08-06 15:44:41,518 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.RegionStates: Offlined 2a16f0086072207fedd5efb5f679bde4 from slave1,16020,1407363145796
2014-08-06 15:44:42,011 INFO  [defaultRpcServer.handler=23,queue=3,port=16020] master.RegionStates: Transition {d39bb1863f3baceabd838c633cba9f20 state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796} to {d39bb1863f3baceabd838c633cba9f20 state=OFFLINE, ts=1407365082011, server=slave1,16020,1407363145796}
2014-08-06 15:44:42,011 INFO  [defaultRpcServer.handler=23,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20. with state=OFFLINE
2014-08-06 15:44:42,013 INFO  [defaultRpcServer.handler=23,queue=3,port=16020] master.RegionStates: Offlined d39bb1863f3baceabd838c633cba9f20 from slave1,16020,1407363145796
2014-08-06 15:44:42,504 INFO  [defaultRpcServer.handler=11,queue=1,port=16020] master.RegionStates: Transition {941f3daa3189b04c7d5d94a986175384 state=PENDING_CLOSE, ts=1407365073129, server=slave1,16020,1407363145796} to {941f3daa3189b04c7d5d94a986175384 state=OFFLINE, ts=1407365082504, server=slave1,16020,1407363145796}
2014-08-06 15:44:42,504 INFO  [defaultRpcServer.handler=11,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384. with state=OFFLINE
2014-08-06 15:44:42,508 INFO  [defaultRpcServer.handler=11,queue=1,port=16020] master.RegionStates: Offlined 941f3daa3189b04c7d5d94a986175384 from slave1,16020,1407363145796
2014-08-06 15:44:43,185 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Transition {97e653821cd22644b2780dc096d62059 state=PENDING_CLOSE, ts=1407365073122, server=slave1,16020,1407363145796} to {97e653821cd22644b2780dc096d62059 state=OFFLINE, ts=1407365083185, server=slave1,16020,1407363145796}
2014-08-06 15:44:43,185 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059. with state=OFFLINE
2014-08-06 15:44:43,188 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Offlined 97e653821cd22644b2780dc096d62059 from slave1,16020,1407363145796
2014-08-06 15:44:46,761 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Transition {0c3669f1b658ec2cfea4662920345d3f state=PENDING_CLOSE, ts=1407365073122, server=slave1,16020,1407363145796} to {0c3669f1b658ec2cfea4662920345d3f state=OFFLINE, ts=1407365086761, server=slave1,16020,1407363145796}
2014-08-06 15:44:46,761 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f. with state=OFFLINE
2014-08-06 15:44:46,765 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Offlined 0c3669f1b658ec2cfea4662920345d3f from slave1,16020,1407363145796
2014-08-06 15:44:47,136 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from DISABLING to DISABLED
2014-08-06 15:44:47,142 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Disabled table, usertable, is done=true
2014-08-06 15:44:51,518 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.HMaster: Client=hadoop//9.1.143.58 delete usertable
2014-08-06 15:44:51,548 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.TableEventHandler: Handling table operation C_M_DELETE_TABLE on table usertable
2014-08-06 15:44:51,657 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] hbase.MetaTableAccessor: Deleted [{ENCODED => 9a384662d9ff6cc90600a0f3504ef123, NAME => 'usertable,,1407361750441.9a384662d9ff6cc90600a0f3504ef123.', STARTKEY => '', ENDKEY => 'user1'}, {ENCODED => 2baa40eaf58caba1c3e44d5edf1ac795, NAME => 'usertable,user1,1407361750441.2baa40eaf58caba1c3e44d5edf1ac795.', STARTKEY => 'user1', ENDKEY => 'user2'}, {ENCODED => a390894508c5f49c438ee5a3585e0f78, NAME => 'usertable,user2,1407361750441.a390894508c5f49c438ee5a3585e0f78.', STARTKEY => 'user2', ENDKEY => 'user3'}, {ENCODED => 2a16f0086072207fedd5efb5f679bde4, NAME => 'usertable,user3,1407361750441.2a16f0086072207fedd5efb5f679bde4.', STARTKEY => 'user3', ENDKEY => 'user4'}, {ENCODED => 0c3669f1b658ec2cfea4662920345d3f, NAME => 'usertable,user4,1407361750441.0c3669f1b658ec2cfea4662920345d3f.', STARTKEY => 'user4', ENDKEY => 'user5'}, {ENCODED => 97e653821cd22644b2780dc096d62059, NAME => 'usertable,user5,1407361750441.97e653821cd22644b2780dc096d62059.', STARTKEY => 'user5', ENDKEY => 'user6'}, {ENCODED => e9de6146bf9f987041eeac083bc7b535, NAME => 'usertable,user6,1407361750441.e9de6146bf9f987041eeac083bc7b535.', STARTKEY => 'user6', ENDKEY => 'user7'}, {ENCODED => d39bb1863f3baceabd838c633cba9f20, NAME => 'usertable,user7,1407361750441.d39bb1863f3baceabd838c633cba9f20.', STARTKEY => 'user7', ENDKEY => 'user8'}, {ENCODED => 941f3daa3189b04c7d5d94a986175384, NAME => 'usertable,user8,1407361750441.941f3daa3189b04c7d5d94a986175384.', STARTKEY => 'user8', ENDKEY => 'user9'}, {ENCODED => 3ce788492d2889f8070653d1a524657a, NAME => 'usertable,user9,1407361750441.3ce788492d2889f8070653d1a524657a.', STARTKEY => 'user9', ENDKEY => ''}]
2014-08-06 15:44:53,828 INFO  [PriorityRpcServer.handler=6,queue=0,port=16020] regionserver.RSRpcServices: Compacting hbase:meta,,1.1588230740
2014-08-06 15:44:53,829 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HRegion: Starting compaction on info in region hbase:meta,,1.1588230740
2014-08-06 15:44:53,830 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HStore: Starting compaction of 1 file(s) in info of hbase:meta,,1.1588230740 into tmpdir=hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp, totalSize=28.3 K
2014-08-06 15:44:53,830 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:44:53,917 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.HStore: Completed major compaction of 1 (all) file(s) in info of hbase:meta,,1.1588230740 into d75f39f4d4ed4b3cbe8ee5616934805f(size=28.3 K), total size for store is 28.3 K. This selection was in queue for 0sec, and took 0sec to execute.
2014-08-06 15:44:53,917 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407363074017] regionserver.CompactSplitThread: Completed compaction: Request = regionName=hbase:meta,,1.1588230740, storeName=info, fileCount=1, fileSize=28.3 K, priority=1, time=1023501192385307; duration=0sec
2014-08-06 15:45:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.00 MB, freeSize=5.94 GB, max=5.95 GB, accesses=1173, hits=1165, hitRatio=99.32%, , cachingAccesses=1167, cachingHits=1159, cachingHitsRatio=99.31%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 15:45:22,470 INFO  [defaultRpcServer.handler=48,queue=3,port=16020] compress.CodecPool: Got brand-new compressor [.gz]
2014-08-06 15:45:22,471 INFO  [defaultRpcServer.handler=48,queue=3,port=16020] master.HMaster: Client=hadoop//9.1.143.58 create 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}
2014-08-06 15:45:22,544 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.CreateTableHandler: Create table usertable
2014-08-06 15:45:22,597 INFO  [RegionOpenAndInitThread-usertable-1] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,598 INFO  [RegionOpenAndInitThread-usertable-2] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,598 INFO  [RegionOpenAndInitThread-usertable-3] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,598 INFO  [RegionOpenAndInitThread-usertable-4] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,599 INFO  [RegionOpenAndInitThread-usertable-5] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,599 INFO  [RegionOpenAndInitThread-usertable-6] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,600 INFO  [RegionOpenAndInitThread-usertable-7] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,600 INFO  [RegionOpenAndInitThread-usertable-8] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,600 INFO  [RegionOpenAndInitThread-usertable-9] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,601 INFO  [RegionOpenAndInitThread-usertable-10] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-06 15:45:22,670 INFO  [RegionOpenAndInitThread-usertable-6] regionserver.HRegion: Closed usertable,user5,1407365122455.943fa17c811d717fc6a0f13bb88e6911.
2014-08-06 15:45:22,670 INFO  [RegionOpenAndInitThread-usertable-3] regionserver.HRegion: Closed usertable,user2,1407365122455.e6c7c683b45079b9acd8a18b7b278d42.
2014-08-06 15:45:22,675 INFO  [RegionOpenAndInitThread-usertable-5] regionserver.HRegion: Closed usertable,user4,1407365122455.38223ff9dfb30bb65fa6de83292a48e8.
2014-08-06 15:45:22,682 INFO  [RegionOpenAndInitThread-usertable-1] regionserver.HRegion: Closed usertable,,1407365122455.d732118d66d79f0d4393bf3e588dfa69.
2014-08-06 15:45:22,683 INFO  [RegionOpenAndInitThread-usertable-8] regionserver.HRegion: Closed usertable,user7,1407365122455.ca800a83b73090be700d6b9f0f8d57fd.
2014-08-06 15:45:22,685 INFO  [RegionOpenAndInitThread-usertable-7] regionserver.HRegion: Closed usertable,user6,1407365122455.abc0e305d80058afb26a2b1e7fd80164.
2014-08-06 15:45:22,685 INFO  [RegionOpenAndInitThread-usertable-4] regionserver.HRegion: Closed usertable,user3,1407365122455.7c60e4001f3fe75c3420d470e480a63d.
2014-08-06 15:45:22,686 INFO  [RegionOpenAndInitThread-usertable-9] regionserver.HRegion: Closed usertable,user8,1407365122455.798fa05b035c8f0a80531bf21c766e50.
2014-08-06 15:45:22,687 INFO  [RegionOpenAndInitThread-usertable-10] regionserver.HRegion: Closed usertable,user9,1407365122455.e4e7f5b29db88c799799a751a289dbb3.
2014-08-06 15:45:22,725 INFO  [RegionOpenAndInitThread-usertable-2] regionserver.HRegion: Closed usertable,user1,1407365122455.c47756840faa023ee76d32ad246cc447.
2014-08-06 15:45:22,740 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] hbase.MetaTableAccessor: Added 10
2014-08-06 15:45:22,787 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407363145796
2014-08-06 15:45:22,787 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {943fa17c811d717fc6a0f13bb88e6911 state=OFFLINE, ts=1407365122740, server=null} to {943fa17c811d717fc6a0f13bb88e6911 state=PENDING_OPEN, ts=1407365122787, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,787 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user5,1407365122455.943fa17c811d717fc6a0f13bb88e6911. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,790 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {e6c7c683b45079b9acd8a18b7b278d42 state=OFFLINE, ts=1407365122740, server=null} to {e6c7c683b45079b9acd8a18b7b278d42 state=PENDING_OPEN, ts=1407365122790, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,790 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user2,1407365122455.e6c7c683b45079b9acd8a18b7b278d42. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,792 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {38223ff9dfb30bb65fa6de83292a48e8 state=OFFLINE, ts=1407365122740, server=null} to {38223ff9dfb30bb65fa6de83292a48e8 state=PENDING_OPEN, ts=1407365122792, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,792 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user4,1407365122455.38223ff9dfb30bb65fa6de83292a48e8. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,794 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {d732118d66d79f0d4393bf3e588dfa69 state=OFFLINE, ts=1407365122740, server=null} to {d732118d66d79f0d4393bf3e588dfa69 state=PENDING_OPEN, ts=1407365122794, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,794 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,,1407365122455.d732118d66d79f0d4393bf3e588dfa69. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,796 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {ca800a83b73090be700d6b9f0f8d57fd state=OFFLINE, ts=1407365122740, server=null} to {ca800a83b73090be700d6b9f0f8d57fd state=PENDING_OPEN, ts=1407365122796, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,796 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user7,1407365122455.ca800a83b73090be700d6b9f0f8d57fd. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,799 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {abc0e305d80058afb26a2b1e7fd80164 state=OFFLINE, ts=1407365122740, server=null} to {abc0e305d80058afb26a2b1e7fd80164 state=PENDING_OPEN, ts=1407365122799, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,799 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user6,1407365122455.abc0e305d80058afb26a2b1e7fd80164. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,801 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {7c60e4001f3fe75c3420d470e480a63d state=OFFLINE, ts=1407365122740, server=null} to {7c60e4001f3fe75c3420d470e480a63d state=PENDING_OPEN, ts=1407365122801, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,802 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user3,1407365122455.7c60e4001f3fe75c3420d470e480a63d. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,804 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {798fa05b035c8f0a80531bf21c766e50 state=OFFLINE, ts=1407365122740, server=null} to {798fa05b035c8f0a80531bf21c766e50 state=PENDING_OPEN, ts=1407365122804, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,804 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user8,1407365122455.798fa05b035c8f0a80531bf21c766e50. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,807 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {e4e7f5b29db88c799799a751a289dbb3 state=OFFLINE, ts=1407365122740, server=null} to {e4e7f5b29db88c799799a751a289dbb3 state=PENDING_OPEN, ts=1407365122807, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,807 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user9,1407365122455.e4e7f5b29db88c799799a751a289dbb3. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,810 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {c47756840faa023ee76d32ad246cc447 state=OFFLINE, ts=1407365122740, server=null} to {c47756840faa023ee76d32ad246cc447 state=PENDING_OPEN, ts=1407365122810, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,810 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user1,1407365122455.c47756840faa023ee76d32ad246cc447. with state=PENDING_OPEN&sn=slave1,16020,1407363145796
2014-08-06 15:45:22,853 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from ENABLING to ENABLED
2014-08-06 15:45:22,862 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Transition {e6c7c683b45079b9acd8a18b7b278d42 state=PENDING_OPEN, ts=1407365122790, server=slave1,16020,1407363145796} to {e6c7c683b45079b9acd8a18b7b278d42 state=OPEN, ts=1407365122862, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,863 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user2,1407365122455.e6c7c683b45079b9acd8a18b7b278d42. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,863 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Transition {943fa17c811d717fc6a0f13bb88e6911 state=PENDING_OPEN, ts=1407365122787, server=slave1,16020,1407363145796} to {943fa17c811d717fc6a0f13bb88e6911 state=OPEN, ts=1407365122863, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,863 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.CreateTableHandler: failed. null
2014-08-06 15:45:22,863 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user5,1407365122455.943fa17c811d717fc6a0f13bb88e6911. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,866 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Onlined e6c7c683b45079b9acd8a18b7b278d42 on slave1,16020,1407363145796
2014-08-06 15:45:22,866 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Onlined 943fa17c811d717fc6a0f13bb88e6911 on slave1,16020,1407363145796
2014-08-06 15:45:22,888 INFO  [defaultRpcServer.handler=19,queue=4,port=16020] master.RegionStates: Transition {38223ff9dfb30bb65fa6de83292a48e8 state=PENDING_OPEN, ts=1407365122792, server=slave1,16020,1407363145796} to {38223ff9dfb30bb65fa6de83292a48e8 state=OPEN, ts=1407365122888, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,888 INFO  [defaultRpcServer.handler=19,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user4,1407365122455.38223ff9dfb30bb65fa6de83292a48e8. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,890 INFO  [defaultRpcServer.handler=19,queue=4,port=16020] master.RegionStates: Onlined 38223ff9dfb30bb65fa6de83292a48e8 on slave1,16020,1407363145796
2014-08-06 15:45:22,895 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Transition {d732118d66d79f0d4393bf3e588dfa69 state=PENDING_OPEN, ts=1407365122794, server=slave1,16020,1407363145796} to {d732118d66d79f0d4393bf3e588dfa69 state=OPEN, ts=1407365122895, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,895 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStateStore: Updating row usertable,,1407365122455.d732118d66d79f0d4393bf3e588dfa69. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,895 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Transition {ca800a83b73090be700d6b9f0f8d57fd state=PENDING_OPEN, ts=1407365122796, server=slave1,16020,1407363145796} to {ca800a83b73090be700d6b9f0f8d57fd state=OPEN, ts=1407365122895, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,895 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user7,1407365122455.ca800a83b73090be700d6b9f0f8d57fd. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,898 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Onlined d732118d66d79f0d4393bf3e588dfa69 on slave1,16020,1407363145796
2014-08-06 15:45:22,898 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Onlined ca800a83b73090be700d6b9f0f8d57fd on slave1,16020,1407363145796
2014-08-06 15:45:22,921 INFO  [defaultRpcServer.handler=24,queue=4,port=16020] master.RegionStates: Transition {abc0e305d80058afb26a2b1e7fd80164 state=PENDING_OPEN, ts=1407365122799, server=slave1,16020,1407363145796} to {abc0e305d80058afb26a2b1e7fd80164 state=OPEN, ts=1407365122921, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,921 INFO  [defaultRpcServer.handler=24,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user6,1407365122455.abc0e305d80058afb26a2b1e7fd80164. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,923 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStates: Transition {7c60e4001f3fe75c3420d470e480a63d state=PENDING_OPEN, ts=1407365122801, server=slave1,16020,1407363145796} to {7c60e4001f3fe75c3420d470e480a63d state=OPEN, ts=1407365122923, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,923 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user3,1407365122455.7c60e4001f3fe75c3420d470e480a63d. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,924 INFO  [defaultRpcServer.handler=24,queue=4,port=16020] master.RegionStates: Onlined abc0e305d80058afb26a2b1e7fd80164 on slave1,16020,1407363145796
2014-08-06 15:45:22,926 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStates: Onlined 7c60e4001f3fe75c3420d470e480a63d on slave1,16020,1407363145796
2014-08-06 15:45:22,931 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStates: Transition {798fa05b035c8f0a80531bf21c766e50 state=PENDING_OPEN, ts=1407365122804, server=slave1,16020,1407363145796} to {798fa05b035c8f0a80531bf21c766e50 state=OPEN, ts=1407365122931, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,932 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user8,1407365122455.798fa05b035c8f0a80531bf21c766e50. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,937 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStates: Onlined 798fa05b035c8f0a80531bf21c766e50 on slave1,16020,1407363145796
2014-08-06 15:45:22,964 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Transition {e4e7f5b29db88c799799a751a289dbb3 state=PENDING_OPEN, ts=1407365122807, server=slave1,16020,1407363145796} to {e4e7f5b29db88c799799a751a289dbb3 state=OPEN, ts=1407365122964, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,964 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user9,1407365122455.e4e7f5b29db88c799799a751a289dbb3. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:22,967 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Onlined e4e7f5b29db88c799799a751a289dbb3 on slave1,16020,1407363145796
2014-08-06 15:45:22,998 INFO  [defaultRpcServer.handler=8,queue=3,port=16020] master.RegionStates: Transition {c47756840faa023ee76d32ad246cc447 state=PENDING_OPEN, ts=1407365122810, server=slave1,16020,1407363145796} to {c47756840faa023ee76d32ad246cc447 state=OPEN, ts=1407365122998, server=slave1,16020,1407363145796}
2014-08-06 15:45:22,998 INFO  [defaultRpcServer.handler=8,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user1,1407365122455.c47756840faa023ee76d32ad246cc447. with state=OPEN&openSeqNum=2&server=slave1,16020,1407363145796
2014-08-06 15:45:23,002 INFO  [defaultRpcServer.handler=8,queue=3,port=16020] master.RegionStates: Onlined c47756840faa023ee76d32ad246cc447 on slave1,16020,1407363145796
2014-08-06 15:45:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:45:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407364835238, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365135161, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:45:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:45:35,171 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:45:35,177 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:45:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:45:35,178 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:45:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365135161, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365135178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:45:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:45:35,182 INFO  [AM.-pool1-t12] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:45:35,182 INFO  [AM.-pool1-t12] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365135178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365135182, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:45:35,182 INFO  [AM.-pool1-t12] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:45:35,185 INFO  [AM.-pool1-t12] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:45:35,195 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:45:35,196 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:45:35,225 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:45:35,226 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:45:35,226 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365135182, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365135226, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:45:35,227 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:46:26,106 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 7024338432 and memstore heap size to 3192880896
2014-08-06 15:50:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=6.54 GB, max=6.54 GB, accesses=1206, hits=1195, hitRatio=99.09%, , cachingAccesses=1200, cachingHits=1189, cachingHitsRatio=99.08%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 15:50:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:50:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365135226, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365435162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:50:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:50:35,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:50:35,177 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:50:35,177 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:50:35,177 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:50:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365435162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365435178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:50:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:50:35,183 INFO  [AM.-pool1-t13] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:50:35,183 INFO  [AM.-pool1-t13] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365435178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365435183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:50:35,183 INFO  [AM.-pool1-t13] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:50:35,185 INFO  [AM.-pool1-t13] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:50:35,192 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:50:35,193 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:50:35,209 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:50:35,209 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:50:35,210 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365435183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365435210, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:50:35,210 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:55:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=6.54 GB, max=6.54 GB, accesses=1209, hits=1198, hitRatio=99.09%, , cachingAccesses=1203, cachingHits=1192, cachingHitsRatio=99.09%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 15:55:35,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:55:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365435210, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365735162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:55:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 15:55:35,177 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:55:35,182 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 15:55:35,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:55:35,182 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 15:55:35,183 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407365735162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365735183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:55:35,183 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 15:55:35,186 INFO  [AM.-pool1-t14] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 15:55:35,186 INFO  [AM.-pool1-t14] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407365735183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365735186, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:55:35,186 INFO  [AM.-pool1-t14] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 15:55:35,189 INFO  [AM.-pool1-t14] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:55:35,200 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 15:55:35,201 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 15:55:35,222 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 15:55:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 15:55:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407365735186, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365735223, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 15:55:35,223 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:00:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=6.54 GB, max=6.54 GB, accesses=1212, hits=1201, hitRatio=99.09%, , cachingAccesses=1206, cachingHits=1195, cachingHitsRatio=99.09%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 16:00:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:00:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407365735223, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366035162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:00:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 16:00:35,170 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:00:35,177 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 16:00:35,177 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:00:35,178 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 16:00:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366035162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366035178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:00:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 16:00:35,184 INFO  [AM.-pool1-t15] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:00:35,184 INFO  [AM.-pool1-t15] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366035178, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366035184, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:00:35,184 INFO  [AM.-pool1-t15] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 16:00:35,187 INFO  [AM.-pool1-t15] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:00:35,198 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 16:00:35,199 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 16:00:35,218 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 16:00:35,219 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:00:35,219 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366035184, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407366035219, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:00:35,220 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:05:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=6.54 GB, max=6.54 GB, accesses=1215, hits=1204, hitRatio=99.09%, , cachingAccesses=1209, cachingHits=1198, cachingHitsRatio=99.09%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 16:05:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:05:35,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407366035219, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366335162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:05:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 16:05:35,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:05:35,177 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 16:05:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:05:35,178 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 16:05:35,179 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366335162, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366335179, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:05:35,179 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 16:05:35,186 INFO  [AM.-pool1-t16] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:05:35,186 INFO  [AM.-pool1-t16] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366335179, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366335186, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:05:35,186 INFO  [AM.-pool1-t16] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 16:05:35,191 INFO  [AM.-pool1-t16] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:05:35,202 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 16:05:35,203 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 16:05:35,224 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 16:05:35,225 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:05:35,225 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366335186, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407366335225, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:05:35,225 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:10:02,441 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.02 MB, freeSize=6.54 GB, max=6.54 GB, accesses=1218, hits=1207, hitRatio=99.10%, , cachingAccesses=1212, cachingHits=1201, cachingHitsRatio=99.09%, evictions=0, evicted=8, evictedPerRun=Infinity
2014-08-06 16:10:25,709 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407363025164 with entries=0, filesize=17 B; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407366625612
2014-08-06 16:10:30,018 INFO  [RS_OPEN_META-sceplus-vm48:16020-0-MetaLogRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407363029810.meta with entries=275, filesize=73.96 KB; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407363002241/sceplus-vm48.almaden.ibm.com%2C16020%2C1407363002241.1407366629863.meta
2014-08-06 16:10:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a., src=sceplus-vm48.almaden.ibm.com,16020,1407361669146, dest=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:10:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407366335225, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366635163, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:10:35,163 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_CLOSE
2014-08-06 16:10:35,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407363002241-BalancerChore] regionserver.RSRpcServices: Close fe1c441ecb6e3789e0fdd62314d7b06a, moving to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:10:35,178 INFO  [StoreCloserThread-hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.-1] regionserver.HStore: Closed info
2014-08-06 16:10:35,178 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:10:35,179 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: fe1c441ecb6e3789e0fdd62314d7b06a to self.
2014-08-06 16:10:35,179 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_CLOSE, ts=1407366635163, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366635179, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:10:35,179 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=CLOSED
2014-08-06 16:10:35,183 INFO  [AM.-pool1-t17] master.AssignmentManager: Assigning hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. to sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:10:35,183 INFO  [AM.-pool1-t17] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=CLOSED, ts=1407366635179, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366635183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:10:35,183 INFO  [AM.-pool1-t17] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=PENDING_OPEN
2014-08-06 16:10:35,186 INFO  [AM.-pool1-t17] regionserver.RSRpcServices: Open hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:10:35,194 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@3ffbe014, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-06 16:10:35,194 INFO  [StoreOpener-fe1c441ecb6e3789e0fdd62314d7b06a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-06 16:10:35,209 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined fe1c441ecb6e3789e0fdd62314d7b06a; next sequenceid=8
2014-08-06 16:10:35,210 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a.
2014-08-06 16:10:35,210 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStates: Transition {fe1c441ecb6e3789e0fdd62314d7b06a state=PENDING_OPEN, ts=1407366635183, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241} to {fe1c441ecb6e3789e0fdd62314d7b06a state=OPEN, ts=1407366635210, server=sceplus-vm48.almaden.ibm.com,16020,1407363002241}
2014-08-06 16:10:35,210 INFO  [PostOpenDeployTasks:fe1c441ecb6e3789e0fdd62314d7b06a] master.RegionStateStore: Updating row hbase:namespace,,1406986075482.fe1c441ecb6e3789e0fdd62314d7b06a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407363002241
2014-08-06 16:10:35,597 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher] regionserver.HRegionServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher requesting flush for region hbase:meta,,1.1588230740 after a delay of 8737
2014-08-06 16:10:44,394 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=80012536, memsize=132.4 K, hasBloomFilter=false, into tmp file hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp/9f714656977f4476a10150b44f557583
2014-08-06 16:10:44,401 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 9f714656977f4476a10150b44f557583
2014-08-06 16:10:44,411 INFO  [MemStoreFlusher.0] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 9f714656977f4476a10150b44f557583
2014-08-06 16:10:44,411 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/9f714656977f4476a10150b44f557583, entries=132, sequenceid=80012536, filesize=20.0 K
2014-08-06 16:10:44,416 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~132.36 KB/135536, currentsize=0 B/0 for region hbase:meta,,1.1588230740 in 77ms, sequenceid=80012536, compaction requested=false
