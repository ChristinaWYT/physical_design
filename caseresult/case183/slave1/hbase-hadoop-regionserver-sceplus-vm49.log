Mon Jul 21 01:55:19 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-21 01:55:19,722 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-21 01:55:19,723 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-21 01:55:19,723 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-21 01:55:19,953 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-21 01:55:19,953 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-21 01:55:19,953 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 46764 22
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-21 01:55:19,954 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 46764 9.1.143.59 22
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-21 01:55:19,955 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-21 01:55:19,957 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=98
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-21 01:55:19,958 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-21 01:55:19,959 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-21 01:55:19,959 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-21 01:55:19,961 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-21 01:55:19,961 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-21 01:55:20,190 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-21 01:55:20,652 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-21 01:55:20,767 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-21 01:55:20,782 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-21 01:55:20,858 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-21 01:55:20,860 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-21 01:55:20,860 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-21 01:55:20,867 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-21 01:55:20,873 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-21 01:55:20,975 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-21 01:55:20,975 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-21 01:55:20,980 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-21 01:55:20,983 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-21 01:55:21,072 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-21 01:55:21,140 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-21 01:55:21,152 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-21 01:55:21,154 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-21 01:55:21,154 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-21 01:55:21,155 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-21 01:55:21,512 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-21 01:55:21,558 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-21 01:55:21,561 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-21 01:55:21,561 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-21 01:55:21,584 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-21 01:55:21,587 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-21 01:55:21,591 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-21 01:55:21,599 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-21 01:55:21,710 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2014-07-21 01:55:21,710 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-21 01:55:22,078 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-21 01:55:22,078 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-21 01:55:22,093 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1475821e3c60000, negotiated timeout = 90000
2014-07-21 01:55:53,908 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x762eefed, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-21 01:55:53,910 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x762eefed connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-21 01:55:53,910 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-21 01:55:53,911 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-21 01:55:53,915 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1475821e3c60002, negotiated timeout = 90000
2014-07-21 01:55:54,196 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@17897c4d
2014-07-21 01:55:54,201 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-21 01:55:54,207 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-21 01:55:54,214 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-21 01:55:54,252 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-21 01:55:54,259 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-21 01:55:54,263 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-21 01:55:54,281 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405932920500 with port=60020, startcode=1405932920887
2014-07-21 01:55:54,599 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-21 01:55:54,599 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-21 01:55:54,599 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-21 01:55:54,625 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-21 01:55:54,634 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887
2014-07-21 01:55:54,674 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-21 01:55:54,686 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-21 01:55:54,772 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405932954691
2014-07-21 01:55:54,785 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-21 01:55:54,790 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-21 01:55:54,794 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-21 01:55:54,799 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-21 01:55:54,801 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-21 01:55:54,802 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-21 01:55:54,802 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-21 01:55:54,802 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-21 01:55:54,802 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-21 01:55:54,810 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1405932922324, slave1,60020,1405932920887] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1405932922324, slave1,60020,1405932920887]
2014-07-21 01:55:54,832 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-21 01:55:54,835 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x74d37bdd, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-21 01:55:54,836 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x74d37bdd connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-21 01:55:54,836 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-21 01:55:54,837 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-21 01:55:54,841 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x475821ea980004, negotiated timeout = 90000
2014-07-21 01:55:54,847 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-21 01:55:54,847 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-21 01:55:54,894 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405932920887, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x1475821e3c60000
2014-07-21 01:55:54,894 INFO  [SplitLogWorker-slave1,60020,1405932920887] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405932920887 starting
2014-07-21 01:55:54,894 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-21 01:55:54,894 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405932920887
2014-07-21 01:55:54,894 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405932920887'
2014-07-21 01:55:54,895 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-21 01:55:54,896 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-21 01:55:54,896 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-21 01:55:59,540 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:55:59,678 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:55:59,678 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ddad2a8ab23bf81d8e3825dc6fd3e50d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,679 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:55:59,680 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d475eaaf0a2bb0a5de8d229390854422 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,682 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-21 01:55:59,682 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 29fb436f66188416368f4fdf09a317d8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,699 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:55:59,699 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:55:59,709 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ddad2a8ab23bf81d8e3825dc6fd3e50d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,709 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 29fb436f66188416368f4fdf09a317d8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,709 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d475eaaf0a2bb0a5de8d229390854422 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:55:59,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => ddad2a8ab23bf81d8e3825dc6fd3e50d, NAME => 'usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-21 01:55:59,725 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => d475eaaf0a2bb0a5de8d229390854422, NAME => 'usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-21 01:55:59,726 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 29fb436f66188416368f4fdf09a317d8, NAME => 'usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-21 01:55:59,751 INFO  [RS_OPEN_REGION-slave1:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-21 01:55:59,752 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 29fb436f66188416368f4fdf09a317d8
2014-07-21 01:55:59,752 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ddad2a8ab23bf81d8e3825dc6fd3e50d
2014-07-21 01:55:59,752 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable d475eaaf0a2bb0a5de8d229390854422
2014-07-21 01:55:59,753 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:55:59,753 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:55:59,753 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:55:59,761 INFO  [RS_OPEN_REGION-slave1:60020-0] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-21 01:55:59,763 INFO  [RS_OPEN_REGION-slave1:60020-0] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-21 01:55:59,765 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-21 01:55:59,765 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-21 01:55:59,766 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-21 01:55:59,855 INFO  [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 01:55:59,855 INFO  [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 01:55:59,855 INFO  [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 01:55:59,929 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-21 01:55:59,984 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-21 01:55:59,984 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-21 01:55:59,984 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-21 01:55:59,998 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/032c4dde794b4f9598081a0f9b7b709b, isReference=false, isBulkLoadResult=false, seqid=20314, majorCompaction=false
2014-07-21 01:55:59,998 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/04e29a2220d94dc7bf927fe0ee33ec37, isReference=false, isBulkLoadResult=false, seqid=19523, majorCompaction=false
2014-07-21 01:55:59,998 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/0069b7aa5dde47c4bbcbac1f7a6052ea, isReference=false, isBulkLoadResult=false, seqid=17076, majorCompaction=false
2014-07-21 01:56:00,036 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/035f06f2e9ef4d7fac4af53ff019d9e8, isReference=false, isBulkLoadResult=false, seqid=18083, majorCompaction=false
2014-07-21 01:56:00,041 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/07a8fd211437492c83f2e3cf06ebc512, isReference=false, isBulkLoadResult=false, seqid=15598, majorCompaction=false
2014-07-21 01:56:00,044 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/04ccf2c0c9414a5cbe3a3ce46f5a43b6, isReference=false, isBulkLoadResult=false, seqid=23197, majorCompaction=false
2014-07-21 01:56:00,058 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/0c6f80be86d04ea0a8f99a03fdbca4a8, isReference=false, isBulkLoadResult=false, seqid=21429, majorCompaction=false
2014-07-21 01:56:00,072 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/0d66987c58cf4c28bce36ec67f988894, isReference=false, isBulkLoadResult=false, seqid=18171, majorCompaction=false
2014-07-21 01:56:00,076 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/0b2e512f35a94772a721b9b5a872bec7, isReference=false, isBulkLoadResult=false, seqid=14979, majorCompaction=false
2014-07-21 01:56:00,090 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/1522b217f15c4847ad0fe83867e4e45d, isReference=false, isBulkLoadResult=false, seqid=22269, majorCompaction=false
2014-07-21 01:56:00,104 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/110d147cbee6408fb5d5f004334638aa, isReference=false, isBulkLoadResult=false, seqid=16224, majorCompaction=false
2014-07-21 01:56:00,104 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/0e38af656c5e447abed6bd6dcb07bb0f, isReference=false, isBulkLoadResult=false, seqid=13885, majorCompaction=false
2014-07-21 01:56:00,115 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/18ef0307dc9f452d9c933ab5e8272883, isReference=false, isBulkLoadResult=false, seqid=25505, majorCompaction=false
2014-07-21 01:56:00,150 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/141e358ae4ea4db3b181b1ec8d9af1ed, isReference=false, isBulkLoadResult=false, seqid=21563, majorCompaction=false
2014-07-21 01:56:00,171 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/2d9a12525dd048e8be9283adc11ce308, isReference=false, isBulkLoadResult=false, seqid=14477, majorCompaction=false
2014-07-21 01:56:00,180 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/10c03c1567fd479a9f563f8d49f389ff, isReference=false, isBulkLoadResult=false, seqid=20223, majorCompaction=false
2014-07-21 01:56:00,184 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/25d1c2325e4c4f0ab332eaf7cd9ef86c, isReference=false, isBulkLoadResult=false, seqid=11257, majorCompaction=false
2014-07-21 01:56:00,209 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/2d00c55f0fe0454ab86e40b9129d7371, isReference=false, isBulkLoadResult=false, seqid=15117, majorCompaction=false
2014-07-21 01:56:00,213 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/12a17d5cda0e4546a743b0a3557c0c77, isReference=false, isBulkLoadResult=false, seqid=16498, majorCompaction=false
2014-07-21 01:56:00,220 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/32cdb1f79ef54f8caf4c3a12d967ddd6, isReference=false, isBulkLoadResult=false, seqid=11544, majorCompaction=false
2014-07-21 01:56:00,237 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/133eb321ebac4b469c73b708a132a664, isReference=false, isBulkLoadResult=false, seqid=14382, majorCompaction=false
2014-07-21 01:56:00,245 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/349d9ae9f01548b7af5904b46ae91232, isReference=false, isBulkLoadResult=false, seqid=12942, majorCompaction=false
2014-07-21 01:56:00,253 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/3f399481a9a44701ac05684184de5eab, isReference=false, isBulkLoadResult=false, seqid=19015, majorCompaction=false
2014-07-21 01:56:00,269 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/49059fa17119428596264695465a9734, isReference=false, isBulkLoadResult=false, seqid=12171, majorCompaction=false
2014-07-21 01:56:00,274 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/35266608f16e4880b706d67787d19ddf, isReference=false, isBulkLoadResult=false, seqid=24315, majorCompaction=false
2014-07-21 01:56:00,278 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/574da6577d384a5e804af87b6a676415, isReference=false, isBulkLoadResult=false, seqid=21086, majorCompaction=false
2014-07-21 01:56:00,283 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/52a20912d9384588a3d7d4b192bb7281, isReference=false, isBulkLoadResult=false, seqid=26514, majorCompaction=false
2014-07-21 01:56:00,294 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/52f0638f933a41f5b4479b6e1ced9c81, isReference=false, isBulkLoadResult=false, seqid=24825, majorCompaction=false
2014-07-21 01:56:00,306 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/6f36b006afc34fd78e8feae13b02b002, isReference=false, isBulkLoadResult=false, seqid=11018, majorCompaction=false
2014-07-21 01:56:00,311 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/5b5c3a9199c84463b9af5dc71dab9a73, isReference=false, isBulkLoadResult=false, seqid=1108, majorCompaction=true
2014-07-21 01:56:00,325 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/651e1a7cc72c40b5b9b34a9624664cbb, isReference=false, isBulkLoadResult=false, seqid=19569, majorCompaction=false
2014-07-21 01:56:00,330 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/685037ac0d9a46e3829e2cc822f7b498, isReference=false, isBulkLoadResult=false, seqid=7477, majorCompaction=false
2014-07-21 01:56:00,341 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/6fa4463563f5411cb5e00938576917cc, isReference=false, isBulkLoadResult=false, seqid=3030, majorCompaction=false
2014-07-21 01:56:00,342 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/718abc8157c64b12bee00e4dd63d6cdf, isReference=false, isBulkLoadResult=false, seqid=25903, majorCompaction=false
2014-07-21 01:56:00,370 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/73e9383a93104ee7a9cccfde9ec6df4b, isReference=false, isBulkLoadResult=false, seqid=14975, majorCompaction=false
2014-07-21 01:56:00,392 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/6915403a74164b4697c986eff1e8e71e, isReference=false, isBulkLoadResult=false, seqid=6047, majorCompaction=false
2014-07-21 01:56:00,404 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/7a452b6977fd45b0a5e0b488c1e6de2e, isReference=false, isBulkLoadResult=false, seqid=13538, majorCompaction=false
2014-07-21 01:56:00,427 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/6a702a1c605045edaebeacc74fa64b50, isReference=false, isBulkLoadResult=false, seqid=8015, majorCompaction=false
2014-07-21 01:56:00,428 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/744923c4d04240328fb89defea201dff, isReference=false, isBulkLoadResult=false, seqid=7917, majorCompaction=false
2014-07-21 01:56:00,432 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/890361dbd6c045a5bc1b8fbd3fd660be, isReference=false, isBulkLoadResult=false, seqid=3733, majorCompaction=true
2014-07-21 01:56:00,441 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/74b268463d3947c98b99d999595dd31b, isReference=false, isBulkLoadResult=false, seqid=18481, majorCompaction=false
2014-07-21 01:56:00,455 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/870e0e2e9f5c4429a75fff88640899de, isReference=false, isBulkLoadResult=false, seqid=8896, majorCompaction=false
2014-07-21 01:56:00,459 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/8cb9852df3c940418c6d9c9fff60ad38, isReference=false, isBulkLoadResult=false, seqid=20902, majorCompaction=false
2014-07-21 01:56:00,471 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/7d28b0f1ab374f958cbd1457c3783063, isReference=false, isBulkLoadResult=false, seqid=23854, majorCompaction=false
2014-07-21 01:56:00,476 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/981c88330181421e85b322018e73677f, isReference=false, isBulkLoadResult=false, seqid=22323, majorCompaction=false
2014-07-21 01:56:00,483 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/96f0aeb684584f3882e2b2b56f892c50, isReference=false, isBulkLoadResult=false, seqid=13838, majorCompaction=false
2014-07-21 01:56:00,488 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/98bfb05082c24bd591a9ba04f21e17f7, isReference=false, isBulkLoadResult=false, seqid=26530, majorCompaction=false
2014-07-21 01:56:00,497 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/8cd8aed64dc24b038960929a0b953309, isReference=false, isBulkLoadResult=false, seqid=23149, majorCompaction=false
2014-07-21 01:56:00,508 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/a96b38186db14f22b5dcdc75ee730b52, isReference=false, isBulkLoadResult=false, seqid=6132, majorCompaction=false
2014-07-21 01:56:00,515 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/9ed6ccad24cd4a04b2baea719b1456b3, isReference=false, isBulkLoadResult=false, seqid=2529, majorCompaction=false
2014-07-21 01:56:00,534 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/939c67e8e1ed40f895a17c0597d407dc, isReference=false, isBulkLoadResult=false, seqid=8507, majorCompaction=false
2014-07-21 01:56:00,535 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/ba216f1a29b2488a934d2ce789c050cc, isReference=false, isBulkLoadResult=false, seqid=16893, majorCompaction=false
2014-07-21 01:56:00,536 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/a4594d11248545b293590cabdb18b070, isReference=false, isBulkLoadResult=false, seqid=26263, majorCompaction=false
2014-07-21 01:56:00,558 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/9526539210e740c18c3d81bbb54346fd, isReference=false, isBulkLoadResult=false, seqid=22566, majorCompaction=false
2014-07-21 01:56:00,568 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/aca803652f4246dbbfcacaa4ce8e7d7d, isReference=false, isBulkLoadResult=false, seqid=15542, majorCompaction=false
2014-07-21 01:56:00,569 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/969b84f04ee14638b248562302fb566e, isReference=false, isBulkLoadResult=false, seqid=24075, majorCompaction=false
2014-07-21 01:56:00,574 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/d6b736f9ee7348bf8da787ae974265d2, isReference=false, isBulkLoadResult=false, seqid=10588, majorCompaction=false
2014-07-21 01:56:00,588 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/9b19fe40a4b1470c899101ebe2301c14, isReference=false, isBulkLoadResult=false, seqid=18042, majorCompaction=false
2014-07-21 01:56:00,600 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/dc556751715d47e39ffabea451d8d202, isReference=false, isBulkLoadResult=false, seqid=15614, majorCompaction=false
2014-07-21 01:56:00,604 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/b66603c2bc664bf1ae090192676dce76, isReference=false, isBulkLoadResult=false, seqid=24685, majorCompaction=false
2014-07-21 01:56:00,624 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/dd11e2caa7bd44cfa508c4e88d80be4f, isReference=false, isBulkLoadResult=false, seqid=17671, majorCompaction=false
2014-07-21 01:56:00,628 DEBUG [StoreOpener-ddad2a8ab23bf81d8e3825dc6fd3e50d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/f6c9186bcbc445428bc5542a4ae65fce, isReference=false, isBulkLoadResult=false, seqid=25206, majorCompaction=false
2014-07-21 01:56:00,634 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/cea71ee190c9452a8ba976361c8fdd5a, isReference=false, isBulkLoadResult=false, seqid=23831, majorCompaction=false
2014-07-21 01:56:00,643 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/e0b1015040d449aa877151298c355cf8, isReference=false, isBulkLoadResult=false, seqid=23768, majorCompaction=false
2014-07-21 01:56:00,658 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/d2ac2855bc134a73afed5cc5da8baddb, isReference=false, isBulkLoadResult=false, seqid=8272, majorCompaction=false
2014-07-21 01:56:00,663 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/eb378962c67d495d9f87d64e33bb2aae, isReference=false, isBulkLoadResult=false, seqid=16094, majorCompaction=false
2014-07-21 01:56:00,664 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d
2014-07-21 01:56:00,670 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined ddad2a8ab23bf81d8e3825dc6fd3e50d; next sequenceid=26531
2014-07-21 01:56:00,671 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ddad2a8ab23bf81d8e3825dc6fd3e50d
2014-07-21 01:56:00,675 INFO  [PostOpenDeployTasks:ddad2a8ab23bf81d8e3825dc6fd3e50d] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:00,679 DEBUG [PostOpenDeployTasks:ddad2a8ab23bf81d8e3825dc6fd3e50d] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 01:56:00,682 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 01:56:00,686 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/da472a5c29174f3fbd06e97a4359690f, isReference=false, isBulkLoadResult=false, seqid=20015, majorCompaction=false
2014-07-21 01:56:00,690 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 354790343 starting at candidate #12 after considering 132 permutations with 129 in ratio
2014-07-21 01:56:00,693 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.HStore: ddad2a8ab23bf81d8e3825dc6fd3e50d - family: Initiating minor compaction
2014-07-21 01:56:00,693 INFO  [regionserver60020-smallCompactions-1405932960678] regionserver.HRegion: Starting compaction on family in region usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:00,694 INFO  [regionserver60020-smallCompactions-1405932960678] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/.tmp, totalSize=338.4m
2014-07-21 01:56:00,697 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/f13088cb86724d71b8905e7bddbed641, isReference=false, isBulkLoadResult=false, seqid=10516, majorCompaction=false
2014-07-21 01:56:00,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/032c4dde794b4f9598081a0f9b7b709b, keycount=202285, bloomtype=ROW, size=144.0m, encoding=NONE, seqNum=20314
2014-07-21 01:56:00,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/8cb9852df3c940418c6d9c9fff60ad38, keycount=139914, bloomtype=ROW, size=99.6m, encoding=NONE, seqNum=20902
2014-07-21 01:56:00,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/ddad2a8ab23bf81d8e3825dc6fd3e50d/family/141e358ae4ea4db3b181b1ec8d9af1ed, keycount=133032, bloomtype=ROW, size=94.7m, encoding=NONE, seqNum=21563
2014-07-21 01:56:00,706 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/f035db8290194e8ca83690096519c1fc, isReference=false, isBulkLoadResult=false, seqid=9033, majorCompaction=false
2014-07-21 01:56:00,723 DEBUG [StoreOpener-29fb436f66188416368f4fdf09a317d8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8/family/f0f13635ea0e4e55acb1141a01a94121, isReference=false, isBulkLoadResult=false, seqid=21747, majorCompaction=false
2014-07-21 01:56:00,725 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/f43c4ebb4cde4c52a8a5aaddfa4c5bdc, isReference=false, isBulkLoadResult=false, seqid=1974, majorCompaction=true
2014-07-21 01:56:00,726 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/29fb436f66188416368f4fdf09a317d8
2014-07-21 01:56:00,729 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 29fb436f66188416368f4fdf09a317d8; next sequenceid=24076
2014-07-21 01:56:00,730 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 29fb436f66188416368f4fdf09a317d8
2014-07-21 01:56:00,732 INFO  [PostOpenDeployTasks:29fb436f66188416368f4fdf09a317d8] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:00,733 DEBUG [PostOpenDeployTasks:29fb436f66188416368f4fdf09a317d8] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 01:56:00,744 DEBUG [StoreOpener-d475eaaf0a2bb0a5de8d229390854422-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422/family/fbf7949c02b5450a81204ca69f9640df, isReference=false, isBulkLoadResult=false, seqid=22817, majorCompaction=false
2014-07-21 01:56:00,750 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/d475eaaf0a2bb0a5de8d229390854422
2014-07-21 01:56:00,752 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined d475eaaf0a2bb0a5de8d229390854422; next sequenceid=26515
2014-07-21 01:56:00,752 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node d475eaaf0a2bb0a5de8d229390854422
2014-07-21 01:56:00,753 INFO  [PostOpenDeployTasks:d475eaaf0a2bb0a5de8d229390854422] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:00,754 DEBUG [PostOpenDeployTasks:d475eaaf0a2bb0a5de8d229390854422] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-21 01:56:00,782 DEBUG [regionserver60020-smallCompactions-1405932960678] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 01:56:00,836 INFO  [PostOpenDeployTasks:d475eaaf0a2bb0a5de8d229390854422] catalog.MetaEditor: Updated row usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422. with server=slave1,60020,1405932920887
2014-07-21 01:56:00,837 INFO  [PostOpenDeployTasks:29fb436f66188416368f4fdf09a317d8] catalog.MetaEditor: Updated row usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8. with server=slave1,60020,1405932920887
2014-07-21 01:56:00,836 INFO  [PostOpenDeployTasks:ddad2a8ab23bf81d8e3825dc6fd3e50d] catalog.MetaEditor: Updated row usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. with server=slave1,60020,1405932920887
2014-07-21 01:56:00,837 INFO  [PostOpenDeployTasks:d475eaaf0a2bb0a5de8d229390854422] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:00,837 INFO  [PostOpenDeployTasks:ddad2a8ab23bf81d8e3825dc6fd3e50d] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:00,837 INFO  [PostOpenDeployTasks:29fb436f66188416368f4fdf09a317d8] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:00,838 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ddad2a8ab23bf81d8e3825dc6fd3e50d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,838 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d475eaaf0a2bb0a5de8d229390854422 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,839 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 29fb436f66188416368f4fdf09a317d8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ddad2a8ab23bf81d8e3825dc6fd3e50d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned ddad2a8ab23bf81d8e3825dc6fd3e50d to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. on slave1,60020,1405932920887
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 29fb436f66188416368f4fdf09a317d8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 29fb436f66188416368f4fdf09a317d8 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:00,844 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,845 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8. on slave1,60020,1405932920887
2014-07-21 01:56:00,845 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d475eaaf0a2bb0a5de8d229390854422 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,845 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned d475eaaf0a2bb0a5de8d229390854422 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:00,845 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422. on slave1,60020,1405932920887
2014-07-21 01:56:00,845 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cf5c5ce53bff2a0e0ed070b0f00b8a4b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,846 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 139cd3dc941a21a33dc2016906dff923 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,849 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,850 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-21 01:56:00,850 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cf5c5ce53bff2a0e0ed070b0f00b8a4b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,850 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 139cd3dc941a21a33dc2016906dff923 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 01:56:00,850 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-21 01:56:00,851 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-21 01:56:00,851 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => cf5c5ce53bff2a0e0ed070b0f00b8a4b, NAME => 'usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-21 01:56:00,851 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 139cd3dc941a21a33dc2016906dff923, NAME => 'usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-21 01:56:00,851 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 139cd3dc941a21a33dc2016906dff923
2014-07-21 01:56:00,852 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:00,852 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable cf5c5ce53bff2a0e0ed070b0f00b8a4b
2014-07-21 01:56:00,852 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:00,858 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-21 01:56:00,862 INFO  [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 01:56:00,866 INFO  [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 01:56:00,879 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-21 01:56:00,882 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-21 01:56:00,885 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-21 01:56:00,885 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-21 01:56:00,887 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-21 01:56:00,894 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405932920887
2014-07-21 01:56:00,895 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-21 01:56:00,896 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,899 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:00,899 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:00,899 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405932920887
2014-07-21 01:56:00,908 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/023b734f0d444777b3841b4043ca63ff, isReference=false, isBulkLoadResult=false, seqid=20869, majorCompaction=false
2014-07-21 01:56:00,909 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/042142fb782e4a33a53a52185c20fc9d, isReference=false, isBulkLoadResult=false, seqid=22622, majorCompaction=false
2014-07-21 01:56:00,937 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/0655390e73c94a4dadfcf8fc36d03f8c, isReference=false, isBulkLoadResult=false, seqid=18947, majorCompaction=false
2014-07-21 01:56:00,938 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/25d4cb4c6ffa4004935813dc33906a5b, isReference=false, isBulkLoadResult=false, seqid=5002, majorCompaction=false
2014-07-21 01:56:00,975 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/07bb9e882fe445d5a2966e29b2e2ef52, isReference=false, isBulkLoadResult=false, seqid=4674, majorCompaction=true
2014-07-21 01:56:00,978 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/2d697d524c12477ca492e69b2caf83a3, isReference=false, isBulkLoadResult=false, seqid=4392, majorCompaction=false
2014-07-21 01:56:00,990 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/0ae23d85c6c04acd9ac45a3a24099232, isReference=false, isBulkLoadResult=false, seqid=24074, majorCompaction=false
2014-07-21 01:56:01,006 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/360e2f262e834f36abd55863c15a7299, isReference=false, isBulkLoadResult=false, seqid=20152, majorCompaction=false
2014-07-21 01:56:01,012 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/0c93ee8116724a2ba56a7044eafaa8a1, isReference=false, isBulkLoadResult=false, seqid=12426, majorCompaction=false
2014-07-21 01:56:01,028 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/48091e23563c494e9c25475b72688843, isReference=false, isBulkLoadResult=false, seqid=15791, majorCompaction=false
2014-07-21 01:56:01,031 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/100063aa87c04970becdcfab0e48cde2, isReference=false, isBulkLoadResult=false, seqid=14568, majorCompaction=false
2014-07-21 01:56:01,054 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/596799362a3e4a69bb87adffbb26d272, isReference=false, isBulkLoadResult=false, seqid=21739, majorCompaction=false
2014-07-21 01:56:01,059 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/104eb8190004401cbb7ac2d630ccf4ff, isReference=false, isBulkLoadResult=false, seqid=16006, majorCompaction=false
2014-07-21 01:56:01,073 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/5f35e15a4fab4a81b8acada94d1f3d8f, isReference=false, isBulkLoadResult=false, seqid=19409, majorCompaction=false
2014-07-21 01:56:01,081 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/20d7ec44907743cbb0b076c49ede43cc, isReference=false, isBulkLoadResult=false, seqid=15007, majorCompaction=false
2014-07-21 01:56:01,091 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/7c3325ef565d43dc85f6ee3d8baaad49, isReference=false, isBulkLoadResult=false, seqid=23759, majorCompaction=false
2014-07-21 01:56:01,101 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/85acc695aaae4d2f825d3f601c5ad61d, isReference=false, isBulkLoadResult=false, seqid=24063, majorCompaction=false
2014-07-21 01:56:01,114 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/2e8c72bd51994fe2b6b75553c4446b19, isReference=false, isBulkLoadResult=false, seqid=13481, majorCompaction=false
2014-07-21 01:56:01,125 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/8cefa7d2b76d4e97a210a2dc277401ee, isReference=false, isBulkLoadResult=false, seqid=18383, majorCompaction=false
2014-07-21 01:56:01,135 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/358c5b7bbb63471bb99bf2c008abbbda, isReference=false, isBulkLoadResult=false, seqid=15447, majorCompaction=false
2014-07-21 01:56:01,149 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/a22fdee1dd7d43e9a1816bf13cb2970c, isReference=false, isBulkLoadResult=false, seqid=3932, majorCompaction=false
2014-07-21 01:56:01,161 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/4150ebda248442699c18d4e8bd469f94, isReference=false, isBulkLoadResult=false, seqid=11436, majorCompaction=false
2014-07-21 01:56:01,173 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/a995832e8224454dbed6475ed785205f, isReference=false, isBulkLoadResult=false, seqid=10085, majorCompaction=false
2014-07-21 01:56:01,182 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/67ee4bd007a84af3b7099309f1d5fede, isReference=false, isBulkLoadResult=false, seqid=14012, majorCompaction=false
2014-07-21 01:56:01,193 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/b31e6fcbec5f4786b8b844a88eba7d18, isReference=false, isBulkLoadResult=false, seqid=13639, majorCompaction=false
2014-07-21 01:56:01,199 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/7544687725a34a84bfaf84430da99106, isReference=false, isBulkLoadResult=false, seqid=23601, majorCompaction=false
2014-07-21 01:56:01,211 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/b73b0c04769a415693993a5971767358, isReference=false, isBulkLoadResult=false, seqid=22280, majorCompaction=false
2014-07-21 01:56:01,229 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/7dcaafbff071440e944409629751a4fd, isReference=false, isBulkLoadResult=false, seqid=9519, majorCompaction=false
2014-07-21 01:56:01,237 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/c68f500860944aea9690f3d59002e011, isReference=false, isBulkLoadResult=false, seqid=14409, majorCompaction=false
2014-07-21 01:56:01,256 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/956ebedd518f4f3997ed433f956da677, isReference=false, isBulkLoadResult=false, seqid=22886, majorCompaction=false
2014-07-21 01:56:01,263 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/cd06bc468e174b5ea3badccec68b9f0c, isReference=false, isBulkLoadResult=false, seqid=2613, majorCompaction=true
2014-07-21 01:56:01,277 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/ce6f284e8cdb425c9e2ceee006f0382b, isReference=false, isBulkLoadResult=false, seqid=20861, majorCompaction=false
2014-07-21 01:56:01,285 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/99b1ea8f432f454a801f3543be05e541, isReference=false, isBulkLoadResult=false, seqid=12877, majorCompaction=false
2014-07-21 01:56:01,308 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/9d01588df2b748059c9d048cee8105e5, isReference=false, isBulkLoadResult=false, seqid=18121, majorCompaction=false
2014-07-21 01:56:01,310 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/e2142b67b0cd490b8e19801059985bc9, isReference=false, isBulkLoadResult=false, seqid=17392, majorCompaction=false
2014-07-21 01:56:01,322 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/e4593b9cb9ae4c598b9f92bc9b9cd607, isReference=false, isBulkLoadResult=false, seqid=22103, majorCompaction=false
2014-07-21 01:56:01,331 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/ae6a0da234594e17bbb560d9236c1787, isReference=false, isBulkLoadResult=false, seqid=17158, majorCompaction=false
2014-07-21 01:56:01,366 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/e6515af967b4455fac42e1ff64cc5c8e, isReference=false, isBulkLoadResult=false, seqid=18596, majorCompaction=false
2014-07-21 01:56:01,369 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/ae93c953b3ef4bd1810f6beb2a691df8, isReference=false, isBulkLoadResult=false, seqid=23394, majorCompaction=false
2014-07-21 01:56:01,388 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/e93b20ebfd804cbeaee14fb1dbdd19c5, isReference=false, isBulkLoadResult=false, seqid=22461, majorCompaction=false
2014-07-21 01:56:01,395 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/c9e4a0b4876345348d6ac8199829b25c, isReference=false, isBulkLoadResult=false, seqid=11938, majorCompaction=false
2014-07-21 01:56:01,409 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/de5e5ab98d5a407cadd3560e7f002ef9, isReference=false, isBulkLoadResult=false, seqid=20065, majorCompaction=false
2014-07-21 01:56:01,417 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/deb62655d0da4b32862b95d1a8314404, isReference=false, isBulkLoadResult=false, seqid=10915, majorCompaction=false
2014-07-21 01:56:01,418 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/f9042621b3cb4462a22397f75552c0f9, isReference=false, isBulkLoadResult=false, seqid=9764, majorCompaction=false
2014-07-21 01:56:01,435 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/f9d78e4752864e35ae09bd61ab7f75a1, isReference=false, isBulkLoadResult=false, seqid=14029, majorCompaction=false
2014-07-21 01:56:01,437 DEBUG [StoreOpener-cf5c5ce53bff2a0e0ed070b0f00b8a4b-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b/family/df8679d88e864b7eb0d7d7e14066ea23, isReference=false, isBulkLoadResult=false, seqid=22304, majorCompaction=false
2014-07-21 01:56:01,440 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/cf5c5ce53bff2a0e0ed070b0f00b8a4b
2014-07-21 01:56:01,442 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined cf5c5ce53bff2a0e0ed070b0f00b8a4b; next sequenceid=24075
2014-07-21 01:56:01,442 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node cf5c5ce53bff2a0e0ed070b0f00b8a4b
2014-07-21 01:56:01,444 INFO  [PostOpenDeployTasks:cf5c5ce53bff2a0e0ed070b0f00b8a4b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:01,444 DEBUG [PostOpenDeployTasks:cf5c5ce53bff2a0e0ed070b0f00b8a4b] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-21 01:56:01,452 INFO  [PostOpenDeployTasks:cf5c5ce53bff2a0e0ed070b0f00b8a4b] catalog.MetaEditor: Updated row usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b. with server=slave1,60020,1405932920887
2014-07-21 01:56:01,452 INFO  [PostOpenDeployTasks:cf5c5ce53bff2a0e0ed070b0f00b8a4b] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:01,453 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cf5c5ce53bff2a0e0ed070b0f00b8a4b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:01,457 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cf5c5ce53bff2a0e0ed070b0f00b8a4b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:01,457 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned cf5c5ce53bff2a0e0ed070b0f00b8a4b to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:01,457 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b. on slave1,60020,1405932920887
2014-07-21 01:56:01,458 DEBUG [StoreOpener-139cd3dc941a21a33dc2016906dff923-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923/family/fdd338c8c0c3439d953617a740cce106, isReference=false, isBulkLoadResult=false, seqid=23230, majorCompaction=false
2014-07-21 01:56:01,460 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/139cd3dc941a21a33dc2016906dff923
2014-07-21 01:56:01,463 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 139cd3dc941a21a33dc2016906dff923; next sequenceid=24064
2014-07-21 01:56:01,463 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 139cd3dc941a21a33dc2016906dff923
2014-07-21 01:56:01,465 INFO  [PostOpenDeployTasks:139cd3dc941a21a33dc2016906dff923] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:01,465 DEBUG [PostOpenDeployTasks:139cd3dc941a21a33dc2016906dff923] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-21 01:56:01,472 INFO  [PostOpenDeployTasks:139cd3dc941a21a33dc2016906dff923] catalog.MetaEditor: Updated row usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923. with server=slave1,60020,1405932920887
2014-07-21 01:56:01,472 INFO  [PostOpenDeployTasks:139cd3dc941a21a33dc2016906dff923] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:01,472 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 139cd3dc941a21a33dc2016906dff923 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:01,475 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 139cd3dc941a21a33dc2016906dff923 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 01:56:01,475 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 139cd3dc941a21a33dc2016906dff923 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 01:56:01,475 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923. on slave1,60020,1405932920887
2014-07-21 01:56:04,805 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-21 01:56:04,806 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-21 01:56:04,806 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-21 01:56:04,806 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-21 01:56:04,806 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-21 01:56:27,532 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close 139cd3dc941a21a33dc2016906dff923, via zk=yes, znode version=0, on null
2014-07-21 01:56:27,533 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close 29fb436f66188416368f4fdf09a317d8, via zk=yes, znode version=0, on null
2014-07-21 01:56:27,533 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close ddad2a8ab23bf81d8e3825dc6fd3e50d, via zk=yes, znode version=0, on null
2014-07-21 01:56:27,534 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close d475eaaf0a2bb0a5de8d229390854422, via zk=yes, znode version=0, on null
2014-07-21 01:56:27,534 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close cf5c5ce53bff2a0e0ed070b0f00b8a4b, via zk=yes, znode version=0, on null
2014-07-21 01:56:27,538 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:27,538 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:27,542 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:27,545 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.: disabling compactions & flushes
2014-07-21 01:56:27,545 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.: disabling compactions & flushes
2014-07-21 01:56:27,545 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:27,546 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.: disabling compactions & flushes
2014-07-21 01:56:27,545 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:27,546 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:27,599 INFO  [StoreCloserThread-usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.-1] regionserver.HStore: Closed family
2014-07-21 01:56:27,603 INFO  [StoreCloserThread-usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.-1] regionserver.HStore: Closed family
2014-07-21 01:56:27,603 INFO  [StoreCloserThread-usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.-1] regionserver.HStore: Closed family
2014-07-21 01:56:27,604 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:27,604 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:27,604 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:27,604 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 29fb436f66188416368f4fdf09a317d8 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,604 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cf5c5ce53bff2a0e0ed070b0f00b8a4b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,605 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d475eaaf0a2bb0a5de8d229390854422 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,623 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 29fb436f66188416368f4fdf09a317d8 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,623 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8. on slave1,60020,1405932920887
2014-07-21 01:56:27,623 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8.
2014-07-21 01:56:27,623 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cf5c5ce53bff2a0e0ed070b0f00b8a4b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b. on slave1,60020,1405932920887
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b.
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d475eaaf0a2bb0a5de8d229390854422 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422. on slave1,60020,1405932920887
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422.
2014-07-21 01:56:27,624 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.: disabling compactions & flushes
2014-07-21 01:56:27,625 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:27,625 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.: disabling compactions & flushes
2014-07-21 01:56:27,625 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:27,631 INFO  [StoreCloserThread-usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.-1] regionserver.HStore: Closed family
2014-07-21 01:56:27,632 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:27,632 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 139cd3dc941a21a33dc2016906dff923 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,637 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 139cd3dc941a21a33dc2016906dff923 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,637 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923. on slave1,60020,1405932920887
2014-07-21 01:56:27,637 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923.
2014-07-21 01:56:27,837 INFO  [regionserver60020-smallCompactions-1405932960678] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-21 01:56:27,838 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:27,842 INFO  [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d., storeName=family, fileCount=3, fileSize=338.4m (144.0m, 99.6m, 94.7m), priority=-2, time=29941083483305; duration=27sec
2014-07-21 01:56:27,842 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user4,1405929668902.29fb436f66188416368f4fdf09a317d8. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user1,1405929668901.d475eaaf0a2bb0a5de8d229390854422. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user8,1405929668902.139cd3dc941a21a33dc2016906dff923. because compaction request was cancelled
2014-07-21 01:56:27,843 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405929668902.cf5c5ce53bff2a0e0ed070b0f00b8a4b. because compaction request was cancelled
2014-07-21 01:56:27,849 INFO  [StoreCloserThread-usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.-1] regionserver.HStore: Closed family
2014-07-21 01:56:27,849 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 01:56:27,849 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ddad2a8ab23bf81d8e3825dc6fd3e50d from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,853 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ddad2a8ab23bf81d8e3825dc6fd3e50d from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-21 01:56:27,853 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d. on slave1,60020,1405932920887
2014-07-21 01:56:27,853 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1405929668901.ddad2a8ab23bf81d8e3825dc6fd3e50d.
2014-07-21 02:00:20,995 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:01:38,737 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:01:38,749 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96.
2014-07-21 02:01:38,749 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 45ba1d55e2fff38da3118b96932537db from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,750 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:01:38,750 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 539313b4bf6c7d1c57d1ef833f016c96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,751 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:01:38,751 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Open usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:01:38,751 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9362dc1cf01c1ed9bd2d48a68647947f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,755 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 45ba1d55e2fff38da3118b96932537db from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,755 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 539313b4bf6c7d1c57d1ef833f016c96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,756 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 45ba1d55e2fff38da3118b96932537db, NAME => 'usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-21 02:01:38,756 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 539313b4bf6c7d1c57d1ef833f016c96, NAME => 'usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-21 02:01:38,756 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9362dc1cf01c1ed9bd2d48a68647947f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,757 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 539313b4bf6c7d1c57d1ef833f016c96
2014-07-21 02:01:38,757 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96.
2014-07-21 02:01:38,757 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:01:38,757 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 9362dc1cf01c1ed9bd2d48a68647947f, NAME => 'usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-21 02:01:38,757 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:01:38,759 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9362dc1cf01c1ed9bd2d48a68647947f
2014-07-21 02:01:38,759 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:01:38,769 INFO  [StoreOpener-539313b4bf6c7d1c57d1ef833f016c96-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 02:01:38,770 INFO  [StoreOpener-9362dc1cf01c1ed9bd2d48a68647947f-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 02:01:38,773 INFO  [StoreOpener-45ba1d55e2fff38da3118b96932537db-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 02:01:38,774 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/539313b4bf6c7d1c57d1ef833f016c96
2014-07-21 02:01:38,775 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f
2014-07-21 02:01:38,777 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db
2014-07-21 02:01:38,777 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 539313b4bf6c7d1c57d1ef833f016c96; next sequenceid=1
2014-07-21 02:01:38,778 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 539313b4bf6c7d1c57d1ef833f016c96
2014-07-21 02:01:38,778 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 9362dc1cf01c1ed9bd2d48a68647947f; next sequenceid=1
2014-07-21 02:01:38,778 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9362dc1cf01c1ed9bd2d48a68647947f
2014-07-21 02:01:38,779 INFO  [PostOpenDeployTasks:539313b4bf6c7d1c57d1ef833f016c96] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96.
2014-07-21 02:01:38,780 INFO  [PostOpenDeployTasks:9362dc1cf01c1ed9bd2d48a68647947f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:01:38,790 INFO  [PostOpenDeployTasks:9362dc1cf01c1ed9bd2d48a68647947f] catalog.MetaEditor: Updated row usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. with server=slave1,60020,1405932920887
2014-07-21 02:01:38,790 INFO  [PostOpenDeployTasks:9362dc1cf01c1ed9bd2d48a68647947f] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:01:38,791 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9362dc1cf01c1ed9bd2d48a68647947f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,791 INFO  [PostOpenDeployTasks:539313b4bf6c7d1c57d1ef833f016c96] catalog.MetaEditor: Updated row usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96. with server=slave1,60020,1405932920887
2014-07-21 02:01:38,791 INFO  [PostOpenDeployTasks:539313b4bf6c7d1c57d1ef833f016c96] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96.
2014-07-21 02:01:38,792 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 539313b4bf6c7d1c57d1ef833f016c96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,794 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9362dc1cf01c1ed9bd2d48a68647947f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,794 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 9362dc1cf01c1ed9bd2d48a68647947f to OPENED in zk on slave1,60020,1405932920887
2014-07-21 02:01:38,794 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. on slave1,60020,1405932920887
2014-07-21 02:01:38,795 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b921319eb5e9e7dfd2471db139829ab4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,795 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 539313b4bf6c7d1c57d1ef833f016c96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,795 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 539313b4bf6c7d1c57d1ef833f016c96 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 02:01:38,795 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,,1405933300218.539313b4bf6c7d1c57d1ef833f016c96. on slave1,60020,1405932920887
2014-07-21 02:01:38,796 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b607df7a7904509786241899d2d5239b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,798 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b921319eb5e9e7dfd2471db139829ab4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,798 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => b921319eb5e9e7dfd2471db139829ab4, NAME => 'usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-21 02:01:38,799 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:01:38,799 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:01:38,800 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b607df7a7904509786241899d2d5239b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-21 02:01:38,801 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => b607df7a7904509786241899d2d5239b, NAME => 'usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-21 02:01:38,802 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 45ba1d55e2fff38da3118b96932537db; next sequenceid=1
2014-07-21 02:01:38,802 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:01:38,802 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b607df7a7904509786241899d2d5239b
2014-07-21 02:01:38,802 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:01:38,804 INFO  [PostOpenDeployTasks:45ba1d55e2fff38da3118b96932537db] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:01:38,810 INFO  [StoreOpener-b921319eb5e9e7dfd2471db139829ab4-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 02:01:38,810 INFO  [PostOpenDeployTasks:45ba1d55e2fff38da3118b96932537db] catalog.MetaEditor: Updated row usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. with server=slave1,60020,1405932920887
2014-07-21 02:01:38,811 INFO  [PostOpenDeployTasks:45ba1d55e2fff38da3118b96932537db] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:01:38,812 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 45ba1d55e2fff38da3118b96932537db from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,816 INFO  [StoreOpener-b607df7a7904509786241899d2d5239b-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-21 02:01:38,816 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 45ba1d55e2fff38da3118b96932537db from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,817 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 45ba1d55e2fff38da3118b96932537db to OPENED in zk on slave1,60020,1405932920887
2014-07-21 02:01:38,817 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. on slave1,60020,1405932920887
2014-07-21 02:01:38,817 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:01:38,820 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b
2014-07-21 02:01:38,821 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined b921319eb5e9e7dfd2471db139829ab4; next sequenceid=1
2014-07-21 02:01:38,821 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:01:38,825 INFO  [PostOpenDeployTasks:b921319eb5e9e7dfd2471db139829ab4] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:01:38,832 INFO  [PostOpenDeployTasks:b921319eb5e9e7dfd2471db139829ab4] catalog.MetaEditor: Updated row usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. with server=slave1,60020,1405932920887
2014-07-21 02:01:38,833 INFO  [PostOpenDeployTasks:b921319eb5e9e7dfd2471db139829ab4] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:01:38,833 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b921319eb5e9e7dfd2471db139829ab4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,846 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b921319eb5e9e7dfd2471db139829ab4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,846 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned b921319eb5e9e7dfd2471db139829ab4 to OPENED in zk on slave1,60020,1405932920887
2014-07-21 02:01:38,846 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. on slave1,60020,1405932920887
2014-07-21 02:01:38,859 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined b607df7a7904509786241899d2d5239b; next sequenceid=1
2014-07-21 02:01:38,859 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b607df7a7904509786241899d2d5239b
2014-07-21 02:01:38,861 INFO  [PostOpenDeployTasks:b607df7a7904509786241899d2d5239b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:01:38,866 INFO  [PostOpenDeployTasks:b607df7a7904509786241899d2d5239b] catalog.MetaEditor: Updated row usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. with server=slave1,60020,1405932920887
2014-07-21 02:01:38,866 INFO  [PostOpenDeployTasks:b607df7a7904509786241899d2d5239b] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:01:38,868 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b607df7a7904509786241899d2d5239b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,872 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1475821e3c60000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b607df7a7904509786241899d2d5239b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-21 02:01:38,873 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned b607df7a7904509786241899d2d5239b to OPENED in zk on slave1,60020,1405932920887
2014-07-21 02:01:38,873 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. on slave1,60020,1405932920887
2014-07-21 02:01:56,688 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:01:56,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103 synced till here 73
2014-07-21 02:01:57,285 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405932954691 with entries=103, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933316689
2014-07-21 02:01:58,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:01:58,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 201 synced till here 198
2014-07-21 02:01:58,722 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933316689 with entries=98, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933318541
2014-07-21 02:02:00,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:00,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 295 synced till here 293
2014-07-21 02:02:00,366 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933318541 with entries=94, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933320224
2014-07-21 02:02:01,956 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:01,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 386 synced till here 383
2014-07-21 02:02:02,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933320224 with entries=91, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933321957
2014-07-21 02:02:04,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:04,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 487 synced till here 482
2014-07-21 02:02:04,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933321957 with entries=101, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933324286
2014-07-21 02:02:06,202 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:06,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 628 synced till here 599
2014-07-21 02:02:07,265 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933324286 with entries=141, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933326203
2014-07-21 02:02:08,068 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:02:08,077 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.9m
2014-07-21 02:02:08,226 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:02:08,226 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.0m
2014-07-21 02:02:09,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:09,493 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:02:09,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 768 synced till here 729
2014-07-21 02:02:10,030 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:10,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933326203 with entries=140, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933329486
2014-07-21 02:02:10,154 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:10,162 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-21 02:02:11,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:11,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 916 synced till here 876
2014-07-21 02:02:11,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933329486 with entries=148, filesize=85.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933331410
2014-07-21 02:02:13,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:13,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1030 synced till here 1027
2014-07-21 02:02:13,103 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933331410 with entries=114, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933333010
2014-07-21 02:02:15,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:15,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1115 synced till here 1110
2014-07-21 02:02:15,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933333010 with entries=85, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933335515
2014-07-21 02:02:17,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:18,023 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933335515 with entries=84, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933337860
2014-07-21 02:02:19,543 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=175, memsize=138.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f962d504705a4de38597dcde6f3bb993
2014-07-21 02:02:19,574 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f962d504705a4de38597dcde6f3bb993 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f962d504705a4de38597dcde6f3bb993
2014-07-21 02:02:19,611 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f962d504705a4de38597dcde6f3bb993, entries=502950, sequenceid=175, filesize=35.8m
2014-07-21 02:02:19,612 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~277.3m/290771680, currentsize=168.2m/176376960 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 11386ms, sequenceid=175, compaction requested=false
2014-07-21 02:02:19,616 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 480.3m
2014-07-21 02:02:19,826 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=196, memsize=145.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/51a10a73263f44af879636b5632bfb26
2014-07-21 02:02:19,871 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/51a10a73263f44af879636b5632bfb26 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/51a10a73263f44af879636b5632bfb26
2014-07-21 02:02:19,888 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/51a10a73263f44af879636b5632bfb26, entries=529810, sequenceid=196, filesize=37.7m
2014-07-21 02:02:19,888 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~281.2m/294871600, currentsize=198.4m/208001920 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 11811ms, sequenceid=196, compaction requested=false
2014-07-21 02:02:20,064 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:20,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:20,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1283 synced till here 1280
2014-07-21 02:02:20,512 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933337860 with entries=84, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933340398
2014-07-21 02:02:22,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:23,190 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1385 synced till here 1384
2014-07-21 02:02:23,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933340398 with entries=102, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933342877
2014-07-21 02:02:23,747 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:02:23,748 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.0m
2014-07-21 02:02:24,099 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:24,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:24,993 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:02:25,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1471 synced till here 1465
2014-07-21 02:02:25,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933342877 with entries=86, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933344993
2014-07-21 02:02:27,416 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:27,443 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1556 synced till here 1552
2014-07-21 02:02:27,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933344993 with entries=85, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933347416
2014-07-21 02:02:28,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:29,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1644 synced till here 1641
2014-07-21 02:02:29,983 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933347416 with entries=88, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933348967
2014-07-21 02:02:32,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:32,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1753 synced till here 1728
2014-07-21 02:02:32,711 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933348967 with entries=109, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933352466
2014-07-21 02:02:34,408 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:34,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1862 synced till here 1850
2014-07-21 02:02:34,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933352466 with entries=109, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933354409
2014-07-21 02:02:35,679 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=366, memsize=183.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/af7559a91c104af7bc566a4bab8f2c29
2014-07-21 02:02:35,707 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/af7559a91c104af7bc566a4bab8f2c29 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/af7559a91c104af7bc566a4bab8f2c29
2014-07-21 02:02:35,727 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/af7559a91c104af7bc566a4bab8f2c29, entries=668200, sequenceid=366, filesize=47.6m
2014-07-21 02:02:35,727 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.1m/271659680, currentsize=206.3m/216338080 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 11979ms, sequenceid=366, compaction requested=false
2014-07-21 02:02:35,728 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 432.8m
2014-07-21 02:02:36,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:36,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1973 synced till here 1949
2014-07-21 02:02:36,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933354409 with entries=111, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933356425
2014-07-21 02:02:36,804 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=329, memsize=236.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/cbb2bbdbc8094ed0a00c4987db9788b0
2014-07-21 02:02:36,918 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/cbb2bbdbc8094ed0a00c4987db9788b0 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/cbb2bbdbc8094ed0a00c4987db9788b0
2014-07-21 02:02:36,940 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/cbb2bbdbc8094ed0a00c4987db9788b0, entries=862450, sequenceid=329, filesize=61.4m
2014-07-21 02:02:36,940 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~480.3m/503640960, currentsize=276.1m/289502080 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 17324ms, sequenceid=329, compaction requested=false
2014-07-21 02:02:37,089 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:02:37,090 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 281.4m
2014-07-21 02:02:37,228 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:48,170 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:02:48,198 WARN  [regionserver60020] util.Sleeper: We slept 13279ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:02:48,198 WARN  [RpcServer.reader=5,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-21 02:02:48,243 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10268ms
GC pool 'ParNew' had collection(s): count=3 time=10533ms
2014-07-21 02:02:48,250 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:02:48,476 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12138,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356041,"queuetimems":1,"class":"HRegionServer","responsesize":10948,"method":"Multi"}
2014-07-21 02:02:48,476 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357587,"queuetimems":1,"class":"HRegionServer","responsesize":6721,"method":"Multi"}
2014-07-21 02:02:48,477 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356113,"queuetimems":1,"class":"HRegionServer","responsesize":11226,"method":"Multi"}
2014-07-21 02:02:48,477 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356427,"queuetimems":0,"class":"HRegionServer","responsesize":11290,"method":"Multi"}
2014-07-21 02:02:48,477 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11764,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356406,"queuetimems":0,"class":"HRegionServer","responsesize":9083,"method":"Multi"}
2014-07-21 02:02:48,477 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356465,"queuetimems":1,"class":"HRegionServer","responsesize":12147,"method":"Multi"}
2014-07-21 02:02:48,492 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356762,"queuetimems":0,"class":"HRegionServer","responsesize":11169,"method":"Multi"}
2014-07-21 02:02:48,493 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12573,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933355722,"queuetimems":0,"class":"HRegionServer","responsesize":12156,"method":"Multi"}
2014-07-21 02:02:48,491 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1398 service: ClientService methodName: Multi size: 1.2m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,492 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357587,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-21 02:02:48,491 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356228,"queuetimems":1,"class":"HRegionServer","responsesize":12983,"method":"Multi"}
2014-07-21 02:02:48,493 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10587,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357592,"queuetimems":0,"class":"HRegionServer","responsesize":4807,"method":"Multi"}
2014-07-21 02:02:48,492 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356587,"queuetimems":0,"class":"HRegionServer","responsesize":11973,"method":"Multi"}
2014-07-21 02:02:48,497 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,497 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1372 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1327 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1395 service: ClientService methodName: Multi size: 869.3k connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1396 service: ClientService methodName: Multi size: 1.3k connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,498 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,504 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1338 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1374 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1306 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1375 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1330 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1333 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,505 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,506 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1357 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,506 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:48,712 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356924,"queuetimems":1,"class":"HRegionServer","responsesize":11948,"method":"Multi"}
2014-07-21 02:02:48,714 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1353 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:48,714 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:49,027 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357051,"queuetimems":68,"class":"HRegionServer","responsesize":12066,"method":"Multi"}
2014-07-21 02:02:49,027 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1349 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,027 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,034 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12401,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356632,"queuetimems":0,"class":"HRegionServer","responsesize":13114,"method":"Multi"}
2014-07-21 02:02:49,034 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1363 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,034 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2067 synced till here 2058
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12293,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356806,"queuetimems":1,"class":"HRegionServer","responsesize":12953,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357417,"queuetimems":0,"class":"HRegionServer","responsesize":11660,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1354 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357227,"queuetimems":0,"class":"HRegionServer","responsesize":11894,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357455,"queuetimems":0,"class":"HRegionServer","responsesize":11030,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12139,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933356960,"queuetimems":0,"class":"HRegionServer","responsesize":12266,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11626,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357473,"queuetimems":1,"class":"HRegionServer","responsesize":9053,"method":"Multi"}
2014-07-21 02:02:49,100 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1343 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1399 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1350 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1401 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1347 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,101 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,124 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933356425 with entries=94, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933369026
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357088,"queuetimems":1,"class":"HRegionServer","responsesize":13011,"method":"Multi"}
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11543,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357643,"queuetimems":1,"class":"HRegionServer","responsesize":9112,"method":"Multi"}
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11390,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357795,"queuetimems":1,"class":"HRegionServer","responsesize":8847,"method":"Multi"}
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1348 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357615,"queuetimems":0,"class":"HRegionServer","responsesize":11302,"method":"Multi"}
2014-07-21 02:02:49,186 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1387 service: ClientService methodName: Multi size: 1.5m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1390 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1392 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,187 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,189 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357257,"queuetimems":0,"class":"HRegionServer","responsesize":12231,"method":"Multi"}
2014-07-21 02:02:49,189 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1346 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,189 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,233 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357282,"queuetimems":0,"class":"HRegionServer","responsesize":13014,"method":"Multi"}
2014-07-21 02:02:49,234 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1345 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,234 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:49,239 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53754","starttimems":1405933357829,"queuetimems":1,"class":"HRegionServer","responsesize":11884,"method":"Multi"}
2014-07-21 02:02:49,239 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1386 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53754: output error
2014-07-21 02:02:49,239 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:02:56,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:02:56,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2182 synced till here 2151
2014-07-21 02:02:58,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933369026 with entries=115, filesize=91.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933376720
2014-07-21 02:02:59,931 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:03:01,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:02,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2294 synced till here 2283
2014-07-21 02:03:02,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933376720 with entries=112, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933381872
2014-07-21 02:03:06,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:07,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2434 synced till here 2403
2014-07-21 02:03:09,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933381872 with entries=140, filesize=96.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933386785
2014-07-21 02:03:10,802 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10838,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933379964,"queuetimems":979,"class":"HRegionServer","responsesize":11884,"method":"Multi"}
2014-07-21 02:03:12,093 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:12,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2579 synced till here 2545
2014-07-21 02:03:12,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933386785 with entries=145, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933392093
2014-07-21 02:03:16,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:16,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2703 synced till here 2676
2014-07-21 02:03:17,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933392093 with entries=124, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933396678
2014-07-21 02:03:19,605 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=499, memsize=175.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/f79c881b5b37402e90cb03a3380eddc0
2014-07-21 02:03:19,623 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/f79c881b5b37402e90cb03a3380eddc0 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/f79c881b5b37402e90cb03a3380eddc0
2014-07-21 02:03:19,654 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/f79c881b5b37402e90cb03a3380eddc0, entries=639440, sequenceid=499, filesize=45.6m
2014-07-21 02:03:19,838 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~291.7m/305883040, currentsize=360.3m/377848080 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 42749ms, sequenceid=499, compaction requested=false
2014-07-21 02:03:19,839 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 614.1m
2014-07-21 02:03:19,882 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:03:22,853 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1264ms
GC pool 'ParNew' had collection(s): count=1 time=1532ms
2014-07-21 02:03:23,498 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:24,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2838 synced till here 2816
2014-07-21 02:03:24,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933396678 with entries=135, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933403499
2014-07-21 02:03:25,802 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:03:30,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:30,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2955 synced till here 2941
2014-07-21 02:03:31,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933403499 with entries=117, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933410124
2014-07-21 02:03:35,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:35,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933410124 with entries=100, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933415881
2014-07-21 02:03:40,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:40,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3142 synced till here 3137
2014-07-21 02:03:41,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933415881 with entries=87, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933420777
2014-07-21 02:03:44,544 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:44,741 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3245 synced till here 3231
2014-07-21 02:03:44,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933420777 with entries=103, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933424545
2014-07-21 02:03:45,341 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=498, memsize=312.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/514be4168d5d47ce8c2827e38160ce55
2014-07-21 02:03:45,375 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/514be4168d5d47ce8c2827e38160ce55 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/514be4168d5d47ce8c2827e38160ce55
2014-07-21 02:03:45,390 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/514be4168d5d47ce8c2827e38160ce55, entries=1137240, sequenceid=498, filesize=80.9m
2014-07-21 02:03:45,390 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~440.3m/461703520, currentsize=257.0m/269436880 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 69662ms, sequenceid=498, compaction requested=false
2014-07-21 02:03:45,391 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 407.8m
2014-07-21 02:03:45,403 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:03:46,748 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:03:47,380 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:47,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933424545 with entries=103, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933427380
2014-07-21 02:03:51,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:03:51,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3480 synced till here 3477
2014-07-21 02:03:51,203 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933427380 with entries=132, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933431097
2014-07-21 02:03:58,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:00,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3633 synced till here 3602
2014-07-21 02:04:01,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933431097 with entries=153, filesize=118.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933438679
2014-07-21 02:04:05,055 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10238,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933434817,"queuetimems":8,"class":"HRegionServer","responsesize":13139,"method":"Multi"}
2014-07-21 02:04:06,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:06,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3767 synced till here 3738
2014-07-21 02:04:07,050 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=740, memsize=167.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/b5532a7104ce493f92513f1662f2f46f
2014-07-21 02:04:07,589 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/b5532a7104ce493f92513f1662f2f46f as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/b5532a7104ce493f92513f1662f2f46f
2014-07-21 02:04:07,647 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/b5532a7104ce493f92513f1662f2f46f, entries=610550, sequenceid=740, filesize=43.5m
2014-07-21 02:04:07,648 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~642.2m/673399520, currentsize=401.7m/421172800 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 47809ms, sequenceid=740, compaction requested=true
2014-07-21 02:04:07,853 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:04:07,853 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-21 02:04:07,854 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-21 02:04:07,858 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:04:07,858 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:04:07,858 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:04:07,902 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 780.9m
2014-07-21 02:04:07,932 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:04:07,946 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933437005,"queuetimems":0,"class":"HRegionServer","responsesize":13140,"method":"Multi"}
2014-07-21 02:04:07,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933438679 with entries=134, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933446014
2014-07-21 02:04:11,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:11,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3903 synced till here 3866
2014-07-21 02:04:11,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933446014 with entries=136, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933451211
2014-07-21 02:04:12,187 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:04:15,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:15,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4007 synced till here 4002
2014-07-21 02:04:15,279 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=844, memsize=144.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/10b6a3360a3b460f94a8a9f5905006cb
2014-07-21 02:04:15,313 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/10b6a3360a3b460f94a8a9f5905006cb as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/10b6a3360a3b460f94a8a9f5905006cb
2014-07-21 02:04:15,334 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933451211 with entries=104, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933455241
2014-07-21 02:04:15,334 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/10b6a3360a3b460f94a8a9f5905006cb, entries=525590, sequenceid=844, filesize=37.4m
2014-07-21 02:04:15,334 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~410.9m/430904720, currentsize=88.3m/92595760 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 29943ms, sequenceid=844, compaction requested=false
2014-07-21 02:04:15,335 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 411.5m
2014-07-21 02:04:16,263 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:04:21,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:21,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4118 synced till here 4100
2014-07-21 02:04:22,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933455241 with entries=111, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933461014
2014-07-21 02:04:22,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405932954691
2014-07-21 02:04:22,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933316689
2014-07-21 02:04:22,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933318541
2014-07-21 02:04:22,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933320224
2014-07-21 02:04:22,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933321957
2014-07-21 02:04:22,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933324286
2014-07-21 02:04:22,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933326203
2014-07-21 02:04:22,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933329486
2014-07-21 02:04:22,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933331410
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933333010
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933335515
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933337860
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933340398
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933342877
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933344993
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933347416
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933348967
2014-07-21 02:04:22,562 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933352466
2014-07-21 02:04:25,965 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:26,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4240 synced till here 4236
2014-07-21 02:04:26,102 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933461014 with entries=122, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933465965
2014-07-21 02:04:30,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:30,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4365 synced till here 4364
2014-07-21 02:04:30,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933465965 with entries=125, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933470229
2014-07-21 02:04:33,362 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:33,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4456 synced till here 4450
2014-07-21 02:04:33,651 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933470229 with entries=91, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933473362
2014-07-21 02:04:36,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:36,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4562 synced till here 4549
2014-07-21 02:04:36,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933473362 with entries=106, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933476235
2014-07-21 02:04:38,748 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=931, memsize=169.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/70e2bf431b134aa2abf59370ced03e56
2014-07-21 02:04:38,773 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/70e2bf431b134aa2abf59370ced03e56 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/70e2bf431b134aa2abf59370ced03e56
2014-07-21 02:04:38,885 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/70e2bf431b134aa2abf59370ced03e56, entries=616960, sequenceid=931, filesize=44.0m
2014-07-21 02:04:38,885 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~414.7m/434805520, currentsize=168.2m/176368880 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 23550ms, sequenceid=931, compaction requested=true
2014-07-21 02:04:38,891 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:04:38,891 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-21 02:04:38,891 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-21 02:04:38,891 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 738.9m
2014-07-21 02:04:38,891 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:04:38,891 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:04:38,891 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:04:39,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:39,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4681 synced till here 4658
2014-07-21 02:04:39,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933476235 with entries=119, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933479024
2014-07-21 02:04:39,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933354409
2014-07-21 02:04:40,969 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:04:41,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:41,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4802 synced till here 4793
2014-07-21 02:04:41,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933479024 with entries=121, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933481119
2014-07-21 02:04:43,530 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:43,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4909 synced till here 4896
2014-07-21 02:04:43,732 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=975, memsize=190.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/3444fd4ee3cc47f993812a567be4196d
2014-07-21 02:04:43,771 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/3444fd4ee3cc47f993812a567be4196d as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/3444fd4ee3cc47f993812a567be4196d
2014-07-21 02:04:43,924 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933481119 with entries=107, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933483530
2014-07-21 02:04:43,937 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/3444fd4ee3cc47f993812a567be4196d, entries=694390, sequenceid=975, filesize=49.5m
2014-07-21 02:04:43,947 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~806.2m/845322320, currentsize=429.8m/450728880 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 36036ms, sequenceid=975, compaction requested=true
2014-07-21 02:04:43,953 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:04:43,953 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-21 02:04:43,953 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-21 02:04:43,953 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:04:43,953 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:04:43,954 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:04:44,135 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:04:44,138 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 433.3m
2014-07-21 02:04:45,525 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:04:46,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:46,553 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:04:49,248 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5110 synced till here 5090
2014-07-21 02:04:50,132 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933483530 with entries=201, filesize=119.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933486298
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933356425
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933369026
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933376720
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933381872
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933386785
2014-07-21 02:04:50,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933392093
2014-07-21 02:04:52,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:52,644 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5236 synced till here 5218
2014-07-21 02:04:53,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933486298 with entries=126, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933492609
2014-07-21 02:04:55,815 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:55,838 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5335 synced till here 5330
2014-07-21 02:04:56,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933492609 with entries=99, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933495816
2014-07-21 02:04:58,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:04:58,876 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5444 synced till here 5424
2014-07-21 02:04:59,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933495816 with entries=109, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933498828
2014-07-21 02:05:00,328 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:05:02,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:02,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5587 synced till here 5551
2014-07-21 02:05:03,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933498828 with entries=143, filesize=97.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933502295
2014-07-21 02:05:06,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:06,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5718 synced till here 5668
2014-07-21 02:05:08,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933502295 with entries=131, filesize=93.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933506108
2014-07-21 02:05:10,364 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:10,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5874 synced till here 5843
2014-07-21 02:05:11,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933506108 with entries=156, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933510365
2014-07-21 02:05:12,801 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1241, memsize=220.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/ab4c57397be446e5a75be8d428e58d14
2014-07-21 02:05:12,820 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/ab4c57397be446e5a75be8d428e58d14 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/ab4c57397be446e5a75be8d428e58d14
2014-07-21 02:05:12,951 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/ab4c57397be446e5a75be8d428e58d14, entries=801790, sequenceid=1241, filesize=57.1m
2014-07-21 02:05:12,978 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~438.9m/460232720, currentsize=425.2m/445818480 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 28840ms, sequenceid=1241, compaction requested=true
2014-07-21 02:05:12,984 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:05:12,984 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-21 02:05:12,984 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-21 02:05:12,984 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:05:12,984 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 398.7m
2014-07-21 02:05:12,984 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:05:12,984 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:05:13,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:13,662 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:05:13,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6015 synced till here 5990
2014-07-21 02:05:14,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933510365 with entries=141, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933513571
2014-07-21 02:05:14,752 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:05:16,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:16,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6153 synced till here 6123
2014-07-21 02:05:17,036 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933513571 with entries=138, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933516158
2014-07-21 02:05:19,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:19,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6294 synced till here 6272
2014-07-21 02:05:20,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933516158 with entries=141, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933519161
2014-07-21 02:05:21,187 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:05:21,660 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1184, memsize=305.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/874054ad8c8e4ba9bcf791e3eb7b5eba
2014-07-21 02:05:21,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:21,848 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/874054ad8c8e4ba9bcf791e3eb7b5eba as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/874054ad8c8e4ba9bcf791e3eb7b5eba
2014-07-21 02:05:21,871 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/874054ad8c8e4ba9bcf791e3eb7b5eba, entries=1112360, sequenceid=1184, filesize=79.2m
2014-07-21 02:05:21,879 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~746.8m/783127120, currentsize=721.1m/756080400 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 42988ms, sequenceid=1184, compaction requested=true
2014-07-21 02:05:21,880 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-21 02:05:21,881 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-21 02:05:21,881 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:05:21,881 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:05:21,881 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:05:21,881 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:05:21,881 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 379.2m
2014-07-21 02:05:21,885 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6424 synced till here 6399
2014-07-21 02:05:21,960 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:05:22,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933519161 with entries=130, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933521848
2014-07-21 02:05:22,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933396678
2014-07-21 02:05:22,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933403499
2014-07-21 02:05:22,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933410124
2014-07-21 02:05:22,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933415881
2014-07-21 02:05:22,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933420777
2014-07-21 02:05:23,562 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:05:23,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:24,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6546 synced till here 6515
2014-07-21 02:05:24,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933521848 with entries=122, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933523981
2014-07-21 02:05:28,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:29,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6689 synced till here 6677
2014-07-21 02:05:29,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933523981 with entries=143, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933528825
2014-07-21 02:05:32,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:32,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6810 synced till here 6796
2014-07-21 02:05:33,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933528825 with entries=121, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933532860
2014-07-21 02:05:36,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:36,981 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933532860 with entries=98, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933536958
2014-07-21 02:05:43,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:43,189 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7030 synced till here 7017
2014-07-21 02:05:43,790 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933536958 with entries=122, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933543123
2014-07-21 02:05:46,050 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1637, memsize=127.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4b5eb17648414f8c9756df4b8e57aa44
2014-07-21 02:05:46,069 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4b5eb17648414f8c9756df4b8e57aa44 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4b5eb17648414f8c9756df4b8e57aa44
2014-07-21 02:05:46,080 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4b5eb17648414f8c9756df4b8e57aa44, entries=465520, sequenceid=1637, filesize=33.2m
2014-07-21 02:05:46,080 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~382.3m/400818400, currentsize=89.1m/93474960 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 24199ms, sequenceid=1637, compaction requested=false
2014-07-21 02:05:46,081 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 918.8m
2014-07-21 02:05:46,660 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:46,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933543123 with entries=92, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933546660
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933424545
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933427380
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933431097
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933438679
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933446014
2014-07-21 02:05:46,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933451211
2014-07-21 02:05:47,219 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:05:47,231 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1462, memsize=266.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/2749062d72294b7f8377e606b3e29dc7
2014-07-21 02:05:47,245 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/2749062d72294b7f8377e606b3e29dc7 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/2749062d72294b7f8377e606b3e29dc7
2014-07-21 02:05:47,258 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/2749062d72294b7f8377e606b3e29dc7, entries=969210, sequenceid=1462, filesize=69.0m
2014-07-21 02:05:47,259 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~407.0m/426748720, currentsize=144.8m/151850480 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 34275ms, sequenceid=1462, compaction requested=true
2014-07-21 02:05:47,259 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:05:47,259 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-21 02:05:47,260 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-21 02:05:47,260 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1016.8m
2014-07-21 02:05:47,260 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:05:47,260 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:05:47,260 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:05:48,802 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:05:50,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:50,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7235 synced till here 7228
2014-07-21 02:05:50,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933546660 with entries=113, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933550198
2014-07-21 02:05:50,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933455241
2014-07-21 02:05:50,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933461014
2014-07-21 02:05:50,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933465965
2014-07-21 02:05:50,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933470229
2014-07-21 02:05:50,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933473362
2014-07-21 02:05:53,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:53,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7336 synced till here 7334
2014-07-21 02:05:53,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933550198 with entries=101, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933553431
2014-07-21 02:05:56,515 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:05:56,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7446 synced till here 7428
2014-07-21 02:05:57,081 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933553431 with entries=110, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933556515
2014-07-21 02:06:00,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:00,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7560 synced till here 7533
2014-07-21 02:06:01,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933556515 with entries=114, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933560242
2014-07-21 02:06:01,662 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:06:04,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:04,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7692 synced till here 7669
2014-07-21 02:06:04,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933560242 with entries=132, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933564332
2014-07-21 02:06:06,679 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:08,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7837 synced till here 7831
2014-07-21 02:06:08,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933564332 with entries=145, filesize=97.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933566679
2014-07-21 02:06:11,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:11,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7943 synced till here 7941
2014-07-21 02:06:11,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933566679 with entries=106, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933571034
2014-07-21 02:06:43,790 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 34177ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:06:43,790 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 34177ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:06:43,792 WARN  [regionserver60020] util.Sleeper: We slept 32535ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:06:43,792 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 30545ms
GC pool 'ParNew' had collection(s): count=2 time=166ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=30772ms
2014-07-21 02:06:43,886 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933571681,"queuetimems":0,"class":"HRegionServer","responsesize":12005,"method":"Multi"}
2014-07-21 02:06:43,886 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5793 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:43,888 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572992,"queuetimems":0,"class":"HRegionServer","responsesize":261,"method":"Multi"}
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572992,"queuetimems":0,"class":"HRegionServer","responsesize":183,"method":"Multi"}
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5852 service: ClientService methodName: Multi size: 53.7k connection: 9.1.143.53:53755: output error
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5850 service: ClientService methodName: Multi size: 37.5k connection: 9.1.143.53:53755: output error
2014-07-21 02:06:43,890 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,035 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933573011,"queuetimems":1,"class":"HRegionServer","responsesize":3926,"method":"Multi"}
2014-07-21 02:06:44,035 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572263,"queuetimems":1,"class":"HRegionServer","responsesize":11961,"method":"Multi"}
2014-07-21 02:06:44,035 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32330,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933571704,"queuetimems":1,"class":"HRegionServer","responsesize":12275,"method":"Multi"}
2014-07-21 02:06:44,036 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5849 service: ClientService methodName: Multi size: 711.9k connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,036 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,036 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5817 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,036 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,036 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5797 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,037 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32080,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572018,"queuetimems":0,"class":"HRegionServer","responsesize":11689,"method":"Multi"}
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32118,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933571980,"queuetimems":0,"class":"HRegionServer","responsesize":9149,"method":"Multi"}
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5843 service: ClientService methodName: Multi size: 710.7k connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5807 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5809 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,099 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,100 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5844 service: ClientService methodName: Multi size: 736.9k connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,100 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,187 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572334,"queuetimems":1,"class":"HRegionServer","responsesize":12048,"method":"Multi"}
2014-07-21 02:06:44,188 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5820 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,188 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:44,379 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31696,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572682,"queuetimems":3,"class":"HRegionServer","responsesize":11722,"method":"Multi"}
2014-07-21 02:06:44,379 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31386,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572992,"queuetimems":0,"class":"HRegionServer","responsesize":12013,"method":"Multi"}
2014-07-21 02:06:44,379 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572713,"queuetimems":0,"class":"HRegionServer","responsesize":11913,"method":"Multi"}
2014-07-21 02:06:44,380 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5819 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,380 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,380 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5829 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,380 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,382 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5825 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,382 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,385 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572739,"queuetimems":1,"class":"HRegionServer","responsesize":12156,"method":"Multi"}
2014-07-21 02:06:44,386 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5835 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,386 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,467 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31513,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53755","starttimems":1405933572953,"queuetimems":0,"class":"HRegionServer","responsesize":12238,"method":"Multi"}
2014-07-21 02:06:44,467 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5831 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,467 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,582 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 5846 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:53755: output error
2014-07-21 02:06:44,582 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:06:44,754 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8042 synced till here 8041
2014-07-21 02:06:44,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933571034 with entries=99, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933604281
2014-07-21 02:06:45,535 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1844, memsize=141.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/ddc9881e942f44248cd135ec8c484448
2014-07-21 02:06:45,557 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/ddc9881e942f44248cd135ec8c484448 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/ddc9881e942f44248cd135ec8c484448
2014-07-21 02:06:45,571 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/ddc9881e942f44248cd135ec8c484448, entries=516780, sequenceid=1844, filesize=36.8m
2014-07-21 02:06:45,571 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~918.8m/963438960, currentsize=404.8m/424487760 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 59490ms, sequenceid=1844, compaction requested=true
2014-07-21 02:06:45,572 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:06:45,572 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-21 02:06:45,572 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-21 02:06:45,572 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:06:45,573 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 325.9m
2014-07-21 02:06:45,573 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:06:45,573 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:06:45,915 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:06:45,955 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:06:49,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:50,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933604281 with entries=103, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933609985
2014-07-21 02:06:52,123 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1844, memsize=216.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8c19a86dd48d4d9d8b516ed0da75610a
2014-07-21 02:06:52,141 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8c19a86dd48d4d9d8b516ed0da75610a as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8c19a86dd48d4d9d8b516ed0da75610a
2014-07-21 02:06:52,174 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8c19a86dd48d4d9d8b516ed0da75610a, entries=787950, sequenceid=1844, filesize=56.2m
2014-07-21 02:06:52,174 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1020.3m/1069901200, currentsize=468.0m/490782960 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 64914ms, sequenceid=1844, compaction requested=true
2014-07-21 02:06:52,175 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:06:52,175 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 471.2m
2014-07-21 02:06:52,175 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-21 02:06:52,175 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-21 02:06:52,175 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:06:52,175 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:06:52,175 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:06:52,179 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:06:52,311 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:52,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8238 synced till here 8234
2014-07-21 02:06:52,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933609985 with entries=93, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933612312
2014-07-21 02:06:52,388 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933476235
2014-07-21 02:06:52,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933479024
2014-07-21 02:06:52,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933481119
2014-07-21 02:06:52,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933483530
2014-07-21 02:06:52,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933486298
2014-07-21 02:06:52,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933492609
2014-07-21 02:06:52,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933495816
2014-07-21 02:06:52,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933498828
2014-07-21 02:06:52,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933502295
2014-07-21 02:06:52,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933506108
2014-07-21 02:06:52,622 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:06:54,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:54,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933612312 with entries=84, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933614372
2014-07-21 02:06:55,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:55,668 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:06:55,694 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8414 synced till here 8409
2014-07-21 02:06:55,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933614372 with entries=92, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933615668
2014-07-21 02:06:57,096 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1842, memsize=183.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1d13fa8cd4434c42b44e174f8986b43a
2014-07-21 02:06:57,117 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1d13fa8cd4434c42b44e174f8986b43a as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1d13fa8cd4434c42b44e174f8986b43a
2014-07-21 02:06:57,135 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1d13fa8cd4434c42b44e174f8986b43a, entries=668590, sequenceid=1842, filesize=47.7m
2014-07-21 02:06:57,136 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~325.9m/341704160, currentsize=134.0m/140539680 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 11563ms, sequenceid=1842, compaction requested=true
2014-07-21 02:06:57,136 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:06:57,136 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-21 02:06:57,136 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-21 02:06:57,136 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:06:57,136 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 577.3m
2014-07-21 02:06:57,136 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:06:57,137 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:06:57,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:57,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8503 synced till here 8501
2014-07-21 02:06:57,493 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933615668 with entries=89, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933617440
2014-07-21 02:06:57,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933510365
2014-07-21 02:06:57,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933513571
2014-07-21 02:06:57,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933516158
2014-07-21 02:06:57,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933519161
2014-07-21 02:06:57,848 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:06:59,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:06:59,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8591 synced till here 8589
2014-07-21 02:06:59,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933617440 with entries=88, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933619739
2014-07-21 02:07:03,896 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:03,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8690 synced till here 8689
2014-07-21 02:07:03,938 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933619739 with entries=99, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933623897
2014-07-21 02:07:04,753 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2114, memsize=248.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/1d29c459ee1a4f6e9f115af4750a9bae
2014-07-21 02:07:04,774 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/1d29c459ee1a4f6e9f115af4750a9bae as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/1d29c459ee1a4f6e9f115af4750a9bae
2014-07-21 02:07:04,788 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/1d29c459ee1a4f6e9f115af4750a9bae, entries=904190, sequenceid=2114, filesize=64.4m
2014-07-21 02:07:04,788 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~473.1m/496071280, currentsize=191.0m/200294560 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 12613ms, sequenceid=2114, compaction requested=true
2014-07-21 02:07:04,789 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:04,789 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-21 02:07:04,789 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 289.7m
2014-07-21 02:07:04,789 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-21 02:07:04,790 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:04,790 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:04,790 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:07:05,050 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:07,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:07,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933623897 with entries=110, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933627135
2014-07-21 02:07:08,137 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:07:08,391 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:07:08,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:08,475 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8890 synced till here 8888
2014-07-21 02:07:08,750 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933627135 with entries=90, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933628452
2014-07-21 02:07:11,149 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2217, memsize=128.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/ed705549a9f84b80ad3f1330534f0b1c
2014-07-21 02:07:11,166 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/ed705549a9f84b80ad3f1330534f0b1c as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/ed705549a9f84b80ad3f1330534f0b1c
2014-07-21 02:07:11,190 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/ed705549a9f84b80ad3f1330534f0b1c, entries=465970, sequenceid=2217, filesize=33.2m
2014-07-21 02:07:11,191 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~289.7m/303764560, currentsize=30.8m/32348080 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 6402ms, sequenceid=2217, compaction requested=true
2014-07-21 02:07:11,191 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:11,191 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-21 02:07:11,191 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-21 02:07:11,191 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:11,191 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:11,191 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 300.1m
2014-07-21 02:07:11,191 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:07:11,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:11,247 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933628452 with entries=93, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933631219
2014-07-21 02:07:11,248 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933521848
2014-07-21 02:07:11,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933523981
2014-07-21 02:07:11,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933528825
2014-07-21 02:07:11,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933532860
2014-07-21 02:07:11,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933536958
2014-07-21 02:07:11,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933543123
2014-07-21 02:07:11,371 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:12,073 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2171, memsize=335.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/4d43e2f629304ec09ce5f76ce01cca4b
2014-07-21 02:07:12,096 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/4d43e2f629304ec09ce5f76ce01cca4b as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/4d43e2f629304ec09ce5f76ce01cca4b
2014-07-21 02:07:12,115 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/4d43e2f629304ec09ce5f76ce01cca4b, entries=1221080, sequenceid=2171, filesize=87.0m
2014-07-21 02:07:12,115 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~581.2m/609413280, currentsize=201.2m/210923120 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 14979ms, sequenceid=2171, compaction requested=true
2014-07-21 02:07:12,116 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:12,116 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-21 02:07:12,116 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-21 02:07:12,116 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:12,116 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 315.3m
2014-07-21 02:07:12,116 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:12,116 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:07:12,283 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:16,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:16,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933631219 with entries=99, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933636562
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933546660
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933550198
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933553431
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933556515
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933560242
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933564332
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933566679
2014-07-21 02:07:16,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933571034
2014-07-21 02:07:20,413 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2175, memsize=298.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f262b8cdd05d45c4bb208a7c5e5a1124
2014-07-21 02:07:20,433 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f262b8cdd05d45c4bb208a7c5e5a1124 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f262b8cdd05d45c4bb208a7c5e5a1124
2014-07-21 02:07:20,445 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f262b8cdd05d45c4bb208a7c5e5a1124, entries=1086580, sequenceid=2175, filesize=77.4m
2014-07-21 02:07:20,445 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~300.1m/314706480, currentsize=61.0m/64003200 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 9254ms, sequenceid=2175, compaction requested=true
2014-07-21 02:07:20,446 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:20,446 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-21 02:07:20,446 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-21 02:07:20,446 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:20,446 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:20,446 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:07:21,665 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2291, memsize=308.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/82c48055527a4ba18d4bef0fb1707fcd
2014-07-21 02:07:21,683 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/82c48055527a4ba18d4bef0fb1707fcd as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/82c48055527a4ba18d4bef0fb1707fcd
2014-07-21 02:07:21,694 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/82c48055527a4ba18d4bef0fb1707fcd, entries=1122270, sequenceid=2291, filesize=79.9m
2014-07-21 02:07:21,694 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~315.3m/330613600, currentsize=39.1m/41033040 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 9578ms, sequenceid=2291, compaction requested=true
2014-07-21 02:07:21,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-21 02:07:21,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-21 02:07:21,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:21,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:21,695 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:07:21,695 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:22,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:22,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9190 synced till here 9186
2014-07-21 02:07:22,081 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933636562 with entries=108, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933642045
2014-07-21 02:07:22,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933604281
2014-07-21 02:07:22,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933609985
2014-07-21 02:07:22,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933612312
2014-07-21 02:07:22,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933614372
2014-07-21 02:07:22,130 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:07:22,130 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 256.8m
2014-07-21 02:07:22,591 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:23,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:23,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9273 synced till here 9272
2014-07-21 02:07:23,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933642045 with entries=83, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933643200
2014-07-21 02:07:26,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:26,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9373 synced till here 9372
2014-07-21 02:07:26,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933643200 with entries=100, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933646244
2014-07-21 02:07:29,858 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1587ms
GC pool 'ParNew' had collection(s): count=1 time=1611ms
2014-07-21 02:07:30,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:30,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9472 synced till here 9466
2014-07-21 02:07:30,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933646244 with entries=99, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933650199
2014-07-21 02:07:32,157 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2319, memsize=258.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/572186ed3cac40f58815344571025c81
2014-07-21 02:07:32,322 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/572186ed3cac40f58815344571025c81 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/572186ed3cac40f58815344571025c81
2014-07-21 02:07:32,336 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/572186ed3cac40f58815344571025c81, entries=941720, sequenceid=2319, filesize=67.1m
2014-07-21 02:07:32,337 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.6m/271208080, currentsize=115.0m/120576240 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 10207ms, sequenceid=2319, compaction requested=true
2014-07-21 02:07:32,338 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:32,338 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-21 02:07:32,338 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-21 02:07:32,338 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:32,338 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:32,338 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:07:33,630 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:33,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9580 synced till here 9573
2014-07-21 02:07:33,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933650199 with entries=108, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933653631
2014-07-21 02:07:33,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933615668
2014-07-21 02:07:33,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933617440
2014-07-21 02:07:33,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933619739
2014-07-21 02:07:36,328 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:36,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9670 synced till here 9669
2014-07-21 02:07:36,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933653631 with entries=90, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933656328
2014-07-21 02:07:37,340 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:07:37,341 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 257.5m
2014-07-21 02:07:37,684 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:37,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:37,711 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:07:37,712 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 262.4m
2014-07-21 02:07:37,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9763 synced till here 9754
2014-07-21 02:07:37,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933656328 with entries=93, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933657711
2014-07-21 02:07:38,009 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:40,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:40,292 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9860 synced till here 9857
2014-07-21 02:07:40,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933657711 with entries=97, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933660259
2014-07-21 02:07:40,382 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:07:42,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:42,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933660259 with entries=91, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933662418
2014-07-21 02:07:46,954 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2439, memsize=258.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/06d6832970bd45258dc4910210a23a8f
2014-07-21 02:07:46,981 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/06d6832970bd45258dc4910210a23a8f as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/06d6832970bd45258dc4910210a23a8f
2014-07-21 02:07:47,003 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/06d6832970bd45258dc4910210a23a8f, entries=940940, sequenceid=2439, filesize=67.0m
2014-07-21 02:07:47,004 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.8m/280800720, currentsize=81.4m/85303600 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 9292ms, sequenceid=2439, compaction requested=true
2014-07-21 02:07:47,004 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:47,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-21 02:07:47,004 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 292.3m
2014-07-21 02:07:47,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-21 02:07:47,005 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:47,005 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:47,005 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:07:47,131 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:07:47,276 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2475, memsize=269.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/41d1a144e11c45c3b053db545cc55e77
2014-07-21 02:07:47,288 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/41d1a144e11c45c3b053db545cc55e77 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/41d1a144e11c45c3b053db545cc55e77
2014-07-21 02:07:47,297 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/41d1a144e11c45c3b053db545cc55e77, entries=979820, sequenceid=2475, filesize=69.8m
2014-07-21 02:07:47,297 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~271.8m/285007440, currentsize=97.1m/101822560 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 9956ms, sequenceid=2475, compaction requested=true
2014-07-21 02:07:47,297 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:47,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-21 02:07:47,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-21 02:07:47,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:47,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:47,298 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:07:49,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:49,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933662418 with entries=96, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933669802
2014-07-21 02:07:52,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:52,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10144 synced till here 10141
2014-07-21 02:07:52,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933669802 with entries=97, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933672336
2014-07-21 02:07:55,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:07:55,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10241 synced till here 10239
2014-07-21 02:07:55,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933672336 with entries=97, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933675148
2014-07-21 02:07:57,196 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2490, memsize=283.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8b9d353b4fb1459abba2d4e7ed6a89e3
2014-07-21 02:07:57,215 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8b9d353b4fb1459abba2d4e7ed6a89e3 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8b9d353b4fb1459abba2d4e7ed6a89e3
2014-07-21 02:07:57,229 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8b9d353b4fb1459abba2d4e7ed6a89e3, entries=1030770, sequenceid=2490, filesize=73.4m
2014-07-21 02:07:57,230 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~292.3m/306500160, currentsize=100.5m/105367840 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 10226ms, sequenceid=2490, compaction requested=true
2014-07-21 02:07:57,230 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:07:57,230 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-21 02:07:57,230 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-21 02:07:57,230 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:07:57,230 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:07:57,231 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:08:01,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:01,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933675148 with entries=106, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933681740
2014-07-21 02:08:25,691 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:25,769 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:08:25,769 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 257.2m
2014-07-21 02:08:25,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10458 synced till here 10454
2014-07-21 02:08:25,965 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:26,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933681740 with entries=111, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933705692
2014-07-21 02:08:26,838 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:08:26,838 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 257.6m
2014-07-21 02:08:26,973 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:29,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:29,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933705692 with entries=143, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933709241
2014-07-21 02:08:33,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:33,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933709241 with entries=84, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933713627
2014-07-21 02:08:34,454 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:08:35,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:35,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10787 synced till here 10783
2014-07-21 02:08:35,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933713627 with entries=102, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933715615
2014-07-21 02:08:36,550 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2748, memsize=263.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/6148a82b396747f2a79d41e08d2c6d0a
2014-07-21 02:08:36,586 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/6148a82b396747f2a79d41e08d2c6d0a as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/6148a82b396747f2a79d41e08d2c6d0a
2014-07-21 02:08:36,651 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/6148a82b396747f2a79d41e08d2c6d0a, entries=960760, sequenceid=2748, filesize=68.4m
2014-07-21 02:08:36,651 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~263.9m/276691280, currentsize=120.2m/126048480 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 10882ms, sequenceid=2748, compaction requested=true
2014-07-21 02:08:36,652 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:08:36,652 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-21 02:08:36,652 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-21 02:08:36,652 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:08:36,652 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:08:36,652 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:08:36,652 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 297.9m
2014-07-21 02:08:37,506 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:37,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:37,544 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10893 synced till here 10886
2014-07-21 02:08:37,607 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933715615 with entries=106, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933717527
2014-07-21 02:08:38,400 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2592, memsize=257.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/49ece300e75645a3be559d2c6b646017
2014-07-21 02:08:38,418 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/49ece300e75645a3be559d2c6b646017 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/49ece300e75645a3be559d2c6b646017
2014-07-21 02:08:38,430 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/49ece300e75645a3be559d2c6b646017, entries=938030, sequenceid=2592, filesize=66.8m
2014-07-21 02:08:38,431 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.6m/270146880, currentsize=163.7m/171604480 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 11593ms, sequenceid=2592, compaction requested=true
2014-07-21 02:08:38,432 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:08:38,432 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-21 02:08:38,432 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-21 02:08:38,432 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:08:38,432 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:08:38,433 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:08:38,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:38,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10976 synced till here 10975
2014-07-21 02:08:38,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933717527 with entries=83, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933718712
2014-07-21 02:08:41,688 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:08:41,688 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.3m
2014-07-21 02:08:41,822 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:41,848 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11062 synced till here 11061
2014-07-21 02:08:41,849 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:41,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933718712 with entries=86, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933721822
2014-07-21 02:08:43,170 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:43,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933721822 with entries=98, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933723170
2014-07-21 02:08:43,524 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:08:47,179 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2653, memsize=264.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/83d6bb8d79154aab99c6841d5848579d
2014-07-21 02:08:47,202 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/83d6bb8d79154aab99c6841d5848579d as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/83d6bb8d79154aab99c6841d5848579d
2014-07-21 02:08:47,215 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/83d6bb8d79154aab99c6841d5848579d, entries=964670, sequenceid=2653, filesize=68.7m
2014-07-21 02:08:47,215 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~302.0m/316655040, currentsize=139.7m/146530480 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 10563ms, sequenceid=2653, compaction requested=true
2014-07-21 02:08:47,216 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:08:47,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-21 02:08:47,216 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 266.1m
2014-07-21 02:08:47,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-21 02:08:47,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:08:47,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:08:47,216 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:08:47,360 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:50,151 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2692, memsize=240.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/3b4f7b0b79eb4205bc56065a79d4de8f
2014-07-21 02:08:50,182 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/3b4f7b0b79eb4205bc56065a79d4de8f as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/3b4f7b0b79eb4205bc56065a79d4de8f
2014-07-21 02:08:50,196 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/3b4f7b0b79eb4205bc56065a79d4de8f, entries=873910, sequenceid=2692, filesize=62.2m
2014-07-21 02:08:50,196 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.3m/268760320, currentsize=18.7m/19591120 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8508ms, sequenceid=2692, compaction requested=true
2014-07-21 02:08:50,197 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:08:50,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-21 02:08:50,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-21 02:08:50,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:08:50,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:08:50,197 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:08:53,079 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:08:53,080 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.3m
2014-07-21 02:08:53,396 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:08:54,004 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2734, memsize=206.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/4d03c01f5fce4717b3f6c6ba4faf7d55
2014-07-21 02:08:54,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:54,051 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/4d03c01f5fce4717b3f6c6ba4faf7d55 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/4d03c01f5fce4717b3f6c6ba4faf7d55
2014-07-21 02:08:54,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11285 synced till here 11283
2014-07-21 02:08:54,071 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933723170 with entries=125, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933734030
2014-07-21 02:08:54,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933623897
2014-07-21 02:08:54,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933627135
2014-07-21 02:08:54,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933628452
2014-07-21 02:08:54,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933631219
2014-07-21 02:08:54,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933636562
2014-07-21 02:08:54,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933642045
2014-07-21 02:08:54,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933643200
2014-07-21 02:08:54,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933646244
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933650199
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933653631
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933656328
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933657711
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933660259
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933662418
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933669802
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933672336
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933675148
2014-07-21 02:08:54,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933681740
2014-07-21 02:08:54,075 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/4d03c01f5fce4717b3f6c6ba4faf7d55, entries=751860, sequenceid=2734, filesize=53.6m
2014-07-21 02:08:54,076 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~266.1m/279070880, currentsize=14.4m/15056160 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 6859ms, sequenceid=2734, compaction requested=true
2014-07-21 02:08:54,076 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:08:54,076 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-21 02:08:54,076 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-21 02:08:54,076 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:08:54,076 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:08:54,077 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:08:56,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:56,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11384 synced till here 11380
2014-07-21 02:08:56,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933734030 with entries=99, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933736772
2014-07-21 02:08:59,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:08:59,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11472 synced till here 11464
2014-07-21 02:08:59,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933736772 with entries=88, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933739130
2014-07-21 02:09:00,365 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:09:00,365 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.3m
2014-07-21 02:09:00,551 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:00,892 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:00,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11559 synced till here 11555
2014-07-21 02:09:00,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933739130 with entries=87, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933740892
2014-07-21 02:09:01,943 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3076, memsize=228.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/7787728ec8c74aa592d51f14cd756a54
2014-07-21 02:09:01,960 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/7787728ec8c74aa592d51f14cd756a54 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/7787728ec8c74aa592d51f14cd756a54
2014-07-21 02:09:01,973 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/7787728ec8c74aa592d51f14cd756a54, entries=832680, sequenceid=3076, filesize=59.3m
2014-07-21 02:09:01,974 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.3m/268765360, currentsize=126.6m/132773120 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8894ms, sequenceid=3076, compaction requested=true
2014-07-21 02:09:01,974 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:01,975 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-21 02:09:01,975 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-21 02:09:01,975 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:01,975 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:01,975 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:09:02,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:03,241 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933740892 with entries=116, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933742836
2014-07-21 02:09:03,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933705692
2014-07-21 02:09:03,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933709241
2014-07-21 02:09:03,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933713627
2014-07-21 02:09:04,697 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:04,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11762 synced till here 11753
2014-07-21 02:09:04,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933742836 with entries=87, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933744697
2014-07-21 02:09:07,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:07,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11854 synced till here 11851
2014-07-21 02:09:07,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933744697 with entries=92, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933747739
2014-07-21 02:09:08,490 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:09:08,491 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 256.9m
2014-07-21 02:09:08,594 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2793, memsize=225.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/1fb5154296cd44ab94c483fd33a38e0f
2014-07-21 02:09:08,613 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/1fb5154296cd44ab94c483fd33a38e0f as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/1fb5154296cd44ab94c483fd33a38e0f
2014-07-21 02:09:08,709 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/1fb5154296cd44ab94c483fd33a38e0f, entries=819050, sequenceid=2793, filesize=58.3m
2014-07-21 02:09:08,709 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.3m/271880480, currentsize=151.7m/159026080 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8344ms, sequenceid=2793, compaction requested=true
2014-07-21 02:09:08,710 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:08,710 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-21 02:09:08,710 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-21 02:09:08,710 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:08,710 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:08,710 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:09:08,747 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:09,643 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:09:09,643 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 258.5m
2014-07-21 02:09:09,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:09,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933747739 with entries=80, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933749663
2014-07-21 02:09:09,697 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933715615
2014-07-21 02:09:09,697 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933717527
2014-07-21 02:09:09,800 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:16,022 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2883, memsize=249.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7b5c8acaa024450da8f34a0bd0799445
2014-07-21 02:09:16,042 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7b5c8acaa024450da8f34a0bd0799445 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7b5c8acaa024450da8f34a0bd0799445
2014-07-21 02:09:16,061 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7b5c8acaa024450da8f34a0bd0799445, entries=910030, sequenceid=2883, filesize=64.8m
2014-07-21 02:09:16,061 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~266.1m/279000320, currentsize=23.1m/24268800 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 7570ms, sequenceid=2883, compaction requested=true
2014-07-21 02:09:16,061 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:16,062 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-21 02:09:16,062 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-21 02:09:16,062 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:16,062 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:16,062 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:09:16,870 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3300, memsize=244.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/6040448612b24ec7965b3afec9ac13e5
2014-07-21 02:09:16,891 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/6040448612b24ec7965b3afec9ac13e5 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/6040448612b24ec7965b3afec9ac13e5
2014-07-21 02:09:16,904 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/6040448612b24ec7965b3afec9ac13e5, entries=888840, sequenceid=3300, filesize=63.3m
2014-07-21 02:09:16,905 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.5m/271035040, currentsize=4.1m/4308320 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 7262ms, sequenceid=3300, compaction requested=true
2014-07-21 02:09:16,905 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:16,905 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-21 02:09:16,905 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-21 02:09:16,906 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:16,906 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:16,906 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:09:27,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:27,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12053 synced till here 12051
2014-07-21 02:09:27,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933749663 with entries=119, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933767806
2014-07-21 02:09:29,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:29,360 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12148 synced till here 12142
2014-07-21 02:09:29,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933767806 with entries=95, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933769340
2014-07-21 02:09:30,782 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:09:30,782 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.4m
2014-07-21 02:09:31,030 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:31,584 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:31,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12234 synced till here 12232
2014-07-21 02:09:31,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933769340 with entries=86, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933771585
2014-07-21 02:09:33,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:33,838 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12319 synced till here 12317
2014-07-21 02:09:33,851 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933771585 with entries=85, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933773823
2014-07-21 02:09:35,912 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:36,120 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12424 synced till here 12423
2014-07-21 02:09:36,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933773823 with entries=105, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933775913
2014-07-21 02:09:37,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:37,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12516 synced till here 12514
2014-07-21 02:09:38,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933775913 with entries=92, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933777967
2014-07-21 02:09:39,346 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2942, memsize=249.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/d89d0c7f82fa4499affff9587b1ece6f
2014-07-21 02:09:39,365 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/d89d0c7f82fa4499affff9587b1ece6f as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/d89d0c7f82fa4499affff9587b1ece6f
2014-07-21 02:09:39,380 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/d89d0c7f82fa4499affff9587b1ece6f, entries=907340, sequenceid=2942, filesize=64.6m
2014-07-21 02:09:39,381 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.9m/271507120, currentsize=143.8m/150827040 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8599ms, sequenceid=2942, compaction requested=true
2014-07-21 02:09:39,381 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:39,381 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-21 02:09:39,381 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-21 02:09:39,382 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:39,382 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:39,382 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:09:39,986 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:09:39,986 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 257.1m
2014-07-21 02:09:40,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:40,175 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12604 synced till here 12601
2014-07-21 02:09:40,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933777967 with entries=88, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933780158
2014-07-21 02:09:40,203 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:40,780 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:09:40,780 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.5m
2014-07-21 02:09:40,913 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:09:42,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:42,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933780158 with entries=96, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933782346
2014-07-21 02:09:46,456 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:09:46,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12804 synced till here 12800
2014-07-21 02:09:46,825 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933782346 with entries=104, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933786456
2014-07-21 02:09:48,549 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3032, memsize=259.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/53005591b54a4245857b568a20fbc1fb
2014-07-21 02:09:48,571 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/53005591b54a4245857b568a20fbc1fb as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/53005591b54a4245857b568a20fbc1fb
2014-07-21 02:09:48,589 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/53005591b54a4245857b568a20fbc1fb, entries=943080, sequenceid=3032, filesize=67.2m
2014-07-21 02:09:48,590 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.0m/271598000, currentsize=84.6m/88704800 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8604ms, sequenceid=3032, compaction requested=true
2014-07-21 02:09:48,590 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:48,591 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-21 02:09:48,591 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-21 02:09:48,591 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:48,591 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:48,591 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:09:48,835 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3570, memsize=256.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/4ab069f80377405c976a476755e9d2b8
2014-07-21 02:09:48,847 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/4ab069f80377405c976a476755e9d2b8 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/4ab069f80377405c976a476755e9d2b8
2014-07-21 02:09:48,861 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/4ab069f80377405c976a476755e9d2b8, entries=933980, sequenceid=3570, filesize=66.5m
2014-07-21 02:09:48,861 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.5m/268979200, currentsize=84.0m/88128560 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8081ms, sequenceid=3570, compaction requested=true
2014-07-21 02:09:48,862 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:09:48,862 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-21 02:09:48,862 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-21 02:09:48,862 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:09:48,862 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:09:48,862 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:10:01,464 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:10:01,465 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.2m
2014-07-21 02:10:01,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:01,631 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:01,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933786456 with entries=131, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933801547
2014-07-21 02:10:03,540 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:03,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13024 synced till here 13023
2014-07-21 02:10:03,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933801547 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933803541
2014-07-21 02:10:05,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:05,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13114 synced till here 13113
2014-07-21 02:10:05,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933803541 with entries=90, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933805122
2014-07-21 02:10:07,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:07,814 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933805122 with entries=97, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933807776
2014-07-21 02:10:09,719 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3087, memsize=257.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/a545d84f8ff441bea37c91bc367772b4
2014-07-21 02:10:09,735 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/a545d84f8ff441bea37c91bc367772b4 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/a545d84f8ff441bea37c91bc367772b4
2014-07-21 02:10:09,749 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/a545d84f8ff441bea37c91bc367772b4, entries=936410, sequenceid=3087, filesize=66.7m
2014-07-21 02:10:09,749 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.2m/269678240, currentsize=116.3m/121967440 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8284ms, sequenceid=3087, compaction requested=true
2014-07-21 02:10:09,750 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:09,751 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-21 02:10:09,751 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-21 02:10:09,751 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:09,751 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:09,751 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:10:13,817 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:13,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13313 synced till here 13311
2014-07-21 02:10:14,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933807776 with entries=102, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933813817
2014-07-21 02:10:14,202 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:10:14,203 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 256.2m
2014-07-21 02:10:14,333 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:14,683 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:10:14,683 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.0m
2014-07-21 02:10:14,835 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:15,763 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:10:15,890 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:15,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13398 synced till here 13394
2014-07-21 02:10:16,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933813817 with entries=85, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933815891
2014-07-21 02:10:17,246 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:17,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13482 synced till here 13481
2014-07-21 02:10:17,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933815891 with entries=84, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933817247
2014-07-21 02:10:19,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:20,042 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13578 synced till here 13577
2014-07-21 02:10:20,048 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933817247 with entries=96, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933819789
2014-07-21 02:10:20,331 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:10:20,993 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:10:21,362 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:22,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933819789 with entries=108, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933821362
2014-07-21 02:10:22,911 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3174, memsize=256.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/8debcec62eb14242852746a9f8291d12
2014-07-21 02:10:22,928 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/8debcec62eb14242852746a9f8291d12 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/8debcec62eb14242852746a9f8291d12
2014-07-21 02:10:22,943 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/8debcec62eb14242852746a9f8291d12, entries=932880, sequenceid=3174, filesize=66.4m
2014-07-21 02:10:22,944 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.2m/268661840, currentsize=165.0m/173030160 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8741ms, sequenceid=3174, compaction requested=true
2014-07-21 02:10:22,944 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:22,944 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-21 02:10:22,944 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-21 02:10:22,944 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 392.8m
2014-07-21 02:10:22,944 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:22,944 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:22,944 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:10:23,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:23,509 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3169, memsize=249.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/7dbf6b8ffa544d4fb69a86dce548095a
2014-07-21 02:10:23,530 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/7dbf6b8ffa544d4fb69a86dce548095a as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/7dbf6b8ffa544d4fb69a86dce548095a
2014-07-21 02:10:23,547 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/7dbf6b8ffa544d4fb69a86dce548095a, entries=909460, sequenceid=3169, filesize=64.7m
2014-07-21 02:10:23,547 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.0m/268468560, currentsize=47.6m/49928000 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8864ms, sequenceid=3169, compaction requested=true
2014-07-21 02:10:23,547 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:23,548 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-21 02:10:23,548 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-21 02:10:23,548 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 312.2m
2014-07-21 02:10:23,548 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:23,548 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:23,548 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:10:23,700 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:32,898 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3266, memsize=312.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c3ba94171b0b424193925985edab0ca5
2014-07-21 02:10:32,926 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c3ba94171b0b424193925985edab0ca5 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c3ba94171b0b424193925985edab0ca5
2014-07-21 02:10:32,976 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c3ba94171b0b424193925985edab0ca5, entries=1136680, sequenceid=3266, filesize=80.9m
2014-07-21 02:10:32,977 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~312.2m/327355840, currentsize=0.0/0 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 9428ms, sequenceid=3266, compaction requested=true
2014-07-21 02:10:32,977 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:32,978 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-21 02:10:32,978 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-21 02:10:32,978 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:32,978 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:32,978 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:10:34,194 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3992, memsize=392.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/5bb65bef2d0247ba889e2ad0288fbaad
2014-07-21 02:10:34,207 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/5bb65bef2d0247ba889e2ad0288fbaad as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/5bb65bef2d0247ba889e2ad0288fbaad
2014-07-21 02:10:34,227 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/5bb65bef2d0247ba889e2ad0288fbaad, entries=1430280, sequenceid=3992, filesize=101.8m
2014-07-21 02:10:34,227 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~392.8m/411908080, currentsize=14.2m/14909520 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 11283ms, sequenceid=3992, compaction requested=true
2014-07-21 02:10:34,227 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:34,228 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-21 02:10:34,228 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-21 02:10:34,228 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:34,228 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:34,228 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:10:37,792 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:37,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13802 synced till here 13801
2014-07-21 02:10:37,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933821362 with entries=116, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933837793
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933718712
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933721822
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933723170
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933734030
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933736772
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933739130
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933740892
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933742836
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933744697
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933747739
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933749663
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933767806
2014-07-21 02:10:37,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933769340
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933771585
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933773823
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933775913
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933777967
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933780158
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933782346
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933786456
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933801547
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933803541
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933805122
2014-07-21 02:10:37,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933807776
2014-07-21 02:10:39,915 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:40,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13914 synced till here 13912
2014-07-21 02:10:40,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933837793 with entries=112, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933839916
2014-07-21 02:10:41,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:41,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13999 synced till here 13992
2014-07-21 02:10:41,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933839916 with entries=85, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933841487
2014-07-21 02:10:41,755 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:10:41,755 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 257.2m
2014-07-21 02:10:41,955 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:43,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:43,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14099 synced till here 14098
2014-07-21 02:10:43,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933841487 with entries=100, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933843460
2014-07-21 02:10:45,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:45,952 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14192 synced till here 14191
2014-07-21 02:10:45,973 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933843460 with entries=93, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933845932
2014-07-21 02:10:49,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:49,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14313 synced till here 14312
2014-07-21 02:10:50,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933845932 with entries=121, filesize=87.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933849206
2014-07-21 02:10:50,345 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3325, memsize=261.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/c56961140a6f4b97887fcc0500cee9ef
2014-07-21 02:10:50,360 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/c56961140a6f4b97887fcc0500cee9ef as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/c56961140a6f4b97887fcc0500cee9ef
2014-07-21 02:10:50,370 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/c56961140a6f4b97887fcc0500cee9ef, entries=951250, sequenceid=3325, filesize=67.8m
2014-07-21 02:10:50,371 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.3m/273954720, currentsize=115.7m/121317360 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8616ms, sequenceid=3325, compaction requested=true
2014-07-21 02:10:50,371 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:10:50,371 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-21 02:10:50,371 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-21 02:10:50,371 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:10:50,372 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:10:50,372 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:10:52,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:52,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14409 synced till here 14406
2014-07-21 02:10:52,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933849206 with entries=96, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933852128
2014-07-21 02:10:52,819 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:10:52,820 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.6m
2014-07-21 02:10:52,931 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:10:52,932 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.0m
2014-07-21 02:10:53,027 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:53,071 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:10:53,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:53,732 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933852128 with entries=91, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933853586
2014-07-21 02:10:55,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:10:55,714 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933853586 with entries=73, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933855681
2014-07-21 02:11:00,650 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4252, memsize=260.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/cdde7a3048254923a83fd2f3f10eb9c0
2014-07-21 02:11:00,769 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3418, memsize=257.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c6aeaab458004deda53787c11605a2b7
2014-07-21 02:11:00,961 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c6aeaab458004deda53787c11605a2b7 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c6aeaab458004deda53787c11605a2b7
2014-07-21 02:11:00,961 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/cdde7a3048254923a83fd2f3f10eb9c0 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/cdde7a3048254923a83fd2f3f10eb9c0
2014-07-21 02:11:00,984 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c6aeaab458004deda53787c11605a2b7, entries=935570, sequenceid=3418, filesize=66.7m
2014-07-21 02:11:00,985 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.0m/269435920, currentsize=54.7m/57376640 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8053ms, sequenceid=3418, compaction requested=true
2014-07-21 02:11:00,986 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:00,986 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-21 02:11:00,986 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-21 02:11:00,986 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:00,986 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:00,986 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:11:01,020 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/cdde7a3048254923a83fd2f3f10eb9c0, entries=947460, sequenceid=4252, filesize=67.5m
2014-07-21 02:11:01,020 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.2m/272861680, currentsize=65.1m/68282560 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8200ms, sequenceid=4252, compaction requested=true
2014-07-21 02:11:01,021 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:01,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-21 02:11:01,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-21 02:11:01,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:01,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:01,021 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:11:14,801 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:14,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14704 synced till here 14702
2014-07-21 02:11:14,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933855681 with entries=131, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933874802
2014-07-21 02:11:15,262 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:11:15,263 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 256.4m
2014-07-21 02:11:15,450 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:17,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:18,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933874802 with entries=95, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933877826
2014-07-21 02:11:20,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:21,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933877826 with entries=84, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933880977
2014-07-21 02:11:22,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:22,650 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14964 synced till here 14962
2014-07-21 02:11:22,667 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933880977 with entries=81, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933882636
2014-07-21 02:11:24,379 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3479, memsize=258.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/174fb75f8f7a4aac9fdbef7168b039e4
2014-07-21 02:11:24,413 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/174fb75f8f7a4aac9fdbef7168b039e4 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/174fb75f8f7a4aac9fdbef7168b039e4
2014-07-21 02:11:24,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:24,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15055 synced till here 15051
2014-07-21 02:11:24,667 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/174fb75f8f7a4aac9fdbef7168b039e4, entries=939270, sequenceid=3479, filesize=66.9m
2014-07-21 02:11:24,667 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.0m/270503680, currentsize=125.1m/131125520 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 9404ms, sequenceid=3479, compaction requested=true
2014-07-21 02:11:24,668 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:24,668 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-21 02:11:24,669 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-21 02:11:24,671 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:24,671 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:24,671 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:11:24,678 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933882636 with entries=91, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933884517
2014-07-21 02:11:24,835 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:11:24,835 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 260.0m
2014-07-21 02:11:25,054 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:26,196 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:11:26,196 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 256.3m
2014-07-21 02:11:26,253 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:26,407 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:26,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15159 synced till here 15153
2014-07-21 02:11:26,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933884517 with entries=104, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933886253
2014-07-21 02:11:28,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:28,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15248 synced till here 15246
2014-07-21 02:11:28,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933886253 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933888541
2014-07-21 02:11:31,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:32,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15335 synced till here 15329
2014-07-21 02:11:32,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933888541 with entries=87, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933891954
2014-07-21 02:11:33,257 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:11:33,699 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4484, memsize=245.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f01b5f3f47a745d4979f4fac5a22aba1
2014-07-21 02:11:33,717 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f01b5f3f47a745d4979f4fac5a22aba1 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f01b5f3f47a745d4979f4fac5a22aba1
2014-07-21 02:11:33,739 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f01b5f3f47a745d4979f4fac5a22aba1, entries=891870, sequenceid=4484, filesize=63.5m
2014-07-21 02:11:33,739 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~266.0m/278945920, currentsize=122.9m/128867680 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8904ms, sequenceid=4484, compaction requested=true
2014-07-21 02:11:33,740 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:33,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-21 02:11:33,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-21 02:11:33,740 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 277.1m
2014-07-21 02:11:33,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:33,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:33,740 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:11:33,899 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:33,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:33,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933891954 with entries=99, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933893914
2014-07-21 02:11:34,654 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3565, memsize=252.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/09346eeb30bc461ba5e244ae8bc7dd15
2014-07-21 02:11:34,673 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/09346eeb30bc461ba5e244ae8bc7dd15 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/09346eeb30bc461ba5e244ae8bc7dd15
2014-07-21 02:11:34,688 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/09346eeb30bc461ba5e244ae8bc7dd15, entries=918510, sequenceid=3565, filesize=65.4m
2014-07-21 02:11:34,689 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.2m/270788000, currentsize=120.2m/126086160 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8493ms, sequenceid=3565, compaction requested=true
2014-07-21 02:11:34,689 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:34,689 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-21 02:11:34,689 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-21 02:11:34,690 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:34,690 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:34,690 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:11:42,266 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3637, memsize=262.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/e600b2f026cd48f1b46b7d99aaab4dbe
2014-07-21 02:11:42,281 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/e600b2f026cd48f1b46b7d99aaab4dbe as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/e600b2f026cd48f1b46b7d99aaab4dbe
2014-07-21 02:11:42,296 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/e600b2f026cd48f1b46b7d99aaab4dbe, entries=954130, sequenceid=3637, filesize=68.0m
2014-07-21 02:11:42,297 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~277.1m/290516000, currentsize=4.9m/5189680 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8557ms, sequenceid=3637, compaction requested=true
2014-07-21 02:11:42,297 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:42,297 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-21 02:11:42,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-21 02:11:42,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:42,298 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:42,298 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:11:48,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:48,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933893914 with entries=105, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933908750
2014-07-21 02:11:51,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:51,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15635 synced till here 15632
2014-07-21 02:11:51,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933908750 with entries=96, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933911223
2014-07-21 02:11:51,380 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:11:51,381 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.5m
2014-07-21 02:11:51,549 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:52,503 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:52,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15731 synced till here 15726
2014-07-21 02:11:52,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933911223 with entries=96, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933912503
2014-07-21 02:11:53,065 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:11:53,065 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.1m
2014-07-21 02:11:53,242 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:11:57,742 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:11:57,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15837 synced till here 15835
2014-07-21 02:11:57,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933912503 with entries=106, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933917743
2014-07-21 02:11:58,246 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:11:59,689 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3662, memsize=252.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/e0c09ba0ae92403f884b0d37821b3a5a
2014-07-21 02:11:59,701 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/e0c09ba0ae92403f884b0d37821b3a5a as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/e0c09ba0ae92403f884b0d37821b3a5a
2014-07-21 02:11:59,712 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/e0c09ba0ae92403f884b0d37821b3a5a, entries=918100, sequenceid=3662, filesize=65.4m
2014-07-21 02:11:59,712 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.5m/269010240, currentsize=26.6m/27875200 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8331ms, sequenceid=3662, compaction requested=true
2014-07-21 02:11:59,713 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:11:59,713 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-21 02:11:59,713 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-21 02:11:59,713 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:11:59,713 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 269.6m
2014-07-21 02:11:59,713 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:11:59,713 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:12:00,107 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:00,743 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4733, memsize=244.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/8f0a8baa7a8d45f1968c6a28e6c24e08
2014-07-21 02:12:00,758 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/8f0a8baa7a8d45f1968c6a28e6c24e08 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/8f0a8baa7a8d45f1968c6a28e6c24e08
2014-07-21 02:12:00,770 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/8f0a8baa7a8d45f1968c6a28e6c24e08, entries=890670, sequenceid=4733, filesize=63.5m
2014-07-21 02:12:00,771 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.2m/268619360, currentsize=53.9m/56472400 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 7706ms, sequenceid=4733, compaction requested=true
2014-07-21 02:12:00,771 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:00,771 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-21 02:12:00,772 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-21 02:12:00,772 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:00,772 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:00,772 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:12:00,956 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:00,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15952 synced till here 15950
2014-07-21 02:12:01,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933917743 with entries=115, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933920957
2014-07-21 02:12:01,001 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933813817
2014-07-21 02:12:01,001 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933815891
2014-07-21 02:12:01,001 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933817247
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933819789
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933821362
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933837793
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933839916
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933841487
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933843460
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933845932
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933849206
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933852128
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933853586
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933855681
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933874802
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933877826
2014-07-21 02:12:01,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933880977
2014-07-21 02:12:01,003 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933882636
2014-07-21 02:12:02,167 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:02,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16039 synced till here 16038
2014-07-21 02:12:02,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933920957 with entries=87, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933922167
2014-07-21 02:12:04,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:04,124 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933922167 with entries=95, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933924073
2014-07-21 02:12:05,689 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:12:05,689 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 256.3m
2014-07-21 02:12:05,827 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:08,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:08,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933924073 with entries=115, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933928774
2014-07-21 02:12:09,221 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3723, memsize=266.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/a95c871594da4044a76d2b23214c65da
2014-07-21 02:12:09,255 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/a95c871594da4044a76d2b23214c65da as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/a95c871594da4044a76d2b23214c65da
2014-07-21 02:12:09,277 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/a95c871594da4044a76d2b23214c65da, entries=968710, sequenceid=3723, filesize=69.0m
2014-07-21 02:12:09,278 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~273.6m/286891760, currentsize=122.3m/128251520 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 9565ms, sequenceid=3723, compaction requested=true
2014-07-21 02:12:09,278 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:09,278 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-21 02:12:09,278 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-21 02:12:09,278 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:09,279 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:09,279 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:12:10,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:10,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933928774 with entries=93, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933930714
2014-07-21 02:12:10,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933884517
2014-07-21 02:12:10,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933886253
2014-07-21 02:12:10,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933888541
2014-07-21 02:12:13,638 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3783, memsize=256.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7bb2acc96369447493fd0b9167259b30
2014-07-21 02:12:13,656 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7bb2acc96369447493fd0b9167259b30 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7bb2acc96369447493fd0b9167259b30
2014-07-21 02:12:13,671 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7bb2acc96369447493fd0b9167259b30, entries=933280, sequenceid=3783, filesize=66.4m
2014-07-21 02:12:13,671 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.3m/268776160, currentsize=79.0m/82849200 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 7982ms, sequenceid=3783, compaction requested=true
2014-07-21 02:12:13,672 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:13,672 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-21 02:12:13,672 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-21 02:12:13,672 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:13,672 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:13,672 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:12:25,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:25,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933930714 with entries=89, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933945121
2014-07-21 02:12:25,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933891954
2014-07-21 02:12:25,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933893914
2014-07-21 02:12:25,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933908750
2014-07-21 02:12:28,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:28,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16552 synced till here 16547
2014-07-21 02:12:28,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933945121 with entries=121, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933948810
2014-07-21 02:12:28,935 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:12:28,936 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 256.9m
2014-07-21 02:12:29,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:31,044 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:12:31,044 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 256.5m
2014-07-21 02:12:31,170 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:32,139 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:32,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16631 synced till here 16630
2014-07-21 02:12:32,193 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933948810 with entries=79, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933952140
2014-07-21 02:12:33,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:33,491 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16715 synced till here 16712
2014-07-21 02:12:33,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933952140 with entries=84, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933953472
2014-07-21 02:12:34,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:34,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16798 synced till here 16797
2014-07-21 02:12:34,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933953472 with entries=83, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933954687
2014-07-21 02:12:35,519 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:12:35,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:35,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933954687 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933955571
2014-07-21 02:12:37,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:37,634 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933955571 with entries=95, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933957559
2014-07-21 02:12:37,680 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5070, memsize=261.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f2c4c342472d49d98ff829104dd6ff22
2014-07-21 02:12:37,692 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f2c4c342472d49d98ff829104dd6ff22 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f2c4c342472d49d98ff829104dd6ff22
2014-07-21 02:12:37,701 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f2c4c342472d49d98ff829104dd6ff22, entries=951990, sequenceid=5070, filesize=67.7m
2014-07-21 02:12:37,702 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.5m/274166160, currentsize=169.5m/177734000 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8766ms, sequenceid=5070, compaction requested=true
2014-07-21 02:12:37,703 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:37,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-21 02:12:37,703 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 300.6m
2014-07-21 02:12:37,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-21 02:12:37,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:37,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:37,704 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:12:38,287 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:39,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:39,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933957559 with entries=123, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933959059
2014-07-21 02:12:39,818 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3867, memsize=256.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/01c2ca7f3d5e45a48d95633d7789229f
2014-07-21 02:12:39,833 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/01c2ca7f3d5e45a48d95633d7789229f as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/01c2ca7f3d5e45a48d95633d7789229f
2014-07-21 02:12:40,067 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/01c2ca7f3d5e45a48d95633d7789229f, entries=933920, sequenceid=3867, filesize=66.5m
2014-07-21 02:12:40,067 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.5m/268961520, currentsize=181.8m/190679200 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 9023ms, sequenceid=3867, compaction requested=true
2014-07-21 02:12:40,068 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:40,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-21 02:12:40,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-21 02:12:40,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:40,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:40,068 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:12:41,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:12:42,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17189 synced till here 17188
2014-07-21 02:12:42,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933959059 with entries=84, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933961966
2014-07-21 02:12:42,727 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:12:42,727 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 257.3m
2014-07-21 02:12:43,277 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:12:47,252 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3957, memsize=284.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/6bb4400606dc49d2b607de67414b7648
2014-07-21 02:12:47,267 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/6bb4400606dc49d2b607de67414b7648 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/6bb4400606dc49d2b607de67414b7648
2014-07-21 02:12:47,281 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/6bb4400606dc49d2b607de67414b7648, entries=1036450, sequenceid=3957, filesize=73.8m
2014-07-21 02:12:47,282 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~302.5m/317245680, currentsize=96.6m/101249120 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 9579ms, sequenceid=3957, compaction requested=true
2014-07-21 02:12:47,282 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:47,282 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-21 02:12:47,282 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-21 02:12:47,283 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:47,283 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:47,283 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:12:50,469 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5289, memsize=241.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1180d99202564778a68102158341c1fb
2014-07-21 02:12:50,486 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1180d99202564778a68102158341c1fb as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1180d99202564778a68102158341c1fb
2014-07-21 02:12:50,506 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1180d99202564778a68102158341c1fb, entries=880870, sequenceid=5289, filesize=62.8m
2014-07-21 02:12:50,506 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.9m/271426080, currentsize=25.1m/26354000 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 7779ms, sequenceid=5289, compaction requested=true
2014-07-21 02:12:50,507 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:12:50,507 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-21 02:12:50,507 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-21 02:12:50,507 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:12:50,507 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:12:50,507 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:13:00,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:00,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933961966 with entries=105, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933980847
2014-07-21 02:13:01,365 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 256.5m
2014-07-21 02:13:01,365 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:13:01,596 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:03,524 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:03,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17409 synced till here 17407
2014-07-21 02:13:03,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933980847 with entries=115, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933983524
2014-07-21 02:13:08,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:08,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17534 synced till here 17503
2014-07-21 02:13:08,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933983524 with entries=125, filesize=93.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933988222
2014-07-21 02:13:10,448 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:10,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17619 synced till here 17615
2014-07-21 02:13:10,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933988222 with entries=85, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933990448
2014-07-21 02:13:11,588 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:13:11,589 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 261.9m
2014-07-21 02:13:11,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:11,704 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17706 synced till here 17705
2014-07-21 02:13:11,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933990448 with entries=87, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933991663
2014-07-21 02:13:11,803 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:12,968 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:13,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17819 synced till here 17813
2014-07-21 02:13:13,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933991663 with entries=113, filesize=92.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933992969
2014-07-21 02:13:15,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:15,159 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:13:15,233 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4015, memsize=260.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e87acd52e5024d7c945e749858a76333
2014-07-21 02:13:15,259 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e87acd52e5024d7c945e749858a76333 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e87acd52e5024d7c945e749858a76333
2014-07-21 02:13:15,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17907 synced till here 17902
2014-07-21 02:13:15,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933992969 with entries=88, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933995159
2014-07-21 02:13:15,656 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e87acd52e5024d7c945e749858a76333, entries=947810, sequenceid=4015, filesize=67.5m
2014-07-21 02:13:15,656 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.3m/272960960, currentsize=246.5m/258520160 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 14291ms, sequenceid=4015, compaction requested=true
2014-07-21 02:13:15,657 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:15,657 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-21 02:13:15,657 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-21 02:13:15,657 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 259.9m
2014-07-21 02:13:15,657 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:15,657 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:15,657 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:13:15,833 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:13:15,911 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:13:15,984 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:17,311 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:17,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933995159 with entries=95, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933997311
2014-07-21 02:13:18,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:19,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18135 synced till here 18130
2014-07-21 02:13:19,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933997311 with entries=133, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933998881
2014-07-21 02:13:20,513 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4101, memsize=218.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/bef30b724c8d4bafac47967868b86513
2014-07-21 02:13:20,541 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/bef30b724c8d4bafac47967868b86513 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/bef30b724c8d4bafac47967868b86513
2014-07-21 02:13:20,559 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/bef30b724c8d4bafac47967868b86513, entries=796330, sequenceid=4101, filesize=56.7m
2014-07-21 02:13:20,559 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~265.3m/278179440, currentsize=206.1m/216094320 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8970ms, sequenceid=4101, compaction requested=true
2014-07-21 02:13:20,560 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:20,560 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-21 02:13:20,560 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-21 02:13:20,560 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 291.7m
2014-07-21 02:13:20,560 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:20,560 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:20,561 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:13:20,766 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:23,655 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:23,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18247 synced till here 18235
2014-07-21 02:13:23,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933998881 with entries=112, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934003655
2014-07-21 02:13:24,821 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:13:25,550 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:25,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934003655 with entries=89, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934005550
2014-07-21 02:13:26,504 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5540, memsize=209.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/657b82a1a59e4fe29a3a36ba7570f471
2014-07-21 02:13:26,538 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/657b82a1a59e4fe29a3a36ba7570f471 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/657b82a1a59e4fe29a3a36ba7570f471
2014-07-21 02:13:26,554 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/657b82a1a59e4fe29a3a36ba7570f471, entries=761230, sequenceid=5540, filesize=54.2m
2014-07-21 02:13:26,555 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.3m/273960400, currentsize=125.4m/131451440 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 10898ms, sequenceid=5540, compaction requested=true
2014-07-21 02:13:26,555 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:26,555 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-21 02:13:26,555 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-21 02:13:26,555 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 418.5m
2014-07-21 02:13:26,555 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:26,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:26,556 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:13:26,808 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:31,655 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4204, memsize=253.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/e1910fc499c04367b875ab51f687a17b
2014-07-21 02:13:31,677 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/e1910fc499c04367b875ab51f687a17b as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/e1910fc499c04367b875ab51f687a17b
2014-07-21 02:13:31,694 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/e1910fc499c04367b875ab51f687a17b, entries=922890, sequenceid=4204, filesize=65.7m
2014-07-21 02:13:31,694 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~291.7m/305910400, currentsize=18.4m/19276640 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 11134ms, sequenceid=4204, compaction requested=true
2014-07-21 02:13:31,695 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:31,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-21 02:13:31,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-21 02:13:31,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:31,695 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 268.9m
2014-07-21 02:13:31,695 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:31,695 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:13:31,862 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:36,114 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4247, memsize=283.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e35f268487424015b0adb9b6d870adb8
2014-07-21 02:13:36,133 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e35f268487424015b0adb9b6d870adb8 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e35f268487424015b0adb9b6d870adb8
2014-07-21 02:13:36,151 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e35f268487424015b0adb9b6d870adb8, entries=1030760, sequenceid=4247, filesize=73.4m
2014-07-21 02:13:36,151 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~418.5m/438850640, currentsize=0.0/0 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 9596ms, sequenceid=4247, compaction requested=true
2014-07-21 02:13:36,151 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:36,151 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-21 02:13:36,152 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-21 02:13:36,152 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:36,152 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:36,152 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:13:37,520 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4252, memsize=190.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/adfa6b4f458743afa45b543348c9d0dc
2014-07-21 02:13:37,533 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/adfa6b4f458743afa45b543348c9d0dc as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/adfa6b4f458743afa45b543348c9d0dc
2014-07-21 02:13:37,544 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/adfa6b4f458743afa45b543348c9d0dc, entries=695190, sequenceid=4252, filesize=49.5m
2014-07-21 02:13:37,545 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~268.9m/281941200, currentsize=0.0/0 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 5850ms, sequenceid=4252, compaction requested=true
2014-07-21 02:13:37,545 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:37,546 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-21 02:13:37,546 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-21 02:13:37,546 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:13:37,546 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:37,546 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:13:39,378 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:39,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18442 synced till here 18435
2014-07-21 02:13:39,604 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934005550 with entries=106, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934019379
2014-07-21 02:13:39,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933911223
2014-07-21 02:13:39,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933912503
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933917743
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933920957
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933922167
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933924073
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933928774
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933930714
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933945121
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933948810
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933952140
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933953472
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933954687
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933955571
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933957559
2014-07-21 02:13:39,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933959059
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933961966
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933980847
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933983524
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933988222
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933990448
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933991663
2014-07-21 02:13:39,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933992969
2014-07-21 02:13:41,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:42,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18544 synced till here 18543
2014-07-21 02:13:42,013 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934019379 with entries=102, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934021989
2014-07-21 02:13:43,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:43,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18634 synced till here 18630
2014-07-21 02:13:44,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934021989 with entries=90, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934023695
2014-07-21 02:13:44,959 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:13:44,959 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 257.4m
2014-07-21 02:13:45,142 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:45,959 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:46,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934023695 with entries=95, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934025959
2014-07-21 02:13:50,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:50,082 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18841 synced till here 18839
2014-07-21 02:13:50,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934025959 with entries=112, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934030055
2014-07-21 02:13:52,161 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:52,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934030055 with entries=118, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934032162
2014-07-21 02:13:53,134 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5809, memsize=246.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/ac96218015e1404b9d1318763e9132b9
2014-07-21 02:13:53,154 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/ac96218015e1404b9d1318763e9132b9 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/ac96218015e1404b9d1318763e9132b9
2014-07-21 02:13:53,196 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/ac96218015e1404b9d1318763e9132b9, entries=895850, sequenceid=5809, filesize=63.8m
2014-07-21 02:13:53,196 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~263.2m/275945280, currentsize=102.1m/107066160 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 8237ms, sequenceid=5809, compaction requested=true
2014-07-21 02:13:53,197 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:13:53,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-21 02:13:53,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-21 02:13:53,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:13:53,197 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:13:53,197 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:13:54,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:54,138 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934032162 with entries=88, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934034029
2014-07-21 02:13:54,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933995159
2014-07-21 02:13:54,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933997311
2014-07-21 02:13:54,498 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:13:54,499 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 257.0m
2014-07-21 02:13:54,981 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:13:54,982 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 256.5m
2014-07-21 02:13:55,088 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:55,214 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:13:55,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:55,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934034029 with entries=97, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934035807
2014-07-21 02:13:57,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:13:57,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19252 synced till here 19250
2014-07-21 02:13:57,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934035807 with entries=108, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934037398
2014-07-21 02:14:02,786 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4391, memsize=260.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e9d746bbc2ba4026aea197a83907ea6d
2014-07-21 02:14:02,833 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/e9d746bbc2ba4026aea197a83907ea6d as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e9d746bbc2ba4026aea197a83907ea6d
2014-07-21 02:14:02,963 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/e9d746bbc2ba4026aea197a83907ea6d, entries=949400, sequenceid=4391, filesize=67.6m
2014-07-21 02:14:02,963 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.8m/273420000, currentsize=73.0m/76562560 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 8464ms, sequenceid=4391, compaction requested=true
2014-07-21 02:14:02,964 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:02,964 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-21 02:14:02,964 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-21 02:14:02,964 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:02,965 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:02,965 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:14:03,196 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4397, memsize=262.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/0519b5a3a5a7416585bf2fe46458838b
2014-07-21 02:14:03,216 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/0519b5a3a5a7416585bf2fe46458838b as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/0519b5a3a5a7416585bf2fe46458838b
2014-07-21 02:14:03,228 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/0519b5a3a5a7416585bf2fe46458838b, entries=954620, sequenceid=4397, filesize=68.0m
2014-07-21 02:14:03,228 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.2m/274922400, currentsize=71.5m/75007200 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 8246ms, sequenceid=4397, compaction requested=true
2014-07-21 02:14:03,229 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:03,229 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:14:03,229 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:14:03,229 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:03,229 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:03,230 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:14:16,980 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:17,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19395 synced till here 19394
2014-07-21 02:14:17,197 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934037398 with entries=143, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934056981
2014-07-21 02:14:18,279 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:14:18,280 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 258.1m
2014-07-21 02:14:18,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:18,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:14:18,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19513 synced till here 19511
2014-07-21 02:14:18,754 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934056981 with entries=118, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934058298
2014-07-21 02:14:21,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:21,650 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19603 synced till here 19601
2014-07-21 02:14:21,667 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934058298 with entries=90, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934061635
2014-07-21 02:14:24,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:24,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934061635 with entries=122, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934064221
2014-07-21 02:14:26,142 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:14:26,143 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 256.8m
2014-07-21 02:14:26,283 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:14:27,264 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6160, memsize=267.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/c3efae091d9345eca32ec8b279dc5a7f
2014-07-21 02:14:27,280 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/c3efae091d9345eca32ec8b279dc5a7f as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/c3efae091d9345eca32ec8b279dc5a7f
2014-07-21 02:14:27,301 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/c3efae091d9345eca32ec8b279dc5a7f, entries=975170, sequenceid=6160, filesize=69.4m
2014-07-21 02:14:27,301 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.8m/280840800, currentsize=123.7m/129686000 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 9021ms, sequenceid=6160, compaction requested=true
2014-07-21 02:14:27,302 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:27,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-21 02:14:27,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-21 02:14:27,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:27,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:27,302 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:14:27,390 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:14:27,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:27,436 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:14:27,436 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:27,436 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:14:27,437 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:14:27,437 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:27,437 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:27,437 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:14:27,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19809 synced till here 19808
2014-07-21 02:14:27,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934064221 with entries=84, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934067423
2014-07-21 02:14:28,743 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:28,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19906 synced till here 19904
2014-07-21 02:14:28,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934067423 with entries=97, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934068744
2014-07-21 02:14:32,269 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:32,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934068744 with entries=87, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934072269
2014-07-21 02:14:35,287 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:35,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20083 synced till here 20081
2014-07-21 02:14:35,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934072269 with entries=90, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934075288
2014-07-21 02:14:35,475 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4537, memsize=256.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/75208b09a25c473c9f748a9f882cf081
2014-07-21 02:14:35,504 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/75208b09a25c473c9f748a9f882cf081 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/75208b09a25c473c9f748a9f882cf081
2014-07-21 02:14:35,521 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/75208b09a25c473c9f748a9f882cf081, entries=935140, sequenceid=4537, filesize=66.6m
2014-07-21 02:14:35,522 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.8m/269312880, currentsize=121.2m/127047440 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 9379ms, sequenceid=4537, compaction requested=true
2014-07-21 02:14:35,522 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:35,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:14:35,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:14:35,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:35,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:35,522 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:14:49,273 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:14:49,273 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 257.7m
2014-07-21 02:14:49,440 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:14:51,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:51,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20216 synced till here 20214
2014-07-21 02:14:51,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934075288 with entries=133, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934091151
2014-07-21 02:14:52,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:52,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20310 synced till here 20308
2014-07-21 02:14:52,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934091151 with entries=94, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934092708
2014-07-21 02:14:53,459 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:53,478 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20407 synced till here 20398
2014-07-21 02:14:53,523 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934092708 with entries=97, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934093459
2014-07-21 02:14:55,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:56,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934093459 with entries=95, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934095949
2014-07-21 02:14:56,119 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:14:56,119 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:14:56,124 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:14:56,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:14:56,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:14:56,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:14:56,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:14:56,125 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:14:57,454 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:14:57,454 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.0m
2014-07-21 02:14:57,608 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:14:58,822 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:14:58,874 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20596 synced till here 20595
2014-07-21 02:14:58,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934095949 with entries=94, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934098822
2014-07-21 02:15:00,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:00,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20684 synced till here 20682
2014-07-21 02:15:00,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934098822 with entries=88, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934100361
2014-07-21 02:15:00,955 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6400, memsize=257.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/87d61fe1bc7b41ecaec22f73e4d67bfd
2014-07-21 02:15:00,973 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/87d61fe1bc7b41ecaec22f73e4d67bfd as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/87d61fe1bc7b41ecaec22f73e4d67bfd
2014-07-21 02:15:00,997 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/87d61fe1bc7b41ecaec22f73e4d67bfd, entries=938400, sequenceid=6400, filesize=66.8m
2014-07-21 02:15:00,998 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.7m/270252240, currentsize=194.2m/203613280 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 11725ms, sequenceid=6400, compaction requested=true
2014-07-21 02:15:00,998 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:15:00,998 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:15:00,998 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:15:00,998 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:15:00,998 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:15:00,999 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:15:02,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:02,633 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20778 synced till here 20777
2014-07-21 02:15:02,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934100361 with entries=94, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934102609
2014-07-21 02:15:04,642 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:04,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20871 synced till here 20869
2014-07-21 02:15:04,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934102609 with entries=93, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934104642
2014-07-21 02:15:04,933 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:15:04,933 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:15:04,934 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:15:04,934 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:15:04,934 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:15:04,934 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:15:04,934 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:15:04,934 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:15:06,883 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4676, memsize=249.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/2683b0690b6d4659b5a34d45b2bd327f
2014-07-21 02:15:06,902 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/2683b0690b6d4659b5a34d45b2bd327f as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/2683b0690b6d4659b5a34d45b2bd327f
2014-07-21 02:15:06,923 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/2683b0690b6d4659b5a34d45b2bd327f, entries=909800, sequenceid=4676, filesize=64.8m
2014-07-21 02:15:06,924 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.6m/269063600, currentsize=45.0m/47210560 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 9470ms, sequenceid=4676, compaction requested=true
2014-07-21 02:15:06,924 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:15:06,924 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-21 02:15:06,924 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-21 02:15:06,924 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:15:06,924 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:15:06,924 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:15:20,993 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:15:22,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:23,410 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934104642 with entries=104, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934122872
2014-07-21 02:15:23,410 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405933998881
2014-07-21 02:15:23,410 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934003655
2014-07-21 02:15:23,410 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934005550
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934019379
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934021989
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934023695
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934025959
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934030055
2014-07-21 02:15:23,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934032162
2014-07-21 02:15:25,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:26,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21078 synced till here 21074
2014-07-21 02:15:26,278 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934122872 with entries=103, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934125676
2014-07-21 02:15:29,237 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:29,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934125676 with entries=81, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934129238
2014-07-21 02:15:30,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:30,745 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21244 synced till here 21243
2014-07-21 02:15:30,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934129238 with entries=85, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934130718
2014-07-21 02:15:33,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:33,111 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21333 synced till here 21326
2014-07-21 02:15:33,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934130718 with entries=89, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934133074
2014-07-21 02:15:35,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:35,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21420 synced till here 21419
2014-07-21 02:15:35,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934133074 with entries=87, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934135730
2014-07-21 02:15:37,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:37,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21514 synced till here 21503
2014-07-21 02:15:38,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934135730 with entries=94, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934137872
2014-07-21 02:15:39,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:39,663 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21600 synced till here 21595
2014-07-21 02:15:39,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934137872 with entries=86, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934139646
2014-07-21 02:15:41,095 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:41,122 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934139646 with entries=90, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934141096
2014-07-21 02:15:44,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:15:44,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21788 synced till here 21778
2014-07-21 02:15:44,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934141096 with entries=98, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934144628
2014-07-21 02:15:57,999 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90609ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:15:58,000 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.0g
2014-07-21 02:15:58,622 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:16:03,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:03,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21917 synced till here 21912
2014-07-21 02:16:03,523 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934144628 with entries=129, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934163390
2014-07-21 02:16:04,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:04,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22000 synced till here 21998
2014-07-21 02:16:04,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934163390 with entries=83, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934164767
2014-07-21 02:16:05,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:06,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934164767 with entries=103, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934165916
2014-07-21 02:16:08,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:08,917 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22201 synced till here 22199
2014-07-21 02:16:08,981 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934165916 with entries=98, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934168794
2014-07-21 02:16:10,874 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:11,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22330 synced till here 22329
2014-07-21 02:16:11,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934168794 with entries=129, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934170874
2014-07-21 02:16:13,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:13,821 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934170874 with entries=99, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934173799
2014-07-21 02:16:16,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:16,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934173799 with entries=102, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934176219
2014-07-21 02:16:17,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:18,447 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934176219 with entries=110, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934177977
2014-07-21 02:16:26,960 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90841ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:16:26,960 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.1g
2014-07-21 02:16:27,766 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:16:29,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:29,948 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934177977 with entries=119, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934189514
2014-07-21 02:16:30,453 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:16:30,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:30,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934189514 with entries=85, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934190792
2014-07-21 02:16:32,638 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4985, memsize=1008.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/41d2f358c79e48a5b0d69a0f101a4c97
2014-07-21 02:16:32,673 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/41d2f358c79e48a5b0d69a0f101a4c97 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/41d2f358c79e48a5b0d69a0f101a4c97
2014-07-21 02:16:32,702 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/41d2f358c79e48a5b0d69a0f101a4c97, entries=3671460, sequenceid=4985, filesize=261.2m
2014-07-21 02:16:32,703 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1092678960, currentsize=383.3m/401931520 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 34704ms, sequenceid=4985, compaction requested=true
2014-07-21 02:16:32,703 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:16:32,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:16:32,703 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:16:32,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:16:32,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:16:32,704 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 269.5m
2014-07-21 02:16:32,704 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:16:32,861 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:16:33,175 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:16:33,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:34,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934190792 with entries=125, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934193613
2014-07-21 02:16:34,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934034029
2014-07-21 02:16:34,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934035807
2014-07-21 02:16:34,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934037398
2014-07-21 02:16:34,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934056981
2014-07-21 02:16:34,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934058298
2014-07-21 02:16:34,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934061635
2014-07-21 02:16:40,951 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5182, memsize=259.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/5236138f2c6e4411927d3fb24679857a
2014-07-21 02:16:40,968 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/5236138f2c6e4411927d3fb24679857a as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/5236138f2c6e4411927d3fb24679857a
2014-07-21 02:16:41,064 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/5236138f2c6e4411927d3fb24679857a, entries=945330, sequenceid=5182, filesize=67.3m
2014-07-21 02:16:41,065 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~269.5m/282556880, currentsize=3.7m/3924960 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8361ms, sequenceid=5182, compaction requested=true
2014-07-21 02:16:41,066 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:16:41,066 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:16:41,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-21 02:16:41,066 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:16:41,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-21 02:16:41,066 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 96133ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:16:41,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:16:41,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:16:41,066 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 1.0g
2014-07-21 02:16:41,066 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:16:41,067 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:16:41,067 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:16:41,067 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:16:41,067 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:16:41,067 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:16:41,751 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:16:46,796 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:47,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23090 synced till here 23061
2014-07-21 02:16:47,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934193613 with entries=120, filesize=80.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934206796
2014-07-21 02:16:49,553 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:49,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23179 synced till here 23178
2014-07-21 02:16:49,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934206796 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934209553
2014-07-21 02:16:50,936 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:51,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934209553 with entries=106, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934210936
2014-07-21 02:16:53,144 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:16:53,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23368 synced till here 23367
2014-07-21 02:16:53,182 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934210936 with entries=83, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934213145
2014-07-21 02:17:03,263 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5147, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/7474a424be0a4c8cbfdac291b5510bd4
2014-07-21 02:17:03,281 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/7474a424be0a4c8cbfdac291b5510bd4 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/7474a424be0a4c8cbfdac291b5510bd4
2014-07-21 02:17:03,319 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/7474a424be0a4c8cbfdac291b5510bd4, entries=3830790, sequenceid=5147, filesize=272.6m
2014-07-21 02:17:03,320 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1138967840, currentsize=289.5m/303563760 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 36359ms, sequenceid=5147, compaction requested=true
2014-07-21 02:17:03,320 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:17:03,320 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:17:03,320 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:17:03,320 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:17:03,320 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:17:03,320 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:17:05,905 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:17:05,906 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:17:05,906 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:17:05,906 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:17:05,907 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:17:05,907 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:17:05,907 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:17:05,907 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:17:05,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:06,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23464 synced till here 23462
2014-07-21 02:17:06,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934213145 with entries=96, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934225940
2014-07-21 02:17:06,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934064221
2014-07-21 02:17:06,244 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934067423
2014-07-21 02:17:06,245 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934068744
2014-07-21 02:17:06,245 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934072269
2014-07-21 02:17:09,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:09,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23602 synced till here 23599
2014-07-21 02:17:09,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934225940 with entries=138, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934229831
2014-07-21 02:17:11,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:11,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23687 synced till here 23686
2014-07-21 02:17:11,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934229831 with entries=85, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934231526
2014-07-21 02:17:13,205 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:13,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23780 synced till here 23776
2014-07-21 02:17:13,325 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934231526 with entries=93, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934233206
2014-07-21 02:17:14,912 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:14,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23874 synced till here 23873
2014-07-21 02:17:14,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934233206 with entries=94, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934234912
2014-07-21 02:17:17,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:17,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23975 synced till here 23974
2014-07-21 02:17:17,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934234912 with entries=101, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934237476
2014-07-21 02:17:18,465 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7467, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/86adf5cde7c740d28e7b27b055159a54
2014-07-21 02:17:18,483 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/86adf5cde7c740d28e7b27b055159a54 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/86adf5cde7c740d28e7b27b055159a54
2014-07-21 02:17:18,515 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/86adf5cde7c740d28e7b27b055159a54, entries=3728790, sequenceid=7467, filesize=265.3m
2014-07-21 02:17:18,516 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1091607840, currentsize=358.3m/375690880 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 37450ms, sequenceid=7467, compaction requested=true
2014-07-21 02:17:18,516 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:17:18,516 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:17:18,517 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:17:18,517 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:17:18,517 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:17:18,517 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:17:18,521 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:17:18,522 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:17:18,522 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:17:18,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:17:18,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:17:18,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:17:18,522 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:17:18,522 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:17:19,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:19,468 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24061 synced till here 24057
2014-07-21 02:17:19,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934237476 with entries=86, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934239449
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934075288
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934091151
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934092708
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934093459
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934095949
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934098822
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934100361
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934102609
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934104642
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934122872
2014-07-21 02:17:19,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934125676
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934129238
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934130718
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934133074
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934135730
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934137872
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934139646
2014-07-21 02:17:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934141096
2014-07-21 02:17:22,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:22,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934239449 with entries=94, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934242047
2014-07-21 02:17:24,069 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:24,103 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934242047 with entries=79, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934244070
2014-07-21 02:17:38,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:38,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24330 synced till here 24329
2014-07-21 02:17:38,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934244070 with entries=96, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934258127
2014-07-21 02:17:40,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:40,676 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934258127 with entries=93, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934260614
2014-07-21 02:17:42,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:42,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24535 synced till here 24533
2014-07-21 02:17:42,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934260614 with entries=112, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934262220
2014-07-21 02:17:44,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:44,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934262220 with entries=97, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934264286
2014-07-21 02:17:46,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:46,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24713 synced till here 24712
2014-07-21 02:17:46,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934264286 with entries=81, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934266711
2014-07-21 02:17:52,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:52,730 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934266711 with entries=110, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934272678
2014-07-21 02:17:54,754 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:54,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24915 synced till here 24913
2014-07-21 02:17:54,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934272678 with entries=92, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934274755
2014-07-21 02:17:56,474 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:56,491 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25018 synced till here 25015
2014-07-21 02:17:56,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934274755 with entries=103, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934276474
2014-07-21 02:17:58,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:17:58,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25108 synced till here 25106
2014-07-21 02:17:58,458 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934276474 with entries=90, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934278410
2014-07-21 02:17:58,460 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:18:03,248 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90073ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:18:03,248 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.2g
2014-07-21 02:18:03,959 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:18:17,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:17,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25201 synced till here 25198
2014-07-21 02:18:17,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934278410 with entries=93, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934297535
2014-07-21 02:18:17,877 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:18:17,877 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.4m
2014-07-21 02:18:18,048 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:18:21,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:21,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934297535 with entries=127, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934301583
2014-07-21 02:18:22,849 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:22,868 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25418 synced till here 25411
2014-07-21 02:18:22,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934301583 with entries=90, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934302850
2014-07-21 02:18:24,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:24,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25519 synced till here 25518
2014-07-21 02:18:24,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934302850 with entries=101, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934304865
2014-07-21 02:18:26,426 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5657, memsize=255.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/27bc1943049541ef9112fd450ac3e90d
2014-07-21 02:18:26,440 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/27bc1943049541ef9112fd450ac3e90d as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/27bc1943049541ef9112fd450ac3e90d
2014-07-21 02:18:26,455 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/27bc1943049541ef9112fd450ac3e90d, entries=931310, sequenceid=5657, filesize=66.3m
2014-07-21 02:18:26,456 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268882320, currentsize=33.4m/35008960 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8579ms, sequenceid=5657, compaction requested=true
2014-07-21 02:18:26,456 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:18:26,456 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-21 02:18:26,456 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-21 02:18:26,456 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:18:26,456 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:18:26,457 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:18:28,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:28,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25620 synced till here 25610
2014-07-21 02:18:28,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934304865 with entries=101, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934308003
2014-07-21 02:18:29,405 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:29,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25708 synced till here 25705
2014-07-21 02:18:29,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934308003 with entries=88, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934309405
2014-07-21 02:18:32,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:32,581 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25812 synced till here 25807
2014-07-21 02:18:32,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934309405 with entries=104, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934312563
2014-07-21 02:18:34,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:34,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25904 synced till here 25901
2014-07-21 02:18:34,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934312563 with entries=92, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934314269
2014-07-21 02:18:35,145 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:18:35,267 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.2g vs best flushable region's 78.9m. Choosing the bigger.
2014-07-21 02:18:35,267 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:18:35,267 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.2g
2014-07-21 02:18:36,150 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:18:36,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:37,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26000 synced till here 25997
2014-07-21 02:18:37,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934314269 with entries=96, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934316998
2014-07-21 02:18:38,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:38,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26082 synced till here 26081
2014-07-21 02:18:38,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934316998 with entries=82, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934318816
2014-07-21 02:18:39,045 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:39,050 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:39,078 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:41,163 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:41,196 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:43,167 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:18:43,937 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5666, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/6c50212da871437384e80aae992bbfc2
2014-07-21 02:18:43,962 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/6c50212da871437384e80aae992bbfc2 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/6c50212da871437384e80aae992bbfc2
2014-07-21 02:18:44,036 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/6c50212da871437384e80aae992bbfc2, entries=4456940, sequenceid=5666, filesize=317.1m
2014-07-21 02:18:44,037 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1285699520, currentsize=330.0m/346074400 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 40789ms, sequenceid=5666, compaction requested=true
2014-07-21 02:18:44,037 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:18:44,038 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:18:44,038 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 872ms
2014-07-21 02:18:44,038 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:18:44,038 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,038 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:18:44,038 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2843ms
2014-07-21 02:18:44,038 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,038 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:18:44,039 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2876ms
2014-07-21 02:18:44,039 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,039 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:18:44,042 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4963ms
2014-07-21 02:18:44,042 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,043 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4992ms
2014-07-21 02:18:44,043 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,043 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4998ms
2014-07-21 02:18:44,044 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:18:44,165 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:18:44,165 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:18:44,165 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:18:44,165 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:18:44,165 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:18:44,166 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:18:44,166 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:18:44,166 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:18:48,539 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90018ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:18:48,540 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 1.1g
2014-07-21 02:18:49,188 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:18:55,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:55,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934318816 with entries=114, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934335744
2014-07-21 02:18:55,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934144628
2014-07-21 02:18:55,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934163390
2014-07-21 02:18:55,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934164767
2014-07-21 02:18:55,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934165916
2014-07-21 02:18:55,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934168794
2014-07-21 02:18:55,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934170874
2014-07-21 02:18:55,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934173799
2014-07-21 02:18:55,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934176219
2014-07-21 02:18:57,917 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:18:57,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26301 synced till here 26292
2014-07-21 02:18:58,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934335744 with entries=105, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934337917
2014-07-21 02:19:00,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:01,020 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934337917 with entries=129, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934340992
2014-07-21 02:19:02,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:02,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934340992 with entries=122, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934342426
2014-07-21 02:19:05,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:05,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26638 synced till here 26637
2014-07-21 02:19:05,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934342426 with entries=86, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934345374
2014-07-21 02:19:07,540 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:07,579 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934345374 with entries=89, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934347540
2014-07-21 02:19:10,135 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:10,167 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934347540 with entries=98, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934350135
2014-07-21 02:19:12,017 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:12,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26930 synced till here 26928
2014-07-21 02:19:12,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934350135 with entries=105, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934352017
2014-07-21 02:19:14,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:14,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27024 synced till here 27020
2014-07-21 02:19:14,316 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934352017 with entries=94, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934354266
2014-07-21 02:19:15,542 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5810, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/9940ee78de1b4bdd9178fbe0a6046224
2014-07-21 02:19:15,562 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/9940ee78de1b4bdd9178fbe0a6046224 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/9940ee78de1b4bdd9178fbe0a6046224
2014-07-21 02:19:15,580 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/9940ee78de1b4bdd9178fbe0a6046224, entries=4375610, sequenceid=5810, filesize=311.3m
2014-07-21 02:19:15,581 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1262395040, currentsize=381.4m/399905600 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 40314ms, sequenceid=5810, compaction requested=true
2014-07-21 02:19:15,582 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:19:15,582 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:19:15,582 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:19:15,582 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:19:15,582 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:19:15,582 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:19:25,969 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:19:25,969 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:19:25,970 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:19:25,970 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:19:25,970 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:19:25,970 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:19:25,970 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:19:25,971 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:19:27,692 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8658, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/2346d7b27bb845a2818d707af9a36068
2014-07-21 02:19:27,708 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/2346d7b27bb845a2818d707af9a36068 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/2346d7b27bb845a2818d707af9a36068
2014-07-21 02:19:27,721 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/2346d7b27bb845a2818d707af9a36068, entries=4168610, sequenceid=8658, filesize=296.6m
2014-07-21 02:19:27,722 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1200526000, currentsize=342.9m/359543840 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 39182ms, sequenceid=8658, compaction requested=true
2014-07-21 02:19:27,722 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:19:27,722 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:19:27,723 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:19:27,723 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:19:27,723 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:19:27,723 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:19:27,727 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:19:27,727 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:19:27,727 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:19:27,727 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:19:27,727 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:19:27,727 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:19:27,727 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:19:27,727 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:19:28,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:28,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934354266 with entries=131, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934368382
2014-07-21 02:19:28,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934177977
2014-07-21 02:19:28,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934189514
2014-07-21 02:19:28,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934190792
2014-07-21 02:19:28,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934193613
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934206796
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934209553
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934210936
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934213145
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934225940
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934229831
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934231526
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934233206
2014-07-21 02:19:28,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934234912
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934237476
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934239449
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934242047
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934244070
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934258127
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934260614
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934262220
2014-07-21 02:19:28,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934264286
2014-07-21 02:19:28,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934266711
2014-07-21 02:19:28,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934272678
2014-07-21 02:19:28,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934274755
2014-07-21 02:19:28,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934276474
2014-07-21 02:19:30,021 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:30,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934368382 with entries=95, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934370022
2014-07-21 02:19:34,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:34,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934370022 with entries=90, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934374062
2014-07-21 02:19:35,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:35,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27419 synced till here 27417
2014-07-21 02:19:35,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934374062 with entries=79, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934375522
2014-07-21 02:19:37,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:38,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27511 synced till here 27510
2014-07-21 02:19:38,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934375522 with entries=92, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934377939
2014-07-21 02:19:39,436 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:19:39,436 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.0m
2014-07-21 02:19:39,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:19:39,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:39,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934377939 with entries=83, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934379687
2014-07-21 02:19:41,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:41,114 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27669 synced till here 27667
2014-07-21 02:19:41,135 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934379687 with entries=75, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934381097
2014-07-21 02:19:42,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:42,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27754 synced till here 27747
2014-07-21 02:19:42,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934381097 with entries=85, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934382465
2014-07-21 02:19:45,243 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:19:45,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27847 synced till here 27846
2014-07-21 02:19:45,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934382465 with entries=93, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934385244
2014-07-21 02:19:48,072 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6119, memsize=250.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/920f88fb3012495a84f47f356c3c8736
2014-07-21 02:19:48,092 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/920f88fb3012495a84f47f356c3c8736 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/920f88fb3012495a84f47f356c3c8736
2014-07-21 02:19:48,102 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/920f88fb3012495a84f47f356c3c8736, entries=912640, sequenceid=6119, filesize=64.9m
2014-07-21 02:19:48,102 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.7m/269123760, currentsize=36.8m/38623600 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 8666ms, sequenceid=6119, compaction requested=true
2014-07-21 02:19:48,102 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:19:48,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-21 02:19:48,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-21 02:19:48,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:19:48,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:19:48,103 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:20:03,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:03,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27979 synced till here 27976
2014-07-21 02:20:03,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934385244 with entries=132, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934403806
2014-07-21 02:20:04,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:04,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28064 synced till here 28062
2014-07-21 02:20:05,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934403806 with entries=85, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934404780
2014-07-21 02:20:08,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:08,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934404780 with entries=88, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934408864
2014-07-21 02:20:11,408 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:11,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934408864 with entries=83, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934411408
2014-07-21 02:20:12,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:13,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28318 synced till here 28315
2014-07-21 02:20:13,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934411408 with entries=83, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934412993
2014-07-21 02:20:13,096 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:20:14,840 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90675ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:20:14,840 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.1g
2014-07-21 02:20:15,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:15,211 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28417 synced till here 28413
2014-07-21 02:20:15,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934412993 with entries=99, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934415189
2014-07-21 02:20:15,675 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:20:17,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:17,645 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28498 synced till here 28496
2014-07-21 02:20:17,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934415189 with entries=81, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934417627
2014-07-21 02:20:19,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:19,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934417627 with entries=98, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934419426
2014-07-21 02:20:20,993 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:20:31,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:31,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28703 synced till here 28701
2014-07-21 02:20:31,642 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934419426 with entries=107, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934431542
2014-07-21 02:20:34,358 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:34,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28828 synced till here 28817
2014-07-21 02:20:34,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934431542 with entries=125, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934434358
2014-07-21 02:20:35,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:35,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934434358 with entries=90, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934435906
2014-07-21 02:20:41,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:42,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29023 synced till here 29021
2014-07-21 02:20:42,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934435906 with entries=105, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934441988
2014-07-21 02:20:44,964 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:44,974 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:20:44,975 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.1g vs best flushable region's 169.3m. Choosing the bigger.
2014-07-21 02:20:44,975 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:20:44,975 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.1g
2014-07-21 02:20:45,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29132 synced till here 29109
2014-07-21 02:20:45,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934441988 with entries=109, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934444964
2014-07-21 02:20:46,398 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:20:47,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:47,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934444964 with entries=92, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934447613
2014-07-21 02:20:48,056 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,070 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,083 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,091 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,097 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,103 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,106 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,402 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,411 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,441 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,462 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,503 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,523 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,534 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,535 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,579 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,583 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,626 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,654 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,679 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,723 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,766 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,809 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,836 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,850 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,926 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,942 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:48,997 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,026 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,064 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,272 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,317 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,331 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,356 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,383 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,410 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,429 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,487 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,516 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,530 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:49,991 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,012 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,039 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,049 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,098 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,655 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,658 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,694 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,749 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:50,917 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:20:53,588 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-21 02:20:53,589 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5487ms
2014-07-21 02:20:53,589 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5533ms
2014-07-21 02:20:53,589 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-21 02:20:53,590 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5520ms
2014-07-21 02:20:53,590 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5499ms
2014-07-21 02:20:53,590 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5494ms
2014-07-21 02:20:53,590 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5484ms
2014-07-21 02:20:53,591 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5189ms
2014-07-21 02:20:53,591 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-07-21 02:20:53,591 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5150ms
2014-07-21 02:20:53,591 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5129ms
2014-07-21 02:20:53,592 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5089ms
2014-07-21 02:20:53,592 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5069ms
2014-07-21 02:20:53,592 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5058ms
2014-07-21 02:20:53,592 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5013ms
2014-07-21 02:20:53,592 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5057ms
2014-07-21 02:20:53,627 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:53,655 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:20:53,680 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:53,723 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:53,766 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:53,809 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:53,836 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:53,851 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:53,926 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:53,942 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:53,998 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:54,027 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:54,064 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:54,273 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:54,294 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6312, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/90cf6c9a4baa4cb3833f9f6a807cab15
2014-07-21 02:20:54,305 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/90cf6c9a4baa4cb3833f9f6a807cab15 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/90cf6c9a4baa4cb3833f9f6a807cab15
2014-07-21 02:20:54,317 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:54,326 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/90cf6c9a4baa4cb3833f9f6a807cab15, entries=4177210, sequenceid=6312, filesize=297.1m
2014-07-21 02:20:54,326 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1233429760, currentsize=324.6m/340404000 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 39486ms, sequenceid=6312, compaction requested=true
2014-07-21 02:20:54,327 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:20:54,327 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:20:54,327 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-21 02:20:54,327 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,327 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:20:54,327 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5056ms
2014-07-21 02:20:54,328 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,327 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:20:54,328 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5264ms
2014-07-21 02:20:54,328 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,328 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:20:54,328 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:20:54,328 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5302ms
2014-07-21 02:20:54,328 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,329 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5332ms
2014-07-21 02:20:54,329 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,330 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5388ms
2014-07-21 02:20:54,330 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,330 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5404ms
2014-07-21 02:20:54,330 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,330 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5480ms
2014-07-21 02:20:54,330 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,331 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:54,331 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,334 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5498ms
2014-07-21 02:20:54,334 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,337 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5528ms
2014-07-21 02:20:54,337 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,337 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5572ms
2014-07-21 02:20:54,338 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,338 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5615ms
2014-07-21 02:20:54,338 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,338 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5659ms
2014-07-21 02:20:54,338 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,338 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5685ms
2014-07-21 02:20:54,338 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,338 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5712ms
2014-07-21 02:20:54,338 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,344 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5809ms
2014-07-21 02:20:54,344 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,344 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5765ms
2014-07-21 02:20:54,344 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,344 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5810ms
2014-07-21 02:20:54,344 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,344 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5821ms
2014-07-21 02:20:54,345 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,346 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5843ms
2014-07-21 02:20:54,346 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,346 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5884ms
2014-07-21 02:20:54,346 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,347 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5906ms
2014-07-21 02:20:54,347 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,347 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5936ms
2014-07-21 02:20:54,347 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,349 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5947ms
2014-07-21 02:20:54,349 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,350 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6244ms
2014-07-21 02:20:54,350 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,350 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6254ms
2014-07-21 02:20:54,351 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,351 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6260ms
2014-07-21 02:20:54,351 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,351 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6281ms
2014-07-21 02:20:54,351 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,357 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:20:54,357 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,357 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6274ms
2014-07-21 02:20:54,357 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,361 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6305ms
2014-07-21 02:20:54,361 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,361 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6259ms
2014-07-21 02:20:54,361 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,362 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5779ms
2014-07-21 02:20:54,362 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,367 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3450ms
2014-07-21 02:20:54,367 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,367 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3618ms
2014-07-21 02:20:54,367 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,367 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3673ms
2014-07-21 02:20:54,368 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,373 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3715ms
2014-07-21 02:20:54,373 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,373 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3718ms
2014-07-21 02:20:54,374 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,374 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4276ms
2014-07-21 02:20:54,374 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,377 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4328ms
2014-07-21 02:20:54,377 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,383 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:20:54,383 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,385 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4346ms
2014-07-21 02:20:54,385 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,386 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4373ms
2014-07-21 02:20:54,386 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,393 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4402ms
2014-07-21 02:20:54,393 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,393 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4863ms
2014-07-21 02:20:54,394 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,394 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4879ms
2014-07-21 02:20:54,394 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,401 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4914ms
2014-07-21 02:20:54,401 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,401 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4972ms
2014-07-21 02:20:54,402 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,402 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4992ms
2014-07-21 02:20:54,402 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:20:54,634 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:20:54,634 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:20:54,635 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:20:54,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:20:54,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:20:54,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:20:54,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:20:54,635 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:20:54,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:54,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29335 synced till here 29323
2014-07-21 02:20:54,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934447613 with entries=111, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934454829
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934278410
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934297535
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934301583
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934302850
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934304865
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934308003
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934309405
2014-07-21 02:20:54,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934312563
2014-07-21 02:20:56,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:56,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29464 synced till here 29448
2014-07-21 02:20:56,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934454829 with entries=129, filesize=95.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934456750
2014-07-21 02:20:58,042 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90315ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:20:58,043 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 1.2g
2014-07-21 02:20:58,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:20:58,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29601 synced till here 29579
2014-07-21 02:20:59,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934456750 with entries=137, filesize=104.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934458787
2014-07-21 02:21:00,109 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:21:00,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:00,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29714 synced till here 29689
2014-07-21 02:21:00,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934458787 with entries=113, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934460697
2014-07-21 02:21:02,804 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:02,918 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:21:03,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29867 synced till here 29848
2014-07-21 02:21:03,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934460697 with entries=153, filesize=117.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934462805
2014-07-21 02:21:04,407 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:05,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29972 synced till here 29955
2014-07-21 02:21:05,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934462805 with entries=105, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934464408
2014-07-21 02:21:06,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:06,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934464408 with entries=85, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934466052
2014-07-21 02:21:12,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:12,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30177 synced till here 30176
2014-07-21 02:21:12,490 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934466052 with entries=120, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934472436
2014-07-21 02:21:12,607 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,651 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,654 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,666 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,672 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,682 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,683 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,694 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,694 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,697 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,702 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,704 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,719 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,863 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,864 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,864 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,865 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,865 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,866 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:12,949 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,019 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,058 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,900 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,903 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,903 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,904 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,908 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,915 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,916 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,918 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:13,972 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,000 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,045 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,076 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,107 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,147 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,188 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,223 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,231 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,246 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,258 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,273 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,343 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,378 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,418 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,443 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,466 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,495 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,525 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:14,548 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:17,698 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-21 02:21:17,698 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,699 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5091ms
2014-07-21 02:21:17,699 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-07-21 02:21:17,699 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5045ms
2014-07-21 02:21:17,699 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5033ms
2014-07-21 02:21:17,700 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5028ms
2014-07-21 02:21:17,700 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5018ms
2014-07-21 02:21:17,700 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-07-21 02:21:17,700 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5017ms
2014-07-21 02:21:17,702 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,705 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,720 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,864 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,864 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,864 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:17,865 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,866 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,867 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:17,949 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:18,020 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,058 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:18,901 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,904 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,904 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,906 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,909 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:18,916 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:18,916 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:18,918 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:18,973 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,000 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,045 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,077 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,108 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,148 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,189 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,224 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,231 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,247 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,259 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,273 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,343 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,379 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,418 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,443 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,467 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,495 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:21:19,525 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:19,549 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:21:22,762 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10043ms
2014-07-21 02:21:22,763 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10097ms
2014-07-21 02:21:22,763 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1064ms
GC pool 'ParNew' had collection(s): count=1 time=1069ms
2014-07-21 02:21:22,764 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10092ms
2014-07-21 02:21:22,765 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10082ms
2014-07-21 02:21:22,765 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10072ms
2014-07-21 02:21:22,765 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10082ms
2014-07-21 02:21:22,766 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10064ms
2014-07-21 02:21:22,766 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10062ms
2014-07-21 02:21:22,766 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10072ms
2014-07-21 02:21:22,767 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10070ms
2014-07-21 02:21:22,767 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10160ms
2014-07-21 02:21:22,768 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10116ms
2014-07-21 02:21:22,768 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10114ms
2014-07-21 02:21:22,864 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,864 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,865 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,866 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,866 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,867 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:22,949 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:23,021 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:23,058 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:23,902 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:23,904 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:23,906 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:23,906 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:23,910 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:23,917 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:23,917 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:23,919 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:23,973 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,001 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,046 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,078 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:24,108 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,149 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,189 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,225 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,232 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,248 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,259 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,273 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:24,343 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:24,379 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,419 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:24,444 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,468 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:24,495 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:21:24,526 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:21:24,549 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:21:27,763 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15044ms
2014-07-21 02:21:27,764 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15098ms
2014-07-21 02:21:27,764 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15092ms
2014-07-21 02:21:27,765 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15083ms
2014-07-21 02:21:27,766 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15072ms
2014-07-21 02:21:27,766 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15083ms
2014-07-21 02:21:27,766 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15065ms
2014-07-21 02:21:27,767 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15063ms
2014-07-21 02:21:27,767 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15073ms
2014-07-21 02:21:27,767 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15070ms
2014-07-21 02:21:27,768 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15161ms
2014-07-21 02:21:27,768 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15114ms
2014-07-21 02:21:27,769 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15118ms
2014-07-21 02:21:28,739 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15681ms
2014-07-21 02:21:28,740 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15877ms
2014-07-21 02:21:28,740 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15876ms
2014-07-21 02:21:28,740 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15876ms
2014-07-21 02:21:28,741 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15877ms
2014-07-21 02:21:28,741 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15876ms
2014-07-21 02:21:28,741 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15875ms
2014-07-21 02:21:28,741 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15792ms
2014-07-21 02:21:28,741 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15722ms
2014-07-21 02:21:28,902 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:28,905 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:21:28,906 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:21:28,907 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:28,910 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:28,917 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:28,917 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:28,920 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:28,974 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,001 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,046 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,078 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,108 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,149 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,190 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,225 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,232 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,248 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,260 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,273 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-21 02:21:29,344 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,380 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,420 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,445 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,468 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,496 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:21:29,526 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:29,550 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:21:30,734 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6459, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/51bc20ae085049389d396e94c93da51d
2014-07-21 02:21:30,756 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/51bc20ae085049389d396e94c93da51d as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/51bc20ae085049389d396e94c93da51d
2014-07-21 02:21:30,773 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/51bc20ae085049389d396e94c93da51d, entries=4165390, sequenceid=6459, filesize=296.3m
2014-07-21 02:21:30,774 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1230342400, currentsize=424.4m/445025280 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 45798ms, sequenceid=6459, compaction requested=true
2014-07-21 02:21:30,774 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:21:30,774 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:21:30,774 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16226ms
2014-07-21 02:21:30,775 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,775 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:21:30,775 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16251ms
2014-07-21 02:21:30,775 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:21:30,775 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 304.6m
2014-07-21 02:21:30,775 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:21:30,775 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,775 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:21:30,775 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16280ms
2014-07-21 02:21:30,776 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,776 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16310ms
2014-07-21 02:21:30,776 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,776 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16333ms
2014-07-21 02:21:30,776 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,776 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16358ms
2014-07-21 02:21:30,776 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,777 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16399ms
2014-07-21 02:21:30,777 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,789 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16446ms
2014-07-21 02:21:30,789 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16516ms
2014-07-21 02:21:30,789 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,797 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16539ms
2014-07-21 02:21:30,797 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,797 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16551ms
2014-07-21 02:21:30,797 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,798 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16567ms
2014-07-21 02:21:30,798 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,809 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16586ms
2014-07-21 02:21:30,809 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,809 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16621ms
2014-07-21 02:21:30,809 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,813 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16666ms
2014-07-21 02:21:30,813 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,813 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16706ms
2014-07-21 02:21:30,813 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,813 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16737ms
2014-07-21 02:21:30,814 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,814 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16769ms
2014-07-21 02:21:30,814 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,814 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16814ms
2014-07-21 02:21:30,814 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,825 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16853ms
2014-07-21 02:21:30,825 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,825 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16907ms
2014-07-21 02:21:30,825 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,826 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16910ms
2014-07-21 02:21:30,826 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,826 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16911ms
2014-07-21 02:21:30,826 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,826 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16918ms
2014-07-21 02:21:30,826 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,826 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16922ms
2014-07-21 02:21:30,826 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,827 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16924ms
2014-07-21 02:21:30,827 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,827 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16925ms
2014-07-21 02:21:30,827 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,828 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16928ms
2014-07-21 02:21:30,828 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,828 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17809ms
2014-07-21 02:21:30,829 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,829 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17880ms
2014-07-21 02:21:30,829 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,830 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17964ms
2014-07-21 02:21:30,830 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,832 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17967ms
2014-07-21 02:21:30,832 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,832 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17969ms
2014-07-21 02:21:30,832 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,832 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17968ms
2014-07-21 02:21:30,832 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,832 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17968ms
2014-07-21 02:21:30,832 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,833 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17970ms
2014-07-21 02:21:30,833 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,833 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17775ms
2014-07-21 02:21:30,833 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,833 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18182ms
2014-07-21 02:21:30,833 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,833 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18179ms
2014-07-21 02:21:30,833 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,834 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18227ms
2014-07-21 02:21:30,834 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,834 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18137ms
2014-07-21 02:21:30,834 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,834 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18140ms
2014-07-21 02:21:30,834 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,836 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18132ms
2014-07-21 02:21:30,836 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,837 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18135ms
2014-07-21 02:21:30,837 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,837 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18154ms
2014-07-21 02:21:30,837 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,837 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18144ms
2014-07-21 02:21:30,837 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,838 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18156ms
2014-07-21 02:21:30,839 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,841 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18169ms
2014-07-21 02:21:30,841 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,841 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18175ms
2014-07-21 02:21:30,841 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,842 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18122ms
2014-07-21 02:21:30,842 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:30,985 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16714,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474271,"queuetimems":1,"class":"HRegionServer","responsesize":4202,"method":"Multi"}
2014-07-21 02:21:31,045 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18182,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472863,"queuetimems":0,"class":"HRegionServer","responsesize":14,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18182,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472863,"queuetimems":0,"class":"HRegionServer","responsesize":56,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16789,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474257,"queuetimems":1,"class":"HRegionServer","responsesize":4201,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17129,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473916,"queuetimems":0,"class":"HRegionServer","responsesize":177,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17130,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473915,"queuetimems":0,"class":"HRegionServer","responsesize":44,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18182,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472864,"queuetimems":1,"class":"HRegionServer","responsesize":147,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18352,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472694,"queuetimems":1,"class":"HRegionServer","responsesize":20,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472665,"queuetimems":0,"class":"HRegionServer","responsesize":4074,"method":"Multi"}
2014-07-21 02:21:31,052 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17150,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473902,"queuetimems":0,"class":"HRegionServer","responsesize":207,"method":"Multi"}
2014-07-21 02:21:31,046 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16817,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474229,"queuetimems":0,"class":"HRegionServer","responsesize":4028,"method":"Multi"}
2014-07-21 02:21:31,052 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472865,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-21 02:21:31,057 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473902,"queuetimems":0,"class":"HRegionServer","responsesize":813,"method":"Multi"}
2014-07-21 02:21:31,058 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472693,"queuetimems":0,"class":"HRegionServer","responsesize":68,"method":"Multi"}
2014-07-21 02:21:31,060 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472666,"queuetimems":0,"class":"HRegionServer","responsesize":237,"method":"Multi"}
2014-07-21 02:21:31,062 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:21:31,063 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472701,"queuetimems":0,"class":"HRegionServer","responsesize":1072,"method":"Multi"}
2014-07-21 02:21:31,063 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473898,"queuetimems":0,"class":"HRegionServer","responsesize":860,"method":"Multi"}
2014-07-21 02:21:31,064 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473899,"queuetimems":0,"class":"HRegionServer","responsesize":597,"method":"Multi"}
2014-07-21 02:21:31,064 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18382,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472681,"queuetimems":0,"class":"HRegionServer","responsesize":900,"method":"Multi"}
2014-07-21 02:21:31,065 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472865,"queuetimems":0,"class":"HRegionServer","responsesize":868,"method":"Multi"}
2014-07-21 02:21:31,066 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474243,"queuetimems":0,"class":"HRegionServer","responsesize":4216,"method":"Multi"}
2014-07-21 02:21:31,070 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472863,"queuetimems":0,"class":"HRegionServer","responsesize":1753,"method":"Multi"}
2014-07-21 02:21:31,072 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18378,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472693,"queuetimems":0,"class":"HRegionServer","responsesize":3897,"method":"Multi"}
2014-07-21 02:21:31,078 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472716,"queuetimems":0,"class":"HRegionServer","responsesize":3859,"method":"Multi"}
2014-07-21 02:21:31,078 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17163,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473915,"queuetimems":0,"class":"HRegionServer","responsesize":3725,"method":"Multi"}
2014-07-21 02:21:31,078 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18378,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472699,"queuetimems":0,"class":"HRegionServer","responsesize":2248,"method":"Multi"}
2014-07-21 02:21:31,078 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18399,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472679,"queuetimems":1,"class":"HRegionServer","responsesize":3759,"method":"Multi"}
2014-07-21 02:21:32,535 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:32,536 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472563,"queuetimems":0,"class":"HRegionServer","responsesize":12056,"method":"Multi"}
2014-07-21 02:21:32,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30289 synced till here 30266
2014-07-21 02:21:32,830 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18285,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474545,"queuetimems":0,"class":"HRegionServer","responsesize":11377,"method":"Multi"}
2014-07-21 02:21:32,831 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18339,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474492,"queuetimems":0,"class":"HRegionServer","responsesize":11278,"method":"Multi"}
2014-07-21 02:21:32,830 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18308,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474522,"queuetimems":0,"class":"HRegionServer","responsesize":8974,"method":"Multi"}
2014-07-21 02:21:32,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934472436 with entries=112, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934492535
2014-07-21 02:21:32,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934314269
2014-07-21 02:21:32,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934316998
2014-07-21 02:21:33,068 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18963,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474105,"queuetimems":0,"class":"HRegionServer","responsesize":11585,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473962,"queuetimems":0,"class":"HRegionServer","responsesize":12609,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18628,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474440,"queuetimems":0,"class":"HRegionServer","responsesize":12061,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18730,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474338,"queuetimems":0,"class":"HRegionServer","responsesize":11963,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474463,"queuetimems":0,"class":"HRegionServer","responsesize":12294,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474185,"queuetimems":1,"class":"HRegionServer","responsesize":11863,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20419,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472649,"queuetimems":0,"class":"HRegionServer","responsesize":11741,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473005,"queuetimems":1,"class":"HRegionServer","responsesize":12113,"method":"Multi"}
2014-07-21 02:21:33,068 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18693,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474375,"queuetimems":0,"class":"HRegionServer","responsesize":11703,"method":"Multi"}
2014-07-21 02:21:33,070 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18654,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474415,"queuetimems":0,"class":"HRegionServer","responsesize":12075,"method":"Multi"}
2014-07-21 02:21:33,070 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20025,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473044,"queuetimems":0,"class":"HRegionServer","responsesize":11803,"method":"Multi"}
2014-07-21 02:21:33,068 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473997,"queuetimems":1,"class":"HRegionServer","responsesize":12066,"method":"Multi"}
2014-07-21 02:21:33,068 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18998,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474070,"queuetimems":1,"class":"HRegionServer","responsesize":11578,"method":"Multi"}
2014-07-21 02:21:33,070 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472603,"queuetimems":1,"class":"HRegionServer","responsesize":12142,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18925,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474144,"queuetimems":0,"class":"HRegionServer","responsesize":11661,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474041,"queuetimems":0,"class":"HRegionServer","responsesize":12078,"method":"Multi"}
2014-07-21 02:21:33,069 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18848,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934474220,"queuetimems":0,"class":"HRegionServer","responsesize":12084,"method":"Multi"}
2014-07-21 02:21:33,562 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934472948,"queuetimems":0,"class":"HRegionServer","responsesize":11962,"method":"Multi"}
2014-07-21 02:21:33,583 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19685,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934473897,"queuetimems":1,"class":"HRegionServer","responsesize":13311,"method":"Multi"}
2014-07-21 02:21:34,431 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:34,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30432 synced till here 30400
2014-07-21 02:21:34,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934492535 with entries=143, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934494432
2014-07-21 02:21:36,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:36,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30590 synced till here 30558
2014-07-21 02:21:36,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934494432 with entries=158, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934496335
2014-07-21 02:21:37,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:38,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30756 synced till here 30719
2014-07-21 02:21:38,265 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934496335 with entries=166, filesize=107.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934497747
2014-07-21 02:21:39,783 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:39,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30889 synced till here 30857
2014-07-21 02:21:40,237 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934497747 with entries=133, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934499783
2014-07-21 02:21:41,900 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:42,234 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31025 synced till here 31022
2014-07-21 02:21:42,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934499783 with entries=136, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934501900
2014-07-21 02:21:43,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:43,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31143 synced till here 31119
2014-07-21 02:21:44,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934501900 with entries=118, filesize=83.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934503692
2014-07-21 02:21:44,363 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,363 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,364 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,365 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,365 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,442 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,445 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,451 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,451 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,452 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,452 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,452 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,453 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,453 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,453 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,453 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,453 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,454 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,454 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,454 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,454 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,456 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,457 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,457 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,457 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,457 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,458 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,458 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,464 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,520 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,520 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,520 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,521 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,521 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,522 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,522 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,523 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,524 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,526 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,526 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,527 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,528 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,528 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,529 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,536 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,536 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6687, memsize=219.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/7642f30b70464c9088e595a2f0d37f33
2014-07-21 02:21:44,537 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,538 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,538 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,541 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,545 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:44,556 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/7642f30b70464c9088e595a2f0d37f33 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/7642f30b70464c9088e595a2f0d37f33
2014-07-21 02:21:44,569 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/7642f30b70464c9088e595a2f0d37f33, entries=797680, sequenceid=6687, filesize=56.8m
2014-07-21 02:21:44,570 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~304.6m/319371280, currentsize=127.4m/133563200 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 13795ms, sequenceid=6687, compaction requested=true
2014-07-21 02:21:44,570 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:21:44,570 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-21 02:21:44,570 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-21 02:21:44,570 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:21:44,570 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25ms
2014-07-21 02:21:44,571 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,571 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:21:44,571 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-07-21 02:21:44,571 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,570 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:21:44,571 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33ms
2014-07-21 02:21:44,571 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:21:44,571 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,571 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:21:44,571 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:21:44,571 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:21:44,571 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:21:44,572 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:21:44,572 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:21:44,571 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34ms
2014-07-21 02:21:44,572 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,573 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 36ms
2014-07-21 02:21:44,573 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,573 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 37ms
2014-07-21 02:21:44,573 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,573 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 45ms
2014-07-21 02:21:44,573 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,574 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 46ms
2014-07-21 02:21:44,574 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,574 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 46ms
2014-07-21 02:21:44,574 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,574 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 47ms
2014-07-21 02:21:44,574 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,578 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 56ms
2014-07-21 02:21:44,578 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,579 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 53ms
2014-07-21 02:21:44,579 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,579 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 56ms
2014-07-21 02:21:44,579 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,579 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 57ms
2014-07-21 02:21:44,579 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,579 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 57ms
2014-07-21 02:21:44,579 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,580 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 57ms
2014-07-21 02:21:44,580 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,580 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 59ms
2014-07-21 02:21:44,580 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,580 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 59ms
2014-07-21 02:21:44,580 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,580 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 60ms
2014-07-21 02:21:44,580 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,585 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 65ms
2014-07-21 02:21:44,586 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,586 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 67ms
2014-07-21 02:21:44,586 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,586 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 122ms
2014-07-21 02:21:44,586 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,586 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 130ms
2014-07-21 02:21:44,586 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,593 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 139ms
2014-07-21 02:21:44,593 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,593 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 139ms
2014-07-21 02:21:44,593 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,593 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 136ms
2014-07-21 02:21:44,593 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,597 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 140ms
2014-07-21 02:21:44,597 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,597 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 140ms
2014-07-21 02:21:44,598 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,602 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 148ms
2014-07-21 02:21:44,602 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,602 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 148ms
2014-07-21 02:21:44,602 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,602 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 148ms
2014-07-21 02:21:44,603 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,603 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 151ms
2014-07-21 02:21:44,603 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,603 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 158ms
2014-07-21 02:21:44,603 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,603 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 150ms
2014-07-21 02:21:44,603 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,613 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 167ms
2014-07-21 02:21:44,613 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,621 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 174ms
2014-07-21 02:21:44,621 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,629 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 182ms
2014-07-21 02:21:44,629 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,629 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 182ms
2014-07-21 02:21:44,630 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,633 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-07-21 02:21:44,633 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,633 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-07-21 02:21:44,633 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,634 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 185ms
2014-07-21 02:21:44,634 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,634 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 184ms
2014-07-21 02:21:44,634 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,635 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 184ms
2014-07-21 02:21:44,635 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,641 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 196ms
2014-07-21 02:21:44,641 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,641 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 199ms
2014-07-21 02:21:44,642 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,647 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 284ms
2014-07-21 02:21:44,647 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,653 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 288ms
2014-07-21 02:21:44,653 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,653 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 289ms
2014-07-21 02:21:44,653 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,661 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 298ms
2014-07-21 02:21:44,661 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:44,661 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 298ms
2014-07-21 02:21:44,662 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:45,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:45,885 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:21:45,886 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files, but is 1.1g vs best flushable region's 139.2m. Choosing the bigger.
2014-07-21 02:21:45,886 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. due to global heap pressure
2014-07-21 02:21:45,886 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.1g
2014-07-21 02:21:45,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31284 synced till here 31256
2014-07-21 02:21:46,406 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934503692 with entries=141, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934505871
2014-07-21 02:21:47,015 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:47,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31403 synced till here 31361
2014-07-21 02:21:47,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934505871 with entries=119, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934507016
2014-07-21 02:21:48,042 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,044 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,044 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,045 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,045 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,049 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,052 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,054 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,056 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,056 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,152 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,154 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,154 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,154 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,205 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:21:48,262 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,262 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,262 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,264 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,266 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,267 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,267 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,268 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,348 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,349 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,350 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,350 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,352 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,353 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,353 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,353 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,353 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,353 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,354 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,354 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,354 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,354 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,354 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,355 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,356 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:48,356 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:21:50,508 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9988, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/bbbdfdc1d7b84c4db615ecf3cde7c05c
2014-07-21 02:21:50,530 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/bbbdfdc1d7b84c4db615ecf3cde7c05c as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/bbbdfdc1d7b84c4db615ecf3cde7c05c
2014-07-21 02:21:50,550 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/bbbdfdc1d7b84c4db615ecf3cde7c05c, entries=4265550, sequenceid=9988, filesize=303.4m
2014-07-21 02:21:50,550 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1302763760, currentsize=439.2m/460583440 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 52508ms, sequenceid=9988, compaction requested=true
2014-07-21 02:21:50,550 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:21:50,551 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:21:50,551 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:21:50,551 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2195ms
2014-07-21 02:21:50,551 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,551 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:21:50,551 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:21:50,551 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:21:50,553 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2197ms
2014-07-21 02:21:50,553 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,554 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2198ms
2014-07-21 02:21:50,554 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,554 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2200ms
2014-07-21 02:21:50,554 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,554 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2200ms
2014-07-21 02:21:50,554 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,555 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2202ms
2014-07-21 02:21:50,555 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,556 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2202ms
2014-07-21 02:21:50,556 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,559 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2206ms
2014-07-21 02:21:50,559 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,559 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2207ms
2014-07-21 02:21:50,559 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,559 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2207ms
2014-07-21 02:21:50,560 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,560 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2208ms
2014-07-21 02:21:50,560 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,560 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2207ms
2014-07-21 02:21:50,560 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,561 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2208ms
2014-07-21 02:21:50,561 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,561 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2209ms
2014-07-21 02:21:50,562 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,565 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2215ms
2014-07-21 02:21:50,565 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,565 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2215ms
2014-07-21 02:21:50,565 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,566 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2216ms
2014-07-21 02:21:50,566 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,569 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2221ms
2014-07-21 02:21:50,569 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,569 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2301ms
2014-07-21 02:21:50,569 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,569 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2302ms
2014-07-21 02:21:50,570 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,573 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2306ms
2014-07-21 02:21:50,573 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,574 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2308ms
2014-07-21 02:21:50,574 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,574 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2310ms
2014-07-21 02:21:50,574 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,575 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2312ms
2014-07-21 02:21:50,575 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,575 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2313ms
2014-07-21 02:21:50,575 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,583 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2320ms
2014-07-21 02:21:50,583 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,585 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2431ms
2014-07-21 02:21:50,585 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,586 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2431ms
2014-07-21 02:21:50,586 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,586 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2432ms
2014-07-21 02:21:50,587 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,587 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2435ms
2014-07-21 02:21:50,588 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,589 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2544ms
2014-07-21 02:21:50,589 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,593 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2537ms
2014-07-21 02:21:50,593 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,597 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2543ms
2014-07-21 02:21:50,597 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,602 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2549ms
2014-07-21 02:21:50,602 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,605 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2556ms
2014-07-21 02:21:50,605 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,605 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2560ms
2014-07-21 02:21:50,606 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,610 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2564ms
2014-07-21 02:21:50,611 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,611 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2567ms
2014-07-21 02:21:50,611 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,614 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2569ms
2014-07-21 02:21:50,614 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:50,615 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2572ms
2014-07-21 02:21:50,615 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:21:51,011 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:51,021 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:21:51,028 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:21:51,028 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:21:51,029 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 20 blocking
2014-07-21 02:21:51,029 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-21 02:21:51,029 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:21:51,029 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:21:51,029 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:21:51,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31520 synced till here 31503
2014-07-21 02:21:51,123 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934507016 with entries=117, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934511011
2014-07-21 02:21:51,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934318816
2014-07-21 02:21:51,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934335744
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934337917
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934340992
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934342426
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934345374
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934347540
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934350135
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934352017
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934354266
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934368382
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934370022
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934374062
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934375522
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934377939
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934379687
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934381097
2014-07-21 02:21:51,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934382465
2014-07-21 02:21:51,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934385244
2014-07-21 02:21:51,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934403806
2014-07-21 02:21:51,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934404780
2014-07-21 02:21:51,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934408864
2014-07-21 02:21:51,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934411408
2014-07-21 02:21:56,842 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:56,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31664 synced till here 31663
2014-07-21 02:21:56,896 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934511011 with entries=144, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934516842
2014-07-21 02:21:58,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:58,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31744 synced till here 31743
2014-07-21 02:21:58,075 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934516842 with entries=80, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934518040
2014-07-21 02:21:59,200 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:21:59,361 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31860 synced till here 31858
2014-07-21 02:21:59,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934518040 with entries=116, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934519201
2014-07-21 02:22:00,496 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:00,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31956 synced till here 31949
2014-07-21 02:22:00,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934519201 with entries=96, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934520497
2014-07-21 02:22:01,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:01,841 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32040 synced till here 32035
2014-07-21 02:22:01,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934520497 with entries=84, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934521811
2014-07-21 02:22:02,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:02,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32133 synced till here 32129
2014-07-21 02:22:03,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934521811 with entries=93, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934522775
2014-07-21 02:22:03,729 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.4m
2014-07-21 02:22:03,733 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:22:03,903 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:22:04,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:05,193 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32231 synced till here 32227
2014-07-21 02:22:05,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934522775 with entries=98, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934524901
2014-07-21 02:22:06,648 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7186, memsize=70.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/d53889819d9c41da917f57d9ef9071e6
2014-07-21 02:22:06,660 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/d53889819d9c41da917f57d9ef9071e6 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/d53889819d9c41da917f57d9ef9071e6
2014-07-21 02:22:06,672 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/d53889819d9c41da917f57d9ef9071e6, entries=255550, sequenceid=7186, filesize=18.2m
2014-07-21 02:22:06,673 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268834480, currentsize=9.6m/10081520 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 2944ms, sequenceid=7186, compaction requested=true
2014-07-21 02:22:06,673 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:22:06,673 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-21 02:22:06,674 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-21 02:22:06,674 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:22:06,674 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:22:06,674 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:22:08,113 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6977, memsize=498.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/f77b8c866f4843d68f86b01405f8e659
2014-07-21 02:22:08,135 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/f77b8c866f4843d68f86b01405f8e659 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/f77b8c866f4843d68f86b01405f8e659
2014-07-21 02:22:08,147 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/f77b8c866f4843d68f86b01405f8e659, entries=1813190, sequenceid=6977, filesize=129.1m
2014-07-21 02:22:08,147 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1222668240, currentsize=424.6m/445212480 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 22261ms, sequenceid=6977, compaction requested=true
2014-07-21 02:22:08,148 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:22:08,148 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:22:08,148 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:22:08,148 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:22:08,148 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:22:08,148 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:22:08,268 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:22:08,268 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:22:08,269 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:22:08,269 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:22:08,269 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:22:08,269 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:22:08,269 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:22:08,269 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:22:09,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:09,167 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934524901 with entries=91, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934529132
2014-07-21 02:22:09,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934412993
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934415189
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934417627
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934419426
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934431542
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934434358
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934435906
2014-07-21 02:22:09,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934441988
2014-07-21 02:22:10,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:10,894 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32417 synced till here 32416
2014-07-21 02:22:10,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934529132 with entries=95, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934530788
2014-07-21 02:22:12,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:12,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934530788 with entries=93, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934532774
2014-07-21 02:22:14,230 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:14,253 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934532774 with entries=84, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934534230
2014-07-21 02:22:17,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:18,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32707 synced till here 32703
2014-07-21 02:22:18,358 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934534230 with entries=113, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934537862
2014-07-21 02:22:20,237 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:20,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32825 synced till here 32823
2014-07-21 02:22:20,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934537862 with entries=118, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934540237
2014-07-21 02:22:23,037 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:23,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32916 synced till here 32911
2014-07-21 02:22:23,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934540237 with entries=91, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934543037
2014-07-21 02:22:23,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:24,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:24,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934543037 with entries=86, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934544759
2014-07-21 02:22:24,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:25,036 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90402ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:22:25,036 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 682.5m
2014-07-21 02:22:25,517 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:22:26,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:26,579 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33092 synced till here 33088
2014-07-21 02:22:26,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934544759 with entries=90, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934546563
2014-07-21 02:22:26,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:29,347 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:30,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33190 synced till here 33188
2014-07-21 02:22:30,069 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934546563 with entries=98, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934549348
2014-07-21 02:22:30,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:33,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:33,233 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934549348 with entries=87, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934553190
2014-07-21 02:22:33,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:34,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:34,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934553190 with entries=82, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934554713
2014-07-21 02:22:34,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:22:36,807 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:22:36,808 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.7g vs best flushable region's 142.0m. Choosing the bigger.
2014-07-21 02:22:36,808 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:22:36,808 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.7g
2014-07-21 02:22:37,017 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:37,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33446 synced till here 33444
2014-07-21 02:22:37,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934554713 with entries=87, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934557018
2014-07-21 02:22:38,098 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:22:40,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:40,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33545 synced till here 33540
2014-07-21 02:22:40,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934557018 with entries=99, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934560173
2014-07-21 02:22:40,781 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,785 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,798 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,803 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,804 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,814 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,831 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,832 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,871 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:40,882 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,481 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,484 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,498 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,498 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,501 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,515 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,530 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,531 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,545 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,547 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,551 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,555 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,600 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,614 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,678 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,723 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,735 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,755 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:41,844 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,063 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,077 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,092 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,106 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,131 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,154 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,188 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,223 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,265 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,267 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,303 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,343 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,408 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,447 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:42,869 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,730 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,762 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,796 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,799 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,817 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:43,973 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:45,781 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,785 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,798 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,804 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,804 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,814 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,831 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,832 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,871 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:45,882 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:46,959 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5115ms
2014-07-21 02:22:46,960 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5359ms
2014-07-21 02:22:46,960 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5225ms
2014-07-21 02:22:46,960 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5237ms
2014-07-21 02:22:46,960 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5347ms
2014-07-21 02:22:46,961 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5206ms
2014-07-21 02:22:46,961 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5283ms
2014-07-21 02:22:46,961 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5430ms
2014-07-21 02:22:46,962 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5416ms
2014-07-21 02:22:46,962 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5415ms
2014-07-21 02:22:46,962 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5482ms
2014-07-21 02:22:46,962 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5478ms
2014-07-21 02:22:46,963 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5465ms
2014-07-21 02:22:46,963 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5465ms
2014-07-21 02:22:46,963 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5462ms
2014-07-21 02:22:46,963 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5448ms
2014-07-21 02:22:46,964 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5434ms
2014-07-21 02:22:46,964 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5414ms
2014-07-21 02:22:46,965 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5410ms
2014-07-21 02:22:47,063 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,077 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:22:47,092 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,107 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:22:47,132 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,155 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:22:47,188 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,223 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,265 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,267 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,303 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,305 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7379, memsize=544.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/931bb5a7c87343a18c261906e9878908
2014-07-21 02:22:47,321 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/931bb5a7c87343a18c261906e9878908 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/931bb5a7c87343a18c261906e9878908
2014-07-21 02:22:47,336 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/931bb5a7c87343a18c261906e9878908, entries=1981690, sequenceid=7379, filesize=141.0m
2014-07-21 02:22:47,336 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~684.2m/717414480, currentsize=225.8m/236758960 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 22300ms, sequenceid=7379, compaction requested=true
2014-07-21 02:22:47,337 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:22:47,337 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:22:47,337 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5034ms
2014-07-21 02:22:47,337 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:22:47,337 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,337 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:22:47,337 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5070ms
2014-07-21 02:22:47,337 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:22:47,338 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,338 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:22:47,341 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5076ms
2014-07-21 02:22:47,341 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,341 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5118ms
2014-07-21 02:22:47,342 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,342 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-21 02:22:47,342 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,342 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-21 02:22:47,342 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,342 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5211ms
2014-07-21 02:22:47,342 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,343 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,343 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,349 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5243ms
2014-07-21 02:22:47,349 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,349 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5257ms
2014-07-21 02:22:47,350 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,350 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5274ms
2014-07-21 02:22:47,350 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,350 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5287ms
2014-07-21 02:22:47,350 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,351 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5796ms
2014-07-21 02:22:47,351 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,351 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5801ms
2014-07-21 02:22:47,352 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,352 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5823ms
2014-07-21 02:22:47,352 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,353 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5838ms
2014-07-21 02:22:47,353 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,353 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5852ms
2014-07-21 02:22:47,353 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,365 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5867ms
2014-07-21 02:22:47,365 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,365 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5867ms
2014-07-21 02:22:47,366 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,366 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5882ms
2014-07-21 02:22:47,366 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,366 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5886ms
2014-07-21 02:22:47,366 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,366 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5819ms
2014-07-21 02:22:47,366 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,372 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5827ms
2014-07-21 02:22:47,372 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,372 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5841ms
2014-07-21 02:22:47,372 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,372 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5694ms
2014-07-21 02:22:47,372 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,374 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5619ms
2014-07-21 02:22:47,374 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,374 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5761ms
2014-07-21 02:22:47,374 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,374 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5651ms
2014-07-21 02:22:47,375 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,375 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5640ms
2014-07-21 02:22:47,375 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,381 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5781ms
2014-07-21 02:22:47,382 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,382 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5538ms
2014-07-21 02:22:47,382 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,383 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6501ms
2014-07-21 02:22:47,383 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,384 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6512ms
2014-07-21 02:22:47,384 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,384 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6552ms
2014-07-21 02:22:47,384 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,385 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6554ms
2014-07-21 02:22:47,385 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,389 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6575ms
2014-07-21 02:22:47,389 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,389 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6585ms
2014-07-21 02:22:47,389 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,390 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6587ms
2014-07-21 02:22:47,390 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,397 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6599ms
2014-07-21 02:22:47,397 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,405 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6620ms
2014-07-21 02:22:47,405 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,405 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6624ms
2014-07-21 02:22:47,405 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,406 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3433ms
2014-07-21 02:22:47,406 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,408 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:22:47,408 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,413 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3596ms
2014-07-21 02:22:47,413 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,413 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3614ms
2014-07-21 02:22:47,414 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,414 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3618ms
2014-07-21 02:22:47,414 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,414 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3652ms
2014-07-21 02:22:47,414 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,421 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3691ms
2014-07-21 02:22:47,421 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,421 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4552ms
2014-07-21 02:22:47,421 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:47,433 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4987ms
2014-07-21 02:22:47,433 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:22:48,020 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:49,350 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33660 synced till here 33642
2014-07-21 02:22:49,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934560173 with entries=115, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934568021
2014-07-21 02:22:49,817 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:22:49,817 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:22:49,818 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:22:49,818 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:22:49,819 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:22:49,819 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:22:49,819 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:22:49,819 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:22:50,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:51,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33782 synced till here 33752
2014-07-21 02:22:51,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934568021 with entries=122, filesize=99.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934570455
2014-07-21 02:22:53,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:53,337 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33921 synced till here 33900
2014-07-21 02:22:53,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934570455 with entries=139, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934573234
2014-07-21 02:22:54,098 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:22:54,101 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files, but is 1.3g vs best flushable region's 199.6m. Choosing the bigger.
2014-07-21 02:22:54,101 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. due to global heap pressure
2014-07-21 02:22:54,105 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 1.3g
2014-07-21 02:22:55,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:22:55,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34050 synced till here 34035
2014-07-21 02:22:55,882 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934573234 with entries=129, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934575585
2014-07-21 02:22:56,284 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,284 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,284 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,285 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,285 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,286 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,289 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,290 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,291 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,294 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,295 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,296 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,296 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,297 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,298 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,300 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,300 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,304 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,307 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,308 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,308 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,308 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,308 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,309 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,310 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,310 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,313 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,318 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,320 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,407 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,408 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,409 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,410 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,410 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,411 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,411 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,413 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,414 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,415 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:56,416 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,157 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,158 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,158 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,158 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,159 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,159 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,159 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,159 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,165 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,165 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:22:57,394 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:23:01,284 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,284 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,284 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,285 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,286 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,286 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,289 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,290 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,291 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,294 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,295 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,296 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,297 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5012ms
2014-07-21 02:23:01,298 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,298 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,300 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,301 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,305 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,307 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,308 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,308 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5008ms
2014-07-21 02:23:01,309 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,309 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,310 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:23:01,310 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,310 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:01,313 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:23:02,394 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5229ms
2014-07-21 02:23:02,394 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5236ms
2014-07-21 02:23:02,395 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5979ms
2014-07-21 02:23:02,395 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5984ms
2014-07-21 02:23:02,395 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5988ms
2014-07-21 02:23:02,395 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5231ms
2014-07-21 02:23:02,395 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5982ms
2014-07-21 02:23:02,396 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5982ms
2014-07-21 02:23:02,396 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5981ms
2014-07-21 02:23:02,396 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5239ms
2014-07-21 02:23:02,396 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,396 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5238ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5989ms
2014-07-21 02:23:02,397 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5989ms
2014-07-21 02:23:02,398 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5989ms
2014-07-21 02:23:02,398 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5988ms
2014-07-21 02:23:02,398 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5988ms
2014-07-21 02:23:02,398 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6080ms
2014-07-21 02:23:02,399 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6079ms
2014-07-21 02:23:06,284 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,285 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,285 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,286 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,286 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,286 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,289 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,290 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,292 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:23:06,295 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,296 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,297 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,298 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10014ms
2014-07-21 02:23:06,298 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,299 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,301 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,301 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,306 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,308 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,309 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,309 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10009ms
2014-07-21 02:23:06,309 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,310 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:06,310 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:23:06,310 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,311 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:23:06,314 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:23:07,394 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10229ms
2014-07-21 02:23:07,395 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10237ms
2014-07-21 02:23:07,396 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10988ms
2014-07-21 02:23:07,396 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10983ms
2014-07-21 02:23:07,396 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10232ms
2014-07-21 02:23:07,397 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10239ms
2014-07-21 02:23:07,397 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10238ms
2014-07-21 02:23:07,397 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10986ms
2014-07-21 02:23:07,398 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10981ms
2014-07-21 02:23:07,398 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10990ms
2014-07-21 02:23:07,399 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10990ms
2014-07-21 02:23:07,399 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10240ms
2014-07-21 02:23:07,400 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10240ms
2014-07-21 02:23:07,400 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10241ms
2014-07-21 02:23:07,400 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10242ms
2014-07-21 02:23:07,400 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10243ms
2014-07-21 02:23:07,401 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10985ms
2014-07-21 02:23:07,401 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10987ms
2014-07-21 02:23:07,402 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11081ms
2014-07-21 02:23:07,402 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11084ms
2014-07-21 02:23:07,403 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10992ms
2014-07-21 02:23:07,403 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10993ms
2014-07-21 02:23:07,403 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10994ms
2014-07-21 02:23:11,285 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,285 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,286 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,286 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,287 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,287 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,290 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,291 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,292 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,295 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,297 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,298 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,299 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15014ms
2014-07-21 02:23:11,299 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,299 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,301 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,302 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,306 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,308 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,309 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,309 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15009ms
2014-07-21 02:23:11,310 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,310 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:11,311 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,311 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:23:11,311 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:23:11,315 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:23:12,395 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15230ms
2014-07-21 02:23:12,395 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15237ms
2014-07-21 02:23:12,396 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15989ms
2014-07-21 02:23:12,397 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15983ms
2014-07-21 02:23:12,397 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15233ms
2014-07-21 02:23:12,398 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15987ms
2014-07-21 02:23:12,398 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15239ms
2014-07-21 02:23:12,398 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15240ms
2014-07-21 02:23:12,398 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15982ms
2014-07-21 02:23:12,399 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15991ms
2014-07-21 02:23:12,399 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15991ms
2014-07-21 02:23:12,400 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15241ms
2014-07-21 02:23:12,400 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15241ms
2014-07-21 02:23:12,400 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15241ms
2014-07-21 02:23:12,400 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15243ms
2014-07-21 02:23:12,401 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15243ms
2014-07-21 02:23:12,401 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15986ms
2014-07-21 02:23:12,401 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15987ms
2014-07-21 02:23:12,402 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16082ms
2014-07-21 02:23:12,402 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16084ms
2014-07-21 02:23:12,403 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15993ms
2014-07-21 02:23:12,403 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15993ms
2014-07-21 02:23:12,404 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15994ms
2014-07-21 02:23:16,286 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-21 02:23:16,287 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,287 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-21 02:23:16,287 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-21 02:23:16,287 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,288 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,290 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,291 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,293 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,296 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,297 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,298 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,299 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20015ms
2014-07-21 02:23:16,300 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,300 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,301 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,302 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,306 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,309 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,309 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,310 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20010ms
2014-07-21 02:23:16,310 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:16,311 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-21 02:23:16,311 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,311 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20003ms
2014-07-21 02:23:16,311 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:23:16,315 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:23:17,395 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20230ms
2014-07-21 02:23:17,396 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20238ms
2014-07-21 02:23:17,397 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20990ms
2014-07-21 02:23:17,397 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20984ms
2014-07-21 02:23:17,398 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20234ms
2014-07-21 02:23:17,398 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20987ms
2014-07-21 02:23:17,398 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20239ms
2014-07-21 02:23:17,399 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20240ms
2014-07-21 02:23:17,399 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20983ms
2014-07-21 02:23:17,399 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20991ms
2014-07-21 02:23:17,400 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20991ms
2014-07-21 02:23:17,400 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20241ms
2014-07-21 02:23:17,400 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20241ms
2014-07-21 02:23:17,401 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20242ms
2014-07-21 02:23:17,401 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20244ms
2014-07-21 02:23:17,401 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20243ms
2014-07-21 02:23:17,401 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20986ms
2014-07-21 02:23:17,402 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20988ms
2014-07-21 02:23:17,402 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21082ms
2014-07-21 02:23:17,403 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21084ms
2014-07-21 02:23:17,403 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20993ms
2014-07-21 02:23:17,403 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20993ms
2014-07-21 02:23:17,404 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20995ms
2014-07-21 02:23:21,084 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7473, memsize=863.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/eefb658f9dd24254b6eb79d7c6ddd489
2014-07-21 02:23:21,108 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/eefb658f9dd24254b6eb79d7c6ddd489 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/eefb658f9dd24254b6eb79d7c6ddd489
2014-07-21 02:23:21,120 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/eefb658f9dd24254b6eb79d7c6ddd489, entries=3143770, sequenceid=7473, filesize=223.6m
2014-07-21 02:23:21,121 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.7g/1826456400, currentsize=318.4m/333899120 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 44313ms, sequenceid=7473, compaction requested=true
2014-07-21 02:23:21,121 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:21,122 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:23:21,122 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:23:21,122 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24713ms
2014-07-21 02:23:21,122 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:21,122 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,122 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:21,122 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24712ms
2014-07-21 02:23:21,123 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,122 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:23:21,123 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24713ms
2014-07-21 02:23:21,123 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,123 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24805ms
2014-07-21 02:23:21,123 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,123 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24803ms
2014-07-21 02:23:21,123 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,124 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24710ms
2014-07-21 02:23:21,124 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,125 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24710ms
2014-07-21 02:23:21,125 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,125 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23967ms
2014-07-21 02:23:21,125 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,125 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23968ms
2014-07-21 02:23:21,125 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,125 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23966ms
2014-07-21 02:23:21,125 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,126 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23967ms
2014-07-21 02:23:21,126 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,126 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23967ms
2014-07-21 02:23:21,126 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,126 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24718ms
2014-07-21 02:23:21,126 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,127 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24719ms
2014-07-21 02:23:21,127 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,127 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24711ms
2014-07-21 02:23:21,128 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,128 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23970ms
2014-07-21 02:23:21,128 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,128 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23969ms
2014-07-21 02:23:21,128 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,129 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24718ms
2014-07-21 02:23:21,129 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,130 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23966ms
2014-07-21 02:23:21,130 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,131 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24718ms
2014-07-21 02:23:21,131 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,131 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24724ms
2014-07-21 02:23:21,132 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,132 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23974ms
2014-07-21 02:23:21,132 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,133 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23967ms
2014-07-21 02:23:21,133 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,133 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24820ms
2014-07-21 02:23:21,133 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,134 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24823ms
2014-07-21 02:23:21,134 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,134 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24826ms
2014-07-21 02:23:21,135 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,135 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24825ms
2014-07-21 02:23:21,135 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,136 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24828ms
2014-07-21 02:23:21,136 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,141 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24833ms
2014-07-21 02:23:21,141 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,142 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24841ms
2014-07-21 02:23:21,142 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,144 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24837ms
2014-07-21 02:23:21,144 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,145 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24838ms
2014-07-21 02:23:21,145 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,145 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24841ms
2014-07-21 02:23:21,145 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,150 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24850ms
2014-07-21 02:23:21,150 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,150 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24850ms
2014-07-21 02:23:21,150 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,157 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24859ms
2014-07-21 02:23:21,157 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,157 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24860ms
2014-07-21 02:23:21,158 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,158 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24874ms
2014-07-21 02:23:21,158 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,158 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24862ms
2014-07-21 02:23:21,158 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,159 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24864ms
2014-07-21 02:23:21,159 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,161 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24867ms
2014-07-21 02:23:21,161 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,165 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24875ms
2014-07-21 02:23:21,165 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,174 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24884ms
2014-07-21 02:23:21,174 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,175 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24886ms
2014-07-21 02:23:21,176 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,177 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24891ms
2014-07-21 02:23:21,178 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,178 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24893ms
2014-07-21 02:23:21,178 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,179 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24895ms
2014-07-21 02:23:21,179 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,179 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24895ms
2014-07-21 02:23:21,179 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,179 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24894ms
2014-07-21 02:23:21,179 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,185 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24902ms
2014-07-21 02:23:21,185 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:23:21,201 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:21,225 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25754,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575470,"queuetimems":5351,"class":"HRegionServer","responsesize":11170,"method":"Multi"}
2014-07-21 02:23:21,238 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575719,"queuetimems":4030,"class":"HRegionServer","responsesize":7028,"method":"Multi"}
2014-07-21 02:23:21,243 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24843,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576400,"queuetimems":4322,"class":"HRegionServer","responsesize":3967,"method":"Multi"}
2014-07-21 02:23:21,250 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576402,"queuetimems":4306,"class":"HRegionServer","responsesize":1100,"method":"Multi"}
2014-07-21 02:23:21,252 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575722,"queuetimems":4020,"class":"HRegionServer","responsesize":7963,"method":"Multi"}
2014-07-21 02:23:21,262 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575461,"queuetimems":5513,"class":"HRegionServer","responsesize":10789,"method":"Multi"}
2014-07-21 02:23:21,262 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25784,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575478,"queuetimems":5290,"class":"HRegionServer","responsesize":11566,"method":"Multi"}
2014-07-21 02:23:21,265 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575461,"queuetimems":5486,"class":"HRegionServer","responsesize":10345,"method":"Multi"}
2014-07-21 02:23:21,265 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575461,"queuetimems":5442,"class":"HRegionServer","responsesize":8992,"method":"Multi"}
2014-07-21 02:23:21,265 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575461,"queuetimems":5463,"class":"HRegionServer","responsesize":9098,"method":"Multi"}
2014-07-21 02:23:21,266 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575515,"queuetimems":4073,"class":"HRegionServer","responsesize":8931,"method":"Multi"}
2014-07-21 02:23:21,270 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25791,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575478,"queuetimems":5207,"class":"HRegionServer","responsesize":11033,"method":"Multi"}
2014-07-21 02:23:21,270 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25809,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575461,"queuetimems":5390,"class":"HRegionServer","responsesize":10628,"method":"Multi"}
2014-07-21 02:23:21,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34140 synced till here 34132
2014-07-21 02:23:21,349 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576282,"queuetimems":4284,"class":"HRegionServer","responsesize":4823,"method":"Multi"}
2014-07-21 02:23:21,358 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576282,"queuetimems":4282,"class":"HRegionServer","responsesize":1241,"method":"Multi"}
2014-07-21 02:23:21,365 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576290,"queuetimems":4281,"class":"HRegionServer","responsesize":1159,"method":"Multi"}
2014-07-21 02:23:21,369 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576285,"queuetimems":4279,"class":"HRegionServer","responsesize":2347,"method":"Multi"}
2014-07-21 02:23:21,374 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576285,"queuetimems":4278,"class":"HRegionServer","responsesize":273,"method":"Multi"}
2014-07-21 02:23:21,374 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576284,"queuetimems":4282,"class":"HRegionServer","responsesize":923,"method":"Multi"}
2014-07-21 02:23:21,379 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576279,"queuetimems":4290,"class":"HRegionServer","responsesize":4491,"method":"Multi"}
2014-07-21 02:23:21,386 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25102,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576283,"queuetimems":4283,"class":"HRegionServer","responsesize":225,"method":"Multi"}
2014-07-21 02:23:21,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934575585 with entries=90, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934601201
2014-07-21 02:23:21,411 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934444964
2014-07-21 02:23:21,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934447613
2014-07-21 02:23:21,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934454829
2014-07-21 02:23:21,696 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:23:21,696 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:23:21,696 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:21,696 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:23:21,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:23:21,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:21,697 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:21,697 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:23:21,877 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25723,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576153,"queuetimems":4271,"class":"HRegionServer","responsesize":8731,"method":"Multi"}
2014-07-21 02:23:22,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:22,249 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575725,"queuetimems":4009,"class":"HRegionServer","responsesize":6415,"method":"Multi"}
2014-07-21 02:23:22,249 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26755,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575494,"queuetimems":5113,"class":"HRegionServer","responsesize":11040,"method":"Multi"}
2014-07-21 02:23:22,249 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26096,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576152,"queuetimems":4309,"class":"HRegionServer","responsesize":11222,"method":"Multi"}
2014-07-21 02:23:22,250 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576274,"queuetimems":4312,"class":"HRegionServer","responsesize":8300,"method":"Multi"}
2014-07-21 02:23:22,249 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26622,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575626,"queuetimems":3958,"class":"HRegionServer","responsesize":8663,"method":"Multi"}
2014-07-21 02:23:22,257 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575834,"queuetimems":4065,"class":"HRegionServer","responsesize":11155,"method":"Multi"}
2014-07-21 02:23:22,266 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576153,"queuetimems":4246,"class":"HRegionServer","responsesize":12268,"method":"Multi"}
2014-07-21 02:23:22,266 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575506,"queuetimems":4107,"class":"HRegionServer","responsesize":11694,"method":"Multi"}
2014-07-21 02:23:22,269 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26117,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576152,"queuetimems":4294,"class":"HRegionServer","responsesize":8422,"method":"Multi"}
2014-07-21 02:23:22,270 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575586,"queuetimems":3953,"class":"HRegionServer","responsesize":8780,"method":"Multi"}
2014-07-21 02:23:22,270 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25998,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576272,"queuetimems":4351,"class":"HRegionServer","responsesize":6867,"method":"Multi"}
2014-07-21 02:23:22,274 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575533,"queuetimems":4037,"class":"HRegionServer","responsesize":8983,"method":"Multi"}
2014-07-21 02:23:22,275 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575619,"queuetimems":3967,"class":"HRegionServer","responsesize":8307,"method":"Multi"}
2014-07-21 02:23:22,269 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575830,"queuetimems":4085,"class":"HRegionServer","responsesize":10386,"method":"Multi"}
2014-07-21 02:23:22,280 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575514,"queuetimems":4095,"class":"HRegionServer","responsesize":11514,"method":"Multi"}
2014-07-21 02:23:22,280 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26786,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575494,"queuetimems":5069,"class":"HRegionServer","responsesize":11506,"method":"Multi"}
2014-07-21 02:23:22,281 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26787,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575494,"queuetimems":4142,"class":"HRegionServer","responsesize":12037,"method":"Multi"}
2014-07-21 02:23:22,282 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575362,"queuetimems":5502,"class":"HRegionServer","responsesize":12303,"method":"Multi"}
2014-07-21 02:23:22,282 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26780,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575502,"queuetimems":4121,"class":"HRegionServer","responsesize":11350,"method":"Multi"}
2014-07-21 02:23:22,290 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575541,"queuetimems":3976,"class":"HRegionServer","responsesize":11809,"method":"Multi"}
2014-07-21 02:23:22,298 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575838,"queuetimems":4027,"class":"HRegionServer","responsesize":11314,"method":"Multi"}
2014-07-21 02:23:22,299 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26813,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934575486,"queuetimems":5162,"class":"HRegionServer","responsesize":12252,"method":"Multi"}
2014-07-21 02:23:22,298 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576400,"queuetimems":4304,"class":"HRegionServer","responsesize":9098,"method":"Multi"}
2014-07-21 02:23:22,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34257 synced till here 34236
2014-07-21 02:23:22,358 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25958,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576399,"queuetimems":4348,"class":"HRegionServer","responsesize":10628,"method":"Multi"}
2014-07-21 02:23:22,358 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25958,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576399,"queuetimems":4327,"class":"HRegionServer","responsesize":11566,"method":"Multi"}
2014-07-21 02:23:22,366 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25966,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576399,"queuetimems":4367,"class":"HRegionServer","responsesize":11170,"method":"Multi"}
2014-07-21 02:23:22,426 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934601201 with entries=117, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934602248
2014-07-21 02:23:23,290 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576274,"queuetimems":4327,"class":"HRegionServer","responsesize":11308,"method":"Multi"}
2014-07-21 02:23:23,444 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934576279,"queuetimems":4297,"class":"HRegionServer","responsesize":10880,"method":"Multi"}
2014-07-21 02:23:23,746 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:23,938 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34386 synced till here 34341
2014-07-21 02:23:24,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934602248 with entries=129, filesize=94.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934603746
2014-07-21 02:23:25,246 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:23:25,246 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.0m
2014-07-21 02:23:25,697 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:23:25,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:25,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34533 synced till here 34512
2014-07-21 02:23:26,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934603746 with entries=147, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934605893
2014-07-21 02:23:27,207 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:27,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34645 synced till here 34607
2014-07-21 02:23:27,461 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934605893 with entries=112, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934607207
2014-07-21 02:23:29,128 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:29,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34820 synced till here 34770
2014-07-21 02:23:29,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934607207 with entries=175, filesize=121.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934609129
2014-07-21 02:23:30,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:30,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34946 synced till here 34933
2014-07-21 02:23:31,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934609129 with entries=126, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934610901
2014-07-21 02:23:32,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:32,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35089 synced till here 35062
2014-07-21 02:23:32,786 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934610901 with entries=143, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934612470
2014-07-21 02:23:34,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:34,580 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35226 synced till here 35199
2014-07-21 02:23:34,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934612470 with entries=137, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934614528
2014-07-21 02:23:36,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:36,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35332 synced till here 35327
2014-07-21 02:23:36,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934614528 with entries=106, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934616463
2014-07-21 02:23:36,558 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7723, memsize=183.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/6ba509983ddf41ba81ec0ffda82b1a51
2014-07-21 02:23:36,591 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/6ba509983ddf41ba81ec0ffda82b1a51 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/6ba509983ddf41ba81ec0ffda82b1a51
2014-07-21 02:23:36,612 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/6ba509983ddf41ba81ec0ffda82b1a51, entries=668730, sequenceid=7723, filesize=47.6m
2014-07-21 02:23:36,613 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~270.6m/283710960, currentsize=106.6m/111810960 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 11367ms, sequenceid=7723, compaction requested=true
2014-07-21 02:23:36,614 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:36,614 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-21 02:23:36,614 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-21 02:23:36,614 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:23:36,614 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:36,614 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:23:37,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:37,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35443 synced till here 35442
2014-07-21 02:23:37,635 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934616463 with entries=111, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934617597
2014-07-21 02:23:38,037 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11251, memsize=926.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/d8ca18a1fae44b2d8110705ad2e3784c
2014-07-21 02:23:38,053 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/d8ca18a1fae44b2d8110705ad2e3784c as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/d8ca18a1fae44b2d8110705ad2e3784c
2014-07-21 02:23:38,306 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/d8ca18a1fae44b2d8110705ad2e3784c, entries=3373500, sequenceid=11251, filesize=240.0m
2014-07-21 02:23:38,307 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1395990640, currentsize=399.4m/418853280 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 44202ms, sequenceid=11251, compaction requested=true
2014-07-21 02:23:38,307 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:38,307 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:23:38,307 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:23:38,307 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:38,308 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:38,308 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:23:38,310 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:23:38,310 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:23:38,310 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:38,310 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 20 blocking
2014-07-21 02:23:38,310 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-21 02:23:38,310 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:38,310 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:38,310 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:23:38,414 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90146ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:23:38,415 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1001.2m
2014-07-21 02:23:39,178 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:39,180 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:23:39,196 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35595 synced till here 35594
2014-07-21 02:23:39,224 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934617597 with entries=152, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934619179
2014-07-21 02:23:39,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934456750
2014-07-21 02:23:39,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934458787
2014-07-21 02:23:39,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934460697
2014-07-21 02:23:39,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934462805
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934464408
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934466052
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934472436
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934492535
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934494432
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934496335
2014-07-21 02:23:39,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934497747
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934499783
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934501900
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934503692
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934505871
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934507016
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934511011
2014-07-21 02:23:39,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934516842
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934518040
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934519201
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934520497
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934521811
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934522775
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934524901
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934529132
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934530788
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934532774
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934534230
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934537862
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934540237
2014-07-21 02:23:39,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934543037
2014-07-21 02:23:40,536 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:40,823 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35699 synced till here 35685
2014-07-21 02:23:40,887 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934619179 with entries=104, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934620537
2014-07-21 02:23:44,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:44,038 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934620537 with entries=89, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934624001
2014-07-21 02:23:46,421 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:46,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35880 synced till here 35871
2014-07-21 02:23:46,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934624001 with entries=92, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934626422
2014-07-21 02:23:48,721 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:48,752 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35985 synced till here 35973
2014-07-21 02:23:48,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934626422 with entries=105, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934628722
2014-07-21 02:23:50,448 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:50,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36071 synced till here 36069
2014-07-21 02:23:50,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934628722 with entries=86, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934630449
2014-07-21 02:23:51,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:51,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36168 synced till here 36163
2014-07-21 02:23:51,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934630449 with entries=97, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934631586
2014-07-21 02:23:54,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:54,057 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36268 synced till here 36266
2014-07-21 02:23:54,133 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934631586 with entries=100, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934634030
2014-07-21 02:23:55,167 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8010, memsize=339.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/2215d28856bc49ca93688e276d1b59e8
2014-07-21 02:23:55,190 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/2215d28856bc49ca93688e276d1b59e8 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/2215d28856bc49ca93688e276d1b59e8
2014-07-21 02:23:55,219 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/2215d28856bc49ca93688e276d1b59e8, entries=1237690, sequenceid=8010, filesize=88.1m
2014-07-21 02:23:55,220 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1001.2m/1049796560, currentsize=284.1m/297934240 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 16805ms, sequenceid=8010, compaction requested=true
2014-07-21 02:23:55,221 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:55,221 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:23:55,221 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:23:55,221 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:55,221 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:55,221 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:23:55,254 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:23:55,254 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:23:55,254 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:23:55,254 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:23:55,254 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:23:55,254 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:23:55,254 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:23:55,255 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:23:56,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:56,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934634030 with entries=94, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934636413
2014-07-21 02:23:56,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934544759
2014-07-21 02:23:56,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934546563
2014-07-21 02:23:56,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934549348
2014-07-21 02:23:56,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934553190
2014-07-21 02:23:58,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:23:58,181 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36447 synced till here 36442
2014-07-21 02:23:58,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934636413 with entries=85, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934638154
2014-07-21 02:24:00,691 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:01,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934638154 with entries=95, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934640691
2014-07-21 02:24:02,931 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:02,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36625 synced till here 36621
2014-07-21 02:24:02,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934640691 with entries=83, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934642931
2014-07-21 02:24:03,571 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:24:03,572 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.2m
2014-07-21 02:24:03,737 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:24:05,390 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:05,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934642931 with entries=99, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934645390
2014-07-21 02:24:07,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:07,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36817 synced till here 36814
2014-07-21 02:24:07,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934645390 with entries=93, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934647184
2014-07-21 02:24:09,005 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8263, memsize=150.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/16c17fe50d6d4b31bcb6f9ef6da2d1a6
2014-07-21 02:24:09,021 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/16c17fe50d6d4b31bcb6f9ef6da2d1a6 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/16c17fe50d6d4b31bcb6f9ef6da2d1a6
2014-07-21 02:24:09,037 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/16c17fe50d6d4b31bcb6f9ef6da2d1a6, entries=546480, sequenceid=8263, filesize=38.9m
2014-07-21 02:24:09,038 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.2m/268655280, currentsize=26.7m/28016720 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 5467ms, sequenceid=8263, compaction requested=true
2014-07-21 02:24:09,039 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:24:09,039 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-21 02:24:09,039 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-21 02:24:09,039 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:24:09,039 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:24:09,040 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:24:09,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:09,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934647184 with entries=85, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934649146
2014-07-21 02:24:12,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:12,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36995 synced till here 36990
2014-07-21 02:24:12,107 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934649146 with entries=93, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934652045
2014-07-21 02:24:12,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:13,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:13,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37086 synced till here 37080
2014-07-21 02:24:13,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934652045 with entries=91, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934653272
2014-07-21 02:24:13,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:16,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:16,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37179 synced till here 37178
2014-07-21 02:24:16,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934653272 with entries=93, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934656594
2014-07-21 02:24:16,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:18,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:18,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37266 synced till here 37262
2014-07-21 02:24:18,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934656594 with entries=87, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934658273
2014-07-21 02:24:18,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:20,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:20,358 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934658273 with entries=91, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934660222
2014-07-21 02:24:20,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:20,463 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90646ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:24:20,464 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 708.7m
2014-07-21 02:24:20,924 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:24:21,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:21,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37451 synced till here 37446
2014-07-21 02:24:21,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934660222 with entries=94, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934661534
2014-07-21 02:24:21,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:24,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:24,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37557 synced till here 37556
2014-07-21 02:24:24,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934661534 with entries=106, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934664289
2014-07-21 02:24:24,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:26,479 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:26,690 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37653 synced till here 37650
2014-07-21 02:24:26,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934664289 with entries=96, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934666479
2014-07-21 02:24:26,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:24:27,654 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:24:27,654 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.6g vs best flushable region's 117.5m. Choosing the bigger.
2014-07-21 02:24:27,655 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:24:27,655 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.6g
2014-07-21 02:24:29,099 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:29,114 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:24:29,118 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37752 synced till here 37751
2014-07-21 02:24:29,134 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934666479 with entries=99, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934669101
2014-07-21 02:24:33,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:33,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934669101 with entries=127, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934673219
2014-07-21 02:24:33,949 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:33,949 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:33,952 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:33,968 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:33,971 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:33,987 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,036 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,084 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,126 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,176 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,323 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,373 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,382 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,429 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,475 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,520 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,561 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,562 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,579 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,599 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,612 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,672 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,701 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,731 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:34,763 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,301 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,303 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,304 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,307 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,310 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,312 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,314 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,319 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,332 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,368 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,368 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,371 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,371 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,390 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,400 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,401 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,442 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,488 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,524 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,535 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,547 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,754 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,802 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,856 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:35,894 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:38,949 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:38,950 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:38,953 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:38,968 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:38,972 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:38,988 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,036 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,084 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,127 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,176 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,323 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,383 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,446 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5017ms
2014-07-21 02:24:39,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,520 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,561 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,563 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,579 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,600 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,613 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,672 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:39,702 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:24:39,731 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:24:41,255 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1452ms
GC pool 'ParNew' had collection(s): count=1 time=1386ms
2014-07-21 02:24:41,123 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5228ms
2014-07-21 02:24:41,255 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5399ms
2014-07-21 02:24:41,256 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6492ms
2014-07-21 02:24:41,256 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5955ms
2014-07-21 02:24:41,256 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5953ms
2014-07-21 02:24:41,256 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5952ms
2014-07-21 02:24:41,257 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5949ms
2014-07-21 02:24:41,257 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5947ms
2014-07-21 02:24:41,258 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5945ms
2014-07-21 02:24:41,259 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5945ms
2014-07-21 02:24:41,259 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5941ms
2014-07-21 02:24:41,259 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5927ms
2014-07-21 02:24:41,260 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5893ms
2014-07-21 02:24:41,260 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5892ms
2014-07-21 02:24:41,261 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-07-21 02:24:41,261 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-07-21 02:24:41,262 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5871ms
2014-07-21 02:24:41,262 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5862ms
2014-07-21 02:24:41,263 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5862ms
2014-07-21 02:24:41,263 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5821ms
2014-07-21 02:24:41,263 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5775ms
2014-07-21 02:24:41,263 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5739ms
2014-07-21 02:24:41,263 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5728ms
2014-07-21 02:24:41,264 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5716ms
2014-07-21 02:24:41,264 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-21 02:24:41,264 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5462ms
2014-07-21 02:24:43,950 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:43,951 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:43,954 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:43,968 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:43,972 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:43,988 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,038 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,084 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:24:44,127 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,177 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:24:44,324 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,375 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,384 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,447 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10018ms
2014-07-21 02:24:44,475 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:24:44,521 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,562 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,564 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,579 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:24:44,600 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,613 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,673 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:44,702 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:46,145 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:24:46,147 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1389ms
GC pool 'ParNew' had collection(s): count=1 time=1412ms
2014-07-21 02:24:46,255 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10361ms
2014-07-21 02:24:46,256 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10400ms
2014-07-21 02:24:46,257 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10953ms
2014-07-21 02:24:46,257 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11494ms
2014-07-21 02:24:46,258 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10951ms
2014-07-21 02:24:46,258 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10957ms
2014-07-21 02:24:46,259 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10947ms
2014-07-21 02:24:46,259 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10949ms
2014-07-21 02:24:46,259 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10955ms
2014-07-21 02:24:46,260 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10945ms
2014-07-21 02:24:46,261 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10893ms
2014-07-21 02:24:46,261 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10929ms
2014-07-21 02:24:46,261 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10943ms
2014-07-21 02:24:46,262 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10890ms
2014-07-21 02:24:46,263 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10891ms
2014-07-21 02:24:46,263 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10895ms
2014-07-21 02:24:46,263 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10863ms
2014-07-21 02:24:46,263 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10873ms
2014-07-21 02:24:46,264 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10862ms
2014-07-21 02:24:46,264 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10729ms
2014-07-21 02:24:46,264 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10740ms
2014-07-21 02:24:46,264 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10776ms
2014-07-21 02:24:46,265 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10822ms
2014-07-21 02:24:46,265 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10511ms
2014-07-21 02:24:46,265 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10718ms
2014-07-21 02:24:46,266 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10463ms
2014-07-21 02:24:46,416 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8436, memsize=681.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/a0bedad67bbd4180a6e59d0314077ae0
2014-07-21 02:24:46,429 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/a0bedad67bbd4180a6e59d0314077ae0 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/a0bedad67bbd4180a6e59d0314077ae0
2014-07-21 02:24:46,441 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/a0bedad67bbd4180a6e59d0314077ae0, entries=2480520, sequenceid=8436, filesize=176.5m
2014-07-21 02:24:46,441 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~708.7m/743154000, currentsize=174.2m/182710320 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 25977ms, sequenceid=8436, compaction requested=true
2014-07-21 02:24:46,442 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:24:46,442 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:24:46,442 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:24:46,443 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:24:46,443 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10641ms
2014-07-21 02:24:46,443 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,443 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:24:46,443 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10896ms
2014-07-21 02:24:46,443 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,443 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:24:46,443 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10689ms
2014-07-21 02:24:46,443 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,444 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11002ms
2014-07-21 02:24:46,444 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,444 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10956ms
2014-07-21 02:24:46,444 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,444 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10920ms
2014-07-21 02:24:46,444 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,457 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10922ms
2014-07-21 02:24:46,457 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,457 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11056ms
2014-07-21 02:24:46,457 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,461 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11071ms
2014-07-21 02:24:46,461 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,461 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11061ms
2014-07-21 02:24:46,462 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,462 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11094ms
2014-07-21 02:24:46,462 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,462 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11091ms
2014-07-21 02:24:46,462 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,462 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11091ms
2014-07-21 02:24:46,462 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,464 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11146ms
2014-07-21 02:24:46,464 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,465 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11133ms
2014-07-21 02:24:46,465 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,469 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11102ms
2014-07-21 02:24:46,469 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,469 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11155ms
2014-07-21 02:24:46,470 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,470 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11166ms
2014-07-21 02:24:46,470 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,470 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11160ms
2014-07-21 02:24:46,470 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,470 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11158ms
2014-07-21 02:24:46,470 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,470 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11169ms
2014-07-21 02:24:46,470 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,481 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11174ms
2014-07-21 02:24:46,481 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,482 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11719ms
2014-07-21 02:24:46,482 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,482 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11179ms
2014-07-21 02:24:46,482 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,484 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10628ms
2014-07-21 02:24:46,484 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,485 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10591ms
2014-07-21 02:24:46,485 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,486 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11755ms
2014-07-21 02:24:46,486 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,486 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11785ms
2014-07-21 02:24:46,486 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,491 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11819ms
2014-07-21 02:24:46,491 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,492 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11880ms
2014-07-21 02:24:46,492 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,501 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11902ms
2014-07-21 02:24:46,501 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,509 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11930ms
2014-07-21 02:24:46,509 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,509 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11947ms
2014-07-21 02:24:46,510 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,510 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11949ms
2014-07-21 02:24:46,510 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,510 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11990ms
2014-07-21 02:24:46,510 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,517 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12042ms
2014-07-21 02:24:46,517 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,518 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12088ms
2014-07-21 02:24:46,518 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,525 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12143ms
2014-07-21 02:24:46,525 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,533 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12160ms
2014-07-21 02:24:46,533 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,537 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12214ms
2014-07-21 02:24:46,537 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,538 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12361ms
2014-07-21 02:24:46,538 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,541 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12415ms
2014-07-21 02:24:46,541 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,542 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12457ms
2014-07-21 02:24:46,542 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,542 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12506ms
2014-07-21 02:24:46,542 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,542 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12555ms
2014-07-21 02:24:46,542 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,543 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12571ms
2014-07-21 02:24:46,543 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,553 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12586ms
2014-07-21 02:24:46,553 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,561 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12609ms
2014-07-21 02:24:46,561 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,562 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12613ms
2014-07-21 02:24:46,562 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,563 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12614ms
2014-07-21 02:24:46,563 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675399,"queuetimems":0,"class":"HRegionServer","responsesize":4051,"method":"Multi"}
2014-07-21 02:24:46,871 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675367,"queuetimems":0,"class":"HRegionServer","responsesize":171,"method":"Multi"}
2014-07-21 02:24:46,872 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675400,"queuetimems":0,"class":"HRegionServer","responsesize":751,"method":"Multi"}
2014-07-21 02:24:46,872 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675370,"queuetimems":0,"class":"HRegionServer","responsesize":961,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675387,"queuetimems":0,"class":"HRegionServer","responsesize":4608,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675317,"queuetimems":1,"class":"HRegionServer","responsesize":3841,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675546,"queuetimems":1,"class":"HRegionServer","responsesize":3854,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11336,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675533,"queuetimems":0,"class":"HRegionServer","responsesize":4034,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675330,"queuetimems":0,"class":"HRegionServer","responsesize":4095,"method":"Multi"}
2014-07-21 02:24:46,870 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11501,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675368,"queuetimems":0,"class":"HRegionServer","responsesize":744,"method":"Multi"}
2014-07-21 02:24:46,871 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11558,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675313,"queuetimems":1,"class":"HRegionServer","responsesize":3567,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674379,"queuetimems":0,"class":"HRegionServer","responsesize":3664,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12414,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674596,"queuetimems":0,"class":"HRegionServer","responsesize":3858,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675308,"queuetimems":0,"class":"HRegionServer","responsesize":4151,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673970,"queuetimems":1,"class":"HRegionServer","responsesize":1794,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673984,"queuetimems":0,"class":"HRegionServer","responsesize":3769,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12451,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674560,"queuetimems":0,"class":"HRegionServer","responsesize":755,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675303,"queuetimems":0,"class":"HRegionServer","responsesize":171,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11707,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675303,"queuetimems":0,"class":"HRegionServer","responsesize":4048,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675306,"queuetimems":0,"class":"HRegionServer","responsesize":4098,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12401,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674609,"queuetimems":0,"class":"HRegionServer","responsesize":3741,"method":"Multi"}
2014-07-21 02:24:47,010 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11700,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675310,"queuetimems":0,"class":"HRegionServer","responsesize":3653,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674576,"queuetimems":1,"class":"HRegionServer","responsesize":3637,"method":"Multi"}
2014-07-21 02:24:47,011 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674761,"queuetimems":0,"class":"HRegionServer","responsesize":992,"method":"Multi"}
2014-07-21 02:24:47,239 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:47,241 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673948,"queuetimems":0,"class":"HRegionServer","responsesize":8,"method":"Multi"}
2014-07-21 02:24:47,241 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673948,"queuetimems":0,"class":"HRegionServer","responsesize":886,"method":"Multi"}
2014-07-21 02:24:47,241 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675799,"queuetimems":1,"class":"HRegionServer","responsesize":12087,"method":"Multi"}
2014-07-21 02:24:47,241 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13277,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673964,"queuetimems":0,"class":"HRegionServer","responsesize":3649,"method":"Multi"}
2014-07-21 02:24:48,845 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1197ms
GC pool 'ParNew' had collection(s): count=1 time=1280ms
2014-07-21 02:24:48,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37989 synced till here 37974
2014-07-21 02:24:48,910 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675439,"queuetimems":0,"class":"HRegionServer","responsesize":11091,"method":"Multi"}
2014-07-21 02:24:48,910 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13424,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675485,"queuetimems":0,"class":"HRegionServer","responsesize":11463,"method":"Multi"}
2014-07-21 02:24:48,915 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675849,"queuetimems":0,"class":"HRegionServer","responsesize":11867,"method":"Multi"}
2014-07-21 02:24:48,915 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675367,"queuetimems":0,"class":"HRegionServer","responsesize":12296,"method":"Multi"}
2014-07-21 02:24:48,922 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675752,"queuetimems":1,"class":"HRegionServer","responsesize":11750,"method":"Multi"}
2014-07-21 02:24:48,922 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13400,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675521,"queuetimems":0,"class":"HRegionServer","responsesize":11866,"method":"Multi"}
2014-07-21 02:24:48,926 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674759,"queuetimems":0,"class":"HRegionServer","responsesize":11997,"method":"Multi"}
2014-07-21 02:24:48,949 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934673219 with entries=110, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934687240
2014-07-21 02:24:49,165 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14843,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674319,"queuetimems":0,"class":"HRegionServer","responsesize":11847,"method":"Multi"}
2014-07-21 02:24:49,548 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:24:49,549 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15079,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674469,"queuetimems":0,"class":"HRegionServer","responsesize":11615,"method":"Multi"}
2014-07-21 02:24:49,549 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674697,"queuetimems":0,"class":"HRegionServer","responsesize":12057,"method":"Multi"}
2014-07-21 02:24:49,549 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15184,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674364,"queuetimems":0,"class":"HRegionServer","responsesize":11832,"method":"Multi"}
2014-07-21 02:24:49,564 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:24:49,573 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:24:49,574 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:24:49,574 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:24:49,574 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:24:49,574 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:24:49,574 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:24:49,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:49,725 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674666,"queuetimems":0,"class":"HRegionServer","responsesize":12040,"method":"Multi"}
2014-07-21 02:24:49,725 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674425,"queuetimems":0,"class":"HRegionServer","responsesize":11993,"method":"Multi"}
2014-07-21 02:24:49,726 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674516,"queuetimems":0,"class":"HRegionServer","responsesize":11767,"method":"Multi"}
2014-07-21 02:24:49,729 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15648,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674080,"queuetimems":0,"class":"HRegionServer","responsesize":11969,"method":"Multi"}
2014-07-21 02:24:49,729 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934675889,"queuetimems":1,"class":"HRegionServer","responsesize":11818,"method":"Multi"}
2014-07-21 02:24:49,730 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15610,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674119,"queuetimems":0,"class":"HRegionServer","responsesize":11980,"method":"Multi"}
2014-07-21 02:24:49,730 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15698,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674032,"queuetimems":1,"class":"HRegionServer","responsesize":12182,"method":"Multi"}
2014-07-21 02:24:49,730 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15783,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934673947,"queuetimems":0,"class":"HRegionServer","responsesize":11637,"method":"Multi"}
2014-07-21 02:24:49,729 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15003,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674726,"queuetimems":0,"class":"HRegionServer","responsesize":11991,"method":"Multi"}
2014-07-21 02:24:49,731 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674172,"queuetimems":1,"class":"HRegionServer","responsesize":11670,"method":"Multi"}
2014-07-21 02:24:49,732 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15173,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934674559,"queuetimems":0,"class":"HRegionServer","responsesize":11908,"method":"Multi"}
2014-07-21 02:24:50,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38105 synced till here 38085
2014-07-21 02:24:50,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934687240 with entries=116, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934689724
2014-07-21 02:24:52,855 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:52,919 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38232 synced till here 38206
2014-07-21 02:24:53,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934689724 with entries=127, filesize=99.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934692856
2014-07-21 02:24:53,341 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:24:53,341 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files, but is 1.3g vs best flushable region's 187.3m. Choosing the bigger.
2014-07-21 02:24:53,341 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. due to global heap pressure
2014-07-21 02:24:53,341 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 1.3g
2014-07-21 02:24:54,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:54,882 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38374 synced till here 38351
2014-07-21 02:24:55,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934692856 with entries=142, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934694780
2014-07-21 02:24:55,613 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,614 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,615 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,615 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,615 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,615 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,616 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,616 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,616 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,617 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,617 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,617 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,617 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,619 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,619 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,621 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,621 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:55,622 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,448 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,451 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,451 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,452 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,453 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,457 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,457 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,458 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,459 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,459 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,461 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,461 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,463 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,464 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,464 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,553 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:24:56,554 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,555 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,555 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,556 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,556 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,556 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,556 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,557 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,557 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,558 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,558 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,558 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,561 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,562 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,562 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,565 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,566 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:24:56,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934694780 with entries=99, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934696554
2014-07-21 02:24:56,596 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:25:00,613 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:00,615 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,615 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,616 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,616 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:25:00,616 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,617 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,617 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,617 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,618 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:00,618 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,618 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:25:00,618 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:25:00,619 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:00,620 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:00,621 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:00,622 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:00,623 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-21 02:25:01,684 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5118ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5236ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5233ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5236ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5350ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5347ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5347ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5346ms
2014-07-21 02:25:01,798 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5345ms
2014-07-21 02:25:01,801 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5344ms
2014-07-21 02:25:01,801 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5344ms
2014-07-21 02:25:01,801 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5341ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5341ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5339ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5338ms
2014-07-21 02:25:01,802 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5249ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5249ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5249ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-21 02:25:01,803 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5248ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5248ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5248ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5246ms
2014-07-21 02:25:01,804 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5247ms
2014-07-21 02:25:01,805 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5244ms
2014-07-21 02:25:05,614 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,615 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,616 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,616 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,617 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,617 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,618 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:25:05,619 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-21 02:25:05,620 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:25:05,621 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,622 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:25:05,623 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:25:05,623 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-21 02:25:06,798 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10232ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10346ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10347ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10348ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10348ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10351ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10237ms
2014-07-21 02:25:06,799 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10234ms
2014-07-21 02:25:06,800 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10238ms
2014-07-21 02:25:06,801 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10344ms
2014-07-21 02:25:06,802 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10345ms
2014-07-21 02:25:06,802 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10344ms
2014-07-21 02:25:06,802 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10343ms
2014-07-21 02:25:06,803 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10338ms
2014-07-21 02:25:06,803 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10340ms
2014-07-21 02:25:06,803 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10342ms
2014-07-21 02:25:06,803 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10342ms
2014-07-21 02:25:06,803 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10249ms
2014-07-21 02:25:06,804 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10344ms
2014-07-21 02:25:06,804 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10248ms
2014-07-21 02:25:06,804 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10248ms
2014-07-21 02:25:06,805 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10250ms
2014-07-21 02:25:06,805 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10251ms
2014-07-21 02:25:06,805 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10346ms
2014-07-21 02:25:06,805 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10244ms
2014-07-21 02:25:06,805 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10248ms
2014-07-21 02:25:06,806 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10247ms
2014-07-21 02:25:06,806 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10250ms
2014-07-21 02:25:06,806 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10249ms
2014-07-21 02:25:06,806 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10250ms
2014-07-21 02:25:06,806 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10250ms
2014-07-21 02:25:06,807 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10250ms
2014-07-21 02:25:10,615 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,616 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,617 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,617 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:25:10,618 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:25:10,618 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,618 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,618 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,619 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-21 02:25:10,619 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:25:10,619 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,619 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,620 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-21 02:25:10,620 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:25:10,621 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:25:10,622 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:25:10,623 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:25:10,623 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-21 02:25:11,799 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15233ms
2014-07-21 02:25:11,800 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15235ms
2014-07-21 02:25:11,800 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15238ms
2014-07-21 02:25:11,801 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15353ms
2014-07-21 02:25:11,803 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-21 02:25:11,803 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15340ms
2014-07-21 02:25:11,804 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15353ms
2014-07-21 02:25:11,805 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15353ms
2014-07-21 02:25:11,806 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15352ms
2014-07-21 02:25:11,807 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15249ms
2014-07-21 02:25:11,807 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15251ms
2014-07-21 02:25:11,807 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15249ms
2014-07-21 02:25:11,808 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15251ms
2014-07-21 02:25:11,809 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15247ms
2014-07-21 02:25:11,810 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15350ms
2014-07-21 02:25:11,810 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15256ms
2014-07-21 02:25:11,810 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15256ms
2014-07-21 02:25:11,811 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15255ms
2014-07-21 02:25:11,811 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15255ms
2014-07-21 02:25:11,811 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15352ms
2014-07-21 02:25:11,812 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-21 02:25:11,812 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15258ms
2014-07-21 02:25:11,813 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15351ms
2014-07-21 02:25:11,814 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15349ms
2014-07-21 02:25:11,814 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15355ms
2014-07-21 02:25:11,814 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15356ms
2014-07-21 02:25:11,814 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15357ms
2014-07-21 02:25:11,814 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15357ms
2014-07-21 02:25:11,815 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15253ms
2014-07-21 02:25:11,815 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15259ms
2014-07-21 02:25:11,816 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15260ms
2014-07-21 02:25:11,817 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15260ms
2014-07-21 02:25:14,382 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8506, memsize=951.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/aa71c253ceea4abe9893a6984349bf8e
2014-07-21 02:25:14,411 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/aa71c253ceea4abe9893a6984349bf8e as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/aa71c253ceea4abe9893a6984349bf8e
2014-07-21 02:25:14,426 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/aa71c253ceea4abe9893a6984349bf8e, entries=3465350, sequenceid=8506, filesize=246.5m
2014-07-21 02:25:14,427 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.6g/1758433120, currentsize=339.4m/355853280 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 46772ms, sequenceid=8506, compaction requested=true
2014-07-21 02:25:14,428 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:14,428 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:25:14,428 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17872ms
2014-07-21 02:25:14,428 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 96118ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:25:14,428 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:25:14,428 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,429 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:14,429 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., flushing=true, writesEnabled=true
2014-07-21 02:25:14,429 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:14,429 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17873ms
2014-07-21 02:25:14,429 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,429 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:25:14,433 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17877ms
2014-07-21 02:25:14,433 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,434 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17872ms
2014-07-21 02:25:14,434 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,434 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17977ms
2014-07-21 02:25:14,434 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,434 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17977ms
2014-07-21 02:25:14,434 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,434 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17976ms
2014-07-21 02:25:14,435 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,435 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17976ms
2014-07-21 02:25:14,435 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,448 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17984ms
2014-07-21 02:25:14,448 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,448 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17987ms
2014-07-21 02:25:14,448 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,448 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17894ms
2014-07-21 02:25:14,449 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,449 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17988ms
2014-07-21 02:25:14,449 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,449 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17990ms
2014-07-21 02:25:14,449 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,449 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17893ms
2014-07-21 02:25:14,449 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,449 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17893ms
2014-07-21 02:25:14,449 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,454 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17900ms
2014-07-21 02:25:14,454 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,454 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17900ms
2014-07-21 02:25:14,454 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,458 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695158,"queuetimems":8961,"class":"HRegionServer","responsesize":6633,"method":"Multi"}
2014-07-21 02:25:14,461 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18002ms
2014-07-21 02:25:14,461 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,467 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19432,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695034,"queuetimems":10638,"class":"HRegionServer","responsesize":8738,"method":"Multi"}
2014-07-21 02:25:14,469 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17908ms
2014-07-21 02:25:14,469 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,477 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17920ms
2014-07-21 02:25:14,477 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,480 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17922ms
2014-07-21 02:25:14,480 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,482 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17926ms
2014-07-21 02:25:14,482 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,482 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17925ms
2014-07-21 02:25:14,482 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,482 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695043,"queuetimems":10581,"class":"HRegionServer","responsesize":11468,"method":"Multi"}
2014-07-21 02:25:14,486 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18033ms
2014-07-21 02:25:14,486 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,487 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695045,"queuetimems":10556,"class":"HRegionServer","responsesize":9017,"method":"Multi"}
2014-07-21 02:25:14,487 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18035ms
2014-07-21 02:25:14,487 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,489 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18038ms
2014-07-21 02:25:14,489 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,489 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18026ms
2014-07-21 02:25:14,489 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,497 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18046ms
2014-07-21 02:25:14,497 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,497 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18049ms
2014-07-21 02:25:14,497 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,498 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17936ms
2014-07-21 02:25:14,498 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,498 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17933ms
2014-07-21 02:25:14,498 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,501 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17935ms
2014-07-21 02:25:14,501 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,509 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18892ms
2014-07-21 02:25:14,509 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,510 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18888ms
2014-07-21 02:25:14,510 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,510 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18889ms
2014-07-21 02:25:14,510 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,517 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18898ms
2014-07-21 02:25:14,517 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,518 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18898ms
2014-07-21 02:25:14,518 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,518 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18902ms
2014-07-21 02:25:14,518 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,520 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18903ms
2014-07-21 02:25:14,521 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,521 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18904ms
2014-07-21 02:25:14,521 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,521 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18905ms
2014-07-21 02:25:14,521 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,524 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18069,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696454,"queuetimems":10033,"class":"HRegionServer","responsesize":3858,"method":"Multi"}
2014-07-21 02:25:14,526 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18910ms
2014-07-21 02:25:14,526 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,527 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18910ms
2014-07-21 02:25:14,527 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,527 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18911ms
2014-07-21 02:25:14,527 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,528 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18912ms
2014-07-21 02:25:14,529 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,529 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18914ms
2014-07-21 02:25:14,529 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,538 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18923ms
2014-07-21 02:25:14,538 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,541 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696454,"queuetimems":10004,"class":"HRegionServer","responsesize":3741,"method":"Multi"}
2014-07-21 02:25:14,543 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19385,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695158,"queuetimems":8951,"class":"HRegionServer","responsesize":6388,"method":"Multi"}
2014-07-21 02:25:14,543 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19471,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695072,"queuetimems":8902,"class":"HRegionServer","responsesize":3538,"method":"Multi"}
2014-07-21 02:25:14,543 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695034,"queuetimems":10615,"class":"HRegionServer","responsesize":8863,"method":"Multi"}
2014-07-21 02:25:14,545 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18931ms
2014-07-21 02:25:14,545 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,549 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18935ms
2014-07-21 02:25:14,549 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,550 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18937ms
2014-07-21 02:25:14,550 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:14,555 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695034,"queuetimems":10665,"class":"HRegionServer","responsesize":9026,"method":"Multi"}
2014-07-21 02:25:14,562 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695055,"queuetimems":10514,"class":"HRegionServer","responsesize":11408,"method":"Multi"}
2014-07-21 02:25:14,624 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695126,"queuetimems":8941,"class":"HRegionServer","responsesize":2762,"method":"Multi"}
2014-07-21 02:25:14,624 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695125,"queuetimems":8945,"class":"HRegionServer","responsesize":5046,"method":"Multi"}
2014-07-21 02:25:14,624 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695168,"queuetimems":8936,"class":"HRegionServer","responsesize":6641,"method":"Multi"}
2014-07-21 02:25:14,624 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695070,"queuetimems":10504,"class":"HRegionServer","responsesize":11637,"method":"Multi"}
2014-07-21 02:25:14,654 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19652,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695002,"queuetimems":10750,"class":"HRegionServer","responsesize":6345,"method":"Multi"}
2014-07-21 02:25:14,720 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18268,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696452,"queuetimems":10038,"class":"HRegionServer","responsesize":4151,"method":"Multi"}
2014-07-21 02:25:14,723 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695049,"queuetimems":10533,"class":"HRegionServer","responsesize":11690,"method":"Multi"}
2014-07-21 02:25:14,728 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934694607,"queuetimems":10392,"class":"HRegionServer","responsesize":8781,"method":"Multi"}
2014-07-21 02:25:14,733 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21359,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693374,"queuetimems":10182,"class":"HRegionServer","responsesize":11171,"method":"Multi"}
2014-07-21 02:25:14,739 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21502,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693237,"queuetimems":10476,"class":"HRegionServer","responsesize":11832,"method":"Multi"}
2014-07-21 02:25:14,740 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693261,"queuetimems":10235,"class":"HRegionServer","responsesize":12040,"method":"Multi"}
2014-07-21 02:25:14,746 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21364,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693381,"queuetimems":10167,"class":"HRegionServer","responsesize":10963,"method":"Multi"}
2014-07-21 02:25:14,739 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693509,"queuetimems":10075,"class":"HRegionServer","responsesize":6633,"method":"Multi"}
2014-07-21 02:25:14,864 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19861,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695003,"queuetimems":10726,"class":"HRegionServer","responsesize":8292,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695018,"queuetimems":10702,"class":"HRegionServer","responsesize":8507,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693399,"queuetimems":10046,"class":"HRegionServer","responsesize":12057,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693391,"queuetimems":10088,"class":"HRegionServer","responsesize":11847,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21620,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693246,"queuetimems":10327,"class":"HRegionServer","responsesize":11969,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21623,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693242,"queuetimems":10373,"class":"HRegionServer","responsesize":11980,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19839,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695026,"queuetimems":10685,"class":"HRegionServer","responsesize":8400,"method":"Multi"}
2014-07-21 02:25:14,866 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21639,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934693227,"queuetimems":10599,"class":"HRegionServer","responsesize":11797,"method":"Multi"}
2014-07-21 02:25:15,058 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:25:15,058 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:25:15,059 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:15,059 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:25:15,060 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:25:15,060 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:15,060 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:15,060 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:25:15,702 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695179,"queuetimems":8930,"class":"HRegionServer","responsesize":3721,"method":"Multi"}
2014-07-21 02:25:15,702 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695185,"queuetimems":8923,"class":"HRegionServer","responsesize":6588,"method":"Multi"}
2014-07-21 02:25:15,702 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695166,"queuetimems":8946,"class":"HRegionServer","responsesize":6788,"method":"Multi"}
2014-07-21 02:25:15,702 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934695175,"queuetimems":8934,"class":"HRegionServer","responsesize":5094,"method":"Multi"}
2014-07-21 02:25:15,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:15,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38600 synced till here 38567
2014-07-21 02:25:16,140 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696451,"queuetimems":10124,"class":"HRegionServer","responsesize":3902,"method":"Multi"}
2014-07-21 02:25:16,160 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934696554 with entries=127, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934715827
2014-07-21 02:25:16,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934554713
2014-07-21 02:25:16,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934557018
2014-07-21 02:25:16,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934560173
2014-07-21 02:25:16,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934568021
2014-07-21 02:25:16,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934570455
2014-07-21 02:25:16,320 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696454,"queuetimems":10011,"class":"HRegionServer","responsesize":11797,"method":"Multi"}
2014-07-21 02:25:16,320 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696454,"queuetimems":9964,"class":"HRegionServer","responsesize":11767,"method":"Multi"}
2014-07-21 02:25:16,321 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19765,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696556,"queuetimems":9859,"class":"HRegionServer","responsesize":11993,"method":"Multi"}
2014-07-21 02:25:16,320 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19861,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696459,"queuetimems":9837,"class":"HRegionServer","responsesize":11832,"method":"Multi"}
2014-07-21 02:25:16,322 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696449,"queuetimems":10127,"class":"HRegionServer","responsesize":4899,"method":"Multi"}
2014-07-21 02:25:16,320 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19868,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696452,"queuetimems":10080,"class":"HRegionServer","responsesize":10963,"method":"Multi"}
2014-07-21 02:25:16,320 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696456,"queuetimems":9931,"class":"HRegionServer","responsesize":11456,"method":"Multi"}
2014-07-21 02:25:16,321 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696452,"queuetimems":10103,"class":"HRegionServer","responsesize":12087,"method":"Multi"}
2014-07-21 02:25:16,552 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696448,"queuetimems":10149,"class":"HRegionServer","responsesize":9148,"method":"Multi"}
2014-07-21 02:25:16,552 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696556,"queuetimems":9826,"class":"HRegionServer","responsesize":11991,"method":"Multi"}
2014-07-21 02:25:16,552 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696560,"queuetimems":9781,"class":"HRegionServer","responsesize":11750,"method":"Multi"}
2014-07-21 02:25:16,552 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696452,"queuetimems":10050,"class":"HRegionServer","responsesize":12040,"method":"Multi"}
2014-07-21 02:25:16,559 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20111,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934696448,"queuetimems":10134,"class":"HRegionServer","responsesize":8287,"method":"Multi"}
2014-07-21 02:25:16,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:17,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38736 synced till here 38701
2014-07-21 02:25:17,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934715827 with entries=136, filesize=81.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934716929
2014-07-21 02:25:18,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:18,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38846 synced till here 38815
2014-07-21 02:25:19,330 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:25:19,330 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.1m
2014-07-21 02:25:19,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934716929 with entries=110, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934718441
2014-07-21 02:25:19,883 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:25:20,215 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:21,015 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:25:21,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38983 synced till here 38948
2014-07-21 02:25:21,825 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934718441 with entries=137, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934720216
2014-07-21 02:25:22,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:22,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39116 synced till here 39094
2014-07-21 02:25:23,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934720216 with entries=133, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934722653
2014-07-21 02:25:23,905 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:23,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39240 synced till here 39218
2014-07-21 02:25:24,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934722653 with entries=124, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934723905
2014-07-21 02:25:25,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:25,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39373 synced till here 39339
2014-07-21 02:25:25,501 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934723905 with entries=133, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934725213
2014-07-21 02:25:26,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:26,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39493 synced till here 39458
2014-07-21 02:25:27,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934725213 with entries=120, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934726769
2014-07-21 02:25:28,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:28,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39645 synced till here 39613
2014-07-21 02:25:28,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934726769 with entries=152, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934728622
2014-07-21 02:25:29,065 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8773, memsize=162.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/19ef3acf84c8426382fdf59db19d0560
2014-07-21 02:25:29,112 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/19ef3acf84c8426382fdf59db19d0560 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/19ef3acf84c8426382fdf59db19d0560
2014-07-21 02:25:29,355 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/19ef3acf84c8426382fdf59db19d0560, entries=592550, sequenceid=8773, filesize=42.2m
2014-07-21 02:25:29,355 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.5m/271013600, currentsize=114.5m/120075840 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 10025ms, sequenceid=8773, compaction requested=true
2014-07-21 02:25:29,356 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:29,356 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-21 02:25:29,356 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-21 02:25:29,356 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:25:29,356 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:29,356 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:25:30,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:30,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39773 synced till here 39747
2014-07-21 02:25:30,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934728622 with entries=128, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934730455
2014-07-21 02:25:31,531 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:25:31,531 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files, but is 1.1g vs best flushable region's 144.6m. Choosing the bigger.
2014-07-21 02:25:31,531 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. due to global heap pressure
2014-07-21 02:25:31,532 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.1g
2014-07-21 02:25:31,724 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:31,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39882 synced till here 39878
2014-07-21 02:25:31,800 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934730455 with entries=109, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934731725
2014-07-21 02:25:33,063 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:33,092 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:25:33,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39990 synced till here 39971
2014-07-21 02:25:33,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934731725 with entries=108, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934733063
2014-07-21 02:25:33,877 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,877 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,879 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,880 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,882 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,888 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,904 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,919 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,939 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,940 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,940 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,940 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,941 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,942 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,942 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,952 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,953 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,955 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,974 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,974 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,975 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,975 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,975 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,976 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,976 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,976 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,978 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,980 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:33,988 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,011 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,032 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,034 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,065 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,102 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,102 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,104 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,115 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,145 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,149 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,150 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,176 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,203 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,207 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,217 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,222 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,223 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,224 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,358 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,390 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:34,424 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:25:38,877 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,877 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,879 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,880 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,883 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,888 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,904 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,919 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,940 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,941 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,941 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,941 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,942 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,943 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,944 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:25:38,952 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,953 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,956 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,975 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,975 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,975 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,976 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,976 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,976 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,977 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,977 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,978 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:38,981 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:38,988 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,011 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,033 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,034 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,065 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,102 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,103 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:39,105 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:39,115 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:39,145 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,149 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,151 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,176 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,204 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-21 02:25:39,207 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:25:39,217 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,222 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,223 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,224 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,359 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,398 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5008ms
2014-07-21 02:25:39,424 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12479, memsize=990.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/dbcf7817173c4750a17e29aa78dfb3a3
2014-07-21 02:25:39,424 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:25:39,438 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/dbcf7817173c4750a17e29aa78dfb3a3 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/dbcf7817173c4750a17e29aa78dfb3a3
2014-07-21 02:25:39,448 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/dbcf7817173c4750a17e29aa78dfb3a3, entries=3607540, sequenceid=12479, filesize=256.6m
2014-07-21 02:25:39,449 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1459822960, currentsize=300.0m/314587360 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 46108ms, sequenceid=12479, compaction requested=true
2014-07-21 02:25:39,450 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:39,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:25:39,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:25:39,450 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-21 02:25:39,450 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:39,451 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5061ms
2014-07-21 02:25:39,451 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,451 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:39,451 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5093ms
2014-07-21 02:25:39,451 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,451 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:25:39,451 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5227ms
2014-07-21 02:25:39,452 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,452 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5229ms
2014-07-21 02:25:39,452 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,453 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5231ms
2014-07-21 02:25:39,453 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,453 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5236ms
2014-07-21 02:25:39,453 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,457 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5251ms
2014-07-21 02:25:39,457 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,458 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5256ms
2014-07-21 02:25:39,458 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,458 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5282ms
2014-07-21 02:25:39,459 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,459 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5309ms
2014-07-21 02:25:39,459 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,460 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5310ms
2014-07-21 02:25:39,460 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,464 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5319ms
2014-07-21 02:25:39,465 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,465 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5351ms
2014-07-21 02:25:39,465 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,466 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5362ms
2014-07-21 02:25:39,467 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,467 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5365ms
2014-07-21 02:25:39,467 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,468 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5366ms
2014-07-21 02:25:39,468 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,469 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5403ms
2014-07-21 02:25:39,469 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,469 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5435ms
2014-07-21 02:25:39,469 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,469 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5437ms
2014-07-21 02:25:39,469 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,469 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5458ms
2014-07-21 02:25:39,469 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,470 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5482ms
2014-07-21 02:25:39,470 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,471 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5491ms
2014-07-21 02:25:39,471 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,472 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5494ms
2014-07-21 02:25:39,472 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,473 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5496ms
2014-07-21 02:25:39,473 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,473 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5497ms
2014-07-21 02:25:39,473 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,474 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5497ms
2014-07-21 02:25:39,474 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,474 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5499ms
2014-07-21 02:25:39,474 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,481 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:25:39,481 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:25:39,481 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-21 02:25:39,481 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,481 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:39,481 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 20 blocking
2014-07-21 02:25:39,481 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-21 02:25:39,481 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:39,481 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:39,481 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:25:39,489 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5515ms
2014-07-21 02:25:39,489 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,490 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5515ms
2014-07-21 02:25:39,490 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,491 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5517ms
2014-07-21 02:25:39,491 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,492 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5536ms
2014-07-21 02:25:39,492 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,492 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5539ms
2014-07-21 02:25:39,492 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,493 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5540ms
2014-07-21 02:25:39,493 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,496 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5553ms
2014-07-21 02:25:39,496 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,497 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5554ms
2014-07-21 02:25:39,497 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,502 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5560ms
2014-07-21 02:25:39,502 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,503 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5562ms
2014-07-21 02:25:39,503 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,504 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5563ms
2014-07-21 02:25:39,504 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,504 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5565ms
2014-07-21 02:25:39,504 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,504 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5565ms
2014-07-21 02:25:39,504 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,505 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5587ms
2014-07-21 02:25:39,505 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,506 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5601ms
2014-07-21 02:25:39,506 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,513 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5625ms
2014-07-21 02:25:39,513 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,514 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5631ms
2014-07-21 02:25:39,514 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,519 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5639ms
2014-07-21 02:25:39,519 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,519 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5640ms
2014-07-21 02:25:39,519 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,525 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-07-21 02:25:39,525 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,526 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5649ms
2014-07-21 02:25:39,526 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:25:39,951 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:40,030 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40116 synced till here 40103
2014-07-21 02:25:40,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934733063 with entries=126, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934739951
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934573234
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934575585
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934601201
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934602248
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934603746
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934605893
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934607207
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934609129
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934610901
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934612470
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934614528
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934616463
2014-07-21 02:25:40,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934617597
2014-07-21 02:25:40,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934619179
2014-07-21 02:25:40,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934620537
2014-07-21 02:25:40,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934624001
2014-07-21 02:25:40,107 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934626422
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934628722
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934630449
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934631586
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934634030
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934636413
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934638154
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934640691
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934642931
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934645390
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934647184
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934649146
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934652045
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934653272
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934656594
2014-07-21 02:25:40,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934658273
2014-07-21 02:25:41,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:41,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40246 synced till here 40220
2014-07-21 02:25:41,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934739951 with entries=130, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934741740
2014-07-21 02:25:43,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:43,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40394 synced till here 40362
2014-07-21 02:25:43,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934741740 with entries=148, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934743268
2014-07-21 02:25:44,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:44,927 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40555 synced till here 40515
2014-07-21 02:25:45,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934743268 with entries=161, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934744867
2014-07-21 02:25:45,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:45,867 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40678 synced till here 40659
2014-07-21 02:25:45,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934744867 with entries=123, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934745836
2014-07-21 02:25:47,173 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:47,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40817 synced till here 40816
2014-07-21 02:25:47,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934745836 with entries=139, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934747174
2014-07-21 02:25:48,900 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:25:48,900 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.4m
2014-07-21 02:25:49,117 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:25:49,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:49,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40923 synced till here 40918
2014-07-21 02:25:49,906 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934747174 with entries=106, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934749242
2014-07-21 02:25:50,582 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9090, memsize=274.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/74a8a00be5374728af8be34d2fc9c7d7
2014-07-21 02:25:50,611 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/74a8a00be5374728af8be34d2fc9c7d7 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/74a8a00be5374728af8be34d2fc9c7d7
2014-07-21 02:25:50,634 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/74a8a00be5374728af8be34d2fc9c7d7, entries=999190, sequenceid=9090, filesize=71.2m
2014-07-21 02:25:50,634 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.1g/1156740480, currentsize=402.0m/421505760 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 19102ms, sequenceid=9090, compaction requested=true
2014-07-21 02:25:50,635 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:50,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:25:50,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:25:50,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:50,635 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:50,635 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:25:51,020 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:25:51,020 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:25:51,021 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:51,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:25:51,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:25:51,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:25:51,021 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:51,021 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:25:51,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:51,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934749242 with entries=107, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934751643
2014-07-21 02:25:51,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934660222
2014-07-21 02:25:51,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934661534
2014-07-21 02:25:51,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934664289
2014-07-21 02:25:51,959 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9325, memsize=62.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4ad0c5ca8fa141c3bf860676896109d6
2014-07-21 02:25:51,979 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4ad0c5ca8fa141c3bf860676896109d6 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4ad0c5ca8fa141c3bf860676896109d6
2014-07-21 02:25:51,991 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4ad0c5ca8fa141c3bf860676896109d6, entries=227160, sequenceid=9325, filesize=16.2m
2014-07-21 02:25:51,992 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.4m/268893520, currentsize=19.8m/20768560 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 3092ms, sequenceid=9325, compaction requested=true
2014-07-21 02:25:51,992 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:25:51,992 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-21 02:25:51,993 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-21 02:25:51,993 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:25:51,993 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:25:51,993 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:25:53,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:53,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41113 synced till here 41105
2014-07-21 02:25:53,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934751643 with entries=83, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934753223
2014-07-21 02:25:55,580 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:55,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41211 synced till here 41210
2014-07-21 02:25:55,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934753223 with entries=98, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934755580
2014-07-21 02:25:57,424 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:57,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41295 synced till here 41294
2014-07-21 02:25:57,459 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934755580 with entries=84, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934757425
2014-07-21 02:25:58,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:25:59,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41406 synced till here 41405
2014-07-21 02:25:59,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934757425 with entries=111, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934758669
2014-07-21 02:26:01,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:01,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41500 synced till here 41499
2014-07-21 02:26:01,799 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934758669 with entries=94, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934761769
2014-07-21 02:26:04,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:04,840 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41601 synced till here 41599
2014-07-21 02:26:04,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934761769 with entries=101, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934764817
2014-07-21 02:26:04,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:06,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:06,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41683 synced till here 41681
2014-07-21 02:26:06,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934764817 with entries=82, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934766338
2014-07-21 02:26:06,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:07,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:07,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934766338 with entries=96, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934767509
2014-07-21 02:26:07,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:10,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:10,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41866 synced till here 41865
2014-07-21 02:26:10,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934767509 with entries=87, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934770488
2014-07-21 02:26:10,525 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:12,653 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:12,710 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41956 synced till here 41949
2014-07-21 02:26:12,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934770488 with entries=90, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934772653
2014-07-21 02:26:12,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:15,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:15,062 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934772653 with entries=94, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934775039
2014-07-21 02:26:15,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:17,484 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:17,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42151 synced till here 42148
2014-07-21 02:26:17,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934775039 with entries=101, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934777484
2014-07-21 02:26:17,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 45ba1d55e2fff38da3118b96932537db
2014-07-21 02:26:17,959 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:26:17,959 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.8g vs best flushable region's 152.7m. Choosing the bigger.
2014-07-21 02:26:17,959 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:26:17,960 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.8g
2014-07-21 02:26:18,625 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:26:18,625 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files, but is 999.9m vs best flushable region's 158.3m. Choosing the bigger.
2014-07-21 02:26:18,625 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. due to global heap pressure
2014-07-21 02:26:18,626 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 999.9m
2014-07-21 02:26:18,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:18,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42241 synced till here 42237
2014-07-21 02:26:18,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934777484 with entries=90, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934778759
2014-07-21 02:26:19,634 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:26:19,757 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:26:21,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:22,011 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,020 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,043 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,046 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,097 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,106 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,117 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,149 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,194 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,215 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,521 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,524 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934778759 with entries=96, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934781908
2014-07-21 02:26:22,527 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,533 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,544 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,575 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,586 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,615 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,618 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,629 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,649 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,833 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,866 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,873 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:22,885 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,448 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,509 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,534 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,542 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,577 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,592 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,737 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,773 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,823 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,837 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,858 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,858 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:23,908 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:24,000 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:24,028 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:24,042 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:24,156 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,204 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,449 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,461 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,518 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,699 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,710 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,746 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:25,749 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:26:27,012 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,020 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,043 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,046 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,098 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,107 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,117 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,150 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,194 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,215 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,522 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,524 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,528 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,533 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,544 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,575 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,587 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,615 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,618 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,629 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,649 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:27,834 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:27,867 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:28,832 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5240ms
2014-07-21 02:26:28,833 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5096ms
2014-07-21 02:26:28,833 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5060ms
2014-07-21 02:26:28,833 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-21 02:26:28,834 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5960ms
2014-07-21 02:26:28,834 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5949ms
2014-07-21 02:26:28,834 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5386ms
2014-07-21 02:26:28,834 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5325ms
2014-07-21 02:26:28,835 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5300ms
2014-07-21 02:26:28,836 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5294ms
2014-07-21 02:26:28,836 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5260ms
2014-07-21 02:26:28,837 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:28,858 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:28,858 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:28,908 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:29,000 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:29,028 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:29,042 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:29,156 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,204 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,449 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,462 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,518 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,699 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,711 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:30,746 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:26:30,750 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:26:32,012 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,021 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,044 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,047 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,099 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,107 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,118 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:32,150 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,194 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:32,215 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:32,522 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,525 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,528 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,534 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,545 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,576 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,587 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,615 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:32,619 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:32,629 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:32,650 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:34,020 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10021ms
2014-07-21 02:26:34,020 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10486ms
2014-07-21 02:26:34,020 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11147ms
2014-07-21 02:26:34,021 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11155ms
2014-07-21 02:26:34,021 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10479ms
2014-07-21 02:26:34,021 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1187ms
GC pool 'ParNew' had collection(s): count=1 time=1359ms
2014-07-21 02:26:34,021 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10445ms
2014-07-21 02:26:34,022 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10164ms
2014-07-21 02:26:34,022 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10185ms
2014-07-21 02:26:34,022 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10164ms
2014-07-21 02:26:34,023 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10114ms
2014-07-21 02:26:34,023 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11190ms
2014-07-21 02:26:34,024 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10431ms
2014-07-21 02:26:34,024 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10287ms
2014-07-21 02:26:34,025 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10201ms
2014-07-21 02:26:34,025 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10252ms
2014-07-21 02:26:34,025 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11140ms
2014-07-21 02:26:34,025 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10577ms
2014-07-21 02:26:34,026 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10516ms
2014-07-21 02:26:34,028 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:34,043 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:34,157 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:35,205 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:35,450 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:35,462 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:35,518 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:35,699 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:26:35,711 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:35,747 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:35,750 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:26:37,013 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,022 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,044 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,048 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,099 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,107 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,118 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,151 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,195 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,216 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,524 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:37,525 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,528 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,534 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,545 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,576 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,587 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,616 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,619 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:37,629 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-21 02:26:37,650 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:39,930 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15774ms
2014-07-21 02:26:39,931 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16482ms
2014-07-21 02:26:39,931 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16422ms
2014-07-21 02:26:39,931 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15903ms
2014-07-21 02:26:39,931 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15889ms
2014-07-21 02:26:39,931 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16389ms
2014-07-21 02:26:39,932 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17059ms
2014-07-21 02:26:39,932 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16356ms
2014-07-21 02:26:39,932 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16075ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16095ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16075ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16341ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16025ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17100ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16196ms
2014-07-21 02:26:39,933 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16160ms
2014-07-21 02:26:39,934 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17049ms
2014-07-21 02:26:39,934 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16111ms
2014-07-21 02:26:39,934 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15935ms
2014-07-21 02:26:39,934 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16400ms
2014-07-21 02:26:39,935 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17068ms
2014-07-21 02:26:40,205 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,451 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:26:40,462 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,519 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,700 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,711 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,747 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:40,750 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:26:42,013 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,022 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,044 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,048 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,099 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,108 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,118 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,151 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,195 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,216 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,525 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20004ms
2014-07-21 02:26:42,525 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,528 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,534 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,545 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,576 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,588 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,616 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20002ms
2014-07-21 02:26:42,619 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,630 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:42,650 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20001ms
2014-07-21 02:26:45,927 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20177ms
2014-07-21 02:26:45,927 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20723ms
2014-07-21 02:26:45,927 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21928ms
2014-07-21 02:26:45,928 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20479ms
2014-07-21 02:26:45,928 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20467ms
2014-07-21 02:26:45,928 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22352ms
2014-07-21 02:26:45,929 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21900ms
2014-07-21 02:26:45,929 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20411ms
2014-07-21 02:26:45,930 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20230ms
2014-07-21 02:26:45,930 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20220ms
2014-07-21 02:26:45,931 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20184ms
2014-07-21 02:26:45,932 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21775ms
2014-07-21 02:26:45,932 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22484ms
2014-07-21 02:26:45,933 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22423ms
2014-07-21 02:26:45,933 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21891ms
2014-07-21 02:26:45,934 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22392ms
2014-07-21 02:26:45,934 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23061ms
2014-07-21 02:26:45,935 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22077ms
2014-07-21 02:26:45,935 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22343ms
2014-07-21 02:26:45,936 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22099ms
2014-07-21 02:26:45,936 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22078ms
2014-07-21 02:26:45,936 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22028ms
2014-07-21 02:26:45,937 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23104ms
2014-07-21 02:26:45,937 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22200ms
2014-07-21 02:26:45,937 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22114ms
2014-07-21 02:26:45,937 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22164ms
2014-07-21 02:26:45,938 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23053ms
2014-07-21 02:26:45,938 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22404ms
2014-07-21 02:26:45,938 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23072ms
2014-07-21 02:26:47,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,022 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,045 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,048 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,086 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13406, memsize=695.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/c9e11d4b6661450190e16b16eab7881c
2014-07-21 02:26:47,100 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,105 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/c9e11d4b6661450190e16b16eab7881c as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/c9e11d4b6661450190e16b16eab7881c
2014-07-21 02:26:47,108 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,119 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,122 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/c9e11d4b6661450190e16b16eab7881c, entries=2533220, sequenceid=13406, filesize=180.3m
2014-07-21 02:26:47,123 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1004.6m/1053407600, currentsize=51.8m/54329920 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 28497ms, sequenceid=13406, compaction requested=true
2014-07-21 02:26:47,124 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:26:47,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:26:47,124 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25007ms
2014-07-21 02:26:47,124 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:26:47,124 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25018ms
2014-07-21 02:26:47,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:26:47,124 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,124 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:26:47,124 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25027ms
2014-07-21 02:26:47,125 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:26:47,125 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,125 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25079ms
2014-07-21 02:26:47,125 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,125 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25082ms
2014-07-21 02:26:47,125 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,125 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25105ms
2014-07-21 02:26:47,125 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,126 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25115ms
2014-07-21 02:26:47,126 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,139 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24273ms
2014-07-21 02:26:47,139 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,139 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23605ms
2014-07-21 02:26:47,139 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,139 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24254ms
2014-07-21 02:26:47,139 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,139 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23366ms
2014-07-21 02:26:47,139 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,139 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23316ms
2014-07-21 02:26:47,140 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,140 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23403ms
2014-07-21 02:26:47,140 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,140 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24307ms
2014-07-21 02:26:47,140 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,145 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23237ms
2014-07-21 02:26:47,145 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,145 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23287ms
2014-07-21 02:26:47,145 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,145 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23308ms
2014-07-21 02:26:47,146 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,151 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25002ms
2014-07-21 02:26:47,151 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,153 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23561ms
2014-07-21 02:26:47,153 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,153 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23296ms
2014-07-21 02:26:47,153 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,154 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24280ms
2014-07-21 02:26:47,154 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,154 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23612ms
2014-07-21 02:26:47,154 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,154 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23112ms
2014-07-21 02:26:47,155 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,155 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23646ms
2014-07-21 02:26:47,155 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,155 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23707ms
2014-07-21 02:26:47,155 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,155 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22999ms
2014-07-21 02:26:47,155 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,156 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21410ms
2014-07-21 02:26:47,156 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,158 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21448ms
2014-07-21 02:26:47,158 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,158 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21459ms
2014-07-21 02:26:47,158 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782099,"queuetimems":1,"class":"HRegionServer","responsesize":1106,"method":"Multi"}
2014-07-21 02:26:47,158 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,158 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21640ms
2014-07-21 02:26:47,158 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,164 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23136ms
2014-07-21 02:26:47,164 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,164 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23588ms
2014-07-21 02:26:47,164 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,164 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21703ms
2014-07-21 02:26:47,164 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,164 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21715ms
2014-07-21 02:26:47,164 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,164 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 23165ms
2014-07-21 02:26:47,165 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,165 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21961ms
2014-07-21 02:26:47,165 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,166 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21417ms
2014-07-21 02:26:47,166 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,166 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24517ms
2014-07-21 02:26:47,167 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,167 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24538ms
2014-07-21 02:26:47,167 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,167 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24549ms
2014-07-21 02:26:47,167 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,168 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24553ms
2014-07-21 02:26:47,168 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,168 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24582ms
2014-07-21 02:26:47,169 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,177 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24602ms
2014-07-21 02:26:47,177 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,184 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24640ms
2014-07-21 02:26:47,184 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,189 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24656ms
2014-07-21 02:26:47,189 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,189 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24662ms
2014-07-21 02:26:47,190 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,195 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25001ms
2014-07-21 02:26:47,195 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,197 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25306,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934781890,"queuetimems":0,"class":"HRegionServer","responsesize":10230,"method":"Multi"}
2014-07-21 02:26:47,201 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24677ms
2014-07-21 02:26:47,201 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,202 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24681ms
2014-07-21 02:26:47,202 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,209 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 24994ms
2014-07-21 02:26:47,209 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:26:47,582 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24699,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782883,"queuetimems":0,"class":"HRegionServer","responsesize":3962,"method":"Multi"}
2014-07-21 02:26:47,586 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23732,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783853,"queuetimems":0,"class":"HRegionServer","responsesize":4685,"method":"Multi"}
2014-07-21 02:26:47,590 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24149,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783441,"queuetimems":1,"class":"HRegionServer","responsesize":5534,"method":"Multi"}
2014-07-21 02:26:47,603 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783564,"queuetimems":0,"class":"HRegionServer","responsesize":7026,"method":"Multi"}
2014-07-21 02:26:47,603 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782584,"queuetimems":0,"class":"HRegionServer","responsesize":4297,"method":"Multi"}
2014-07-21 02:26:47,603 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783538,"queuetimems":1,"class":"HRegionServer","responsesize":4422,"method":"Multi"}
2014-07-21 02:26:47,604 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22146,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785457,"queuetimems":0,"class":"HRegionServer","responsesize":4239,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24086,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783523,"queuetimems":0,"class":"HRegionServer","responsesize":4207,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24982,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782627,"queuetimems":1,"class":"HRegionServer","responsesize":3889,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934784025,"queuetimems":1,"class":"HRegionServer","responsesize":901,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782614,"queuetimems":0,"class":"HRegionServer","responsesize":213,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785746,"queuetimems":1,"class":"HRegionServer","responsesize":159,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23776,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783833,"queuetimems":1,"class":"HRegionServer","responsesize":3716,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24739,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782871,"queuetimems":1,"class":"HRegionServer","responsesize":4175,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783509,"queuetimems":0,"class":"HRegionServer","responsesize":201,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21904,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785705,"queuetimems":1,"class":"HRegionServer","responsesize":5728,"method":"Multi"}
2014-07-21 02:26:47,611 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783592,"queuetimems":0,"class":"HRegionServer","responsesize":80,"method":"Multi"}
2014-07-21 02:26:47,610 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783857,"queuetimems":0,"class":"HRegionServer","responsesize":1013,"method":"Multi"}
2014-07-21 02:26:47,611 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934784039,"queuetimems":0,"class":"HRegionServer","responsesize":4082,"method":"Multi"}
2014-07-21 02:26:47,618 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782542,"queuetimems":0,"class":"HRegionServer","responsesize":3899,"method":"Multi"}
2014-07-21 02:26:47,618 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25643,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934781975,"queuetimems":0,"class":"HRegionServer","responsesize":15152,"method":"Multi"}
2014-07-21 02:26:47,618 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782523,"queuetimems":0,"class":"HRegionServer","responsesize":7353,"method":"Multi"}
2014-07-21 02:26:47,618 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782531,"queuetimems":0,"class":"HRegionServer","responsesize":3973,"method":"Multi"}
2014-07-21 02:26:47,622 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782524,"queuetimems":1,"class":"HRegionServer","responsesize":213,"method":"Multi"}
2014-07-21 02:26:47,624 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934781920,"queuetimems":0,"class":"HRegionServer","responsesize":11946,"method":"Multi"}
2014-07-21 02:26:47,618 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782572,"queuetimems":1,"class":"HRegionServer","responsesize":6643,"method":"Multi"}
2014-07-21 02:26:49,022 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:49,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42455 synced till here 42426
2014-07-21 02:26:49,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934781908 with entries=118, filesize=103.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934809023
2014-07-21 02:26:49,397 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27305,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782092,"queuetimems":0,"class":"HRegionServer","responsesize":11601,"method":"Multi"}
2014-07-21 02:26:49,488 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783770,"queuetimems":0,"class":"HRegionServer","responsesize":11042,"method":"Multi"}
2014-07-21 02:26:49,665 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24218,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785446,"queuetimems":0,"class":"HRegionServer","responsesize":8720,"method":"Multi"}
2014-07-21 02:26:49,665 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25761,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783903,"queuetimems":0,"class":"HRegionServer","responsesize":11707,"method":"Multi"}
2014-07-21 02:26:49,665 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782830,"queuetimems":1,"class":"HRegionServer","responsesize":12122,"method":"Multi"}
2014-07-21 02:26:49,665 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27655,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782009,"queuetimems":0,"class":"HRegionServer","responsesize":8720,"method":"Multi"}
2014-07-21 02:26:49,666 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27457,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782208,"queuetimems":0,"class":"HRegionServer","responsesize":9227,"method":"Multi"}
2014-07-21 02:26:49,666 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23978,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785687,"queuetimems":1,"class":"HRegionServer","responsesize":11783,"method":"Multi"}
2014-07-21 02:26:49,666 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785745,"queuetimems":1,"class":"HRegionServer","responsesize":11461,"method":"Multi"}
2014-07-21 02:26:49,674 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24474,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785199,"queuetimems":0,"class":"HRegionServer","responsesize":11601,"method":"Multi"}
2014-07-21 02:26:49,674 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25855,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783818,"queuetimems":0,"class":"HRegionServer","responsesize":11978,"method":"Multi"}
2014-07-21 02:26:49,682 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783733,"queuetimems":0,"class":"HRegionServer","responsesize":11512,"method":"Multi"}
2014-07-21 02:26:49,682 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934783994,"queuetimems":0,"class":"HRegionServer","responsesize":12238,"method":"Multi"}
2014-07-21 02:26:49,687 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782139,"queuetimems":0,"class":"HRegionServer","responsesize":11461,"method":"Multi"}
2014-07-21 02:26:49,687 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782614,"queuetimems":0,"class":"HRegionServer","responsesize":11043,"method":"Multi"}
2014-07-21 02:26:49,687 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27505,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782182,"queuetimems":0,"class":"HRegionServer","responsesize":12036,"method":"Multi"}
2014-07-21 02:26:49,687 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782647,"queuetimems":0,"class":"HRegionServer","responsesize":8810,"method":"Multi"}
2014-07-21 02:26:49,688 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782518,"queuetimems":0,"class":"HRegionServer","responsesize":11965,"method":"Multi"}
2014-07-21 02:26:49,688 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27643,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782044,"queuetimems":1,"class":"HRegionServer","responsesize":11783,"method":"Multi"}
2014-07-21 02:26:49,689 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24188,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934785500,"queuetimems":0,"class":"HRegionServer","responsesize":11612,"method":"Multi"}
2014-07-21 02:26:49,688 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934784152,"queuetimems":0,"class":"HRegionServer","responsesize":11218,"method":"Multi"}
2014-07-21 02:26:49,689 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934782863,"queuetimems":0,"class":"HRegionServer","responsesize":10118,"method":"Multi"}
2014-07-21 02:26:50,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:50,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42587 synced till here 42567
2014-07-21 02:26:51,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934809023 with entries=132, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934810841
2014-07-21 02:26:53,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:53,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42730 synced till here 42697
2014-07-21 02:26:53,195 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934810841 with entries=143, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934813013
2014-07-21 02:26:54,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:54,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42880 synced till here 42844
2014-07-21 02:26:55,028 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934813013 with entries=150, filesize=90.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934814702
2014-07-21 02:26:55,829 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:26:55,830 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.0m
2014-07-21 02:26:56,566 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:26:56,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:56,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43037 synced till here 43026
2014-07-21 02:26:57,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934814702 with entries=157, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934816807
2014-07-21 02:26:58,266 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:26:58,690 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:26:59,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43191 synced till here 43173
2014-07-21 02:26:59,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934816807 with entries=154, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934818691
2014-07-21 02:27:00,234 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,239 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,239 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,242 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,243 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,245 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,245 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,252 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,260 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,267 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,286 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,287 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,288 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,289 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,290 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,291 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,296 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,297 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,297 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,303 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,303 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,339 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,340 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,341 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,382 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,383 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,383 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,384 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,384 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,384 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,468 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:00,470 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,470 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,470 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,471 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,471 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,471 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43322 synced till here 43317
2014-07-21 02:27:00,497 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,497 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,498 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,498 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,503 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,503 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,503 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,504 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,505 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:00,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934818691 with entries=131, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934820468
2014-07-21 02:27:01,996 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9634, memsize=758.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/6b735deb9a9c4358a4b05a72e59a3f40
2014-07-21 02:27:02,024 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/6b735deb9a9c4358a4b05a72e59a3f40 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/6b735deb9a9c4358a4b05a72e59a3f40
2014-07-21 02:27:02,052 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/6b735deb9a9c4358a4b05a72e59a3f40, entries=2762270, sequenceid=9634, filesize=196.6m
2014-07-21 02:27:02,053 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.8g/1943114720, currentsize=206.6m/216642720 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 44093ms, sequenceid=9634, compaction requested=true
2014-07-21 02:27:02,054 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:27:02,054 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1550ms
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:27:02,054 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:27:02,054 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:27:02,054 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1550ms
2014-07-21 02:27:02,054 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:27:02,054 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:27:02,054 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,055 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:27:02,055 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:27:02,055 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:27:02,055 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:27:02,055 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1552ms
2014-07-21 02:27:02,055 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,055 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1552ms
2014-07-21 02:27:02,055 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,056 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1553ms
2014-07-21 02:27:02,056 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,056 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1553ms
2014-07-21 02:27:02,056 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,056 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1552ms
2014-07-21 02:27:02,057 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,057 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1554ms
2014-07-21 02:27:02,057 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,058 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1555ms
2014-07-21 02:27:02,058 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,059 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1556ms
2014-07-21 02:27:02,059 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,060 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1562ms
2014-07-21 02:27:02,060 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,061 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1563ms
2014-07-21 02:27:02,061 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,062 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1564ms
2014-07-21 02:27:02,062 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,062 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1565ms
2014-07-21 02:27:02,062 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,062 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1591ms
2014-07-21 02:27:02,063 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,067 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1596ms
2014-07-21 02:27:02,067 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,067 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1597ms
2014-07-21 02:27:02,067 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,067 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1597ms
2014-07-21 02:27:02,067 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,067 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1597ms
2014-07-21 02:27:02,067 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,067 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1597ms
2014-07-21 02:27:02,067 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,068 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1685ms
2014-07-21 02:27:02,069 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,069 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1686ms
2014-07-21 02:27:02,069 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,071 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1689ms
2014-07-21 02:27:02,072 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,072 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1689ms
2014-07-21 02:27:02,072 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,073 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1690ms
2014-07-21 02:27:02,073 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,074 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1692ms
2014-07-21 02:27:02,074 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,076 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1735ms
2014-07-21 02:27:02,076 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,076 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1736ms
2014-07-21 02:27:02,077 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,077 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1738ms
2014-07-21 02:27:02,077 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,077 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1774ms
2014-07-21 02:27:02,077 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,077 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1774ms
2014-07-21 02:27:02,077 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,078 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1780ms
2014-07-21 02:27:02,078 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,080 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1782ms
2014-07-21 02:27:02,080 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,083 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1784ms
2014-07-21 02:27:02,083 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,083 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1792ms
2014-07-21 02:27:02,083 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,084 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1793ms
2014-07-21 02:27:02,084 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,092 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1804ms
2014-07-21 02:27:02,092 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,092 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1804ms
2014-07-21 02:27:02,092 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,093 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1806ms
2014-07-21 02:27:02,093 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,098 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1811ms
2014-07-21 02:27:02,098 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,100 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1834ms
2014-07-21 02:27:02,100 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,100 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1840ms
2014-07-21 02:27:02,100 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,100 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1849ms
2014-07-21 02:27:02,100 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,105 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1860ms
2014-07-21 02:27:02,105 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,105 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1860ms
2014-07-21 02:27:02,106 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,106 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1863ms
2014-07-21 02:27:02,106 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,109 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1867ms
2014-07-21 02:27:02,110 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,111 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1872ms
2014-07-21 02:27:02,111 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,113 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1874ms
2014-07-21 02:27:02,113 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:02,117 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1883ms
2014-07-21 02:27:02,117 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:03,095 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:03,135 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43474 synced till here 43436
2014-07-21 02:27:03,185 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:27:03,185 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:27:03,210 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:27:03,210 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 20 blocking
2014-07-21 02:27:03,210 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-21 02:27:03,210 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:27:03,210 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:27:03,210 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:27:03,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934820468 with entries=152, filesize=83.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934823095
2014-07-21 02:27:03,383 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934666479
2014-07-21 02:27:03,383 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934669101
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934673219
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934687240
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934689724
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934692856
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934694780
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934696554
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934715827
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934716929
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934718441
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934720216
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934722653
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934723905
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934725213
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934726769
2014-07-21 02:27:03,384 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934728622
2014-07-21 02:27:04,578 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:04,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43621 synced till here 43588
2014-07-21 02:27:04,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934823095 with entries=147, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934824578
2014-07-21 02:27:04,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:06,038 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:06,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43774 synced till here 43768
2014-07-21 02:27:06,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934824578 with entries=153, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934826038
2014-07-21 02:27:06,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:06,501 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9860, memsize=196.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/f028b1d3a17540209d9fffb6e32f541c
2014-07-21 02:27:06,514 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/f028b1d3a17540209d9fffb6e32f541c as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/f028b1d3a17540209d9fffb6e32f541c
2014-07-21 02:27:06,527 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/f028b1d3a17540209d9fffb6e32f541c, entries=714690, sequenceid=9860, filesize=50.9m
2014-07-21 02:27:06,527 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.3m/271905840, currentsize=113.1m/118614880 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 10697ms, sequenceid=9860, compaction requested=true
2014-07-21 02:27:06,528 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-21 02:27:06,528 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-21 02:27:06,528 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:27:06,528 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:27:06,528 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:27:06,528 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:27:07,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:07,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43885 synced till here 43880
2014-07-21 02:27:07,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934826038 with entries=111, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934827293
2014-07-21 02:27:07,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:08,617 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:08,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43981 synced till here 43970
2014-07-21 02:27:08,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934827293 with entries=96, filesize=74.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934828617
2014-07-21 02:27:08,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:10,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:10,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44078 synced till here 44075
2014-07-21 02:27:10,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934828617 with entries=97, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934830028
2014-07-21 02:27:10,067 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:11,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:11,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44183 synced till here 44177
2014-07-21 02:27:11,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934830028 with entries=105, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934831884
2014-07-21 02:27:11,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:15,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:15,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44302 synced till here 44285
2014-07-21 02:27:15,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934831884 with entries=119, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934835551
2014-07-21 02:27:15,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:16,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:17,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934835551 with entries=96, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934836988
2014-07-21 02:27:17,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:18,892 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:18,929 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934836988 with entries=91, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934838893
2014-07-21 02:27:18,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:27:21,481 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90461ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:27:21,481 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.8g
2014-07-21 02:27:21,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:21,663 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44597 synced till here 44596
2014-07-21 02:27:21,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934838893 with entries=108, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934841639
2014-07-21 02:27:23,165 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:23,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44696 synced till here 44693
2014-07-21 02:27:23,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934841639 with entries=99, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934843166
2014-07-21 02:27:23,382 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:27:25,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:25,666 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934843166 with entries=110, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934845423
2014-07-21 02:27:28,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:28,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44904 synced till here 44903
2014-07-21 02:27:28,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934845423 with entries=98, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934848768
2014-07-21 02:27:29,434 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:27:29,434 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files, but is 795.7m vs best flushable region's 244.8m. Choosing the bigger.
2014-07-21 02:27:29,434 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. due to global heap pressure
2014-07-21 02:27:29,435 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 795.7m
2014-07-21 02:27:30,073 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:27:31,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:31,643 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44999 synced till here 44997
2014-07-21 02:27:31,662 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934848768 with entries=95, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934851629
2014-07-21 02:27:32,378 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:27:34,286 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:34,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934851629 with entries=98, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934854287
2014-07-21 02:27:34,530 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,544 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,558 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,560 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,564 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,707 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,707 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,708 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,708 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,709 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,709 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,709 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,710 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,710 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,711 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,718 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,726 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,729 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,733 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,788 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,805 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,816 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,848 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,895 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,912 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,920 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,928 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,972 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,985 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:34,996 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,008 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,022 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,038 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,045 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,114 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,148 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,202 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,520 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,524 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:35,574 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:36,690 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:36,737 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:36,770 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,300 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,320 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,331 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,340 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,358 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,394 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:38,405 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:27:39,530 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,545 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,558 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,560 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,707 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5143ms
2014-07-21 02:27:39,707 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-07-21 02:27:39,707 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-07-21 02:27:39,708 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5044ms
2014-07-21 02:27:39,708 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5047ms
2014-07-21 02:27:39,709 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5075ms
2014-07-21 02:27:39,709 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5108ms
2014-07-21 02:27:39,710 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5111ms
2014-07-21 02:27:39,710 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5123ms
2014-07-21 02:27:39,711 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5136ms
2014-07-21 02:27:39,712 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5147ms
2014-07-21 02:27:39,720 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,727 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,729 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,733 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,805 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,817 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,848 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,896 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,913 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:39,920 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,929 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,973 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,985 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:39,996 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,009 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,023 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:40,038 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,045 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,114 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,148 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,202 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,520 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:40,524 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:40,575 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:42,338 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5568ms
2014-07-21 02:27:42,339 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-07-21 02:27:42,339 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5602ms
2014-07-21 02:27:43,301 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:43,321 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:43,331 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:43,340 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:43,358 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:43,394 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:27:43,406 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:27:44,531 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,545 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,559 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,560 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:44,707 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10143ms
2014-07-21 02:27:44,707 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10024ms
2014-07-21 02:27:44,708 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10030ms
2014-07-21 02:27:44,709 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10045ms
2014-07-21 02:27:44,709 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10048ms
2014-07-21 02:27:44,709 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10075ms
2014-07-21 02:27:44,710 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10109ms
2014-07-21 02:27:44,710 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10111ms
2014-07-21 02:27:44,711 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10123ms
2014-07-21 02:27:44,711 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10136ms
2014-07-21 02:27:44,712 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10147ms
2014-07-21 02:27:44,720 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:27:44,727 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,730 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:44,734 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:44,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,806 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,817 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,849 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,896 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,913 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,920 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,929 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,973 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:44,986 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:27:44,996 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,009 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:45,023 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:45,038 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,045 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,115 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:45,148 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,202 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,522 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-21 02:27:45,524 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-21 02:27:45,575 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:47,293 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1450ms
GC pool 'ParNew' had collection(s): count=1 time=1341ms
2014-07-21 02:27:47,339 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10569ms
2014-07-21 02:27:47,339 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10602ms
2014-07-21 02:27:47,340 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10650ms
2014-07-21 02:27:48,302 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,322 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,333 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,341 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,358 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,395 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:48,406 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-21 02:27:49,459 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14093, memsize=455.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f63488a6e8ed4cad8a2a87dce4128a8c
2014-07-21 02:27:49,473 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/f63488a6e8ed4cad8a2a87dce4128a8c as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f63488a6e8ed4cad8a2a87dce4128a8c
2014-07-21 02:27:49,486 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/f63488a6e8ed4cad8a2a87dce4128a8c, entries=1659040, sequenceid=14093, filesize=118.1m
2014-07-21 02:27:49,486 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~798.6m/837440720, currentsize=69.7m/73132720 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 20051ms, sequenceid=14093, compaction requested=true
2014-07-21 02:27:49,487 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:27:49,487 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:27:49,487 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11082ms
2014-07-21 02:27:49,487 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:27:49,487 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,487 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 267.9m
2014-07-21 02:27:49,487 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:27:49,487 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11093ms
2014-07-21 02:27:49,487 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:27:49,487 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,487 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:27:49,488 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11131ms
2014-07-21 02:27:49,488 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,489 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11149ms
2014-07-21 02:27:49,489 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,489 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11158ms
2014-07-21 02:27:49,490 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,490 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11170ms
2014-07-21 02:27:49,490 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,490 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11190ms
2014-07-21 02:27:49,490 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,506 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12815ms
2014-07-21 02:27:49,506 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,506 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12769ms
2014-07-21 02:27:49,506 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,506 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12736ms
2014-07-21 02:27:49,507 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,508 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13934ms
2014-07-21 02:27:49,508 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,508 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13984ms
2014-07-21 02:27:49,508 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,510 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13991ms
2014-07-21 02:27:49,510 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,510 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14308ms
2014-07-21 02:27:49,510 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,517 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14369ms
2014-07-21 02:27:49,517 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,518 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14404ms
2014-07-21 02:27:49,518 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,529 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14484ms
2014-07-21 02:27:49,529 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,530 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14491ms
2014-07-21 02:27:49,530 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,530 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14508ms
2014-07-21 02:27:49,530 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,530 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14522ms
2014-07-21 02:27:49,530 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,531 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14534ms
2014-07-21 02:27:49,531 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,537 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15007ms
2014-07-21 02:27:49,537 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,541 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14557ms
2014-07-21 02:27:49,541 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,542 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14569ms
2014-07-21 02:27:49,542 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,542 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14614ms
2014-07-21 02:27:49,542 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,542 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14623ms
2014-07-21 02:27:49,542 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,543 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14630ms
2014-07-21 02:27:49,543 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,546 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:27:49,546 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,548 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14653ms
2014-07-21 02:27:49,549 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,554 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14705ms
2014-07-21 02:27:49,554 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,555 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14738ms
2014-07-21 02:27:49,555 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,556 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14750ms
2014-07-21 02:27:49,556 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,557 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14768ms
2014-07-21 02:27:49,557 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,557 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14824ms
2014-07-21 02:27:49,557 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,559 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:27:49,559 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,561 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-21 02:27:49,561 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,562 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14832ms
2014-07-21 02:27:49,562 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,565 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14839ms
2014-07-21 02:27:49,565 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,566 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14848ms
2014-07-21 02:27:49,566 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,577 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15012ms
2014-07-21 02:27:49,577 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,585 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15010ms
2014-07-21 02:27:49,586 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,593 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11266,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858325,"queuetimems":0,"class":"HRegionServer","responsesize":835846,"method":"Multi"}
2014-07-21 02:27:49,593 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-21 02:27:49,593 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,601 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-21 02:27:49,601 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,602 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-21 02:27:49,603 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,610 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14976ms
2014-07-21 02:27:49,610 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,610 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14949ms
2014-07-21 02:27:49,610 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,612 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14948ms
2014-07-21 02:27:49,612 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,616 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14097,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855517,"queuetimems":1,"class":"HRegionServer","responsesize":957769,"method":"Multi"}
2014-07-21 02:27:49,617 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14939ms
2014-07-21 02:27:49,617 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,618 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14935ms
2014-07-21 02:27:49,618 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,621 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15057ms
2014-07-21 02:27:49,621 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:27:49,625 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855572,"queuetimems":0,"class":"HRegionServer","responsesize":1017995,"method":"Multi"}
2014-07-21 02:27:49,639 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855521,"queuetimems":1,"class":"HRegionServer","responsesize":1085573,"method":"Multi"}
2014-07-21 02:27:49,660 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11266,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858297,"queuetimems":0,"class":"HRegionServer","responsesize":750641,"method":"Multi"}
2014-07-21 02:27:50,436 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15511,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854924,"queuetimems":0,"class":"HRegionServer","responsesize":797652,"method":"Multi"}
2014-07-21 02:27:50,440 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15875,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854564,"queuetimems":0,"class":"HRegionServer","responsesize":835846,"method":"Multi"}
2014-07-21 02:27:50,445 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855004,"queuetimems":0,"class":"HRegionServer","responsesize":793243,"method":"Multi"}
2014-07-21 02:27:50,445 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855197,"queuetimems":1,"class":"HRegionServer","responsesize":1106150,"method":"Multi"}
2014-07-21 02:27:50,454 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855018,"queuetimems":1,"class":"HRegionServer","responsesize":865228,"method":"Multi"}
2014-07-21 02:27:50,461 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15929,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854528,"queuetimems":0,"class":"HRegionServer","responsesize":862277,"method":"Multi"}
2014-07-21 02:27:50,461 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854903,"queuetimems":0,"class":"HRegionServer","responsesize":834377,"method":"Multi"}
2014-07-21 02:27:50,475 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858315,"queuetimems":0,"class":"HRegionServer","responsesize":832914,"method":"Multi"}
2014-07-21 02:27:50,481 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12059,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858401,"queuetimems":1,"class":"HRegionServer","responsesize":1107609,"method":"Multi"}
2014-07-21 02:27:50,497 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15954,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854542,"queuetimems":1,"class":"HRegionServer","responsesize":878447,"method":"Multi"}
2014-07-21 02:27:50,497 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15695,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854801,"queuetimems":0,"class":"HRegionServer","responsesize":1009188,"method":"Multi"}
2014-07-21 02:27:50,504 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855042,"queuetimems":0,"class":"HRegionServer","responsesize":741827,"method":"Multi"}
2014-07-21 02:27:50,504 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15842,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854662,"queuetimems":0,"class":"HRegionServer","responsesize":750641,"method":"Multi"}
2014-07-21 02:27:50,505 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854725,"queuetimems":0,"class":"HRegionServer","responsesize":1031222,"method":"Multi"}
2014-07-21 02:27:50,507 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854980,"queuetimems":1,"class":"HRegionServer","responsesize":1116425,"method":"Multi"}
2014-07-21 02:27:50,513 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15838,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854674,"queuetimems":0,"class":"HRegionServer","responsesize":1107609,"method":"Multi"}
2014-07-21 02:27:50,513 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15698,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854814,"queuetimems":1,"class":"HRegionServer","responsesize":737418,"method":"Multi"}
2014-07-21 02:27:50,515 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15799,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854715,"queuetimems":1,"class":"HRegionServer","responsesize":1042978,"method":"Multi"}
2014-07-21 02:27:50,518 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15932,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854585,"queuetimems":0,"class":"HRegionServer","responsesize":832914,"method":"Multi"}
2014-07-21 02:27:50,505 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:27:50,523 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855108,"queuetimems":0,"class":"HRegionServer","responsesize":866701,"method":"Multi"}
2014-07-21 02:27:50,524 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15380,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855143,"queuetimems":1,"class":"HRegionServer","responsesize":1101738,"method":"Multi"}
2014-07-21 02:27:50,524 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854844,"queuetimems":1,"class":"HRegionServer","responsesize":1065015,"method":"Multi"}
2014-07-21 02:27:50,532 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854916,"queuetimems":1,"class":"HRegionServer","responsesize":1091452,"method":"Multi"}
2014-07-21 02:27:50,532 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854992,"queuetimems":1,"class":"HRegionServer","responsesize":834382,"method":"Multi"}
2014-07-21 02:27:50,532 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15497,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934855034,"queuetimems":0,"class":"HRegionServer","responsesize":1022412,"method":"Multi"}
2014-07-21 02:27:50,536 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12181,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858354,"queuetimems":1,"class":"HRegionServer","responsesize":1042978,"method":"Multi"}
2014-07-21 02:27:50,852 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854507,"queuetimems":1,"class":"HRegionServer","responsesize":1084824,"method":"Multi"}
2014-07-21 02:27:50,854 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854448,"queuetimems":0,"class":"HRegionServer","responsesize":1153059,"method":"Multi"}
2014-07-21 02:27:50,858 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858337,"queuetimems":1,"class":"HRegionServer","responsesize":4700,"method":"Multi"}
2014-07-21 02:27:50,858 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854628,"queuetimems":0,"class":"HRegionServer","responsesize":6936,"method":"Multi"}
2014-07-21 02:27:50,866 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16259,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854599,"queuetimems":1,"class":"HRegionServer","responsesize":3965,"method":"Multi"}
2014-07-21 02:27:50,870 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854681,"queuetimems":1,"class":"HRegionServer","responsesize":2090,"method":"Multi"}
2014-07-21 02:27:50,871 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854564,"queuetimems":0,"class":"HRegionServer","responsesize":315,"method":"Multi"}
2014-07-21 02:27:50,875 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854559,"queuetimems":0,"class":"HRegionServer","responsesize":1480,"method":"Multi"}
2014-07-21 02:27:50,875 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854599,"queuetimems":0,"class":"HRegionServer","responsesize":26,"method":"Multi"}
2014-07-21 02:27:50,876 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854785,"queuetimems":1,"class":"HRegionServer","responsesize":4278,"method":"Multi"}
2014-07-21 02:27:50,882 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16309,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854572,"queuetimems":0,"class":"HRegionServer","responsesize":4446,"method":"Multi"}
2014-07-21 02:27:52,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:52,032 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934856730,"queuetimems":0,"class":"HRegionServer","responsesize":1049292,"method":"Multi"}
2014-07-21 02:27:52,038 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17481,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854556,"queuetimems":0,"class":"HRegionServer","responsesize":4700,"method":"Multi"}
2014-07-21 02:27:52,052 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854969,"queuetimems":1,"class":"HRegionServer","responsesize":1095005,"method":"Multi"}
2014-07-21 02:27:52,053 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15363,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934856688,"queuetimems":0,"class":"HRegionServer","responsesize":1077183,"method":"Multi"}
2014-07-21 02:27:52,058 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854891,"queuetimems":1,"class":"HRegionServer","responsesize":1111482,"method":"Multi"}
2014-07-21 02:27:52,687 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15918,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934856768,"queuetimems":0,"class":"HRegionServer","responsesize":1015371,"method":"Multi"}
2014-07-21 02:27:52,700 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45199 synced till here 45193
2014-07-21 02:27:52,768 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934858388,"queuetimems":0,"class":"HRegionServer","responsesize":660148,"method":"Multi"}
2014-07-21 02:27:52,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934854287 with entries=102, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934872031
2014-07-21 02:27:53,015 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18355,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934854659,"queuetimems":1,"class":"HRegionServer","responsesize":834429,"method":"Multi"}
2014-07-21 02:27:54,772 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:55,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934872031 with entries=127, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934874773
2014-07-21 02:27:56,979 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:57,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45455 synced till here 45441
2014-07-21 02:27:57,750 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:27:57,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934874773 with entries=129, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934876979
2014-07-21 02:27:59,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:27:59,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45575 synced till here 45562
2014-07-21 02:27:59,843 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934876979 with entries=120, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934879299
2014-07-21 02:28:01,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:01,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45706 synced till here 45673
2014-07-21 02:28:01,890 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934879299 with entries=131, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934881564
2014-07-21 02:28:02,816 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,817 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,818 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,819 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,819 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,820 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,821 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,822 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,822 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,825 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,826 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,826 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,826 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,827 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:02,828 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,458 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:03,462 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,463 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,464 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,464 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,465 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,465 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,466 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,466 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,466 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,467 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,469 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,469 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,469 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,470 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,470 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,470 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,471 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,471 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,472 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,472 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45823 synced till here 45810
2014-07-21 02:28:03,562 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,563 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,564 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,564 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,565 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,565 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,566 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,566 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,567 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,568 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,568 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,570 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,570 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,572 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,579 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:03,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934881564 with entries=117, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934883459
2014-07-21 02:28:03,604 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10438, memsize=147.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/b617cba4c2624e8b9bda3350894d3ddf
2014-07-21 02:28:03,631 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/b617cba4c2624e8b9bda3350894d3ddf as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/b617cba4c2624e8b9bda3350894d3ddf
2014-07-21 02:28:03,649 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/b617cba4c2624e8b9bda3350894d3ddf, entries=537290, sequenceid=10438, filesize=38.2m
2014-07-21 02:28:03,649 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.9m/280868000, currentsize=104.2m/109227680 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 14162ms, sequenceid=10438, compaction requested=true
2014-07-21 02:28:03,649 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:03,649 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 20 blocking
2014-07-21 02:28:03,650 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 71ms
2014-07-21 02:28:03,650 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=3.8g
2014-07-21 02:28:03,650 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,650 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-21 02:28:03,650 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 78ms
2014-07-21 02:28:03,650 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Under global heap pressure: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files, but is 1.2g vs best flushable region's 104.2m. Choosing the bigger.
2014-07-21 02:28:03,650 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,650 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-21 02:28:03,650 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. due to global heap pressure
2014-07-21 02:28:03,650 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 80ms
2014-07-21 02:28:03,650 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:03,651 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.2g
2014-07-21 02:28:03,651 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:28:03,651 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,651 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 82ms
2014-07-21 02:28:03,651 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,651 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 83ms
2014-07-21 02:28:03,651 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,651 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 83ms
2014-07-21 02:28:03,651 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,652 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 85ms
2014-07-21 02:28:03,652 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,665 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 99ms
2014-07-21 02:28:03,665 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,665 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 99ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,666 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 101ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,666 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 101ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,666 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 102ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,666 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 102ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,666 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 103ms
2014-07-21 02:28:03,666 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,667 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 105ms
2014-07-21 02:28:03,667 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,667 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 195ms
2014-07-21 02:28:03,668 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,668 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 196ms
2014-07-21 02:28:03,668 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,668 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 197ms
2014-07-21 02:28:03,668 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,668 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 197ms
2014-07-21 02:28:03,668 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,681 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-07-21 02:28:03,681 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,682 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 212ms
2014-07-21 02:28:03,682 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,682 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 212ms
2014-07-21 02:28:03,682 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,682 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 213ms
2014-07-21 02:28:03,682 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,706 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 240ms
2014-07-21 02:28:03,706 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,713 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 244ms
2014-07-21 02:28:03,713 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,713 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 246ms
2014-07-21 02:28:03,714 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,721 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 256ms
2014-07-21 02:28:03,721 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,721 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 255ms
2014-07-21 02:28:03,722 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,722 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 257ms
2014-07-21 02:28:03,722 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,722 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 257ms
2014-07-21 02:28:03,722 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,779 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 314ms
2014-07-21 02:28:03,779 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,779 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 315ms
2014-07-21 02:28:03,779 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,781 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 318ms
2014-07-21 02:28:03,781 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,781 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 318ms
2014-07-21 02:28:03,781 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,781 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 319ms
2014-07-21 02:28:03,781 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,781 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 953ms
2014-07-21 02:28:03,781 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,781 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 954ms
2014-07-21 02:28:03,782 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,793 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 971ms
2014-07-21 02:28:03,793 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,795 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 973ms
2014-07-21 02:28:03,795 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,801 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 976ms
2014-07-21 02:28:03,801 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,801 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 982ms
2014-07-21 02:28:03,802 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,802 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 980ms
2014-07-21 02:28:03,802 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,802 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 981ms
2014-07-21 02:28:03,802 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,804 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 983ms
2014-07-21 02:28:03,805 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,809 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 990ms
2014-07-21 02:28:03,809 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,811 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 992ms
2014-07-21 02:28:03,811 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,811 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 992ms
2014-07-21 02:28:03,812 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,812 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 994ms
2014-07-21 02:28:03,812 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,821 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1004ms
2014-07-21 02:28:03,821 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:03,821 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-07-21 02:28:03,822 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:05,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:05,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45941 synced till here 45931
2014-07-21 02:28:05,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934883459 with entries=118, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934885162
2014-07-21 02:28:06,038 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:28:06,522 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:06,527 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:06,528 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:06,539 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:06,562 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,026 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,028 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,035 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,035 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,035 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,036 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,037 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,038 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,043 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,047 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,057 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,060 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,062 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,063 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,073 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,106 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,115 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:07,128 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,189 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,198 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,198 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,199 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,217 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,243 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,274 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,292 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,328 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,338 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,348 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,364 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,935 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,970 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:08,982 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:09,189 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:09,190 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:09,193 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:11,184 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:11,185 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:11,522 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:11,527 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:11,528 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:11,539 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:11,562 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,027 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,028 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,035 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,036 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,036 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,036 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,038 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,038 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,043 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,047 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,058 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,060 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,063 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,063 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:12,074 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,107 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,116 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:12,128 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,189 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,198 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,199 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:13,199 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,218 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:13,243 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,275 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:28:13,292 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,293 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,294 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,297 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,328 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,339 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,348 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,365 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:28:13,493 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,511 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,546 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,556 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:28:13,720 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10333, memsize=1006.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/d83b4f40f73a4f669016424ce06d51e5
2014-07-21 02:28:13,729 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/d83b4f40f73a4f669016424ce06d51e5 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/d83b4f40f73a4f669016424ce06d51e5
2014-07-21 02:28:13,739 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/d83b4f40f73a4f669016424ce06d51e5, entries=3665830, sequenceid=10333, filesize=260.8m
2014-07-21 02:28:13,739 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.8g/1982528320, currentsize=159.5m/167291440 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 52258ms, sequenceid=10333, compaction requested=true
2014-07-21 02:28:13,740 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:13,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:28:13,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:28:13,740 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 184ms
2014-07-21 02:28:13,741 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,740 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:28:13,741 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 195ms
2014-07-21 02:28:13,741 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,741 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:28:13,740 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:13,741 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 230ms
2014-07-21 02:28:13,742 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,741 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:13,742 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 249ms
2014-07-21 02:28:13,742 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,742 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:28:13,742 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5378ms
2014-07-21 02:28:13,742 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,742 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:28:13,742 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5394ms
2014-07-21 02:28:13,742 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,742 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:28:13,742 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5404ms
2014-07-21 02:28:13,743 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,745 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5417ms
2014-07-21 02:28:13,745 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,749 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 452ms
2014-07-21 02:28:13,749 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,749 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 455ms
2014-07-21 02:28:13,749 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,750 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 457ms
2014-07-21 02:28:13,750 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,743 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:13,750 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5458ms
2014-07-21 02:28:13,750 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,756 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5482ms
2014-07-21 02:28:13,756 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,756 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5513ms
2014-07-21 02:28:13,757 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,757 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5540ms
2014-07-21 02:28:13,757 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,750 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:13,761 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:28:13,763 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5564ms
2014-07-21 02:28:13,763 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,763 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5565ms
2014-07-21 02:28:13,763 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,763 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5565ms
2014-07-21 02:28:13,763 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,763 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5574ms
2014-07-21 02:28:13,763 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,764 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6636ms
2014-07-21 02:28:13,764 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,764 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6649ms
2014-07-21 02:28:13,764 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,764 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6658ms
2014-07-21 02:28:13,764 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,769 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6696ms
2014-07-21 02:28:13,769 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,770 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6707ms
2014-07-21 02:28:13,770 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,770 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6708ms
2014-07-21 02:28:13,770 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,770 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6711ms
2014-07-21 02:28:13,770 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,770 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6713ms
2014-07-21 02:28:13,770 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,770 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6723ms
2014-07-21 02:28:13,770 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,781 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6738ms
2014-07-21 02:28:13,781 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,781 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6743ms
2014-07-21 02:28:13,781 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,782 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6745ms
2014-07-21 02:28:13,782 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,782 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6746ms
2014-07-21 02:28:13,782 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,782 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6747ms
2014-07-21 02:28:13,782 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,785 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6750ms
2014-07-21 02:28:13,785 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,785 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6750ms
2014-07-21 02:28:13,786 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,789 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6762ms
2014-07-21 02:28:13,789 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,789 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6763ms
2014-07-21 02:28:13,789 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,790 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7227ms
2014-07-21 02:28:13,790 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,790 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7251ms
2014-07-21 02:28:13,790 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,801 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7274ms
2014-07-21 02:28:13,801 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,801 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7274ms
2014-07-21 02:28:13,801 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,802 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7279ms
2014-07-21 02:28:13,802 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,802 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2617ms
2014-07-21 02:28:13,802 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,802 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2618ms
2014-07-21 02:28:13,802 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,803 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4609ms
2014-07-21 02:28:13,803 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,804 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4613ms
2014-07-21 02:28:13,804 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,810 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4621ms
2014-07-21 02:28:13,810 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,810 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4828ms
2014-07-21 02:28:13,810 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,810 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4840ms
2014-07-21 02:28:13,811 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:13,818 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4882ms
2014-07-21 02:28:13,818 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:28:14,051 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:14,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46070 synced till here 46053
2014-07-21 02:28:14,731 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934885162 with entries=129, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934894052
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934730455
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934731725
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934733063
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934739951
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934741740
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934743268
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934744867
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934745836
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934747174
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934749242
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934751643
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934753223
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934755580
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934757425
2014-07-21 02:28:14,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934758669
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934761769
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934764817
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934766338
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934767509
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934770488
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934772653
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934775039
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934813013
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934814702
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934816807
2014-07-21 02:28:14,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934818691
2014-07-21 02:28:15,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:15,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46175 synced till here 46174
2014-07-21 02:28:15,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934894052 with entries=105, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934895867
2014-07-21 02:28:16,722 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:28:16,723 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:28:16,723 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:16,724 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:28:16,724 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:28:16,724 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:16,724 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:16,724 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:28:17,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:17,413 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46266 synced till here 46265
2014-07-21 02:28:17,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934895867 with entries=91, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934897376
2014-07-21 02:28:19,038 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:19,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934897376 with entries=102, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934899038
2014-07-21 02:28:20,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:20,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46457 synced till here 46453
2014-07-21 02:28:21,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934899038 with entries=89, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934900829
2014-07-21 02:28:23,061 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:23,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46572 synced till here 46538
2014-07-21 02:28:23,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934900829 with entries=115, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934903063
2014-07-21 02:28:24,459 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:24,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46679 synced till here 46674
2014-07-21 02:28:24,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934903063 with entries=107, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934904460
2014-07-21 02:28:26,007 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:26,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46781 synced till here 46772
2014-07-21 02:28:26,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934904460 with entries=102, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934906008
2014-07-21 02:28:28,462 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90196ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:28:28,463 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 800.0m
2014-07-21 02:28:29,042 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:28:32,126 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10422, memsize=600.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/821f6426da874b01a49562925dcc6ac8
2014-07-21 02:28:32,168 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/821f6426da874b01a49562925dcc6ac8 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/821f6426da874b01a49562925dcc6ac8
2014-07-21 02:28:32,200 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/821f6426da874b01a49562925dcc6ac8, entries=2186500, sequenceid=10422, filesize=155.5m
2014-07-21 02:28:32,200 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.2g/1314637440, currentsize=356.3m/373655280 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 28549ms, sequenceid=10422, compaction requested=true
2014-07-21 02:28:32,200 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:32,201 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:28:32,201 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:28:32,201 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:32,201 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:32,201 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:28:33,272 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:28:33,272 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:28:33,273 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:33,273 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 20 blocking
2014-07-21 02:28:33,273 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-21 02:28:33,273 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:33,273 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:33,273 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:28:35,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:35,292 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46909 synced till here 46908
2014-07-21 02:28:35,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934906008 with entries=128, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934915272
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934777484
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934778759
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934781908
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934809023
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934810841
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934820468
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934823095
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934824578
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934826038
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934827293
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934828617
2014-07-21 02:28:35,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934830028
2014-07-21 02:28:35,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934831884
2014-07-21 02:28:35,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934835551
2014-07-21 02:28:35,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934836988
2014-07-21 02:28:39,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:39,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47006 synced till here 47005
2014-07-21 02:28:39,634 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934915272 with entries=97, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934919562
2014-07-21 02:28:40,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:40,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934919562 with entries=71, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934920521
2014-07-21 02:28:42,278 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14778, memsize=335.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/cefdc156ea9c42bfa2e408d0d7b5ec87
2014-07-21 02:28:42,290 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/cefdc156ea9c42bfa2e408d0d7b5ec87 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/cefdc156ea9c42bfa2e408d0d7b5ec87
2014-07-21 02:28:42,300 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/cefdc156ea9c42bfa2e408d0d7b5ec87, entries=1219730, sequenceid=14778, filesize=86.8m
2014-07-21 02:28:42,301 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~800.0m/838892560, currentsize=101.1m/105978480 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 13839ms, sequenceid=14778, compaction requested=true
2014-07-21 02:28:42,302 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:42,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:28:42,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:28:42,302 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:42,303 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:42,303 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:28:43,582 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:28:43,582 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 256.3m
2014-07-21 02:28:43,796 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:28:44,360 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:44,468 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47196 synced till here 47190
2014-07-21 02:28:44,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934920521 with entries=119, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934924360
2014-07-21 02:28:46,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:46,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47298 synced till here 47297
2014-07-21 02:28:46,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934924360 with entries=102, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934926760
2014-07-21 02:28:48,387 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10936, memsize=104.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4ede20edf25c49f68507b78dc73b5f31
2014-07-21 02:28:48,400 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/4ede20edf25c49f68507b78dc73b5f31 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4ede20edf25c49f68507b78dc73b5f31
2014-07-21 02:28:48,414 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/4ede20edf25c49f68507b78dc73b5f31, entries=381510, sequenceid=10936, filesize=27.2m
2014-07-21 02:28:48,414 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.3m/268795360, currentsize=33.6m/35231440 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 4832ms, sequenceid=10936, compaction requested=true
2014-07-21 02:28:48,414 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:48,414 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 20 blocking
2014-07-21 02:28:48,415 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-21 02:28:48,415 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:48,415 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:48,415 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:28:48,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:48,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47382 synced till here 47380
2014-07-21 02:28:48,614 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934926760 with entries=84, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934928569
2014-07-21 02:28:48,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934854287
2014-07-21 02:28:48,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934872031
2014-07-21 02:28:48,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934874773
2014-07-21 02:28:48,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934876979
2014-07-21 02:28:48,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934879299
2014-07-21 02:28:48,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934881564
2014-07-21 02:28:52,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:52,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934928569 with entries=114, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934932692
2014-07-21 02:28:55,064 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:28:55,064 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:28:55,065 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:28:55,065 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:28:55,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:28:55,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:28:55,066 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:28:55,066 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:28:55,081 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:55,098 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47594 synced till here 47593
2014-07-21 02:28:55,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934932692 with entries=98, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934935081
2014-07-21 02:28:57,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:28:57,657 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47676 synced till here 47674
2014-07-21 02:28:57,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934935081 with entries=82, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934937632
2014-07-21 02:29:00,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:00,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934937632 with entries=92, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934940096
2014-07-21 02:29:02,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:03,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934940096 with entries=124, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934942939
2014-07-21 02:29:05,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:05,781 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47984 synced till here 47983
2014-07-21 02:29:06,153 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934942939 with entries=92, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934945764
2014-07-21 02:29:09,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:09,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48066 synced till here 48065
2014-07-21 02:29:09,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934945764 with entries=82, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934949256
2014-07-21 02:29:10,483 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:10,810 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48181 synced till here 48179
2014-07-21 02:29:10,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934949256 with entries=115, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934950483
2014-07-21 02:29:13,509 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:13,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48268 synced till here 48266
2014-07-21 02:29:13,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934950483 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934953509
2014-07-21 02:29:16,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:16,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48365 synced till here 48364
2014-07-21 02:29:16,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934953509 with entries=97, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934956425
2014-07-21 02:29:21,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:21,611 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48454 synced till here 48452
2014-07-21 02:29:21,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934956425 with entries=89, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934961586
2014-07-21 02:29:23,988 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:24,014 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934961586 with entries=79, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934963988
2014-07-21 02:29:24,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:29:25,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:25,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48621 synced till here 48615
2014-07-21 02:29:25,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934963988 with entries=88, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934965352
2014-07-21 02:29:25,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:29:28,535 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90786ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:29:28,536 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 686.3m
2014-07-21 02:29:28,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:28,856 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934965352 with entries=97, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934968794
2014-07-21 02:29:28,856 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:29:28,999 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:29:30,589 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:29:30,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48809 synced till here 48807
2014-07-21 02:29:30,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934968794 with entries=91, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934970589
2014-07-21 02:29:30,630 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:29:56,251 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 28425ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:29:56,252 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 28427ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:29:56,252 WARN  [regionserver60020] util.Sleeper: We slept 23591ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-21 02:29:56,252 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 22082ms
GC pool 'ParNew' had collection(s): count=1 time=909ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=21513ms
2014-07-21 02:29:56,253 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 99531ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:29:56,254 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.3g
2014-07-21 02:29:56,305 WARN  [RpcServer.reader=1,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-21 02:29:56,346 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46417 service: ClientService methodName: Multi size: 721.9k connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,347 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46425 service: ClientService methodName: Multi size: 15.1k connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46419 service: ClientService methodName: Multi size: 708.2k connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46421 service: ClientService methodName: Multi size: 745.6k connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,356 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,467 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934973647,"queuetimems":0,"class":"HRegionServer","responsesize":12222,"method":"Multi"}
2014-07-21 02:29:56,467 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46401 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,467 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,511 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53780","starttimems":1405934973787,"queuetimems":1,"class":"HRegionServer","responsesize":11938,"method":"Multi"}
2014-07-21 02:29:56,511 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46402 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,511 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,524 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46426 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,524 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:56,528 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 46414 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.53:53780: output error
2014-07-21 02:29:56,528 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-21 02:29:57,216 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:30:00,529 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:00,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48917 synced till here 48915
2014-07-21 02:30:00,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934970589 with entries=108, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935000529
2014-07-21 02:30:05,321 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:05,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935000529 with entries=104, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935005322
2014-07-21 02:30:07,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:07,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49133 synced till here 49128
2014-07-21 02:30:07,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935005322 with entries=112, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935007668
2014-07-21 02:30:10,557 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:10,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935007668 with entries=98, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935010558
2014-07-21 02:30:12,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:13,511 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49387 synced till here 49385
2014-07-21 02:30:13,541 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935010558 with entries=156, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935012847
2014-07-21 02:30:14,466 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:30:15,071 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15451, memsize=684.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/3c8b965b00024da4a7194978420b82ff
2014-07-21 02:30:15,089 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/3c8b965b00024da4a7194978420b82ff as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/3c8b965b00024da4a7194978420b82ff
2014-07-21 02:30:15,101 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/3c8b965b00024da4a7194978420b82ff, entries=2490820, sequenceid=15451, filesize=177.4m
2014-07-21 02:30:15,102 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~686.3m/719588880, currentsize=243.0m/254766320 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 46566ms, sequenceid=15451, compaction requested=true
2014-07-21 02:30:15,103 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:30:15,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:30:15,103 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 101831ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:30:15,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:30:15,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:15,103 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:15,103 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.3g
2014-07-21 02:30:15,103 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:30:15,391 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:30:15,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:15,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49480 synced till here 49478
2014-07-21 02:30:15,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935012847 with entries=93, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935015454
2014-07-21 02:30:16,085 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:30:17,572 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:17,610 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935015454 with entries=106, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935017572
2014-07-21 02:30:20,993 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:30:24,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:24,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935017572 with entries=109, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935024631
2014-07-21 02:30:25,838 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:25,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49783 synced till here 49780
2014-07-21 02:30:25,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935024631 with entries=88, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935025839
2014-07-21 02:30:27,565 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:27,585 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49871 synced till here 49870
2014-07-21 02:30:27,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935025839 with entries=88, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935027565
2014-07-21 02:30:29,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:29,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935027565 with entries=115, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935029517
2014-07-21 02:30:34,093 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:34,125 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,126 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,156 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,160 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,175 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,236 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,261 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,429 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935029517 with entries=119, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935034094
2014-07-21 02:30:34,429 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,467 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,505 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,561 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,595 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,598 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,631 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,642 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,657 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,663 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,676 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,701 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,732 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,735 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,761 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,761 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:34,769 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,290 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,389 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,445 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,983 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,985 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,988 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,990 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,992 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:35,995 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:36,242 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:36,273 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:36,296 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:36,323 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,294 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,300 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,302 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,308 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,309 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,320 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,495 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,518 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,544 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,573 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,577 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,588 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:38,589 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:30:39,126 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,126 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,156 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,160 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,176 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,236 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,261 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,429 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,467 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,505 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,561 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,596 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,599 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,631 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,643 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,658 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,663 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,676 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,702 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:39,732 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,735 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,761 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,761 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:39,769 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:40,291 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:40,390 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-21 02:30:40,445 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:30:40,686 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11077, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/820e508aecd24e079c757adfba2ea611
2014-07-21 02:30:40,709 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/820e508aecd24e079c757adfba2ea611 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/820e508aecd24e079c757adfba2ea611
2014-07-21 02:30:40,731 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/820e508aecd24e079c757adfba2ea611, entries=4387650, sequenceid=11077, filesize=312.1m
2014-07-21 02:30:40,732 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.3g/1369910000, currentsize=400.9m/420397280 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 44477ms, sequenceid=11077, compaction requested=true
2014-07-21 02:30:40,732 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:30:40,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:30:40,732 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5287ms
2014-07-21 02:30:40,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:30:40,732 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:40,733 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5343ms
2014-07-21 02:30:40,733 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:40,732 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 325.8m
2014-07-21 02:30:40,733 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:30:40,733 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,733 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5443ms
2014-07-21 02:30:40,733 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,733 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5964ms
2014-07-21 02:30:40,733 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,733 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5972ms
2014-07-21 02:30:40,733 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,734 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5973ms
2014-07-21 02:30:40,734 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,747 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6012ms
2014-07-21 02:30:40,747 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,747 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6015ms
2014-07-21 02:30:40,747 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,747 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6046ms
2014-07-21 02:30:40,748 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,749 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6072ms
2014-07-21 02:30:40,749 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,750 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6086ms
2014-07-21 02:30:40,751 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,751 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6094ms
2014-07-21 02:30:40,751 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,751 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6109ms
2014-07-21 02:30:40,751 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,753 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6121ms
2014-07-21 02:30:40,753 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,755 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6156ms
2014-07-21 02:30:40,755 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,755 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6160ms
2014-07-21 02:30:40,755 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,756 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6195ms
2014-07-21 02:30:40,756 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,757 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6252ms
2014-07-21 02:30:40,757 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,757 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6290ms
2014-07-21 02:30:40,757 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,757 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6329ms
2014-07-21 02:30:40,757 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,757 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6496ms
2014-07-21 02:30:40,758 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,765 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6530ms
2014-07-21 02:30:40,765 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,766 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6591ms
2014-07-21 02:30:40,766 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,766 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6606ms
2014-07-21 02:30:40,766 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,767 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6611ms
2014-07-21 02:30:40,767 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,768 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6641ms
2014-07-21 02:30:40,768 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,771 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6645ms
2014-07-21 02:30:40,771 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,771 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2182ms
2014-07-21 02:30:40,771 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,771 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2184ms
2014-07-21 02:30:40,771 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,771 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2194ms
2014-07-21 02:30:40,772 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,773 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2199ms
2014-07-21 02:30:40,773 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,773 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2229ms
2014-07-21 02:30:40,773 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,779 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2260ms
2014-07-21 02:30:40,779 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,786 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2291ms
2014-07-21 02:30:40,786 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,787 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2467ms
2014-07-21 02:30:40,787 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,788 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2478ms
2014-07-21 02:30:40,788 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,788 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2480ms
2014-07-21 02:30:40,788 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,789 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2487ms
2014-07-21 02:30:40,789 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,789 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2489ms
2014-07-21 02:30:40,789 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,790 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2496ms
2014-07-21 02:30:40,791 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,792 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4469ms
2014-07-21 02:30:40,792 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,801 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4505ms
2014-07-21 02:30:40,801 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,804 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4530ms
2014-07-21 02:30:40,804 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,809 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4567ms
2014-07-21 02:30:40,809 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,810 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4814ms
2014-07-21 02:30:40,810 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,810 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4818ms
2014-07-21 02:30:40,810 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,811 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4821ms
2014-07-21 02:30:40,811 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,811 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4823ms
2014-07-21 02:30:40,811 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,811 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4826ms
2014-07-21 02:30:40,811 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:40,812 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4829ms
2014-07-21 02:30:40,812 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:30:41,013 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:30:41,076 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:30:42,798 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1319ms
GC pool 'ParNew' had collection(s): count=2 time=1581ms
2014-07-21 02:30:43,455 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:43,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50239 synced till here 50214
2014-07-21 02:30:43,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935034094 with entries=134, filesize=99.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935043456
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934838893
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934841639
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934843166
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934845423
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934848768
2014-07-21 02:30:43,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934851629
2014-07-21 02:30:45,075 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1276ms
GC pool 'ParNew' had collection(s): count=1 time=1287ms
2014-07-21 02:30:45,252 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10551,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53784","starttimems":1405935034701,"queuetimems":0,"class":"HRegionServer","responsesize":11975,"method":"Multi"}
2014-07-21 02:30:45,252 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:53784","starttimems":1405935034627,"queuetimems":0,"class":"HRegionServer","responsesize":12818,"method":"Multi"}
2014-07-21 02:30:46,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:46,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50365 synced till here 50337
2014-07-21 02:30:46,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935043456 with entries=126, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935046440
2014-07-21 02:30:47,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:30:47,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50465 synced till here 50464
2014-07-21 02:30:47,986 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935046440 with entries=100, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935047927
2014-07-21 02:30:55,087 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11541, memsize=315.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/18280a6473b244bb85062087ad1fbdf2
2014-07-21 02:30:55,187 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/18280a6473b244bb85062087ad1fbdf2 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/18280a6473b244bb85062087ad1fbdf2
2014-07-21 02:30:55,202 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/18280a6473b244bb85062087ad1fbdf2, entries=1149560, sequenceid=11541, filesize=81.8m
2014-07-21 02:30:55,203 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~325.8m/341656320, currentsize=54.3m/56894160 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 14471ms, sequenceid=11541, compaction requested=true
2014-07-21 02:30:55,204 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:30:55,204 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:30:55,204 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:30:55,204 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:30:55,204 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:30:55,204 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:55,204 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 120140ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:30:55,204 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:55,204 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:30:55,205 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:30:55,205 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 576.3m
2014-07-21 02:30:55,205 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:30:55,205 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:55,205 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:55,205 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:30:55,541 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:30:59,654 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11173, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8c28e06739c14034bb45ec2b9488252b
2014-07-21 02:30:59,673 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/8c28e06739c14034bb45ec2b9488252b as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8c28e06739c14034bb45ec2b9488252b
2014-07-21 02:30:59,687 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/8c28e06739c14034bb45ec2b9488252b, entries=4280810, sequenceid=11173, filesize=304.6m
2014-07-21 02:30:59,687 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1387741280, currentsize=397.6m/416929200 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 44584ms, sequenceid=11173, compaction requested=true
2014-07-21 02:30:59,688 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:30:59,688 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:30:59,688 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:30:59,688 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:30:59,689 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:30:59,689 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:30:59,689 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:30:59,689 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:31:00,089 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:31:00,089 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:31:00,089 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:31:00,089 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 20 blocking
2014-07-21 02:31:00,089 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-21 02:31:00,089 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:31:00,089 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:31:00,089 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:31:03,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:03,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935047927 with entries=128, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935063506
2014-07-21 02:31:03,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934883459
2014-07-21 02:31:03,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934885162
2014-07-21 02:31:03,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934894052
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934895867
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934897376
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934899038
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934900829
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934903063
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934904460
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934906008
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934915272
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934919562
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934920521
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934924360
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934926760
2014-07-21 02:31:03,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934928569
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934932692
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934935081
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934937632
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934940096
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934942939
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934945764
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934949256
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934950483
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934953509
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934956425
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934961586
2014-07-21 02:31:03,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934963988
2014-07-21 02:31:05,908 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:06,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935063506 with entries=98, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935065909
2014-07-21 02:31:08,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:08,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50773 synced till here 50770
2014-07-21 02:31:08,355 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935065909 with entries=82, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935068284
2014-07-21 02:31:09,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:09,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50854 synced till here 50853
2014-07-21 02:31:10,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935068284 with entries=81, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935069651
2014-07-21 02:31:12,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:12,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50943 synced till here 50942
2014-07-21 02:31:12,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935069651 with entries=89, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935072394
2014-07-21 02:31:13,291 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16205, memsize=505.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/7e37d2cab0644fe4a03216e88cbb8889
2014-07-21 02:31:13,307 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/7e37d2cab0644fe4a03216e88cbb8889 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/7e37d2cab0644fe4a03216e88cbb8889
2014-07-21 02:31:13,320 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/7e37d2cab0644fe4a03216e88cbb8889, entries=1841660, sequenceid=16205, filesize=131.2m
2014-07-21 02:31:13,320 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~576.3m/604311440, currentsize=199.3m/208972160 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 18115ms, sequenceid=16205, compaction requested=true
2014-07-21 02:31:13,321 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:31:13,321 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:31:13,321 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:31:13,321 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:31:13,321 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:31:13,321 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:31:13,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:13,558 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51023 synced till here 51021
2014-07-21 02:31:13,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935072394 with entries=80, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935073505
2014-07-21 02:31:13,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934965352
2014-07-21 02:31:13,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934968794
2014-07-21 02:31:19,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:19,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935073505 with entries=133, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935079247
2014-07-21 02:31:19,315 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:31:19,315 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:31:19,315 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:31:19,315 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:31:19,315 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:31:19,316 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:31:19,316 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:31:19,316 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:31:22,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:22,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51258 synced till here 51253
2014-07-21 02:31:22,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935079247 with entries=102, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935082197
2014-07-21 02:31:24,042 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:24,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51340 synced till here 51338
2014-07-21 02:31:24,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935082197 with entries=82, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935084042
2014-07-21 02:31:25,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:25,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51430 synced till here 51425
2014-07-21 02:31:25,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935084042 with entries=90, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935085514
2014-07-21 02:31:27,113 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:27,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51519 synced till here 51517
2014-07-21 02:31:27,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935085514 with entries=89, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935087114
2014-07-21 02:31:28,553 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:28,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935087114 with entries=98, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935088554
2014-07-21 02:31:30,965 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:31,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51703 synced till here 51702
2014-07-21 02:31:31,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935088554 with entries=86, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935090966
2014-07-21 02:31:32,757 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:32,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51786 synced till here 51783
2014-07-21 02:31:32,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935090966 with entries=83, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935092758
2014-07-21 02:31:44,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:44,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935092758 with entries=128, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935104564
2014-07-21 02:31:45,450 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90059ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:31:45,450 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 525.8m
2014-07-21 02:31:45,823 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:31:47,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:47,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52006 synced till here 52001
2014-07-21 02:31:47,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935104564 with entries=92, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935107045
2014-07-21 02:31:49,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:49,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52107 synced till here 52106
2014-07-21 02:31:49,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935107045 with entries=101, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935109624
2014-07-21 02:31:51,641 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:51,657 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52184 synced till here 52181
2014-07-21 02:31:51,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935109624 with entries=77, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935111642
2014-07-21 02:31:51,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:31:52,003 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:31:52,003 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. has too many store files; delaying flush up to 90000ms
2014-07-21 02:31:52,004 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:31:52,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 20 blocking
2014-07-21 02:31:52,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-21 02:31:52,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:31:52,004 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:31:52,004 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:31:53,588 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:31:53,606 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52278 synced till here 52277
2014-07-21 02:31:53,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935111642 with entries=94, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935113588
2014-07-21 02:31:53,637 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:32:02,316 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16748, memsize=482.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/82b872bd9ebf47128cf63afbdbdcb603
2014-07-21 02:32:02,331 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/82b872bd9ebf47128cf63afbdbdcb603 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/82b872bd9ebf47128cf63afbdbdcb603
2014-07-21 02:32:02,348 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/82b872bd9ebf47128cf63afbdbdcb603, entries=1755180, sequenceid=16748, filesize=125.0m
2014-07-21 02:32:02,349 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~529.0m/554716240, currentsize=143.3m/150302800 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 16898ms, sequenceid=16748, compaction requested=true
2014-07-21 02:32:02,349 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:32:02,349 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-21 02:32:02,350 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-21 02:32:02,350 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:32:02,350 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:32:02,350 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:32:05,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:05,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935113588 with entries=136, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935125227
2014-07-21 02:32:05,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:32:07,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:07,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52511 synced till here 52510
2014-07-21 02:32:07,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935125227 with entries=97, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935127372
2014-07-21 02:32:07,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:32:08,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:08,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52602 synced till here 52601
2014-07-21 02:32:09,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935127372 with entries=91, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935128943
2014-07-21 02:32:09,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:32:11,503 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:32:11,503 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:32:11,503 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:32:11,503 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-21 02:32:11,503 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-21 02:32:11,504 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:32:11,504 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:32:11,504 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:32:11,645 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90569ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:32:11,646 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1.4g
2014-07-21 02:32:11,672 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:11,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52688 synced till here 52687
2014-07-21 02:32:11,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935128943 with entries=86, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935131672
2014-07-21 02:32:11,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): b921319eb5e9e7dfd2471db139829ab4
2014-07-21 02:32:11,703 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:32:11,704 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:32:11,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:32:11,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:32:11,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:32:11,704 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:32:11,704 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:32:13,192 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:32:13,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:13,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935131672 with entries=82, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935133687
2014-07-21 02:32:30,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:30,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935133687 with entries=120, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935150169
2014-07-21 02:32:30,935 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90846ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:32:30,936 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.3g
2014-07-21 02:32:31,794 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:32:33,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:33,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52996 synced till here 52990
2014-07-21 02:32:33,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935150169 with entries=106, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935153057
2014-07-21 02:32:34,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:34,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53090 synced till here 53089
2014-07-21 02:32:34,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935153057 with entries=94, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935154091
2014-07-21 02:32:36,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:36,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935154091 with entries=90, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935156528
2014-07-21 02:32:38,908 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:38,928 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53263 synced till here 53262
2014-07-21 02:32:38,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935156528 with entries=83, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935158908
2014-07-21 02:32:44,926 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:32:47,069 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:32:49,289 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:32:49,927 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:32:51,606 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:32:52,069 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:32:54,144 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405932920887: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-21 02:32:54,290 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-21 02:32:54,430 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11850, memsize=1.2g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7c07efbaf5d74926a9cdc6f6de8cb4ca
2014-07-21 02:32:54,456 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7c07efbaf5d74926a9cdc6f6de8cb4ca as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7c07efbaf5d74926a9cdc6f6de8cb4ca
2014-07-21 02:32:54,471 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7c07efbaf5d74926a9cdc6f6de8cb4ca, entries=4318690, sequenceid=11850, filesize=307.5m
2014-07-21 02:32:54,472 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.4g/1496072880, currentsize=246.2m/258199200 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 42826ms, sequenceid=11850, compaction requested=true
2014-07-21 02:32:54,472 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:32:54,473 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-21 02:32:54,473 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5184ms
2014-07-21 02:32:54,473 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-21 02:32:54,473 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 95159ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:32:54,473 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:32:54,473 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:32:54,473 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:32:54,473 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 329ms
2014-07-21 02:32:54,473 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 511.1m
2014-07-21 02:32:54,474 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:32:54,473 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:32:54,474 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7405ms
2014-07-21 02:32:54,474 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:32:54,475 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2869ms
2014-07-21 02:32:54,475 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:32:54,475 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9549ms
2014-07-21 02:32:54,475 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405932920887
2014-07-21 02:32:54,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:54,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935158908 with entries=120, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935174514
2014-07-21 02:32:54,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405934970589
2014-07-21 02:32:54,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935000529
2014-07-21 02:32:54,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935005322
2014-07-21 02:32:54,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935007668
2014-07-21 02:32:54,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935010558
2014-07-21 02:32:54,962 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:32:54,973 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:32:55,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:55,806 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935174514 with entries=85, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935175775
2014-07-21 02:32:57,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:32:57,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935175775 with entries=108, filesize=72.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935177371
2014-07-21 02:33:00,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:00,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53667 synced till here 53666
2014-07-21 02:33:00,426 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935177371 with entries=91, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935180379
2014-07-21 02:33:02,006 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:02,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53753 synced till here 53752
2014-07-21 02:33:02,080 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935180379 with entries=86, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935182007
2014-07-21 02:33:04,011 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:04,395 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935182007 with entries=108, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935184011
2014-07-21 02:33:09,495 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11892, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/2092d5a4d16e40ada20d6b29411d76db
2014-07-21 02:33:09,518 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/2092d5a4d16e40ada20d6b29411d76db as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/2092d5a4d16e40ada20d6b29411d76db
2014-07-21 02:33:09,533 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/2092d5a4d16e40ada20d6b29411d76db, entries=3928980, sequenceid=11892, filesize=279.8m
2014-07-21 02:33:09,533 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.3g/1384417520, currentsize=389.8m/408758720 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 38597ms, sequenceid=11892, compaction requested=true
2014-07-21 02:33:09,534 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:09,534 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:33:09,534 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:33:09,534 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:09,534 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:09,534 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:33:13,210 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17294, memsize=503.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/07b9eb0588f5453a91cf3a2480efbeb4
2014-07-21 02:33:13,230 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/07b9eb0588f5453a91cf3a2480efbeb4 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/07b9eb0588f5453a91cf3a2480efbeb4
2014-07-21 02:33:13,245 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/07b9eb0588f5453a91cf3a2480efbeb4, entries=1834790, sequenceid=17294, filesize=130.7m
2014-07-21 02:33:13,247 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~511.1m/535947760, currentsize=203.0m/212837760 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 18774ms, sequenceid=17294, compaction requested=true
2014-07-21 02:33:13,247 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:13,248 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-21 02:33:13,248 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-21 02:33:13,248 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:13,248 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:13,248 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:33:22,136 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90133ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:33:22,137 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 449.8m
2014-07-21 02:33:22,408 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:33:22,409 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:33:22,409 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:22,409 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 20 blocking
2014-07-21 02:33:22,409 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-21 02:33:22,409 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:22,409 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:22,409 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:33:22,715 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:33:22,729 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:22,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53965 synced till here 53958
2014-07-21 02:33:22,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935184011 with entries=104, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935202729
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935012847
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935015454
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935017572
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935024631
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935025839
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935027565
2014-07-21 02:33:22,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935029517
2014-07-21 02:33:23,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:23,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54053 synced till here 54048
2014-07-21 02:33:23,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935202729 with entries=88, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935203807
2014-07-21 02:33:25,137 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:33:25,137 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:33:25,138 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:25,138 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-21 02:33:25,138 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-21 02:33:25,138 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:25,139 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:25,139 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:33:25,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:26,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935203807 with entries=112, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935205765
2014-07-21 02:33:28,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:28,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54249 synced till here 54248
2014-07-21 02:33:28,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935205765 with entries=84, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935208740
2014-07-21 02:33:30,935 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:30,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935208740 with entries=80, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935210936
2014-07-21 02:33:33,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:33,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54420 synced till here 54417
2014-07-21 02:33:33,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935210936 with entries=91, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935213861
2014-07-21 02:33:37,070 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12360, memsize=374.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/d9859b5e1ed44f86845efe0951e944bf
2014-07-21 02:33:37,091 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/d9859b5e1ed44f86845efe0951e944bf as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/d9859b5e1ed44f86845efe0951e944bf
2014-07-21 02:33:37,103 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/d9859b5e1ed44f86845efe0951e944bf, entries=1363640, sequenceid=12360, filesize=97.1m
2014-07-21 02:33:37,104 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~449.8m/471620000, currentsize=59.5m/62441280 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 14967ms, sequenceid=12360, compaction requested=true
2014-07-21 02:33:37,104 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:37,104 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:33:37,105 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:33:37,105 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:37,105 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:37,105 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:33:41,684 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90182ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:33:41,685 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 406.7m
2014-07-21 02:33:41,911 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:33:42,585 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90882ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:33:42,586 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 667.3m
2014-07-21 02:33:42,951 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:33:45,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:46,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935213861 with entries=130, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935225985
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935034094
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935043456
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935046440
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935047927
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935063506
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935065909
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935068284
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935069651
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935072394
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935073505
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935079247
2014-07-21 02:33:46,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935082197
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935084042
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935085514
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935087114
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935088554
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935090966
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935092758
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935104564
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935107045
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935109624
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935111642
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935113588
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935125227
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935127372
2014-07-21 02:33:46,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935128943
2014-07-21 02:33:47,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:47,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54633 synced till here 54630
2014-07-21 02:33:47,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935225985 with entries=83, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935227922
2014-07-21 02:33:51,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:52,605 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54783 synced till here 54780
2014-07-21 02:33:52,630 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935227922 with entries=150, filesize=114.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935231073
2014-07-21 02:33:54,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:55,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54875 synced till here 54870
2014-07-21 02:33:55,454 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935231073 with entries=92, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935234555
2014-07-21 02:33:56,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:56,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54954 synced till here 54953
2014-07-21 02:33:56,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935234555 with entries=79, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935236801
2014-07-21 02:33:57,653 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17654, memsize=406.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1460be29ec8544f8abb8cdb7170016a8
2014-07-21 02:33:57,665 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/1460be29ec8544f8abb8cdb7170016a8 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1460be29ec8544f8abb8cdb7170016a8
2014-07-21 02:33:57,675 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/1460be29ec8544f8abb8cdb7170016a8, entries=1480820, sequenceid=17654, filesize=105.5m
2014-07-21 02:33:57,675 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~406.7m/426463840, currentsize=201.6m/211406320 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 15990ms, sequenceid=17654, compaction requested=true
2014-07-21 02:33:57,676 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:33:57,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-21 02:33:57,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-21 02:33:57,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:33:57,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:33:57,676 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:33:58,063 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:33:58,082 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55042 synced till here 55037
2014-07-21 02:33:58,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935236801 with entries=88, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935238063
2014-07-21 02:34:05,427 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12231, memsize=651.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7f342a4a394c4bc8bdcdfb2ca9a7ea5d
2014-07-21 02:34:05,441 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/7f342a4a394c4bc8bdcdfb2ca9a7ea5d as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7f342a4a394c4bc8bdcdfb2ca9a7ea5d
2014-07-21 02:34:05,452 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/7f342a4a394c4bc8bdcdfb2ca9a7ea5d, entries=2373100, sequenceid=12231, filesize=169.0m
2014-07-21 02:34:05,453 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~667.3m/699713280, currentsize=238.6m/250155120 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 22867ms, sequenceid=12231, compaction requested=true
2014-07-21 02:34:05,454 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:34:05,454 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-21 02:34:05,454 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-21 02:34:05,454 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:34:05,454 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:34:05,454 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:34:14,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:14,768 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:34:14,768 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:34:14,769 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:34:14,769 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 20 blocking
2014-07-21 02:34:14,769 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-21 02:34:14,769 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:34:14,769 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:34:14,770 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:34:15,422 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55202 synced till here 55200
2014-07-21 02:34:15,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935238063 with entries=160, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935254731
2014-07-21 02:34:15,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935131672
2014-07-21 02:34:15,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935133687
2014-07-21 02:34:15,497 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:34:15,498 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:34:15,498 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:34:15,498 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-21 02:34:15,498 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-21 02:34:15,498 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:34:15,499 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:34:15,499 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:34:17,155 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:17,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55297 synced till here 55296
2014-07-21 02:34:17,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935254731 with entries=95, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935257155
2014-07-21 02:34:19,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:19,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55382 synced till here 55381
2014-07-21 02:34:19,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935257155 with entries=85, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935259125
2014-07-21 02:34:21,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:21,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55476 synced till here 55475
2014-07-21 02:34:21,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935259125 with entries=94, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935261426
2014-07-21 02:34:23,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:23,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935261426 with entries=83, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935263646
2014-07-21 02:34:36,920 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:36,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55675 synced till here 55673
2014-07-21 02:34:37,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935263646 with entries=116, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935276921
2014-07-21 02:34:38,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:38,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55766 synced till here 55757
2014-07-21 02:34:38,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935276921 with entries=91, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935278573
2014-07-21 02:34:40,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:40,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935278573 with entries=79, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935280917
2014-07-21 02:34:43,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:43,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55943 synced till here 55939
2014-07-21 02:34:43,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935280917 with entries=98, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935283476
2014-07-21 02:34:45,048 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:34:45,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56035 synced till here 56034
2014-07-21 02:34:45,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935283476 with entries=92, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935285049
2014-07-21 02:34:53,217 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90809ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:34:53,219 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.2g
2014-07-21 02:34:53,943 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:34:55,917 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90780ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:34:55,918 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 583.7m
2014-07-21 02:34:56,251 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:35:01,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:02,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935285049 with entries=104, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935301990
2014-07-21 02:35:05,541 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:35:05,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:05,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56263 synced till here 56256
2014-07-21 02:35:05,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935301990 with entries=124, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935305696
2014-07-21 02:35:07,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:07,345 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56375 synced till here 56374
2014-07-21 02:35:07,367 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935305696 with entries=112, filesize=78.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935307077
2014-07-21 02:35:10,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:10,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935307077 with entries=119, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935310118
2014-07-21 02:35:12,570 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:12,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935310118 with entries=99, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935312570
2014-07-21 02:35:14,400 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:14,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935312570 with entries=90, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935314400
2014-07-21 02:35:14,512 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18266, memsize=559.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/a26344fc4d4840208feeb47f7c28de67
2014-07-21 02:35:14,533 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/a26344fc4d4840208feeb47f7c28de67 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/a26344fc4d4840208feeb47f7c28de67
2014-07-21 02:35:14,555 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/a26344fc4d4840208feeb47f7c28de67, entries=2037410, sequenceid=18266, filesize=145.0m
2014-07-21 02:35:14,555 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~583.7m/612032720, currentsize=215.0m/225399760 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 18637ms, sequenceid=18266, compaction requested=true
2014-07-21 02:35:14,556 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-21 02:35:14,556 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. has too many store files; delaying flush up to 90000ms
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-21 02:35:14,556 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 20 blocking
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:35:14,556 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:35:20,993 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=11452, hits=4669, hitRatio=40.77%, , cachingAccesses=4673, cachingHits=4668, cachingHitsRatio=99.89%, evictions=0, evicted=3, evictedPerRun=Infinity
2014-07-21 02:35:25,508 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:35:25,508 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:35:25,508 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:35:25,508 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-21 02:35:25,509 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-21 02:35:25,509 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:35:25,509 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:35:25,509 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:35:26,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:26,037 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56803 synced till here 56802
2014-07-21 02:35:26,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935314400 with entries=120, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935326014
2014-07-21 02:35:27,359 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:27,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56885 synced till here 56884
2014-07-21 02:35:27,392 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935326014 with entries=82, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935327360
2014-07-21 02:35:28,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:28,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56978 synced till here 56970
2014-07-21 02:35:28,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935327360 with entries=93, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935328744
2014-07-21 02:35:31,548 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12570, memsize=1.1g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/11ae616174324770889cc90febf48f34
2014-07-21 02:35:31,565 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/11ae616174324770889cc90febf48f34 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/11ae616174324770889cc90febf48f34
2014-07-21 02:35:31,592 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/11ae616174324770889cc90febf48f34, entries=4211390, sequenceid=12570, filesize=299.8m
2014-07-21 02:35:31,593 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.2g/1268284320, currentsize=340.2m/356734640 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 38373ms, sequenceid=12570, compaction requested=true
2014-07-21 02:35:31,593 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:35:31,593 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:35:31,593 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:35:31,593 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:35:31,593 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:35:31,593 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:35:31,675 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:35:31,675 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:35:31,676 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:35:31,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 20 blocking
2014-07-21 02:35:31,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-21 02:35:31,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:35:31,676 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:35:31,676 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:35:31,777 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:31,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935328744 with entries=101, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935331777
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935150169
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935153057
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935154091
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935156528
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935158908
2014-07-21 02:35:31,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935174514
2014-07-21 02:35:31,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935175775
2014-07-21 02:35:31,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935177371
2014-07-21 02:35:31,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935180379
2014-07-21 02:35:31,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935182007
2014-07-21 02:35:31,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935184011
2014-07-21 02:35:33,433 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:33,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57163 synced till here 57160
2014-07-21 02:35:33,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935331777 with entries=84, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935333433
2014-07-21 02:35:45,147 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90379ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:35:45,148 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 1011.3m
2014-07-21 02:35:45,836 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:35:46,048 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90551ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:35:46,048 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 406.5m
2014-07-21 02:35:46,297 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:35:51,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:51,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57300 synced till here 57299
2014-07-21 02:35:51,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935333433 with entries=137, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935351151
2014-07-21 02:35:51,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:35:52,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:53,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57421 synced till here 57420
2014-07-21 02:35:53,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935351151 with entries=121, filesize=92.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935352738
2014-07-21 02:35:53,160 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:35:55,117 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:55,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57516 synced till here 57514
2014-07-21 02:35:55,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935352738 with entries=95, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935355117
2014-07-21 02:35:55,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:35:57,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:35:57,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57615 synced till here 57613
2014-07-21 02:35:57,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935355117 with entries=99, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935357925
2014-07-21 02:35:57,962 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:00,145 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18717, memsize=406.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/167fdec905654d33beb735b7ddc3fb1f
2014-07-21 02:36:00,159 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/167fdec905654d33beb735b7ddc3fb1f as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/167fdec905654d33beb735b7ddc3fb1f
2014-07-21 02:36:00,170 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/167fdec905654d33beb735b7ddc3fb1f, entries=1479920, sequenceid=18717, filesize=105.4m
2014-07-21 02:36:00,170 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~406.5m/426205600, currentsize=174.3m/182748160 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 14122ms, sequenceid=18717, compaction requested=true
2014-07-21 02:36:00,171 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:36:00,171 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-21 02:36:00,171 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-21 02:36:00,171 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:36:00,171 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:36:00,171 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:36:00,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:01,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57714 synced till here 57711
2014-07-21 02:36:01,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935357925 with entries=99, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935360985
2014-07-21 02:36:01,059 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:12,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:12,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57851 synced till here 57844
2014-07-21 02:36:12,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935360985 with entries=137, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935372635
2014-07-21 02:36:12,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:14,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:14,110 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:36:14,111 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:36:14,111 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:36:14,111 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 20 blocking
2014-07-21 02:36:14,111 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-21 02:36:14,111 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:36:14,111 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:36:14,111 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:36:14,394 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57950 synced till here 57947
2014-07-21 02:36:14,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935372635 with entries=99, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935374090
2014-07-21 02:36:14,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:16,377 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:16,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935374090 with entries=98, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935376378
2014-07-21 02:36:16,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:17,017 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12788, memsize=966.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/56941d52ac93483b86076fb614d0c7b0
2014-07-21 02:36:17,045 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/56941d52ac93483b86076fb614d0c7b0 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/56941d52ac93483b86076fb614d0c7b0
2014-07-21 02:36:17,066 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/56941d52ac93483b86076fb614d0c7b0, entries=3519060, sequenceid=12788, filesize=250.5m
2014-07-21 02:36:17,067 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1011.3m/1060382000, currentsize=303.0m/317672480 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 31919ms, sequenceid=12788, compaction requested=true
2014-07-21 02:36:17,067 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:36:17,067 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-21 02:36:17,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-21 02:36:17,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:36:17,068 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:36:17,068 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:36:17,285 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:36:17,285 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. has too many store files; delaying flush up to 90000ms
2014-07-21 02:36:17,285 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:36:17,285 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 20 blocking
2014-07-21 02:36:17,286 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-21 02:36:17,286 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:36:17,286 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:36:17,286 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:36:18,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:18,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935376378 with entries=79, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935378623
2014-07-21 02:36:18,676 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:21,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:21,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58238 synced till here 58236
2014-07-21 02:36:21,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935378623 with entries=111, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935381462
2014-07-21 02:36:21,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): b607df7a7904509786241899d2d5239b
2014-07-21 02:36:36,178 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90637ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:36:36,179 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b., current region memstore size 478.1m
2014-07-21 02:36:36,470 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:36:39,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:39,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58366 synced till here 58365
2014-07-21 02:36:39,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935381462 with entries=128, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935399739
2014-07-21 02:36:40,625 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:40,645 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58452 synced till here 58440
2014-07-21 02:36:41,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935399739 with entries=86, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935400625
2014-07-21 02:36:43,483 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:43,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58541 synced till here 58539
2014-07-21 02:36:43,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935400625 with entries=89, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935403483
2014-07-21 02:36:45,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:45,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58626 synced till here 58622
2014-07-21 02:36:45,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935403483 with entries=85, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935405212
2014-07-21 02:36:46,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:47,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58741 synced till here 58739
2014-07-21 02:36:47,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935405212 with entries=115, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935406547
2014-07-21 02:36:52,365 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13238, memsize=464.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/998cde999ef449db9d628fb1b5cc21c8
2014-07-21 02:36:52,383 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/.tmp/998cde999ef449db9d628fb1b5cc21c8 as hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/998cde999ef449db9d628fb1b5cc21c8
2014-07-21 02:36:52,396 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b607df7a7904509786241899d2d5239b/family/998cde999ef449db9d628fb1b5cc21c8, entries=1692140, sequenceid=13238, filesize=120.5m
2014-07-21 02:36:52,397 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~478.1m/501274800, currentsize=62.1m/65104160 for region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. in 16217ms, sequenceid=13238, compaction requested=true
2014-07-21 02:36:52,399 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:36:52,399 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:36:52,399 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:36:52,399 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:36:52,400 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:36:52,400 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
2014-07-21 02:36:56,025 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90517ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:36:56,027 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 567.8m
2014-07-21 02:36:56,334 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:36:59,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:36:59,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58844 synced till here 58843
2014-07-21 02:36:59,334 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935406547 with entries=103, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935419176
2014-07-21 02:36:59,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935202729
2014-07-21 02:36:59,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935203807
2014-07-21 02:36:59,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935205765
2014-07-21 02:36:59,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935208740
2014-07-21 02:36:59,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935210936
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935213861
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935225985
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935227922
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935231073
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935234555
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935236801
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935238063
2014-07-21 02:36:59,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935254731
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935257155
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935259125
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935261426
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935263646
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935276921
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935278573
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935280917
2014-07-21 02:36:59,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935283476
2014-07-21 02:37:01,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:01,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58962 synced till here 58959
2014-07-21 02:37:01,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935419176 with entries=118, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935421496
2014-07-21 02:37:01,708 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90033ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:37:01,709 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db., current region memstore size 1.0g
2014-07-21 02:37:02,910 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:37:03,076 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:03,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59054 synced till here 59053
2014-07-21 02:37:03,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935421496 with entries=92, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935423076
2014-07-21 02:37:05,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:05,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59152 synced till here 59151
2014-07-21 02:37:05,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935423076 with entries=98, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935425030
2014-07-21 02:37:07,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:07,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935425030 with entries=89, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935427300
2014-07-21 02:37:14,179 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19362, memsize=567.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/b5e74bbc6599457b9c661ad8d548a062
2014-07-21 02:37:14,198 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/b5e74bbc6599457b9c661ad8d548a062 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/b5e74bbc6599457b9c661ad8d548a062
2014-07-21 02:37:14,215 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/b5e74bbc6599457b9c661ad8d548a062, entries=2067290, sequenceid=19362, filesize=147.2m
2014-07-21 02:37:14,215 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~567.8m/595362800, currentsize=186.5m/195606320 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 18188ms, sequenceid=19362, compaction requested=true
2014-07-21 02:37:14,216 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:37:14,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-21 02:37:14,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-21 02:37:14,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:37:14,216 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:37:14,217 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:37:27,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:27,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59339 synced till here 59338
2014-07-21 02:37:27,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935427300 with entries=98, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935447273
2014-07-21 02:37:28,613 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:28,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59423 synced till here 59421
2014-07-21 02:37:28,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935447273 with entries=84, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935448614
2014-07-21 02:37:29,449 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:37:29,449 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:37:29,450 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:37:29,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 20 blocking
2014-07-21 02:37:29,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-21 02:37:29,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:37:29,450 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:37:29,451 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:37:31,023 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:31,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935448614 with entries=89, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935451023
2014-07-21 02:37:32,863 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:32,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59601 synced till here 59599
2014-07-21 02:37:32,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935451023 with entries=89, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935452863
2014-07-21 02:37:34,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:34,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935452863 with entries=87, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935454466
2014-07-21 02:37:35,965 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13140, memsize=1.0g, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c4ddab8504494254b6e97e8377fb8d71
2014-07-21 02:37:35,982 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/.tmp/c4ddab8504494254b6e97e8377fb8d71 as hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c4ddab8504494254b6e97e8377fb8d71
2014-07-21 02:37:35,991 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/45ba1d55e2fff38da3118b96932537db/family/c4ddab8504494254b6e97e8377fb8d71, entries=3774900, sequenceid=13140, filesize=268.8m
2014-07-21 02:37:35,991 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1091324160, currentsize=299.6m/314167760 for region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. in 34282ms, sequenceid=13140, compaction requested=true
2014-07-21 02:37:35,992 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:37:35,992 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-21 02:37:35,992 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-21 02:37:35,992 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:37:35,992 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:37:35,992 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:37:44,921 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Waited 90811ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:37:44,923 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f., current region memstore size 377.6m
2014-07-21 02:37:45,125 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:37:45,810 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db.
2014-07-21 02:37:45,811 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. has too many store files; delaying flush up to 90000ms
2014-07-21 02:37:45,811 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:37:45,811 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 20 blocking
2014-07-21 02:37:45,811 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-21 02:37:45,811 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:37:45,811 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:37:45,812 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user5,1405933300219.45ba1d55e2fff38da3118b96932537db. because compaction request was cancelled
2014-07-21 02:37:45,844 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:45,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59804 synced till here 59802
2014-07-21 02:37:45,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935454466 with entries=116, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935465844
2014-07-21 02:37:45,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935285049
2014-07-21 02:37:45,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935301990
2014-07-21 02:37:45,901 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935305696
2014-07-21 02:37:45,902 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935307077
2014-07-21 02:37:45,902 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935310118
2014-07-21 02:37:45,902 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935312570
2014-07-21 02:37:45,902 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935314400
2014-07-21 02:37:45,905 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935326014
2014-07-21 02:37:45,905 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935327360
2014-07-21 02:37:45,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935328744
2014-07-21 02:37:45,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935331777
2014-07-21 02:37:47,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:47,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59909 synced till here 59905
2014-07-21 02:37:47,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935465844 with entries=105, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935467437
2014-07-21 02:37:47,637 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Waited 90352ms on a compaction to clean up 'too many store files'; waited long enough... proceeding with flush of usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4.
2014-07-21 02:37:47,637 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4., current region memstore size 994.1m
2014-07-21 02:37:48,670 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-21 02:37:48,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:49,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935467437 with entries=92, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935468873
2014-07-21 02:37:52,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:52,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60096 synced till here 60093
2014-07-21 02:37:52,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935468873 with entries=95, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935472811
2014-07-21 02:37:55,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:55,661 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935472811 with entries=110, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935475372
2014-07-21 02:37:57,705 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19744, memsize=377.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/3fe1dc3dad39478ba396b266a0dae685
2014-07-21 02:37:57,720 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/.tmp/3fe1dc3dad39478ba396b266a0dae685 as hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/3fe1dc3dad39478ba396b266a0dae685
2014-07-21 02:37:57,731 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9362dc1cf01c1ed9bd2d48a68647947f/family/3fe1dc3dad39478ba396b266a0dae685, entries=1374960, sequenceid=19744, filesize=97.9m
2014-07-21 02:37:57,731 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~377.6m/395977040, currentsize=175.8m/184320560 for region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. in 12808ms, sequenceid=19744, compaction requested=true
2014-07-21 02:37:57,731 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:37:57,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-21 02:37:57,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-21 02:37:57,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:37:57,732 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:37:57,732 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:37:57,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:37:57,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60303 synced till here 60300
2014-07-21 02:37:57,857 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935475372 with entries=97, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935477831
2014-07-21 02:38:14,632 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:38:14,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60436 synced till here 60430
2014-07-21 02:38:14,676 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935477831 with entries=133, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935494632
2014-07-21 02:38:17,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-21 02:38:17,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935494632 with entries=122, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405932920887/slave1%2C60020%2C1405932920887.1405935497694
2014-07-21 02:38:18,785 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13334, memsize=993.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/3d8114cc4f774849828f108d42c697e6
2014-07-21 02:38:18,799 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/.tmp/3d8114cc4f774849828f108d42c697e6 as hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/3d8114cc4f774849828f108d42c697e6
2014-07-21 02:38:18,808 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b921319eb5e9e7dfd2471db139829ab4/family/3d8114cc4f774849828f108d42c697e6, entries=3616990, sequenceid=13334, filesize=257.6m
2014-07-21 02:38:18,809 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~997.5m/1045978400, currentsize=210.1m/220264720 for region usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. in 31172ms, sequenceid=13334, compaction requested=true
2014-07-21 02:38:18,809 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:38:18,809 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 20 blocking
2014-07-21 02:38:18,810 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-21 02:38:18,810 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:38:18,810 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:38:18,810 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user7,1405933300219.b921319eb5e9e7dfd2471db139829ab4. because compaction request was cancelled
2014-07-21 02:38:20,929 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f.
2014-07-21 02:38:20,929 WARN  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Region usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. has too many store files; delaying flush up to 90000ms
2014-07-21 02:38:20,930 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:38:20,930 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 20 blocking
2014-07-21 02:38:20,930 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-21 02:38:20,930 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:38:20,930 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:38:20,930 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user2,1405933300218.9362dc1cf01c1ed9bd2d48a68647947f. because compaction request was cancelled
2014-07-21 02:38:23,557 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b.
2014-07-21 02:38:23,557 WARN  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Region usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. has too many store files; delaying flush up to 90000ms
2014-07-21 02:38:23,557 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-21 02:38:23,557 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 20 blocking
2014-07-21 02:38:23,558 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-21 02:38:23,558 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 9223372036854775807 because the store might be stuck
2014-07-21 02:38:23,558 DEBUG [regionserver60020-smallCompactions-1405932960678] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-21 02:38:23,558 DEBUG [regionserver60020-smallCompactions-1405932960678] regionserver.CompactSplitThread: Not compacting usertable,user9,1405933300219.b607df7a7904509786241899d2d5239b. because compaction request was cancelled
