Fri Aug  8 23:06:46 PDT 2014 Starting master on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-08-08 23:06:46,726 INFO  [main] util.VersionInfo: HBase 2.0.0-SNAPSHOT
2014-08-08 23:06:46,727 INFO  [main] util.VersionInfo: Subversion git://sceplus-vm48/home/hadoop/hbase-2.0.0-SNAPSHOT -r 50ac59fa8530bbd35c21cd61cfd64d2bd7d3eb57
2014-08-08 23:06:46,727 INFO  [main] util.VersionInfo: Compiled by hadoop on Sun Aug  3 14:37:33 PDT 2014
2014-08-08 23:06:47,006 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/
2014-08-08 23:06:47,006 INFO  [main] util.ServerCommandLine: env:SHLVL=4
2014-08-08 23:06:47,006 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-master-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Dhbase.security.logger=INFO,RFAS
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.53 55349 22
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=12288
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-master.znode
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-08-08 23:06:47,007 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.53 55349 9.1.143.58 22
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-08-08 23:06:47,008 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-08-08 23:06:47,011 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.7.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/../hbase-server/target:/home/hadoop/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/.m2/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/home/hadoop/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/home/hadoop/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/home/hadoop/.m2/repository/com/lmax/disruptor/3.2.0/disruptor-3.2.0.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/hadoop/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/.m2/repository/commons-codec/commons-codec/1.7/commons-codec-1.7.jar:/home/hadoop/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/hadoop/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/hadoop/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/.m2/repository/io/netty/netty-all/4.0.19.Final/netty-all-4.0.19.Final.jar:/home/hadoop/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/hadoop/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/hadoop/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/../lib/tools.jar:/home/hadoop/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/hadoop/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/hadoop/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/home/hadoop/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-hs/2.4.0/hadoop-mapreduce-client-hs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.4.0/hadoop-minicluster-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.4.0/hadoop-yarn-server-applicationhistoryservice-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.4.0/hadoop-yarn-server-nodemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.4.0/hadoop-yarn-server-resourcemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/2.4.0/hadoop-yarn-server-tests-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.4.0/hadoop-yarn-server-web-proxy-2.4.0.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-client/target/hbase-client-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-it/target/hbase-it-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-prefix-tree/target/hbase-prefix-tree-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-protocol/target/hbase-protocol-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-shell/target/hbase-shell-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-testing-util/target/hbase-testing-util-2.0.0-SNAPSHOT.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/home/hadoop/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jettison/jettison/1.3.1/jettison-1.3.1.jar:/home/hadoop/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/hadoop/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/hadoop/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/.m2/repository/org/jboss/netty/netty/3.2.4.Final/netty-3.2.4.Final.jar:/home/hadoop/.m2/repository/org/jruby/jruby-complete/1.6.8/jruby-complete-1.6.8.jar:/home/hadoop/.m2/repository/org/mockito/mockito-all/1.9.0/mockito-all-1.9.0.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/home/hadoop/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/hadoop/.m2/repository/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar:/home/hadoop/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/*.jar:::/home/hadoop/hbase/selfdapql.jar
2014-08-08 23:06:47,012 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-08-08 23:06:47,012 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-08-08 23:06:47,012 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-08-08 23:06:47,012 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_CLASSPATH=:/home/hadoop/hbase/selfdapql.jar
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-master.autorestart
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=4458
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-master-sceplus-vm48.log
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-master-sceplus-vm48
2014-08-08 23:06:47,013 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-08-08 23:06:47,016 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Oracle Corporation, vmVersion=24.51-b03
2014-08-08 23:06:47,017 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_master, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx12288m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-master-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Dhbase.security.logger=INFO,RFAS]
2014-08-08 23:06:47,442 INFO  [main] regionserver.RSRpcServices: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020 server-side HConnection retries=350
2014-08-08 23:06:47,652 INFO  [main] ipc.SimpleRpcScheduler: Using deadline as user call queue, count=5
2014-08-08 23:06:47,692 INFO  [main] ipc.RpcServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020: started 10 reader(s).
2014-08-08 23:06:47,815 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-08-08 23:06:47,831 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-08-08 23:06:47,927 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-08-08 23:06:47,928 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-08-08 23:06:48,178 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2014-08-08 23:06:48,183 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache size=4.76 GB, blockSize=64 KB
2014-08-08 23:06:48,201 INFO  [main] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:06:48,926 INFO  [main] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-08-08 23:06:48,942 WARN  [main] trace.SpanReceiverHost: Class org.cloudera.htrace.impl.LocalFileSpanReceiver cannot be found. org.cloudera.htrace.impl.LocalFileSpanReceiver
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.7.0_55
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Oracle Corporation
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.7.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/../hbase-server/target:/home/hadoop/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/hadoop/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/hadoop/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/hadoop/.m2/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/home/hadoop/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/hadoop/.m2/repository/com/google/inject/extensions/guice-servlet/3.0/guice-servlet-3.0.jar:/home/hadoop/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/hadoop/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar:/home/hadoop/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/home/hadoop/.m2/repository/com/lmax/disruptor/3.2.0/disruptor-3.2.0.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/hadoop/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/hadoop/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/home/hadoop/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/hadoop/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/hadoop/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/hadoop/.m2/repository/commons-codec/commons-codec/1.7/commons-codec-1.7.jar:/home/hadoop/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/hadoop/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/hadoop/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/hadoop/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/hadoop/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/hadoop/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/hadoop/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/hadoop/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/hadoop/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/hadoop/.m2/repository/commons-net/commons-net/3.1/commons-net-3.1.jar:/home/hadoop/.m2/repository/io/netty/netty-all/4.0.19.Final/netty-all-4.0.19.Final.jar:/home/hadoop/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/home/hadoop/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/hadoop/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/hadoop/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/usr/lib/jvm/java-7-openjdk-amd64/jre/../lib/tools.jar:/home/hadoop/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/hadoop/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/home/hadoop/.m2/repository/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar:/home/hadoop/.m2/repository/org/apache/avro/avro/1.7.4/avro-1.7.4.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/home/hadoop/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-common-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/hadoop-hdfs-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-hs/2.4.0/hadoop-mapreduce-client-hs-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobclient-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-minicluster/2.4.0/hadoop-minicluster-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-applicationhistoryservice/2.4.0/hadoop-yarn-server-applicationhistoryservice-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-nodemanager/2.4.0/hadoop-yarn-server-nodemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-resourcemanager/2.4.0/hadoop-yarn-server-resourcemanager-2.4.0.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-tests/2.4.0/hadoop-yarn-server-tests-2.4.0-tests.jar:/home/hadoop/.m2/repository/org/apache/hadoop/hadoop-yarn-server-web-proxy/2.4.0/hadoop-yarn-server-web-proxy-2.4.0.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-client/target/hbase-client-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-common/target/hbase-common-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop-compat/target/hbase-hadoop-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-hadoop2-compat/target/hbase-hadoop2-compat-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-it/target/hbase-it-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-prefix-tree/target/hbase-prefix-tree-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-protocol/target/hbase-protocol-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-server/target/hbase-server-2.0.0-SNAPSHOT-tests.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-shell/target/hbase-shell-2.0.0-SNAPSHOT.jar:/home/hadoop/hbase-2.0.0-SNAPSHOT/hbase-testing-util/target/hbase-testing-util-2.0.0-SNAPSHOT.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpclient/4.2.5/httpclient-4.2.5.jar:/home/hadoop/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/home/hadoop/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/hadoop/.m2/repository/org/codehaus/jettison/jettison/1.3.1/jettison-1.3.1.jar:/home/hadoop/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/hadoop/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/hadoop/.m2/repository/org/htrace/htrace-core/3.0.4/htrace-core-3.0.4.jar:/home/hadoop/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/hadoop/.m2/repository/org/jboss/netty/netty/3.2.4.Final/netty-3.2.4.Final.jar:/home/hadoop/.m2/repository/org/jruby/jruby-complete/1.6.8/jruby-complete-1.6.8.jar:/home/hadoop/.m2/repository/org/mockito/mockito-all/1.9.0/mockito-all-1.9.0.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/hadoop/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/home/hadoop/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/home/hadoop/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/hadoop/.m2/repository/org/xerial/snappy/snappy-java/1.0.4.1/snappy-java-1.0.4.1.jar:/home/hadoop/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/hadoop/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/hadoop/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/*.jar:::/home/hadoop/hbase/selfdapql.jar
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-08-08 23:06:48,966 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-08-08 23:06:48,967 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-08-08 23:06:48,967 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-08-08 23:06:48,967 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-08-08 23:06:48,967 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop
2014-08-08 23:06:48,968 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=master:16020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:06:48,986 INFO  [main] zookeeper.RecoverableZooKeeper: Process identifier=master:16020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:06:48,990 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:06:48,997 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-08 23:06:49,020 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0000, negotiated timeout = 90000
2014-08-08 23:06:49,078 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-08-08 23:06:49,078 INFO  [RpcServer.listener,port=16020] ipc.RpcServer: RpcServer.listener,port=16020: starting
2014-08-08 23:06:49,167 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-08-08 23:06:49,172 INFO  [main] http.HttpRequestLog: Http request log for http.requests.master is not defined
2014-08-08 23:06:49,187 INFO  [main] http.HttpServer: Added global filter 'safety' (class=org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter)
2014-08-08 23:06:49,190 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context master
2014-08-08 23:06:49,191 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2014-08-08 23:06:49,191 INFO  [main] http.HttpServer: Added filter static_user_filter (class=org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2014-08-08 23:06:49,214 INFO  [main] http.HttpServer: Jetty bound to port 16030
2014-08-08 23:06:49,214 INFO  [main] mortbay.log: jetty-6.1.26
2014-08-08 23:06:49,614 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16030
2014-08-08 23:06:49,615 INFO  [main] master.HMaster: hbase.rootdir=hdfs://master:54310/hbase, hbase.cluster.distributed=true
2014-08-08 23:06:49,630 INFO  [main] master.HMaster: Adding ZNode for /hbase/backup-masters/sceplus-vm48.almaden.ibm.com,16020,1407564407993 in backup master directory
2014-08-08 23:06:49,755 INFO  [main] mortbay.log: jetty-6.1.26
2014-08-08 23:06:49,757 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:16010
2014-08-08 23:06:49,817 INFO  [ActiveMasterManager] master.ActiveMasterManager: Deleting ZNode for /hbase/backup-masters/sceplus-vm48.almaden.ibm.com,16020,1407564407993 from backup master directory
2014-08-08 23:06:49,824 INFO  [ActiveMasterManager] master.ActiveMasterManager: Registered Active Master=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:06:49,960 INFO  [ActiveMasterManager] util.FSUtils: Waiting for dfs to exit safe mode...
2014-08-08 23:06:59,967 INFO  [ActiveMasterManager] util.FSUtils: Waiting for dfs to exit safe mode...
2014-08-08 23:07:10,518 INFO  [ActiveMasterManager] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-08-08 23:07:10,542 INFO  [ActiveMasterManager] master.SplitLogManager: Timeout=120000, unassigned timeout=180000, distributedLogReplay=true
2014-08-08 23:07:10,545 INFO  [ActiveMasterManager] master.SplitLogManager: Found 0 orphan tasks and 0 rescan nodes
2014-08-08 23:07:10,625 INFO  [ActiveMasterManager] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x27daa766, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:07:10,627 INFO  [ActiveMasterManager] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x27daa766 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:07:10,627 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:07:10,629 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-08 23:07:10,634 INFO  [ActiveMasterManager-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0002, negotiated timeout = 90000
2014-08-08 23:07:10,798 INFO  [ActiveMasterManager] master.HMaster: Server active/primary master=sceplus-vm48.almaden.ibm.com,16020,1407564407993, sessionid=0x147b9605d6e0000, setting cluster-up flag (Was=false)
2014-08-08 23:07:10,801 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: ClusterId : 581e7401-505b-4f79-8958-9c401fafa742
2014-08-08 23:07:10,824 INFO  [ActiveMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/online-snapshot/acquired /hbase/online-snapshot/reached /hbase/online-snapshot/abort
2014-08-08 23:07:10,835 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.8 G, globalMemStoreLimitLowMark=4.5 G, maxHeap=11.9 G
2014-08-08 23:07:10,836 INFO  [ActiveMasterManager] procedure.ZKProcedureUtil: Clearing all procedure znodes: /hbase/flush-table-proc/acquired /hbase/flush-table-proc/reached /hbase/flush-table-proc/abort
2014-08-08 23:07:10,841 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-08-08 23:07:10,854 INFO  [ActiveMasterManager] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=replicationLogCleaner, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:07:10,854 INFO  [ActiveMasterManager] zookeeper.RecoverableZooKeeper: Process identifier=replicationLogCleaner connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:07:10,855 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:07:10,856 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-08 23:07:10,860 INFO  [ActiveMasterManager-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790002, negotiated timeout = 90000
2014-08-08 23:07:10,864 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,16020,1407564407993 with port=16020, startcode=1407564407993
2014-08-08 23:07:10,868 WARN  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.
2014-08-08 23:07:10,878 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,16020,1407564407993 with port=16020, startcode=1407564407993
2014-08-08 23:07:10,878 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn false
2014-08-08 23:07:10,887 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] master.ServerManager: Registering server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:10,928 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 1, slept for 50 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-08 23:07:10,934 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: WAL configuration: blocksize=128 MB, rollsize=121.60 MB, enabled=true, prefix=sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993, logDir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993, oldLogDir=hdfs://master:54310/hbase/oldWALs
2014-08-08 23:07:11,247 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: Slow sync cost: 243 ms, current pipeline: []
2014-08-08 23:07:11,247 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407564430942
2014-08-08 23:07:11,289 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-08-08 23:07:11,304 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,16020,1407563683255, sceplus-vm48.almaden.ibm.com,16020,1407564407993] other RSs: [sceplus-vm49.almaden.ibm.com,16020,1407564409446, sceplus-vm48.almaden.ibm.com,16020,1407564407993]
2014-08-08 23:07:11,341 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x7416e071, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:07:11,341 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7416e071 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:07:11,342 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:07:11,344 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-08 23:07:11,350 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790004, negotiated timeout = 90000
2014-08-08 23:07:11,392 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,16020,1407564407993 starting
2014-08-08 23:07:11,395 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HeapMemoryManager: Starting HeapMemoryTuner chore.
2014-08-08 23:07:11,401 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,16020,1407564407993, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:16020, sessionid=0x147b9605d6e0000
2014-08-08 23:07:11,401 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x7b755dd4, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:07:11,402 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7b755dd4 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:07:11,402 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:07:11,404 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-08 23:07:11,407 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790005, negotiated timeout = 90000
2014-08-08 23:07:11,444 INFO  [defaultRpcServer.handler=6,queue=1,port=16020] master.ServerManager: Registering server=slave1,16020,1407564409446
2014-08-08 23:07:11,480 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 602 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-08 23:07:12,986 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 2108 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-08 23:07:14,492 INFO  [ActiveMasterManager] master.ServerManager: Waiting for region servers count to settle; currently checked in 2, slept for 3614 ms, expecting minimum of 2, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms, selfCheckedIn true
2014-08-08 23:07:14,986 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving slave1,16020,1407563683255's hlogs to my queue
2014-08-08 23:07:15,396 INFO  [ActiveMasterManager] master.ServerManager: Finished waiting for region servers count to settle; checked in 2, slept for 4517 ms, expecting minimum of 2, maximum of 2147483647, master is running, selfCheckedIn true
2014-08-08 23:07:15,396 INFO  [ActiveMasterManager] master.ServerManager: Registering server=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:15,396 INFO  [ActiveMasterManager] master.HMaster: Registered server found up in zk but who has not yet reported in: sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:15,414 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993 belongs to an existing region server
2014-08-08 23:07:15,415 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255 doesn't belong to a known region server, splitting
2014-08-08 23:07:15,415 INFO  [ActiveMasterManager] master.MasterFileSystem: Log folder hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446 belongs to an existing region server
2014-08-08 23:07:15,509 INFO  [ActiveMasterManager] zookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address=sceplus-vm48.almaden.ibm.com,16020,1407563646474, exception=org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on sceplus-vm48.almaden.ibm.com,16020,1407564407993
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2605)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:795)
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1067)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:20158)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2013)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:114)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:94)
	at java.lang.Thread.run(Thread.java:744)

2014-08-08 23:07:15,543 INFO  [ActiveMasterManager] zookeeper.MetaTableLocator: Unsetting hbase:meta region location in ZooKeeper
2014-08-08 23:07:15,594 INFO  [ActiveMasterManager] master.AssignmentManager: Assigning hbase:meta,,1.1588230740 to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:15,594 INFO  [ActiveMasterManager] master.RegionStates: Transition {1588230740 state=OFFLINE, ts=1407564435435, server=null} to {1588230740 state=PENDING_OPEN, ts=1407564435594, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:07:15,606 INFO  [ActiveMasterManager] regionserver.RSRpcServices: Open hbase:meta,,1.1588230740
2014-08-08 23:07:15,614 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: WAL configuration: blocksize=128 MB, rollsize=121.60 MB, enabled=true, prefix=sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993, logDir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993, oldLogDir=hdfs://master:54310/hbase/oldWALs
2014-08-08 23:07:15,625 INFO  [ActiveMasterManager] master.MasterFileSystem: Log dir for server sceplus-vm48.almaden.ibm.com,16020,1407563646474 does not exist
2014-08-08 23:07:15,645 INFO  [ActiveMasterManager] master.SplitLogManager: dead splitlog workers [sceplus-vm48.almaden.ibm.com,16020,1407563646474, slave1,16020,1407563683255]
2014-08-08 23:07:15,668 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: Slow sync cost: 33 ms, current pipeline: []
2014-08-08 23:07:15,668 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407564435624.meta
2014-08-08 23:07:15,673 INFO  [ActiveMasterManager] master.SplitLogManager: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting is empty dir, no logs to split
2014-08-08 23:07:15,673 INFO  [ActiveMasterManager] master.SplitLogManager: started splitting 0 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting]
2014-08-08 23:07:15,684 INFO  [main-EventThread] zookeeper.RecoveringRegionWatcher: /hbase/recovering-regions/1588230740 znode deleted. Region: 1588230740 completes recovery.
2014-08-08 23:07:15,707 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint from HTD of hbase:meta successfully.
2014-08-08 23:07:15,713 WARN  [ActiveMasterManager] master.SplitLogManager: returning success without actually splitting and deleting all the log files in path hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting
2014-08-08 23:07:15,713 INFO  [ActiveMasterManager] master.SplitLogManager: finished splitting (more than or equal to) 0 bytes in 0 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting] in 39ms
2014-08-08 23:07:15,714 INFO  [ActiveMasterManager] master.ServerManager: AssignmentManager hasn't finished failover cleanup; waiting
2014-08-08 23:07:15,778 INFO  [StoreOpener-1588230740-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:07:15,805 INFO  [StoreOpener-1588230740-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:07:15,880 INFO  [StoreFileOpenerThread-info-1] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for 08b6caa4f4e640a0aacc2c6da495c079
2014-08-08 23:07:15,925 INFO  [RS_OPEN_META-sceplus-vm48:16020-0] regionserver.HRegion: Onlined 1588230740; next sequenceid=3416
2014-08-08 23:07:15,926 INFO  [PostOpenDeployTasks:1588230740] regionserver.HRegionServer: Post open deploy tasks for hbase:meta,,1.1588230740
2014-08-08 23:07:15,928 INFO  [PostOpenDeployTasks:1588230740] zookeeper.MetaTableLocator: Setting hbase:meta region location in ZooKeeper as sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:15,951 INFO  [PostOpenDeployTasks:1588230740] master.RegionStates: Transition {1588230740 state=PENDING_OPEN, ts=1407564435594, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {1588230740 state=OPEN, ts=1407564435951, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:07:15,951 INFO  [PostOpenDeployTasks:1588230740] master.RegionStates: Onlined 1588230740 on sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:15,954 INFO  [ActiveMasterManager] master.HMaster: hbase:meta assigned=1, rit=false, location=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:16,073 INFO  [ActiveMasterManager] hbase.MetaMigrationConvertingToPB: META already up-to date with PB serialization
2014-08-08 23:07:16,140 INFO  [ActiveMasterManager] master.AssignmentManager: Found regions out on cluster or in RIT; presuming failover
2014-08-08 23:07:16,141 WARN  [ActiveMasterManager] master.ServerManager: Expiration of sceplus-vm48.almaden.ibm.com,16020,1407563646474 but server not online
2014-08-08 23:07:16,150 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407563683255 before assignment.
2014-08-08 23:07:16,150 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-08 23:07:16,150 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Splitting logs for sceplus-vm48.almaden.ibm.com,16020,1407563646474 before assignment.
2014-08-08 23:07:16,151 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-08 23:07:16,162 INFO  [ActiveMasterManager] master.AssignmentManager: Joined the cluster in 88ms, failover=true
2014-08-08 23:07:16,265 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=OPEN, ts=1407564436135, server=slave1,16020,1407563683255} to {2d7cdce59988748c536eb4fa2c738b15 state=OFFLINE, ts=1407564436265, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,267 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=OFFLINE
2014-08-08 23:07:16,297 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=OPEN, ts=1407564436132, server=slave1,16020,1407563683255} to {201c1415e88f249634b1e37367f3b55a state=OFFLINE, ts=1407564436297, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,297 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=OFFLINE
2014-08-08 23:07:16,301 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=OPEN, ts=1407564436135, server=slave1,16020,1407563683255} to {f1755ff29dbd940ffbe183f2bac53aac state=OFFLINE, ts=1407564436301, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,302 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=OFFLINE
2014-08-08 23:07:16,306 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=OPEN, ts=1407564436137, server=slave1,16020,1407563683255} to {4f74727d9804f8618b268b7850f1f9fd state=OFFLINE, ts=1407564436306, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,306 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=OFFLINE
2014-08-08 23:07:16,310 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=OPEN, ts=1407564436139, server=slave1,16020,1407563683255} to {de8fb45bce496ccaa88d0d22e867bf36 state=OFFLINE, ts=1407564436310, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,311 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=OFFLINE
2014-08-08 23:07:16,314 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {9ad804e5e097a5b026363aa2e30abfc6 state=OPEN, ts=1407564436136, server=slave1,16020,1407563683255} to {9ad804e5e097a5b026363aa2e30abfc6 state=OFFLINE, ts=1407564436314, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,315 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user5,1407563722043.9ad804e5e097a5b026363aa2e30abfc6. with state=OFFLINE
2014-08-08 23:07:16,319 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=OPEN, ts=1407564436133, server=slave1,16020,1407563683255} to {463b24605f4dcef6d6bb43ac7d82351a state=OFFLINE, ts=1407564436319, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,319 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=OFFLINE
2014-08-08 23:07:16,323 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {f8725ca629bda79e918bbe4be0e5dd4c state=OPEN, ts=1407564436138, server=slave1,16020,1407563683255} to {f8725ca629bda79e918bbe4be0e5dd4c state=OFFLINE, ts=1407564436323, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,323 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user7,1407563722043.f8725ca629bda79e918bbe4be0e5dd4c. with state=OFFLINE
2014-08-08 23:07:16,328 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=OPEN, ts=1407564436138, server=slave1,16020,1407563683255} to {7f4f72e893d4d48452ad33bacbb6bd79 state=OFFLINE, ts=1407564436328, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,328 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=OFFLINE
2014-08-08 23:07:16,332 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStates: Transition {f5526342dd8bbf40a6cef1ca6de1ef04 state=OPEN, ts=1407564436134, server=slave1,16020,1407563683255} to {f5526342dd8bbf40a6cef1ca6de1ef04 state=OFFLINE, ts=1407564436332, server=slave1,16020,1407563683255}
2014-08-08 23:07:16,332 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.RegionStateStore: Updating row usertable,user2,1407563722043.f5526342dd8bbf40a6cef1ca6de1ef04. with state=OFFLINE
2014-08-08 23:07:16,337 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Reassigning 10 region(s) that slave1,16020,1407563683255 was carrying (and 0 regions(s) that were opening on this server)
2014-08-08 23:07:16,337 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407564436132, server=sceplus-vm48.almaden.ibm.com,16020,1407563646474} to {ac22fb07770c81080da256230256da5a state=OFFLINE, ts=1407564436337, server=sceplus-vm48.almaden.ibm.com,16020,1407563646474}
2014-08-08 23:07:16,338 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OFFLINE
2014-08-08 23:07:16,343 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Reassigning 1 region(s) that sceplus-vm48.almaden.ibm.com,16020,1407563646474 was carrying (and 0 regions(s) that were opening on this server)
2014-08-08 23:07:16,376 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Bulk assigning 1 region(s) across 3 server(s), round-robin=true
2014-08-08 23:07:16,378 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.AssignmentManager: Assigning 1 region(s) to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:16,382 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OFFLINE, ts=1407564436337, server=sceplus-vm48.almaden.ibm.com,16020,1407563646474} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407564436382, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:07:16,382 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN&sn=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:16,388 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:07:16,394 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Bulk assigning done
2014-08-08 23:07:16,395 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for ac22fb07770c81080da256230256da5a to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:16,409 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:07:16,410 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:07:16,450 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=40000008
2014-08-08 23:07:16,451 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:07:16,477 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407564436382, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407564436477, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:07:16,477 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=40000008&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:16,482 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Onlined ac22fb07770c81080da256230256da5a on sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:07:16,489 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.MasterFileSystem: Log dir for server sceplus-vm48.almaden.ibm.com,16020,1407563646474 does not exist
2014-08-08 23:07:16,489 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: dead splitlog workers [sceplus-vm48.almaden.ibm.com,16020,1407563646474]
2014-08-08 23:07:16,490 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: started splitting 0 logs in []
2014-08-08 23:07:16,520 INFO  [main-EventThread] zookeeper.RecoveringRegionWatcher: /hbase/recovering-regions/ac22fb07770c81080da256230256da5a znode deleted. Region: ac22fb07770c81080da256230256da5a completes recovery.
2014-08-08 23:07:16,523 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] master.SplitLogManager: finished splitting (more than or equal to) 0 bytes in 0 log files in [] in 33ms
2014-08-08 23:07:16,524 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.LogReplayHandler: Finished processing shutdown of sceplus-vm48.almaden.ibm.com,16020,1407563646474
2014-08-08 23:07:16,549 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Bulk assigning 10 region(s) across 3 server(s), round-robin=true
2014-08-08 23:07:16,550 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.AssignmentManager: Assigning 5 region(s) to sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,550 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.AssignmentManager: Assigning 5 region(s) to slave1,16020,1407564409446
2014-08-08 23:07:16,550 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=OFFLINE, ts=1407564436265, server=slave1,16020,1407563683255} to {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564436550, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:16,551 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,551 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=OFFLINE, ts=1407564436297, server=slave1,16020,1407563683255} to {201c1415e88f249634b1e37367f3b55a state=PENDING_OPEN, ts=1407564436551, server=slave1,16020,1407564409446}
2014-08-08 23:07:16,552 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:16,556 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=OFFLINE, ts=1407564436301, server=slave1,16020,1407563683255} to {f1755ff29dbd940ffbe183f2bac53aac state=PENDING_OPEN, ts=1407564436556, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:16,556 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,558 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=OFFLINE, ts=1407564436306, server=slave1,16020,1407563683255} to {4f74727d9804f8618b268b7850f1f9fd state=PENDING_OPEN, ts=1407564436558, server=slave1,16020,1407564409446}
2014-08-08 23:07:16,558 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:16,561 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=OFFLINE, ts=1407564436310, server=slave1,16020,1407563683255} to {de8fb45bce496ccaa88d0d22e867bf36 state=PENDING_OPEN, ts=1407564436561, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:16,561 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,563 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStates: Transition {9ad804e5e097a5b026363aa2e30abfc6 state=OFFLINE, ts=1407564436314, server=slave1,16020,1407563683255} to {9ad804e5e097a5b026363aa2e30abfc6 state=PENDING_OPEN, ts=1407564436562, server=slave1,16020,1407564409446}
2014-08-08 23:07:16,563 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user5,1407563722043.9ad804e5e097a5b026363aa2e30abfc6. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:16,566 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=OFFLINE, ts=1407564436319, server=slave1,16020,1407563683255} to {463b24605f4dcef6d6bb43ac7d82351a state=PENDING_OPEN, ts=1407564436566, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:16,566 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,567 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStates: Transition {f8725ca629bda79e918bbe4be0e5dd4c state=OFFLINE, ts=1407564436323, server=slave1,16020,1407563683255} to {f8725ca629bda79e918bbe4be0e5dd4c state=PENDING_OPEN, ts=1407564436567, server=slave1,16020,1407564409446}
2014-08-08 23:07:16,567 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user7,1407563722043.f8725ca629bda79e918bbe4be0e5dd4c. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:16,569 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=OFFLINE, ts=1407564436328, server=slave1,16020,1407563683255} to {7f4f72e893d4d48452ad33bacbb6bd79 state=PENDING_OPEN, ts=1407564436569, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:16,569 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-0] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=PENDING_OPEN&sn=sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:16,570 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStates: Transition {f5526342dd8bbf40a6cef1ca6de1ef04 state=OFFLINE, ts=1407564436332, server=slave1,16020,1407563683255} to {f5526342dd8bbf40a6cef1ca6de1ef04 state=PENDING_OPEN, ts=1407564436570, server=slave1,16020,1407564409446}
2014-08-08 23:07:16,571 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-GeneralBulkAssigner-1] master.RegionStateStore: Updating row usertable,user2,1407563722043.f5526342dd8bbf40a6cef1ca6de1ef04. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:16,636 INFO  [ActiveMasterManager] master.HMaster: Master has completed initialization
2014-08-08 23:07:16,960 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Bulk assigning done
2014-08-08 23:07:16,960 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for 2d7cdce59988748c536eb4fa2c738b15 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:17,208 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=PENDING_OPEN, ts=1407564436551, server=slave1,16020,1407564409446} to {201c1415e88f249634b1e37367f3b55a state=OPEN, ts=1407564437208, server=slave1,16020,1407564409446}
2014-08-08 23:07:17,208 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407564409446
2014-08-08 23:07:17,213 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Onlined 201c1415e88f249634b1e37367f3b55a on slave1,16020,1407564409446
2014-08-08 23:07:17,300 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=PENDING_OPEN, ts=1407564436558, server=slave1,16020,1407564409446} to {4f74727d9804f8618b268b7850f1f9fd state=OPEN, ts=1407564437300, server=slave1,16020,1407564409446}
2014-08-08 23:07:17,300 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=OPEN&openSeqNum=40001415&server=slave1,16020,1407564409446
2014-08-08 23:07:17,305 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Onlined 4f74727d9804f8618b268b7850f1f9fd on slave1,16020,1407564409446
2014-08-08 23:07:17,331 ERROR [defaultRpcServer.handler=19,queue=4,port=16020] master.AssignmentManager: Failed to transtion region from {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564436550, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to OPENED by slave1,16020,1407564409446: 2d7cdce59988748c536eb4fa2c738b15 is not pending open on slave1,16020,1407564409446
2014-08-08 23:07:17,352 ERROR [defaultRpcServer.handler=16,queue=1,port=16020] master.MasterRpcServices: Region server slave1,16020,1407564409446 reported a fatal error:
ABORTING region server slave1,16020,1407564409446: Exception running postOpenDeployTasks; region=2d7cdce59988748c536eb4fa2c738b15
Cause:
java.io.IOException: Failed to report opened region to master: usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1730)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:321)

2014-08-08 23:07:17,352 ERROR [defaultRpcServer.handler=24,queue=4,port=16020] master.MasterRpcServices: Region server slave1,16020,1407564409446 reported a fatal error:
ABORTING region server slave1,16020,1407564409446: Exception running postOpenDeployTasks; region=f1755ff29dbd940ffbe183f2bac53aac
Cause:
org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server slave1,16020,1407564409446 not running, aborting
	at org.apache.hadoop.hbase.regionserver.RSRpcServices.checkOpen(RSRpcServices.java:823)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1703)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread.run(OpenRegionHandler.java:321)

2014-08-08 23:07:22,085 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sceplus-vm49.almaden.ibm.com,16020,1407564409446]
2014-08-08 23:07:22,093 INFO  [main-EventThread] replication.ReplicationTrackerZKImpl: /hbase/rs/sceplus-vm49.almaden.ibm.com,16020,1407564409446 znode expired, triggering replicatorRemoved event
2014-08-08 23:07:22,093 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Splitting logs for sceplus-vm49.almaden.ibm.com,16020,1407564409446 before assignment.
2014-08-08 23:07:22,094 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-08 23:07:22,095 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {de8fb45bce496ccaa88d0d22e867bf36 state=PENDING_OPEN, ts=1407564436561, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:22,095 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {f1755ff29dbd940ffbe183f2bac53aac state=PENDING_OPEN, ts=1407564436556, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:22,095 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {7f4f72e893d4d48452ad33bacbb6bd79 state=PENDING_OPEN, ts=1407564436569, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:22,095 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {463b24605f4dcef6d6bb43ac7d82351a state=PENDING_OPEN, ts=1407564436566, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:22,095 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Found region in {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564436550, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to be reassigned by SSH for sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:07:22,103 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/de8fb45bce496ccaa88d0d22e867bf36 already deleted, retry=false
2014-08-08 23:07:22,103 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=PENDING_OPEN, ts=1407564436561, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {de8fb45bce496ccaa88d0d22e867bf36 state=OFFLINE, ts=1407564442103, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:22,103 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=OFFLINE
2014-08-08 23:07:22,115 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/f1755ff29dbd940ffbe183f2bac53aac already deleted, retry=false
2014-08-08 23:07:22,115 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=PENDING_OPEN, ts=1407564436556, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {f1755ff29dbd940ffbe183f2bac53aac state=OFFLINE, ts=1407564442115, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:22,115 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=OFFLINE
2014-08-08 23:07:22,122 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/7f4f72e893d4d48452ad33bacbb6bd79 already deleted, retry=false
2014-08-08 23:07:22,122 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=PENDING_OPEN, ts=1407564436569, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {7f4f72e893d4d48452ad33bacbb6bd79 state=OFFLINE, ts=1407564442122, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:22,122 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=OFFLINE
2014-08-08 23:07:22,175 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/463b24605f4dcef6d6bb43ac7d82351a already deleted, retry=false
2014-08-08 23:07:22,175 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=PENDING_OPEN, ts=1407564436566, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {463b24605f4dcef6d6bb43ac7d82351a state=OFFLINE, ts=1407564442175, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:22,175 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=OFFLINE
2014-08-08 23:07:22,183 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/2d7cdce59988748c536eb4fa2c738b15 already deleted, retry=false
2014-08-08 23:07:22,184 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564436550, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {2d7cdce59988748c536eb4fa2c738b15 state=OFFLINE, ts=1407564442183, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446}
2014-08-08 23:07:22,184 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=OFFLINE
2014-08-08 23:07:22,188 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Reassigning 0 region(s) that sceplus-vm49.almaden.ibm.com,16020,1407564409446 was carrying (and 5 regions(s) that were opening on this server)
2014-08-08 23:07:22,191 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Assigning 5 region(s) to slave1,16020,1407564409446
2014-08-08 23:07:22,191 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=OFFLINE, ts=1407564442103, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {de8fb45bce496ccaa88d0d22e867bf36 state=PENDING_OPEN, ts=1407564442191, server=slave1,16020,1407564409446}
2014-08-08 23:07:22,191 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:22,196 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=OFFLINE, ts=1407564442115, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {f1755ff29dbd940ffbe183f2bac53aac state=PENDING_OPEN, ts=1407564442196, server=slave1,16020,1407564409446}
2014-08-08 23:07:22,196 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:22,201 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=OFFLINE, ts=1407564442122, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {7f4f72e893d4d48452ad33bacbb6bd79 state=PENDING_OPEN, ts=1407564442201, server=slave1,16020,1407564409446}
2014-08-08 23:07:22,201 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:22,205 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=OFFLINE, ts=1407564442175, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {463b24605f4dcef6d6bb43ac7d82351a state=PENDING_OPEN, ts=1407564442205, server=slave1,16020,1407564409446}
2014-08-08 23:07:22,205 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:22,208 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=OFFLINE, ts=1407564442183, server=sceplus-vm49.almaden.ibm.com,16020,1407564409446} to {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564442208, server=slave1,16020,1407564409446}
2014-08-08 23:07:22,209 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=PENDING_OPEN&sn=slave1,16020,1407564409446
2014-08-08 23:07:22,214 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Unable to communicate with slave1,16020,1407564409446 in order to assign regions, 
java.io.IOException: Call to slave1/9.1.143.59:16020 failed on local exception: java.io.IOException: Connection to sceplus-vm49.almaden.ibm.com/9.1.143.59:16020 is closing. Call id=14, waitTime=1
	at org.apache.hadoop.hbase.ipc.RpcClient.wrapException(RpcClient.java:1571)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1542)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.openRegion(AdminProtos.java:20964)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:766)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1679)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2653)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:2634)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:291)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Connection to sceplus-vm49.almaden.ibm.com/9.1.143.59:16020 is closing. Call id=14, waitTime=1
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.cleanupCalls(RpcClient.java:1265)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.close(RpcClient.java:1059)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.run(RpcClient.java:792)
2014-08-08 23:07:22,220 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for de8fb45bce496ccaa88d0d22e867bf36 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:22,227 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=1 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:22,227 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:22,227 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:22,227 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:22,227 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:23,593 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.HMaster: Client=hadoop//9.1.143.58 disable usertable
2014-08-08 23:07:23,648 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Attempting to disable table usertable
2014-08-08 23:07:23,648 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from DISABLING to DISABLING
2014-08-08 23:07:23,653 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Offlining 2 regions.
2014-08-08 23:07:23,659 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=OPEN, ts=1407564437208, server=slave1,16020,1407564409446} to {201c1415e88f249634b1e37367f3b55a state=PENDING_CLOSE, ts=1407564443659, server=slave1,16020,1407564409446}
2014-08-08 23:07:23,659 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=PENDING_CLOSE
2014-08-08 23:07:23,659 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=OPEN, ts=1407564437300, server=slave1,16020,1407564409446} to {4f74727d9804f8618b268b7850f1f9fd state=PENDING_CLOSE, ts=1407564443659, server=slave1,16020,1407564409446}
2014-08-08 23:07:23,660 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=PENDING_CLOSE
2014-08-08 23:07:23,668 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=1 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,231 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,232 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,233 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,234 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,235 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=3 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,235 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=3 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,236 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=3 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:24,235 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:25,423 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm49.almaden.ibm.com,16020,1407564409446's hlogs to my queue
2014-08-08 23:07:25,429 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm49.almaden.ibm.com,16020,1407564409446/lock
2014-08-08 23:07:25,669 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=1 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:25,673 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,237 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,238 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,240 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,241 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=5 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,241 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=4 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,241 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:26,242 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:27,674 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=2 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:27,675 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,243 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,245 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,247 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,248 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=7 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,248 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=6 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,248 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:28,246 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:29,677 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=3 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:29,679 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,251 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,251 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,253 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,255 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=7 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,254 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,255 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=9 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:30,255 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:31,682 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=4 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:31,682 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:31,979 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Timed out on waiting for 2d7cdce59988748c536eb4fa2c738b15 to be assigned.
2014-08-08 23:07:31,980 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Region 2d7cdce59988748c536eb4fa2c738b15 didn't complete assignment in time
2014-08-08 23:07:31,980 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for 201c1415e88f249634b1e37367f3b55a to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:32,256 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,259 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,260 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,261 INFO  [AM.-pool1-t3] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,259 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=9 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,262 WARN  [AM.-pool1-t3] master.RegionStates: Failed to open/close 7f4f72e893d4d48452ad33bacbb6bd79 on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:32,262 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:32,264 INFO  [AM.-pool1-t3] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=PENDING_OPEN, ts=1407564442201, server=slave1,16020,1407564409446} to {7f4f72e893d4d48452ad33bacbb6bd79 state=FAILED_CLOSE, ts=1407564452264, server=slave1,16020,1407564409446}
2014-08-08 23:07:32,265 INFO  [AM.-pool1-t3] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=FAILED_CLOSE
2014-08-08 23:07:32,274 INFO  [AM.-pool1-t3] master.AssignmentManager: Skip assigning {ENCODED => 7f4f72e893d4d48452ad33bacbb6bd79, NAME => 'usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79.', STARTKEY => 'user8', ENDKEY => 'user9'}, we couldn't close it: {7f4f72e893d4d48452ad33bacbb6bd79 state=FAILED_CLOSE, ts=1407564452264, server=slave1,16020,1407564409446}
2014-08-08 23:07:33,684 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=5 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:33,685 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,262 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=9 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,263 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,265 INFO  [AM.-pool1-t2] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac., try=10 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,266 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,265 INFO  [AM.-pool1-t1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,265 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=8 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:34,268 WARN  [AM.-pool1-t1] master.RegionStates: Failed to open/close de8fb45bce496ccaa88d0d22e867bf36 on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:34,266 WARN  [AM.-pool1-t2] master.RegionStates: Failed to open/close f1755ff29dbd940ffbe183f2bac53aac on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:34,269 INFO  [AM.-pool1-t1] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=PENDING_OPEN, ts=1407564442191, server=slave1,16020,1407564409446} to {de8fb45bce496ccaa88d0d22e867bf36 state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446}
2014-08-08 23:07:34,269 INFO  [AM.-pool1-t2] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=PENDING_OPEN, ts=1407564442196, server=slave1,16020,1407564409446} to {f1755ff29dbd940ffbe183f2bac53aac state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446}
2014-08-08 23:07:34,270 INFO  [AM.-pool1-t1] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=FAILED_CLOSE
2014-08-08 23:07:34,270 INFO  [AM.-pool1-t2] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=FAILED_CLOSE
2014-08-08 23:07:34,276 INFO  [AM.-pool1-t1] master.AssignmentManager: Skip assigning {ENCODED => de8fb45bce496ccaa88d0d22e867bf36, NAME => 'usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36.', STARTKEY => 'user9', ENDKEY => ''}, we couldn't close it: {de8fb45bce496ccaa88d0d22e867bf36 state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446}
2014-08-08 23:07:34,278 INFO  [AM.-pool1-t2] master.AssignmentManager: Skip assigning {ENCODED => f1755ff29dbd940ffbe183f2bac53aac, NAME => 'usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac.', STARTKEY => 'user3', ENDKEY => 'user4'}, we couldn't close it: {f1755ff29dbd940ffbe183f2bac53aac state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446}
2014-08-08 23:07:35,687 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=6 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:35,688 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:36,269 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=9 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:36,271 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=9 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:36,271 INFO  [AM.-pool1-t4] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a., try=10 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:36,274 WARN  [AM.-pool1-t4] master.RegionStates: Failed to open/close 463b24605f4dcef6d6bb43ac7d82351a on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:36,275 INFO  [AM.-pool1-t4] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=PENDING_OPEN, ts=1407564442205, server=slave1,16020,1407564409446} to {463b24605f4dcef6d6bb43ac7d82351a state=FAILED_CLOSE, ts=1407564456275, server=slave1,16020,1407564409446}
2014-08-08 23:07:36,275 INFO  [AM.-pool1-t4] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=FAILED_CLOSE
2014-08-08 23:07:36,282 INFO  [AM.-pool1-t4] master.AssignmentManager: Skip assigning {ENCODED => 463b24605f4dcef6d6bb43ac7d82351a, NAME => 'usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a.', STARTKEY => 'user1', ENDKEY => 'user2'}, we couldn't close it: {463b24605f4dcef6d6bb43ac7d82351a state=FAILED_CLOSE, ts=1407564456275, server=slave1,16020,1407564409446}
2014-08-08 23:07:37,284 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for de8fb45bce496ccaa88d0d22e867bf36 to be assigned.
2014-08-08 23:07:37,284 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region de8fb45bce496ccaa88d0d22e867bf36 didn't complete assignment in time
2014-08-08 23:07:37,284 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for f1755ff29dbd940ffbe183f2bac53aac to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:37,690 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=7 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:37,691 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:37,695 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.io.IOException: This connection is closing for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=9 of 10
java.io.IOException: This connection is closing
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:908)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:38,273 INFO  [AM.-pool1-t5] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.forceRegionStateToOffline(AssignmentManager.java:1919)
	at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1548)
	at org.apache.hadoop.hbase.master.AssignCallable.call(AssignCallable.java:48)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:38,279 WARN  [AM.-pool1-t5] master.RegionStates: Failed to open/close 2d7cdce59988748c536eb4fa2c738b15 on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:38,279 INFO  [AM.-pool1-t5] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=PENDING_OPEN, ts=1407564442208, server=slave1,16020,1407564409446} to {2d7cdce59988748c536eb4fa2c738b15 state=FAILED_CLOSE, ts=1407564458279, server=slave1,16020,1407564409446}
2014-08-08 23:07:38,280 INFO  [AM.-pool1-t5] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=FAILED_CLOSE
2014-08-08 23:07:38,285 INFO  [AM.-pool1-t5] master.AssignmentManager: Skip assigning {ENCODED => 2d7cdce59988748c536eb4fa2c738b15, NAME => 'usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15.', STARTKEY => 'user4', ENDKEY => 'user5'}, we couldn't close it: {2d7cdce59988748c536eb4fa2c738b15 state=FAILED_CLOSE, ts=1407564458279, server=slave1,16020,1407564409446}
2014-08-08 23:07:39,696 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=8 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:39,698 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:39,698 WARN  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStates: Failed to open/close 201c1415e88f249634b1e37367f3b55a on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:39,699 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=PENDING_CLOSE, ts=1407564443659, server=slave1,16020,1407564409446} to {201c1415e88f249634b1e37367f3b55a state=FAILED_CLOSE, ts=1407564459699, server=slave1,16020,1407564409446}
2014-08-08 23:07:39,699 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-0] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=FAILED_CLOSE
2014-08-08 23:07:39,699 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned java.net.ConnectException: Connection refused for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=9 of 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupConnection(RpcClient.java:631)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:931)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:41,703 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.AssignmentManager: Server slave1,16020,1407564409446 returned org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020 for usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd., try=10 of 10
org.apache.hadoop.hbase.ipc.RpcClient$FailedServerException: This server is in the failed servers list: slave1/9.1.143.59:16020
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:916)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1100)
	at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1069)
	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1512)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1700)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1765)
	at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$BlockingStub.closeRegion(AdminProtos.java:20976)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.closeRegion(ProtobufUtil.java:1652)
	at org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:797)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1802)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2479)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:2491)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:216)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:07:41,703 WARN  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStates: Failed to open/close 4f74727d9804f8618b268b7850f1f9fd on slave1,16020,1407564409446, set to FAILED_CLOSE
2014-08-08 23:07:41,704 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=PENDING_CLOSE, ts=1407564443659, server=slave1,16020,1407564409446} to {4f74727d9804f8618b268b7850f1f9fd state=FAILED_CLOSE, ts=1407564461704, server=slave1,16020,1407564409446}
2014-08-08 23:07:41,704 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler-1] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=FAILED_CLOSE
2014-08-08 23:07:47,017 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Timed out on waiting for 201c1415e88f249634b1e37367f3b55a to be assigned.
2014-08-08 23:07:47,018 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Region 201c1415e88f249634b1e37367f3b55a didn't complete assignment in time
2014-08-08 23:07:47,018 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for f1755ff29dbd940ffbe183f2bac53aac to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:07:52,326 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for f1755ff29dbd940ffbe183f2bac53aac to be assigned.
2014-08-08 23:07:52,326 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region f1755ff29dbd940ffbe183f2bac53aac didn't complete assignment in time
2014-08-08 23:07:52,326 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 7f4f72e893d4d48452ad33bacbb6bd79 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:02,040 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Timed out on waiting for f1755ff29dbd940ffbe183f2bac53aac to be assigned.
2014-08-08 23:08:02,041 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Region f1755ff29dbd940ffbe183f2bac53aac didn't complete assignment in time
2014-08-08 23:08:02,041 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for 4f74727d9804f8618b268b7850f1f9fd to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:07,349 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for 7f4f72e893d4d48452ad33bacbb6bd79 to be assigned.
2014-08-08 23:08:07,350 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region 7f4f72e893d4d48452ad33bacbb6bd79 didn't complete assignment in time
2014-08-08 23:08:07,350 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 463b24605f4dcef6d6bb43ac7d82351a to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:17,062 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Timed out on waiting for 4f74727d9804f8618b268b7850f1f9fd to be assigned.
2014-08-08 23:08:17,063 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Region 4f74727d9804f8618b268b7850f1f9fd didn't complete assignment in time
2014-08-08 23:08:17,063 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for de8fb45bce496ccaa88d0d22e867bf36 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:22,370 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Timed out on waiting for 463b24605f4dcef6d6bb43ac7d82351a to be assigned.
2014-08-08 23:08:22,370 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Region 463b24605f4dcef6d6bb43ac7d82351a didn't complete assignment in time
2014-08-08 23:08:22,370 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 2d7cdce59988748c536eb4fa2c738b15 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:32,082 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Timed out on waiting for de8fb45bce496ccaa88d0d22e867bf36 to be assigned.
2014-08-08 23:08:32,082 WARN  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] handler.ServerShutdownHandler: Region de8fb45bce496ccaa88d0d22e867bf36 didn't complete assignment in time
2014-08-08 23:08:32,082 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-1] master.AssignmentManager: Waiting for 9ad804e5e097a5b026363aa2e30abfc6 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:08:35,863 INFO  [defaultRpcServer.handler=21,queue=1,port=16020] master.ServerManager: Registering server=slave1,16020,1407564512892
2014-08-08 23:08:35,864 INFO  [defaultRpcServer.handler=21,queue=1,port=16020] master.ServerManager: Triggering server recovery; existingServer slave1,16020,1407564409446 looks stale, new server:slave1,16020,1407564512892
2014-08-08 23:08:35,870 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407564409446 before assignment.
2014-08-08 23:08:35,870 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {de8fb45bce496ccaa88d0d22e867bf36 state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {f8725ca629bda79e918bbe4be0e5dd4c state=PENDING_OPEN, ts=1407564436567, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transitioning {4f74727d9804f8618b268b7850f1f9fd state=FAILED_CLOSE, ts=1407564461704, server=slave1,16020,1407564409446} will be handled by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {f5526342dd8bbf40a6cef1ca6de1ef04 state=PENDING_OPEN, ts=1407564436570, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {f1755ff29dbd940ffbe183f2bac53aac state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,879 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {7f4f72e893d4d48452ad33bacbb6bd79 state=FAILED_CLOSE, ts=1407564452264, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,880 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {463b24605f4dcef6d6bb43ac7d82351a state=FAILED_CLOSE, ts=1407564456275, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,880 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {2d7cdce59988748c536eb4fa2c738b15 state=FAILED_CLOSE, ts=1407564458279, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,880 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transitioning {201c1415e88f249634b1e37367f3b55a state=FAILED_CLOSE, ts=1407564459699, server=slave1,16020,1407564409446} will be handled by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,880 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Found region in {9ad804e5e097a5b026363aa2e30abfc6 state=PENDING_OPEN, ts=1407564436562, server=slave1,16020,1407564409446} to be reassigned by SSH for slave1,16020,1407564409446
2014-08-08 23:08:35,882 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/de8fb45bce496ccaa88d0d22e867bf36 already deleted, retry=false
2014-08-08 23:08:35,882 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {de8fb45bce496ccaa88d0d22e867bf36 state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446} to {de8fb45bce496ccaa88d0d22e867bf36 state=OFFLINE, ts=1407564515882, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,882 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36. with state=OFFLINE
2014-08-08 23:08:35,888 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/f8725ca629bda79e918bbe4be0e5dd4c already deleted, retry=false
2014-08-08 23:08:35,889 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {f8725ca629bda79e918bbe4be0e5dd4c state=PENDING_OPEN, ts=1407564436567, server=slave1,16020,1407564409446} to {f8725ca629bda79e918bbe4be0e5dd4c state=OFFLINE, ts=1407564515889, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,889 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user7,1407563722043.f8725ca629bda79e918bbe4be0e5dd4c. with state=OFFLINE
2014-08-08 23:08:35,893 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/f5526342dd8bbf40a6cef1ca6de1ef04 already deleted, retry=false
2014-08-08 23:08:35,893 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {f5526342dd8bbf40a6cef1ca6de1ef04 state=PENDING_OPEN, ts=1407564436570, server=slave1,16020,1407564409446} to {f5526342dd8bbf40a6cef1ca6de1ef04 state=OFFLINE, ts=1407564515893, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,893 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user2,1407563722043.f5526342dd8bbf40a6cef1ca6de1ef04. with state=OFFLINE
2014-08-08 23:08:35,899 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/f1755ff29dbd940ffbe183f2bac53aac already deleted, retry=false
2014-08-08 23:08:35,899 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {f1755ff29dbd940ffbe183f2bac53aac state=FAILED_CLOSE, ts=1407564454269, server=slave1,16020,1407564409446} to {f1755ff29dbd940ffbe183f2bac53aac state=OFFLINE, ts=1407564515899, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,899 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac. with state=OFFLINE
2014-08-08 23:08:35,903 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/7f4f72e893d4d48452ad33bacbb6bd79 already deleted, retry=false
2014-08-08 23:08:35,903 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {7f4f72e893d4d48452ad33bacbb6bd79 state=FAILED_CLOSE, ts=1407564452264, server=slave1,16020,1407564409446} to {7f4f72e893d4d48452ad33bacbb6bd79 state=OFFLINE, ts=1407564515903, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,903 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79. with state=OFFLINE
2014-08-08 23:08:35,907 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/463b24605f4dcef6d6bb43ac7d82351a already deleted, retry=false
2014-08-08 23:08:35,907 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {463b24605f4dcef6d6bb43ac7d82351a state=FAILED_CLOSE, ts=1407564456275, server=slave1,16020,1407564409446} to {463b24605f4dcef6d6bb43ac7d82351a state=OFFLINE, ts=1407564515907, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,907 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a. with state=OFFLINE
2014-08-08 23:08:35,912 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/2d7cdce59988748c536eb4fa2c738b15 already deleted, retry=false
2014-08-08 23:08:35,912 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {2d7cdce59988748c536eb4fa2c738b15 state=FAILED_CLOSE, ts=1407564458279, server=slave1,16020,1407564409446} to {2d7cdce59988748c536eb4fa2c738b15 state=OFFLINE, ts=1407564515912, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,912 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15. with state=OFFLINE
2014-08-08 23:08:35,916 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] zookeeper.RecoverableZooKeeper: Node /hbase/region-in-transition/9ad804e5e097a5b026363aa2e30abfc6 already deleted, retry=false
2014-08-08 23:08:35,916 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {9ad804e5e097a5b026363aa2e30abfc6 state=PENDING_OPEN, ts=1407564436562, server=slave1,16020,1407564409446} to {9ad804e5e097a5b026363aa2e30abfc6 state=OFFLINE, ts=1407564515916, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,916 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user5,1407563722043.9ad804e5e097a5b026363aa2e30abfc6. with state=OFFLINE
2014-08-08 23:08:35,919 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: Reassigning 2 region(s) that slave1,16020,1407564409446 was carrying (and 0 regions(s) that were opening on this server)
2014-08-08 23:08:35,919 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: The table usertable is disabled.  Hence not assigning region201c1415e88f249634b1e37367f3b55a
2014-08-08 23:08:35,919 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {201c1415e88f249634b1e37367f3b55a state=FAILED_CLOSE, ts=1407564459699, server=slave1,16020,1407564409446} to {201c1415e88f249634b1e37367f3b55a state=OFFLINE, ts=1407564515919, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,920 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a. with state=OFFLINE
2014-08-08 23:08:35,922 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.MasterFileSystem: Log dir for server sceplus-vm49.almaden.ibm.com,16020,1407564409446 does not exist
2014-08-08 23:08:35,922 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: dead splitlog workers [sceplus-vm49.almaden.ibm.com,16020,1407564409446]
2014-08-08 23:08:35,923 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: started splitting 0 logs in []
2014-08-08 23:08:35,925 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: dead splitlog workers [slave1,16020,1407563683255]
2014-08-08 23:08:35,928 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Offlined 201c1415e88f249634b1e37367f3b55a from slave1,16020,1407564409446
2014-08-08 23:08:35,928 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] handler.ServerShutdownHandler: The table usertable is disabled.  Hence not assigning region4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:35,928 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Transition {4f74727d9804f8618b268b7850f1f9fd state=FAILED_CLOSE, ts=1407564461704, server=slave1,16020,1407564409446} to {4f74727d9804f8618b268b7850f1f9fd state=OFFLINE, ts=1407564515928, server=slave1,16020,1407564409446}
2014-08-08 23:08:35,928 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStateStore: Updating row usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd. with state=OFFLINE
2014-08-08 23:08:35,932 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-4] master.RegionStates: Offlined 4f74727d9804f8618b268b7850f1f9fd from slave1,16020,1407564409446
2014-08-08 23:08:35,935 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: started splitting 35 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting]
2014-08-08 23:08:35,937 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] master.SplitLogManager: finished splitting (more than or equal to) 0 bytes in 0 log files in [] in 14ms
2014-08-08 23:08:35,937 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.LogReplayHandler: Finished processing shutdown of sceplus-vm49.almaden.ibm.com,16020,1407564409446
2014-08-08 23:08:35,938 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: dead splitlog workers [slave1,16020,1407564409446]
2014-08-08 23:08:35,942 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: started splitting 1 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting]
2014-08-08 23:08:35,984 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434
2014-08-08 23:08:35,985 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:36,034 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563892434, length=127852742
2014-08-08 23:08:36,034 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:36,066 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563892434
2014-08-08 23:08:36,082 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563892434 after 15ms
2014-08-08 23:08:36,288 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001103.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:36,309 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001095.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:36,348 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001103.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:36,386 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001095.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:36,410 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001097.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:36,414 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001099.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:36,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001092.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:36,517 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492
2014-08-08 23:08:36,518 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:36,532 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563902119 acquired by slave1,16020,1407564512892
2014-08-08 23:08:36,554 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563919492, length=128777032
2014-08-08 23:08:36,554 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:36,557 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563919492
2014-08-08 23:08:36,559 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 36 unassigned = 33 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563913316=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564073473=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492=last_update = 1407564516559 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434=last_update = 1407564516066 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563986573=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563902119=last_update = 1407564516532 last_version = 1 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564013318=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564067305=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 0 error = 0}
2014-08-08 23:08:36,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563919492 after 2ms
2014-08-08 23:08:36,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001271.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:36,633 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001280.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:36,690 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from DISABLING to DISABLED
2014-08-08 23:08:36,690 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001282.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:36,694 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.DisableTableHandler: Disabled table, usertable, is done=true
2014-08-08 23:08:36,697 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001279.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:36,708 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001275.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:36,753 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001271.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:36,816 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001268.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:37,128 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563986573 acquired by slave1,16020,1407564512892
2014-08-08 23:08:37,516 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:37,516 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:37,552 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001103.temp (wrote 17 edits in 191ms)
2014-08-08 23:08:37,553 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001097.temp (wrote 17 edits in 108ms)
2014-08-08 23:08:37,553 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001095.temp (wrote 18 edits in 132ms)
2014-08-08 23:08:37,561 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001097.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001129
2014-08-08 23:08:37,562 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001103.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001135
2014-08-08 23:08:37,575 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001095.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001129
2014-08-08 23:08:37,585 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001092.temp (wrote 18 edits in 60ms)
2014-08-08 23:08:37,587 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001099.temp (wrote 17 edits in 150ms)
2014-08-08 23:08:37,602 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001103.temp (wrote 16 edits in 128ms)
2014-08-08 23:08:37,603 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001092.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001126
2014-08-08 23:08:37,721 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001099.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001131
2014-08-08 23:08:37,721 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001103.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001133
2014-08-08 23:08:37,903 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:37,904 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:37,916 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001282.temp (wrote 17 edits in 86ms)
2014-08-08 23:08:37,919 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001275.temp (wrote 17 edits in 115ms)
2014-08-08 23:08:37,925 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001282.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001314
2014-08-08 23:08:37,927 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001275.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001307
2014-08-08 23:08:37,938 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001268.temp (wrote 17 edits in 100ms)
2014-08-08 23:08:37,940 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001271.temp (wrote 17 edits in 160ms)
2014-08-08 23:08:37,941 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001279.temp (wrote 17 edits in 113ms)
2014-08-08 23:08:37,945 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001268.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001300
2014-08-08 23:08:37,946 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001271.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001303
2014-08-08 23:08:37,956 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001279.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001311
2014-08-08 23:08:37,967 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001271.temp (wrote 18 edits in 152ms)
2014-08-08 23:08:37,968 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001280.temp (wrote 18 edits in 128ms)
2014-08-08 23:08:37,972 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001271.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001305
2014-08-08 23:08:38,009 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001280.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001314
2014-08-08 23:08:38,010 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 121 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563919492 is corrupted = false progress failed = false
2014-08-08 23:08:38,017 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,018 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492 in 1499ms
2014-08-08 23:08:38,019 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,042 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563919492 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563919492
2014-08-08 23:08:38,044 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563919492
2014-08-08 23:08:38,065 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006
2014-08-08 23:08:38,066 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,083 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001095.temp (wrote 18 edits in 175ms)
2014-08-08 23:08:38,088 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001095.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001129
2014-08-08 23:08:38,088 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 121 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563892434 is corrupted = false progress failed = false
2014-08-08 23:08:38,097 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,097 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434 in 2105ms
2014-08-08 23:08:38,098 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,098 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563926006, length=131384499
2014-08-08 23:08:38,099 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:38,103 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563926006
2014-08-08 23:08:38,105 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563926006 after 2ms
2014-08-08 23:08:38,110 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563892434 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563892434
2014-08-08 23:08:38,112 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563892434
2014-08-08 23:08:38,164 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001305.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:38,213 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001313.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:38,213 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001302.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:38,223 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001309.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:38,228 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001307.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:38,255 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001316.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:38,311 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001316.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:38,590 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784
2014-08-08 23:08:38,592 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:38,623 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563976784, length=134124657
2014-08-08 23:08:38,623 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:38,627 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563976784
2014-08-08 23:08:38,629 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563976784 after 1ms
2014-08-08 23:08:38,709 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001603.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:38,721 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001623.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:38,778 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563902119 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:38,791 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001634.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:38,793 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563902119 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563902119
2014-08-08 23:08:38,796 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563902119
2014-08-08 23:08:38,822 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001636.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:38,857 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563913316 acquired by slave1,16020,1407564512892
2014-08-08 23:08:38,904 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001600.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:39,118 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001634.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:39,170 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001633.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:39,216 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563986573 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:39,260 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001638.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:39,487 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563986573 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563986573
2014-08-08 23:08:39,489 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563986573
2014-08-08 23:08:39,536 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564073473 acquired by slave1,16020,1407564512892
2014-08-08 23:08:39,859 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:39,859 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:39,879 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001309.temp (wrote 18 edits in 115ms)
2014-08-08 23:08:39,885 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001309.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001343
2014-08-08 23:08:39,908 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001305.temp (wrote 17 edits in 123ms)
2014-08-08 23:08:39,910 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001316.temp (wrote 18 edits in 119ms)
2014-08-08 23:08:39,925 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001302.temp (wrote 17 edits in 57ms)
2014-08-08 23:08:39,926 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001305.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001337
2014-08-08 23:08:39,932 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001316.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001350
2014-08-08 23:08:39,934 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001302.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001334
2014-08-08 23:08:39,940 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001313.temp (wrote 18 edits in 124ms)
2014-08-08 23:08:39,942 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001316.temp (wrote 17 edits in 155ms)
2014-08-08 23:08:39,946 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001307.temp (wrote 17 edits in 97ms)
2014-08-08 23:08:39,946 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001313.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001347
2014-08-08 23:08:39,948 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001316.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001348
2014-08-08 23:08:39,993 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001307.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001339
2014-08-08 23:08:39,993 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 122 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563926006 is corrupted = false progress failed = false
2014-08-08 23:08:40,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:40,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006 in 1936ms
2014-08-08 23:08:40,016 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:40,030 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563926006 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563926006
2014-08-08 23:08:40,032 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563926006
2014-08-08 23:08:40,052 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217
2014-08-08 23:08:40,053 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:40,086 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563936217, length=128960899
2014-08-08 23:08:40,086 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:40,090 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563936217
2014-08-08 23:08:40,092 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563936217 after 2ms
2014-08-08 23:08:40,165 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001386.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:40,182 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001383.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:40,192 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001388.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:40,225 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001377.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:40,283 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001375.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:40,352 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001372.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:40,406 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001382.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:40,484 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563913316 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:40,515 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563913316 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563913316
2014-08-08 23:08:40,518 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563913316
2014-08-08 23:08:40,529 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564013318 acquired by slave1,16020,1407564512892
2014-08-08 23:08:40,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:40,650 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:40,659 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001638.temp (wrote 17 edits in 168ms)
2014-08-08 23:08:40,664 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001603.temp (wrote 29 edits in 415ms)
2014-08-08 23:08:40,667 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001633.temp (wrote 14 edits in 84ms)
2014-08-08 23:08:40,667 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001638.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001670
2014-08-08 23:08:40,670 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001603.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001651
2014-08-08 23:08:40,677 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001634.temp (wrote 13 edits in 102ms)
2014-08-08 23:08:40,682 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001633.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001658
2014-08-08 23:08:40,685 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001634.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001658
2014-08-08 23:08:40,690 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001600.temp (wrote 27 edits in 99ms)
2014-08-08 23:08:40,691 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001634.temp (wrote 17 edits in 139ms)
2014-08-08 23:08:40,696 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001634.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001666
2014-08-08 23:08:40,696 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001636.temp (wrote 17 edits in 118ms)
2014-08-08 23:08:40,699 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001600.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001644
2014-08-08 23:08:40,702 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001636.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001668
2014-08-08 23:08:40,745 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001623.temp (wrote 21 edits in 149ms)
2014-08-08 23:08:40,752 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001623.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001655
2014-08-08 23:08:40,752 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 155 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563976784 is corrupted = false progress failed = false
2014-08-08 23:08:40,758 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:40,758 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784 in 2167ms
2014-08-08 23:08:40,759 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:40,771 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563976784 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563976784
2014-08-08 23:08:40,772 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563976784
2014-08-08 23:08:40,868 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564073473 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:40,879 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564073473 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564073473
2014-08-08 23:08:40,881 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564073473
2014-08-08 23:08:41,155 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533
2014-08-08 23:08:41,157 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:41,187 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563881533, length=129091620
2014-08-08 23:08:41,187 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:41,191 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563881533
2014-08-08 23:08:41,193 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563881533 after 2ms
2014-08-08 23:08:41,255 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001025.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:41,270 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001018.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:41,308 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001029.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:41,316 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001024.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:41,346 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001031.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:41,364 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001035.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:41,409 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564067305 acquired by slave1,16020,1407564512892
2014-08-08 23:08:41,463 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001035.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:41,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:41,700 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:41,709 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001388.temp (wrote 16 edits in 107ms)
2014-08-08 23:08:41,710 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001375.temp (wrote 19 edits in 140ms)
2014-08-08 23:08:41,713 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001382.temp (wrote 18 edits in 121ms)
2014-08-08 23:08:41,717 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001388.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001418
2014-08-08 23:08:41,724 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001375.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001411
2014-08-08 23:08:41,727 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001372.temp (wrote 19 edits in 84ms)
2014-08-08 23:08:41,729 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001382.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001415
2014-08-08 23:08:41,736 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001383.temp (wrote 18 edits in 87ms)
2014-08-08 23:08:41,737 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001372.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001408
2014-08-08 23:08:41,743 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001386.temp (wrote 17 edits in 112ms)
2014-08-08 23:08:41,744 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001383.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001417
2014-08-08 23:08:41,753 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001377.temp (wrote 18 edits in 102ms)
2014-08-08 23:08:41,757 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001386.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001418
2014-08-08 23:08:41,797 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001377.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001411
2014-08-08 23:08:41,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 125 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563936217 is corrupted = false progress failed = false
2014-08-08 23:08:41,806 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:41,806 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217 in 1754ms
2014-08-08 23:08:41,809 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:41,822 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563936217 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563936217
2014-08-08 23:08:41,824 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563936217
2014-08-08 23:08:41,840 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076
2014-08-08 23:08:41,844 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:41,872 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563992076, length=130578780
2014-08-08 23:08:41,872 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:41,878 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563992076
2014-08-08 23:08:41,881 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563992076 after 3ms
2014-08-08 23:08:41,927 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001730.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:41,936 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001736.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:41,948 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001727.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:41,954 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001734.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:41,960 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001738.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:41,976 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001729.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:41,991 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001722.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:42,037 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001732.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:42,456 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564013318 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:42,468 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564013318 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564013318
2014-08-08 23:08:42,470 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564013318
2014-08-08 23:08:42,498 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945 acquired by slave1,16020,1407564512892
2014-08-08 23:08:42,556 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 26 unassigned = 22 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076=last_update = 1407564521880 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533=last_update = 1407564521191 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945=last_update = 1407564522498 last_version = 1 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564067305=last_update = 1407564521453 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 10 error = 0}
2014-08-08 23:08:42,956 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564067305 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:42,969 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564067305 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564067305
2014-08-08 23:08:42,971 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564067305
2014-08-08 23:08:43,318 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320 acquired by slave1,16020,1407564512892
2014-08-08 23:08:43,529 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:43,532 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:43,543 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:43,543 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:43,547 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001035.temp (wrote 18 edits in 106ms)
2014-08-08 23:08:43,553 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001029.temp (wrote 17 edits in 118ms)
2014-08-08 23:08:43,558 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001730.temp (wrote 18 edits in 96ms)
2014-08-08 23:08:43,561 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001727.temp (wrote 17 edits in 100ms)
2014-08-08 23:08:43,562 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001035.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001069
2014-08-08 23:08:43,563 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001029.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001061
2014-08-08 23:08:43,567 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001730.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001764
2014-08-08 23:08:43,569 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001018.temp (wrote 19 edits in 47ms)
2014-08-08 23:08:43,571 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001727.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001759
2014-08-08 23:08:43,571 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001031.temp (wrote 17 edits in 85ms)
2014-08-08 23:08:43,576 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001018.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001054
2014-08-08 23:08:43,577 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001031.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001063
2014-08-08 23:08:43,578 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001722.temp (wrote 17 edits in 65ms)
2014-08-08 23:08:43,579 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001732.temp (wrote 17 edits in 124ms)
2014-08-08 23:08:43,582 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001035.temp (wrote 17 edits in 88ms)
2014-08-08 23:08:43,583 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001722.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001754
2014-08-08 23:08:43,583 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001732.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001764
2014-08-08 23:08:43,585 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001025.temp (wrote 18 edits in 95ms)
2014-08-08 23:08:43,588 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001035.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001067
2014-08-08 23:08:43,589 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001025.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001059
2014-08-08 23:08:43,617 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001734.temp (wrote 18 edits in 92ms)
2014-08-08 23:08:43,871 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001736.temp (wrote 18 edits in 112ms)
2014-08-08 23:08:43,872 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001734.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001768
2014-08-08 23:08:43,872 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001024.temp (wrote 18 edits in 109ms)
2014-08-08 23:08:43,877 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001736.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001770
2014-08-08 23:08:43,878 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001024.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001057
2014-08-08 23:08:43,879 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 124 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563881533 is corrupted = false progress failed = false
2014-08-08 23:08:43,884 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:43,884 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533 in 2728ms
2014-08-08 23:08:43,886 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:43,887 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001729.temp (wrote 17 edits in 102ms)
2014-08-08 23:08:43,891 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001729.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001761
2014-08-08 23:08:43,932 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563881533 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563881533
2014-08-08 23:08:43,933 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563881533
2014-08-08 23:08:43,952 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080
2014-08-08 23:08:43,954 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:43,959 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001738.temp (wrote 17 edits in 98ms)
2014-08-08 23:08:43,963 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001738.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001770
2014-08-08 23:08:43,963 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 139 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563992076 is corrupted = false progress failed = false
2014-08-08 23:08:43,969 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:43,969 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076 in 2128ms
2014-08-08 23:08:43,970 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:43,979 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563992076 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563992076
2014-08-08 23:08:43,980 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563992076
2014-08-08 23:08:43,986 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564033080, length=129698690
2014-08-08 23:08:43,986 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:43,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564033080
2014-08-08 23:08:43,991 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564033080 after 2ms
2014-08-08 23:08:44,026 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001936.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:44,033 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001944.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:44,043 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001948.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:44,062 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001942.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:44,069 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001944.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:44,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001946.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:44,099 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001948.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:44,150 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001952.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:45,009 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700
2014-08-08 23:08:45,010 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:45,033 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:45,033 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:45,052 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564022700, length=128097982
2014-08-08 23:08:45,052 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:45,330 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001942.temp (wrote 17 edits in 84ms)
2014-08-08 23:08:45,331 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001944.temp (wrote 17 edits in 86ms)
2014-08-08 23:08:45,331 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001952.temp (wrote 18 edits in 118ms)
2014-08-08 23:08:45,482 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001952.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001986
2014-08-08 23:08:45,483 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564022700
2014-08-08 23:08:45,484 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001942.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001974
2014-08-08 23:08:45,485 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001944.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001976
2014-08-08 23:08:45,491 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564022700 after 8ms
2014-08-08 23:08:45,570 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001946.temp (wrote 17 edits in 77ms)
2014-08-08 23:08:45,691 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001948.temp (wrote 17 edits in 95ms)
2014-08-08 23:08:45,692 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001936.temp (wrote 18 edits in 38ms)
2014-08-08 23:08:45,913 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001874.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:45,914 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001946.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001978
2014-08-08 23:08:45,915 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001878.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:45,915 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001874.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:46,125 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001936.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001970
2014-08-08 23:08:46,131 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001948.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001980
2014-08-08 23:08:46,485 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001944.temp (wrote 16 edits in 85ms)
2014-08-08 23:08:46,487 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001948.temp (wrote 19 edits in 97ms)
2014-08-08 23:08:46,487 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001882.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:46,690 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001876.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:46,692 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001878.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:47,165 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001948.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001984
2014-08-08 23:08:47,165 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001944.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001974
2014-08-08 23:08:47,166 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 139 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564033080 is corrupted = false progress failed = false
2014-08-08 23:08:47,557 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 23 unassigned = 19 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320=last_update = 1407564523378 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700=last_update = 1407564525486 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945=last_update = 1407564522777 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080=last_update = 1407564523990 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 13 error = 0}
2014-08-08 23:08:47,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001874.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:47,931 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:47,947 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080 in 3994ms
2014-08-08 23:08:47,947 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:47,950 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001870.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:47,979 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563886945 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563886945
2014-08-08 23:08:47,982 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563886945
2014-08-08 23:08:47,983 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:48,003 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564033080 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564033080
2014-08-08 23:08:48,005 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564033080
2014-08-08 23:08:48,179 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:48,179 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:48,205 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001874.temp (wrote 18 edits in 533ms)
2014-08-08 23:08:48,211 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001874.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001908
2014-08-08 23:08:48,211 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:48,213 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001874.temp (wrote 18 edits in 1191ms)
2014-08-08 23:08:48,217 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001876.temp (wrote 17 edits in 1430ms)
2014-08-08 23:08:48,220 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001874.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001908
2014-08-08 23:08:48,221 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001876.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001908
2014-08-08 23:08:48,224 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563963320 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563963320
2014-08-08 23:08:48,225 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563963320
2014-08-08 23:08:48,226 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001870.temp (wrote 18 edits in 169ms)
2014-08-08 23:08:48,228 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001878.temp (wrote 16 edits in 1434ms)
2014-08-08 23:08:48,247 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001870.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001904
2014-08-08 23:08:48,252 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001878.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001908
2014-08-08 23:08:48,256 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001878.temp (wrote 17 edits in 881ms)
2014-08-08 23:08:48,258 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001874.temp (wrote 18 edits in 419ms)
2014-08-08 23:08:48,261 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001878.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001910
2014-08-08 23:08:48,300 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001874.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001908
2014-08-08 23:08:48,590 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001882.temp (wrote 16 edits in 1279ms)
2014-08-08 23:08:48,594 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001882.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001912
2014-08-08 23:08:48,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 138 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564022700 is corrupted = false progress failed = false
2014-08-08 23:08:48,827 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:48,827 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700 in 3818ms
2014-08-08 23:08:49,189 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836 acquired by slave1,16020,1407564512892
2014-08-08 23:08:49,189 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:49,198 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564022700 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564022700
2014-08-08 23:08:49,200 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564022700
2014-08-08 23:08:49,206 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785
2014-08-08 23:08:49,207 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:49,234 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563997785, length=145543602
2014-08-08 23:08:49,234 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:49,238 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563997785
2014-08-08 23:08:49,239 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563997785 after 1ms
2014-08-08 23:08:49,312 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001772.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:49,320 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001770.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:49,329 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001766.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:49,338 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001761.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:49,350 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001763.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:49,351 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001756.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:49,381 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001766.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:49,383 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001772.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:49,494 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283 acquired by slave1,16020,1407564512892
2014-08-08 23:08:49,978 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830
2014-08-08 23:08:49,978 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:50,009 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564044830, length=129666330
2014-08-08 23:08:50,009 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:50,014 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564044830
2014-08-08 23:08:50,016 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564044830 after 2ms
2014-08-08 23:08:50,071 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000002007.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:50,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000002014.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:50,085 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000002016.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:50,116 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000002012.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:50,123 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000002020.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:50,152 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000002024.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:50,166 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000002012.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:50,249 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000002024.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:50,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:50,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:50,468 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001766.temp (wrote 15 edits in 84ms)
2014-08-08 23:08:50,472 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001761.temp (wrote 16 edits in 82ms)
2014-08-08 23:08:50,474 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001766.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001794
2014-08-08 23:08:50,479 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001761.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001791
2014-08-08 23:08:50,492 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001756.temp (wrote 17 edits in 50ms)
2014-08-08 23:08:50,495 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001766.temp (wrote 16 edits in 81ms)
2014-08-08 23:08:50,500 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001756.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001788
2014-08-08 23:08:50,507 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001766.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001796
2014-08-08 23:08:50,507 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001770.temp (wrote 19 edits in 90ms)
2014-08-08 23:08:50,516 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001770.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001806
2014-08-08 23:08:50,518 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001772.temp (wrote 30 edits in 414ms)
2014-08-08 23:08:50,537 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001763.temp (wrote 17 edits in 80ms)
2014-08-08 23:08:50,537 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001772.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001830
2014-08-08 23:08:50,541 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:50,542 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001763.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001794
2014-08-08 23:08:50,550 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564038836 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564038836
2014-08-08 23:08:50,552 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564038836
2014-08-08 23:08:50,593 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 acquired by slave1,16020,1407564512892
2014-08-08 23:08:50,737 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001772.temp (wrote 30 edits in 146ms)
2014-08-08 23:08:50,742 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001772.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001830
2014-08-08 23:08:50,742 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 160 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563997785 is corrupted = false progress failed = false
2014-08-08 23:08:50,748 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:50,748 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785 in 1542ms
2014-08-08 23:08:50,749 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:50,759 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563997785 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563997785
2014-08-08 23:08:50,760 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563997785
2014-08-08 23:08:50,962 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196
2014-08-08 23:08:50,963 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:50,993 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196, length=129767091
2014-08-08 23:08:50,993 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:50,996 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196
2014-08-08 23:08:50,998 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196 after 2ms
2014-08-08 23:08:51,014 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:51,014 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:51,022 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000002024.temp (wrote 17 edits in 104ms)
2014-08-08 23:08:51,024 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000002014.temp (wrote 18 edits in 106ms)
2014-08-08 23:08:51,025 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000002012.temp (wrote 17 edits in 111ms)
2014-08-08 23:08:51,029 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000002024.temp to hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000002056
2014-08-08 23:08:51,029 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000002014.temp to hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000002048
2014-08-08 23:08:51,034 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000002012.temp to hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000002044
2014-08-08 23:08:51,034 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000002016.temp (wrote 18 edits in 109ms)
2014-08-08 23:08:51,038 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000002007.temp (wrote 18 edits in 38ms)
2014-08-08 23:08:51,039 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000002016.temp to hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000002050
2014-08-08 23:08:51,041 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000002020.temp (wrote 17 edits in 98ms)
2014-08-08 23:08:51,042 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000002007.temp to hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000002041
2014-08-08 23:08:51,046 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000002024.temp (wrote 17 edits in 164ms)
2014-08-08 23:08:51,047 INFO  [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000002020.temp to hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000002052
2014-08-08 23:08:51,049 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000002012.temp (wrote 18 edits in 172ms)
2014-08-08 23:08:51,050 INFO  [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000002024.temp to hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000002056
2014-08-08 23:08:51,096 INFO  [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000002012.temp to hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000002047
2014-08-08 23:08:51,096 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 140 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564044830 is corrupted = false progress failed = false
2014-08-08 23:08:51,101 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:51,101 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830 in 1123ms
2014-08-08 23:08:51,102 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:51,108 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001519.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:51,114 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564044830 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564044830
2014-08-08 23:08:51,117 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564044830
2014-08-08 23:08:51,119 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001524.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:51,122 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:51,137 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001515.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:51,140 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001512.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:51,142 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563968283 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563968283
2014-08-08 23:08:51,144 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563968283
2014-08-08 23:08:51,155 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001519.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:51,160 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001524.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:51,169 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001517.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:51,232 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001524.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:51,416 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156 acquired by slave1,16020,1407564512892
2014-08-08 23:08:51,804 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825
2014-08-08 23:08:51,806 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:51,842 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825, length=129160908
2014-08-08 23:08:51,842 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:51,845 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825
2014-08-08 23:08:51,847 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 after 2ms
2014-08-08 23:08:51,897 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001832.temp region=de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:51,911 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001838.temp region=f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:51,921 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001846.temp region=f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:51,944 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001849.temp region=463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:51,954 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001844.temp region=f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:51,969 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001837.temp region=7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:52,002 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001841.temp region=9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:52,069 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001841.temp region=4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:52,243 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.HMaster: Client=hadoop//9.1.143.58 delete usertable
2014-08-08 23:08:52,289 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.TableEventHandler: Handling table operation C_M_DELETE_TABLE on table usertable
2014-08-08 23:08:52,424 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] hbase.MetaTableAccessor: Deleted [{ENCODED => 201c1415e88f249634b1e37367f3b55a, NAME => 'usertable,,1407563722043.201c1415e88f249634b1e37367f3b55a.', STARTKEY => '', ENDKEY => 'user1'}, {ENCODED => 463b24605f4dcef6d6bb43ac7d82351a, NAME => 'usertable,user1,1407563722043.463b24605f4dcef6d6bb43ac7d82351a.', STARTKEY => 'user1', ENDKEY => 'user2'}, {ENCODED => f5526342dd8bbf40a6cef1ca6de1ef04, NAME => 'usertable,user2,1407563722043.f5526342dd8bbf40a6cef1ca6de1ef04.', STARTKEY => 'user2', ENDKEY => 'user3'}, {ENCODED => f1755ff29dbd940ffbe183f2bac53aac, NAME => 'usertable,user3,1407563722043.f1755ff29dbd940ffbe183f2bac53aac.', STARTKEY => 'user3', ENDKEY => 'user4'}, {ENCODED => 2d7cdce59988748c536eb4fa2c738b15, NAME => 'usertable,user4,1407563722043.2d7cdce59988748c536eb4fa2c738b15.', STARTKEY => 'user4', ENDKEY => 'user5'}, {ENCODED => 9ad804e5e097a5b026363aa2e30abfc6, NAME => 'usertable,user5,1407563722043.9ad804e5e097a5b026363aa2e30abfc6.', STARTKEY => 'user5', ENDKEY => 'user6'}, {ENCODED => 4f74727d9804f8618b268b7850f1f9fd, NAME => 'usertable,user6,1407563722043.4f74727d9804f8618b268b7850f1f9fd.', STARTKEY => 'user6', ENDKEY => 'user7'}, {ENCODED => f8725ca629bda79e918bbe4be0e5dd4c, NAME => 'usertable,user7,1407563722043.f8725ca629bda79e918bbe4be0e5dd4c.', STARTKEY => 'user7', ENDKEY => 'user8'}, {ENCODED => 7f4f72e893d4d48452ad33bacbb6bd79, NAME => 'usertable,user8,1407563722043.7f4f72e893d4d48452ad33bacbb6bd79.', STARTKEY => 'user8', ENDKEY => 'user9'}, {ENCODED => de8fb45bce496ccaa88d0d22e867bf36, NAME => 'usertable,user9,1407563722043.de8fb45bce496ccaa88d0d22e867bf36.', STARTKEY => 'user9', ENDKEY => ''}]
2014-08-08 23:08:52,558 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 15 unassigned = 11 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156=last_update = 1407564531460 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287=last_update = 1407564530648 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825=last_update = 1407564531847 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 1 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196=last_update = 1407564530998 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 21 error = 0}
2014-08-08 23:08:52,738 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:52,738 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:52,748 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001841.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001841.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,755 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001849.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001849.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,762 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001841.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001841.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,767 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001832.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001832.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,776 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001844.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001844.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,781 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001846.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001846.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,790 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001838.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001838.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,829 ERROR [split-log-closeStream-3] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001837.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001837.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 141 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 is corrupted = false progress failed = true
2014-08-08 23:08:52,831 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] regionserver.SplitLogWorker: log splitting of WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 failed, returning error
org.apache.hadoop.io.MultipleIOException: 8 exceptions [org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001841.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001849.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001841.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001832.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001844.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001846.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001838.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001837.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
]
	at org.apache.hadoop.io.MultipleIOException.createIOException(MultipleIOException.java:52)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.close(HLogSplitter.java:1151)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1040)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1732)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:52,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 to final state ERR sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:52,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 in 1130ms
2014-08-08 23:08:52,937 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 entered state: ERR sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:52,937 WARN  [main-EventThread] master.SplitLogManager: Error splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825
2014-08-08 23:08:52,951 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942
2014-08-08 23:08:52,952 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:52,979 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563947942, length=129606251
2014-08-08 23:08:52,979 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:52,983 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563947942
2014-08-08 23:08:52,984 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563947942 after 1ms
2014-08-08 23:08:53,014 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,020 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,025 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,029 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,034 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,049 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,064 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,079 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:53,126 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 entered state: ERR slave1,16020,1407564512892
2014-08-08 23:08:53,126 WARN  [main-EventThread] master.SplitLogManager: Error splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287
2014-08-08 23:08:53,141 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669 acquired by slave1,16020,1407564512892
2014-08-08 23:08:53,251 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:53,251 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:53,258 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001524.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,261 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001519.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001519.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,262 ERROR [split-log-closeStream-3] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001515.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001515.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,265 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001519.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001519.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,269 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001512.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001512.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,270 ERROR [split-log-closeStream-3] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001524.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,272 ERROR [split-log-closeStream-1] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001524.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,276 ERROR [split-log-closeStream-2] wal.HLogSplitter: Couldn't close log at hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001517.temp
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001517.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy18.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:404)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
	at com.sun.proxy.$Proxy19.complete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2116)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2100)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:70)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:103)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.close(ProtobufLogWriter.java:120)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink$2.call(HLogSplitter.java:1075)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,278 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 139 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196 is corrupted = false progress failed = true
2014-08-08 23:08:53,278 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] regionserver.SplitLogWorker: log splitting of WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196 failed, returning error
org.apache.hadoop.io.MultipleIOException: 8 exceptions [org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/4f74727d9804f8618b268b7850f1f9fd/recovered.edits/0000000000000001519.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79/recovered.edits/0000000000000001515.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6/recovered.edits/0000000000000001519.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36/recovered.edits/0000000000000001512.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04/recovered.edits/0000000000000001524.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
, org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c/recovered.edits/0000000000000001517.temp: File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1616446360_1, pendingcreates: 18]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2934)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:2998)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:2978)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:484)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)
]
	at org.apache.hadoop.io.MultipleIOException.createIOException(MultipleIOException.java:52)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.close(HLogSplitter.java:1151)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1040)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogReplayOutputSink.finishWritingAndClose(HLogSplitter.java:1732)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:373)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:231)
	at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:145)
	at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
2014-08-08 23:08:53,413 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 to final state ERR sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:53,413 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 in 2451ms
2014-08-08 23:08:53,415 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 entered state: ERR sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:53,415 WARN  [main-EventThread] master.SplitLogManager: Error splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196
2014-08-08 23:08:53,951 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683
2014-08-08 23:08:53,952 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:53,978 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563907683, length=128965841
2014-08-08 23:08:53,978 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:53,988 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563907683
2014-08-08 23:08:53,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563907683 after 1ms
2014-08-08 23:08:54,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f5526342dd8bbf40a6cef1ca6de1ef04. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,064 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/7f4f72e893d4d48452ad33bacbb6bd79. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,081 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f8725ca629bda79e918bbe4be0e5dd4c. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,083 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/de8fb45bce496ccaa88d0d22e867bf36. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,091 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f1755ff29dbd940ffbe183f2bac53aac. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,101 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/9ad804e5e097a5b026363aa2e30abfc6. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,154 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/463b24605f4dcef6d6bb43ac7d82351a. It is very likely that it was already split so it's safe to discard those edits.
2014-08-08 23:08:54,429 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:54,429 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:54,429 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 139 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563947942 is corrupted = false progress failed = false
2014-08-08 23:08:54,434 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:54,434 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942 in 1483ms
2014-08-08 23:08:54,435 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:54,445 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563947942 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563947942
2014-08-08 23:08:54,446 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563947942
2014-08-08 23:08:54,500 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156 entered state: ERR slave1,16020,1407564512892
2014-08-08 23:08:54,500 WARN  [main-EventThread] master.SplitLogManager: Error splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156
2014-08-08 23:08:54,517 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928 acquired by slave1,16020,1407564512892
2014-08-08 23:08:54,572 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916
2014-08-08 23:08:54,574 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:54,598 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting/slave1%2C16020%2C1407564409446.1407564430916, length=17
2014-08-08 23:08:54,598 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:54,601 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting/slave1%2C16020%2C1407564409446.1407564430916
2014-08-08 23:08:54,602 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting/slave1%2C16020%2C1407564409446.1407564430916 after 1ms
2014-08-08 23:08:54,625 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:54,625 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting/slave1%2C16020%2C1407564409446.1407564430916 is corrupted = false progress failed = false
2014-08-08 23:08:54,629 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:54,629 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916 in 56ms
2014-08-08 23:08:54,631 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:54,638 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting/slave1%2C16020%2C1407564409446.1407564430916 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564409446.1407564430916
2014-08-08 23:08:54,640 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564409446-splitting%2Fslave1%252C16020%252C1407564409446.1407564430916
2014-08-08 23:08:54,680 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] master.SplitLogManager: finished splitting (more than or equal to) 17 bytes in 1 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407564409446-splitting] in 18737ms
2014-08-08 23:08:54,680 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-3] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407564409446
2014-08-08 23:08:55,466 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:55,466 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:55,466 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 121 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563907683 is corrupted = false progress failed = false
2014-08-08 23:08:55,469 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555
2014-08-08 23:08:55,470 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:55,470 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683 in 1519ms
2014-08-08 23:08:55,470 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:55,474 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:55,483 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563907683 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563907683
2014-08-08 23:08:55,484 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563907683
2014-08-08 23:08:55,497 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563942555, length=128594869
2014-08-08 23:08:55,497 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:55,499 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563942555
2014-08-08 23:08:55,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563942555 after 1ms
2014-08-08 23:08:55,564 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x579614cf, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:08:55,565 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x579614cf connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:08:55,565 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:08:55,566 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-08 23:08:55,570 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0008, negotiated timeout = 90000
2014-08-08 23:08:55,582 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:55,603 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:55,643 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:55,669 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:55,676 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:55,711 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:55,717 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:55,793 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:55,809 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:55,852 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:55,867 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563869669 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563869669
2014-08-08 23:08:55,869 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563869669
2014-08-08 23:08:55,892 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711 acquired by slave1,16020,1407564512892
2014-08-08 23:08:56,472 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376
2014-08-08 23:08:56,473 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:56,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564051376, length=140186490
2014-08-08 23:08:56,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:56,504 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564051376
2014-08-08 23:08:56,506 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564051376 after 2ms
2014-08-08 23:08:56,539 INFO  [PriorityRpcServer.handler=2,queue=0,port=16020] regionserver.RSRpcServices: Compacting hbase:meta,,1.1588230740
2014-08-08 23:08:56,542 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:56,545 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407564536545] regionserver.HRegion: Starting compaction on info in region hbase:meta,,1.1588230740
2014-08-08 23:08:56,546 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407564536545] regionserver.HStore: Starting compaction of 2 file(s) in info of hbase:meta,,1.1588230740 into tmpdir=hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp, totalSize=34.1 K
2014-08-08 23:08:56,547 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:56,549 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407564536545] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:08:56,558 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:56,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:56,567 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:56,571 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:56,589 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:56,598 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:56,703 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407564536545] regionserver.HStore: Completed major compaction of 2 (all) file(s) in info of hbase:meta,,1.1588230740 into 9f04d0e2ea3a4991a8941c21236bf9e8(size=18.0 K), total size for store is 18.0 K. This selection was in queue for 0sec, and took 0sec to execute.
2014-08-08 23:08:56,706 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-shortCompactions-1407564536545] regionserver.CompactSplitThread: Completed compaction: Request = regionName=hbase:meta,,1.1588230740, storeName=info, fileCount=2, fileSize=34.1 K, priority=1, time=1222943907096068; duration=0sec
2014-08-08 23:08:56,742 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:56,742 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 10 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563942555 is corrupted = false progress failed = false
2014-08-08 23:08:56,746 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:56,746 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555 in 1276ms
2014-08-08 23:08:56,747 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:56,756 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563942555 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563942555
2014-08-08 23:08:56,757 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563942555
2014-08-08 23:08:57,018 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:57,028 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563980928 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563980928
2014-08-08 23:08:57,030 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563980928
2014-08-08 23:08:57,040 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111 acquired by slave1,16020,1407564512892
2014-08-08 23:08:57,161 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668
2014-08-08 23:08:57,162 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,186 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564027668, length=131758310
2014-08-08 23:08:57,186 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:57,189 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564027668
2014-08-08 23:08:57,191 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564027668 after 2ms
2014-08-08 23:08:57,219 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:08:57,223 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:57,233 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:57,239 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:57,244 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:57,252 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:57,254 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:57,257 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:57,262 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:57,299 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:57,299 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 10 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564051376 is corrupted = false progress failed = false
2014-08-08 23:08:57,302 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,302 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376 in 830ms
2014-08-08 23:08:57,303 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,312 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564051376 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564051376
2014-08-08 23:08:57,313 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564051376
2014-08-08 23:08:57,559 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 4 unassigned = 1 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711=last_update = 1407564535929 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 27 error = 4, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668=last_update = 1407564537190 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 27 error = 4, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 27 error = 4, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = 1407564537075 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 27 error = 4}
2014-08-08 23:08:57,792 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:08:57,795 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422
2014-08-08 23:08:57,801 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563930711 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563930711
2014-08-08 23:08:57,802 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563930711
2014-08-08 23:08:57,805 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,822 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564063422, length=132006594
2014-08-08 23:08:57,822 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:08:57,827 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564063422
2014-08-08 23:08:57,828 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564063422 after 1ms
2014-08-08 23:08:57,856 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:08:57,857 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:57,857 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 8 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564027668 is corrupted = false progress failed = false
2014-08-08 23:08:57,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668 in 699ms
2014-08-08 23:08:57,861 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:57,862 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:08:57,869 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564027668 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564027668
2014-08-08 23:08:57,871 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564027668
2014-08-08 23:08:57,871 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:08:57,876 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:57,883 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:08:57,904 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:08:57,909 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:57,915 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:08:57,930 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:08:57,963 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:08:58,028 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:08:58,117 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:08:58,483 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:08:58,491 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x147b9605d6e0008
2014-08-08 23:08:58,552 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x147b9605d6e0008 closed
2014-08-08 23:08:58,552 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-08 23:08:58,653 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 12 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564063422 is corrupted = false progress failed = false
2014-08-08 23:08:58,842 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:58,842 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422 in 1046ms
2014-08-08 23:08:58,843 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:08:58,907 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564063422 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564063422
2014-08-08 23:08:58,908 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564063422
2014-08-08 23:09:03,559 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 1 unassigned = 0 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111=last_update = 1407564537075 last_version = 2 cur_worker_name = slave1,16020,1407564512892 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 35 done = 30 error = 4}
2014-08-08 23:09:03,830 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:09:03,841 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563897111 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563897111
2014-08-08 23:09:03,843 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563897111
2014-08-08 23:09:04,326 WARN  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] master.SplitLogManager: error while splitting logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting] installed = 35 but only 31 done
2014-08-08 23:09:04,328 ERROR [M_LOG_REPLAY_OPS-sceplus-vm48:16020-2] executor.EventHandler: Caught throwable while processing event M_LOG_REPLAY
java.io.IOException: failed log replay for slave1,16020,1407563683255, will retry
	at org.apache.hadoop.hbase.master.handler.LogReplayHandler.process(LogReplayHandler.java:78)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: error or interrupted while splitting logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting] Task = installed = 35 done = 31 error = 4
	at org.apache.hadoop.hbase.master.SplitLogManager.splitLogDistributed(SplitLogManager.java:350)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:387)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:361)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:283)
	at org.apache.hadoop.hbase.master.handler.LogReplayHandler.process(LogReplayHandler.java:72)
	... 4 more
2014-08-08 23:09:04,331 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: dead splitlog workers [slave1,16020,1407563683255]
2014-08-08 23:09:04,334 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: started splitting 4 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting]
2014-08-08 23:09:04,344 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825
2014-08-08 23:09:04,345 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:04,346 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 acquired by slave1,16020,1407564512892
2014-08-08 23:09:04,371 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825, length=129160908
2014-08-08 23:09:04,371 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:09:04,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825
2014-08-08 23:09:04,375 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 after 1ms
2014-08-08 23:09:04,399 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x270f689, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:09:04,400 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x270f689 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:09:04,401 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:09:04,401 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-08 23:09:04,404 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0009, negotiated timeout = 90000
2014-08-08 23:09:04,416 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:09:04,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:09:04,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:09:04,422 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:09:04,423 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:09:04,427 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:09:04,432 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:09:04,443 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:09:04,447 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:09:04,452 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:09:04,479 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:09:04,965 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287
2014-08-08 23:09:04,966 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:04,983 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156 acquired by slave1,16020,1407564512892
2014-08-08 23:09:04,992 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563875287, length=130974478
2014-08-08 23:09:04,992 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:09:04,995 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563875287
2014-08-08 23:09:04,996 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563875287 after 1ms
2014-08-08 23:09:05,007 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:09:05,009 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x147b9605d6e0009
2014-08-08 23:09:05,012 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] zookeeper.ZooKeeper: Session: 0x147b9605d6e0009 closed
2014-08-08 23:09:05,012 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-08 23:09:05,112 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 11 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 is corrupted = false progress failed = false
2014-08-08 23:09:05,115 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x419558d0, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:09:05,115 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x419558d0 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:09:05,116 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:09:05,116 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:05,117 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 in 771ms
2014-08-08 23:09:05,117 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-08 23:09:05,117 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:05,120 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b9606079000b, negotiated timeout = 90000
2014-08-08 23:09:05,128 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407564016825 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407564016825
2014-08-08 23:09:05,130 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407564016825
2014-08-08 23:09:05,131 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 2d7cdce59988748c536eb4fa2c738b15
2014-08-08 23:09:05,131 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 4f74727d9804f8618b268b7850f1f9fd
2014-08-08 23:09:05,133 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:09:05,136 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f1755ff29dbd940ffbe183f2bac53aac
2014-08-08 23:09:05,137 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f8725ca629bda79e918bbe4be0e5dd4c
2014-08-08 23:09:05,139 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 7f4f72e893d4d48452ad33bacbb6bd79
2014-08-08 23:09:05,142 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region de8fb45bce496ccaa88d0d22e867bf36
2014-08-08 23:09:05,175 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 9ad804e5e097a5b026363aa2e30abfc6
2014-08-08 23:09:05,319 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-0] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region 463b24605f4dcef6d6bb43ac7d82351a
2014-08-08 23:09:05,366 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-1] wal.HLogSplitter: Table usertable doesn't exist. Skip log replay for region f5526342dd8bbf40a6cef1ca6de1ef04
2014-08-08 23:09:05,686 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:09:05,697 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563953156 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563953156
2014-08-08 23:09:05,698 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563953156
2014-08-08 23:09:06,376 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196 entered state: DONE slave1,16020,1407564512892
2014-08-08 23:09:06,386 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563958196 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563958196
2014-08-08 23:09:06,387 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563958196
2014-08-08 23:09:06,861 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:09:06,862 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47b9606079000b
2014-08-08 23:09:06,864 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47b9606079000b closed
2014-08-08 23:09:06,864 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-08 23:09:06,965 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 10 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563875287 is corrupted = false progress failed = false
2014-08-08 23:09:06,967 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:06,967 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 in 2001ms
2014-08-08 23:09:06,968 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:09:06,978 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting/slave1%2C16020%2C1407563683255.1407563875287 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407563683255.1407563875287
2014-08-08 23:09:06,979 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407563683255-splitting%2Fslave1%252C16020%252C1407563683255.1407563875287
2014-08-08 23:09:06,988 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] master.SplitLogManager: finished splitting (more than or equal to) 518546339 bytes in 4 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407563683255-splitting] in 2654ms
2014-08-08 23:09:06,988 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-4] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407563683255
2014-08-08 23:09:25,244 INFO  [defaultRpcServer.handler=48,queue=3,port=16020] compress.CodecPool: Got brand-new compressor [.gz]
2014-08-08 23:09:25,245 INFO  [defaultRpcServer.handler=48,queue=3,port=16020] master.HMaster: Client=hadoop//9.1.143.58 create 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}
2014-08-08 23:09:25,275 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.CreateTableHandler: Create table usertable
2014-08-08 23:09:25,324 INFO  [RegionOpenAndInitThread-usertable-1] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,324 INFO  [RegionOpenAndInitThread-usertable-2] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,324 INFO  [RegionOpenAndInitThread-usertable-3] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,324 INFO  [RegionOpenAndInitThread-usertable-4] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,325 INFO  [RegionOpenAndInitThread-usertable-5] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,325 INFO  [RegionOpenAndInitThread-usertable-6] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,326 INFO  [RegionOpenAndInitThread-usertable-7] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,326 INFO  [RegionOpenAndInitThread-usertable-8] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,326 INFO  [RegionOpenAndInitThread-usertable-9] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,327 INFO  [RegionOpenAndInitThread-usertable-10] regionserver.HRegion: creating HRegion usertable HTD == 'usertable', {NAME => 'family', DATA_BLOCK_ENCODING => 'NONE', BLOOMFILTER => 'ROW', REPLICATION_SCOPE => '0', VERSIONS => '1', COMPRESSION => 'GZ', MIN_VERSIONS => '0', TTL => 'FOREVER', KEEP_DELETED_CELLS => 'false', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'} RootDir = hdfs://master:54310/hbase/.tmp Table name == usertable
2014-08-08 23:09:25,351 INFO  [RegionOpenAndInitThread-usertable-4] regionserver.HRegion: Closed usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b.
2014-08-08 23:09:25,351 INFO  [RegionOpenAndInitThread-usertable-2] regionserver.HRegion: Closed usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f.
2014-08-08 23:09:25,356 INFO  [RegionOpenAndInitThread-usertable-3] regionserver.HRegion: Closed usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5.
2014-08-08 23:09:25,358 INFO  [RegionOpenAndInitThread-usertable-9] regionserver.HRegion: Closed usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd.
2014-08-08 23:09:25,360 INFO  [RegionOpenAndInitThread-usertable-10] regionserver.HRegion: Closed usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9.
2014-08-08 23:09:25,360 INFO  [RegionOpenAndInitThread-usertable-8] regionserver.HRegion: Closed usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1.
2014-08-08 23:09:25,362 INFO  [RegionOpenAndInitThread-usertable-5] regionserver.HRegion: Closed usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525.
2014-08-08 23:09:25,362 INFO  [RegionOpenAndInitThread-usertable-1] regionserver.HRegion: Closed usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361.
2014-08-08 23:09:25,401 INFO  [RegionOpenAndInitThread-usertable-6] regionserver.HRegion: Closed usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe.
2014-08-08 23:09:25,401 INFO  [RegionOpenAndInitThread-usertable-7] regionserver.HRegion: Closed usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710.
2014-08-08 23:09:25,420 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] hbase.MetaTableAccessor: Added 10
2014-08-08 23:09:25,465 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407564512892
2014-08-08 23:09:25,465 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407564565420, server=null} to {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407564565465, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,465 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,468 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407564565421, server=null} to {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407564565468, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,468 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,470 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407564565421, server=null} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407564565470, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,470 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,472 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407564565422, server=null} to {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407564565472, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,473 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,475 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407564565424, server=null} to {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407564565475, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,475 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,476 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407564565424, server=null} to {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407564565476, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,477 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,478 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407564565424, server=null} to {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407564565478, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,478 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,480 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407564565424, server=null} to {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407564565480, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,480 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,482 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407564565424, server=null} to {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407564565482, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,482 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,483 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407564565424, server=null} to {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407564565483, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,484 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=PENDING_OPEN&sn=slave1,16020,1407564512892
2014-08-08 23:09:25,732 WARN  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] zookeeper.ZKTableStateManager: Moving table usertable state from ENABLING to ENABLED
2014-08-08 23:09:25,738 INFO  [MASTER_TABLE_OPERATIONS-sceplus-vm48:16020-0] handler.CreateTableHandler: failed. null
2014-08-08 23:09:25,898 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407564565470, server=slave1,16020,1407564512892} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407564565898, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,898 INFO  [defaultRpcServer.handler=26,queue=1,port=16020] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407564565465, server=slave1,16020,1407564512892} to {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407564565898, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,899 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,899 INFO  [defaultRpcServer.handler=26,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,902 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407564565468, server=slave1,16020,1407564512892} to {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407564565902, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,902 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,902 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Onlined e93cd3088d438d7ce5cbf7674dc6e8b5 on slave1,16020,1407564512892
2014-08-08 23:09:25,903 INFO  [defaultRpcServer.handler=26,queue=1,port=16020] master.RegionStates: Onlined 9c1292974692f9c4f73114174c03102b on slave1,16020,1407564512892
2014-08-08 23:09:25,905 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Onlined 84ff456bfbe98c0f165826a0f12b688f on slave1,16020,1407564512892
2014-08-08 23:09:25,940 INFO  [defaultRpcServer.handler=31,queue=1,port=16020] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407564565472, server=slave1,16020,1407564512892} to {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407564565940, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,940 INFO  [defaultRpcServer.handler=31,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,941 INFO  [defaultRpcServer.handler=47,queue=2,port=16020] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407564565476, server=slave1,16020,1407564512892} to {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407564565941, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,942 INFO  [defaultRpcServer.handler=47,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,944 INFO  [defaultRpcServer.handler=31,queue=1,port=16020] master.RegionStates: Onlined 6e260e9bb95ca1cadb358a9041447ccd on slave1,16020,1407564512892
2014-08-08 23:09:25,945 INFO  [defaultRpcServer.handler=47,queue=2,port=16020] master.RegionStates: Onlined 71ff2fc858512aa6069c415dc539a5d1 on slave1,16020,1407564512892
2014-08-08 23:09:25,956 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407564565475, server=slave1,16020,1407564512892} to {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407564565956, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,957 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,959 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Onlined 543e4d92ab5371cb8d68689c118e60f9 on slave1,16020,1407564512892
2014-08-08 23:09:25,977 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407564565478, server=slave1,16020,1407564512892} to {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407564565977, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,977 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,980 INFO  [defaultRpcServer.handler=36,queue=1,port=16020] master.RegionStates: Onlined 4e94700075ca0745a008d12f3ba7a525 on slave1,16020,1407564512892
2014-08-08 23:09:25,986 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407564565480, server=slave1,16020,1407564512892} to {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407564565985, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,986 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:25,989 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Onlined f65c0619898a2814f37e9033f4c8d361 on slave1,16020,1407564512892
2014-08-08 23:09:25,999 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407564565482, server=slave1,16020,1407564512892} to {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407564565999, server=slave1,16020,1407564512892}
2014-08-08 23:09:25,999 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:26,002 INFO  [defaultRpcServer.handler=3,queue=3,port=16020] master.RegionStates: Onlined 14035e464a191fb59338e10668a4ffbe on slave1,16020,1407564512892
2014-08-08 23:09:26,047 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407564565483, server=slave1,16020,1407564512892} to {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407564566046, server=slave1,16020,1407564512892}
2014-08-08 23:09:26,047 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OPEN&openSeqNum=2&server=slave1,16020,1407564512892
2014-08-08 23:09:26,050 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStates: Onlined 4369dfa620a1c88558f70b80359f6710 on slave1,16020,1407564512892
2014-08-08 23:10:12,704 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 5108610048 and memstore heap size to 5108610048
2014-08-08 23:11:48,202 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=625, hits=617, hitRatio=98.72%, , cachingAccesses=621, cachingHits=613, cachingHitsRatio=98.71%, evictions=0, evicted=4, evictedPerRun=Infinity
2014-08-08 23:12:16,171 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:12:16,171 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407564436477, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407564736171, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:12:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:12:16,180 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:12:16,195 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:12:16,198 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:12:16,198 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:12:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407564736171, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407564736199, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:12:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:12:16,202 INFO  [AM.-pool1-t6] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:12:16,202 INFO  [AM.-pool1-t6] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407564736199, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407564736202, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:12:16,202 INFO  [AM.-pool1-t6] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:12:16,204 INFO  [AM.-pool1-t6] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:12:16,218 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:12:16,219 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:12:16,247 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:12:16,248 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:12:16,248 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407564736202, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407564736248, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:12:16,249 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:13:11,788 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 5108610048 and memstore heap size to 5108610048
2014-08-08 23:14:57,800 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sceplus-vm49.almaden.ibm.com,16020,1407564512892]
2014-08-08 23:14:57,801 WARN  [main-EventThread] zookeeper.RegionServerTracker: sceplus-vm49.almaden.ibm.com,16020,1407564512892 is not online or isn't known to the master.The latter could be caused by a DNS misconfiguration.
2014-08-08 23:14:57,804 INFO  [main-EventThread] replication.ReplicationTrackerZKImpl: /hbase/rs/sceplus-vm49.almaden.ibm.com,16020,1407564512892 znode expired, triggering replicatorRemoved event
2014-08-08 23:15:01,238 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm49.almaden.ibm.com,16020,1407564512892's hlogs to my queue
2014-08-08 23:15:01,243 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm49.almaden.ibm.com,16020,1407564512892/lock
2014-08-08 23:15:37,411 ERROR [defaultRpcServer.handler=49,queue=4,port=16020] master.MasterRpcServices: Region server slave1,16020,1407564512892 reported a fatal error:
ABORTING region server slave1,16020,1407564512892: regionserver:16020-0x47b96060790008, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:16020-0x47b96060790008 received expired from ZooKeeper, aborting
Cause:
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:409)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:320)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)

2014-08-08 23:16:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=1278, hits=1270, hitRatio=99.37%, , cachingAccesses=1274, cachingHits=1266, cachingHitsRatio=99.37%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:17:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407564736248, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565036174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:17:16,182 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:17:16,189 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:17:16,189 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:17:16,189 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:17:16,190 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565036174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565036190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:17:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:17:16,196 INFO  [AM.-pool1-t7] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:17:16,197 INFO  [AM.-pool1-t7] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565036190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565036197, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:17:16,197 INFO  [AM.-pool1-t7] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:17:16,200 INFO  [AM.-pool1-t7] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:17:16,214 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:17:16,215 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:17:16,254 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:17:16,255 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:17:16,255 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565036197, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565036255, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:17:16,255 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:32,597 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.ServerManager: Registering server=slave1,16020,1407565289717
2014-08-08 23:21:32,597 INFO  [defaultRpcServer.handler=15,queue=0,port=16020] master.ServerManager: Triggering server recovery; existingServer slave1,16020,1407564512892 looks stale, new server:slave1,16020,1407565289717
2014-08-08 23:21:32,602 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407564512892 before assignment.
2014-08-08 23:21:32,602 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-08 23:21:32,722 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407564565977, server=slave1,16020,1407564512892} to {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407565292722, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,722 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OFFLINE
2014-08-08 23:21:32,726 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407564565898, server=slave1,16020,1407564512892} to {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407565292726, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,726 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OFFLINE
2014-08-08 23:21:32,728 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407564565985, server=slave1,16020,1407564512892} to {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407565292728, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,728 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OFFLINE
2014-08-08 23:21:32,730 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407564565999, server=slave1,16020,1407564512892} to {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407565292730, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,730 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OFFLINE
2014-08-08 23:21:32,732 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407564565940, server=slave1,16020,1407564512892} to {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407565292732, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,732 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OFFLINE
2014-08-08 23:21:32,734 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407564565956, server=slave1,16020,1407564512892} to {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407565292733, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,734 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OFFLINE
2014-08-08 23:21:32,735 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407564566046, server=slave1,16020,1407564512892} to {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407565292735, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,735 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OFFLINE
2014-08-08 23:21:32,737 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407564565898, server=slave1,16020,1407564512892} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407565292737, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,737 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OFFLINE
2014-08-08 23:21:32,738 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407564565902, server=slave1,16020,1407564512892} to {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407565292738, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,738 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OFFLINE
2014-08-08 23:21:32,740 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407564565941, server=slave1,16020,1407564512892} to {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407565292740, server=slave1,16020,1407564512892}
2014-08-08 23:21:32,740 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OFFLINE
2014-08-08 23:21:32,742 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] handler.ServerShutdownHandler: Reassigning 10 region(s) that slave1,16020,1407564512892 was carrying (and 0 regions(s) that were opening on this server)
2014-08-08 23:21:32,743 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407565289717
2014-08-08 23:21:32,743 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407565292722, server=slave1,16020,1407564512892} to {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407565292743, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,743 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,745 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407565292726, server=slave1,16020,1407564512892} to {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407565292745, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,745 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,747 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407565292728, server=slave1,16020,1407564512892} to {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407565292747, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,747 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,748 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407565292730, server=slave1,16020,1407564512892} to {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407565292748, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,748 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,750 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407565292732, server=slave1,16020,1407564512892} to {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407565292750, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,750 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,751 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407565292733, server=slave1,16020,1407564512892} to {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407565292751, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,751 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,753 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407565292735, server=slave1,16020,1407564512892} to {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407565292753, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,753 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,755 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407565292737, server=slave1,16020,1407564512892} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407565292755, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,755 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,756 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407565292738, server=slave1,16020,1407564512892} to {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407565292756, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,756 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:32,758 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407565292740, server=slave1,16020,1407564512892} to {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407565292758, server=slave1,16020,1407565289717}
2014-08-08 23:21:32,758 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=PENDING_OPEN&sn=slave1,16020,1407565289717
2014-08-08 23:21:33,595 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 4e94700075ca0745a008d12f3ba7a525 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:33,865 INFO  [defaultRpcServer.handler=32,queue=2,port=16020] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407565292747, server=slave1,16020,1407565289717} to {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407565293865, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,865 INFO  [defaultRpcServer.handler=32,queue=2,port=16020] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407565289717
2014-08-08 23:21:33,868 INFO  [defaultRpcServer.handler=32,queue=2,port=16020] master.RegionStates: Onlined f65c0619898a2814f37e9033f4c8d361 on slave1,16020,1407565289717
2014-08-08 23:21:33,908 INFO  [defaultRpcServer.handler=37,queue=2,port=16020] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407565292743, server=slave1,16020,1407565289717} to {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407565293908, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,908 INFO  [defaultRpcServer.handler=37,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OPEN&openSeqNum=40001247&server=slave1,16020,1407565289717
2014-08-08 23:21:33,910 INFO  [defaultRpcServer.handler=37,queue=2,port=16020] master.RegionStates: Onlined 4e94700075ca0745a008d12f3ba7a525 on slave1,16020,1407565289717
2014-08-08 23:21:33,910 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 9c1292974692f9c4f73114174c03102b to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:33,932 INFO  [defaultRpcServer.handler=20,queue=0,port=16020] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407565292745, server=slave1,16020,1407565289717} to {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407565293932, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,933 INFO  [defaultRpcServer.handler=20,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OPEN&openSeqNum=40001231&server=slave1,16020,1407565289717
2014-08-08 23:21:33,934 INFO  [defaultRpcServer.handler=20,queue=0,port=16020] master.RegionStates: Onlined 9c1292974692f9c4f73114174c03102b on slave1,16020,1407565289717
2014-08-08 23:21:33,935 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 14035e464a191fb59338e10668a4ffbe to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:33,964 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407565292751, server=slave1,16020,1407565289717} to {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407565293964, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,965 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407565289717
2014-08-08 23:21:33,967 INFO  [defaultRpcServer.handler=13,queue=3,port=16020] master.RegionStates: Onlined 543e4d92ab5371cb8d68689c118e60f9 on slave1,16020,1407565289717
2014-08-08 23:21:33,972 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407565292748, server=slave1,16020,1407565289717} to {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407565293972, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,972 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OPEN&openSeqNum=40001489&server=slave1,16020,1407565289717
2014-08-08 23:21:33,974 INFO  [defaultRpcServer.handler=9,queue=4,port=16020] master.RegionStates: Onlined 14035e464a191fb59338e10668a4ffbe on slave1,16020,1407565289717
2014-08-08 23:21:33,974 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 6e260e9bb95ca1cadb358a9041447ccd to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:33,997 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407565292750, server=slave1,16020,1407565289717} to {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407565293997, server=slave1,16020,1407565289717}
2014-08-08 23:21:33,997 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OPEN&openSeqNum=40001703&server=slave1,16020,1407565289717
2014-08-08 23:21:33,999 INFO  [defaultRpcServer.handler=41,queue=1,port=16020] master.RegionStates: Onlined 6e260e9bb95ca1cadb358a9041447ccd on slave1,16020,1407565289717
2014-08-08 23:21:33,999 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 4369dfa620a1c88558f70b80359f6710 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:34,052 INFO  [defaultRpcServer.handler=25,queue=0,port=16020] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407565292753, server=slave1,16020,1407565289717} to {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407565294052, server=slave1,16020,1407565289717}
2014-08-08 23:21:34,052 INFO  [defaultRpcServer.handler=25,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OPEN&openSeqNum=40001437&server=slave1,16020,1407565289717
2014-08-08 23:21:34,054 INFO  [defaultRpcServer.handler=25,queue=0,port=16020] master.RegionStates: Onlined 4369dfa620a1c88558f70b80359f6710 on slave1,16020,1407565289717
2014-08-08 23:21:34,055 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for e93cd3088d438d7ce5cbf7674dc6e8b5 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:34,083 INFO  [defaultRpcServer.handler=18,queue=3,port=16020] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407565292755, server=slave1,16020,1407565289717} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407565294083, server=slave1,16020,1407565289717}
2014-08-08 23:21:34,083 INFO  [defaultRpcServer.handler=18,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OPEN&openSeqNum=40001025&server=slave1,16020,1407565289717
2014-08-08 23:21:34,085 INFO  [defaultRpcServer.handler=18,queue=3,port=16020] master.RegionStates: Onlined e93cd3088d438d7ce5cbf7674dc6e8b5 on slave1,16020,1407565289717
2014-08-08 23:21:34,085 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 84ff456bfbe98c0f165826a0f12b688f to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:34,110 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407565292756, server=slave1,16020,1407565289717} to {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407565294110, server=slave1,16020,1407565289717}
2014-08-08 23:21:34,110 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OPEN&openSeqNum=40001015&server=slave1,16020,1407565289717
2014-08-08 23:21:34,112 INFO  [defaultRpcServer.handler=14,queue=4,port=16020] master.RegionStates: Onlined 84ff456bfbe98c0f165826a0f12b688f on slave1,16020,1407565289717
2014-08-08 23:21:34,113 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-0] master.AssignmentManager: Waiting for 71ff2fc858512aa6069c415dc539a5d1 to leave regions-in-transition, timeOut=15000 ms.
2014-08-08 23:21:34,134 INFO  [defaultRpcServer.handler=42,queue=2,port=16020] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407565292758, server=slave1,16020,1407565289717} to {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407565294134, server=slave1,16020,1407565289717}
2014-08-08 23:21:34,135 INFO  [defaultRpcServer.handler=42,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OPEN&openSeqNum=40001627&server=slave1,16020,1407565289717
2014-08-08 23:21:34,136 INFO  [defaultRpcServer.handler=42,queue=2,port=16020] master.RegionStates: Onlined 71ff2fc858512aa6069c415dc539a5d1 on slave1,16020,1407565289717
2014-08-08 23:21:34,147 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-5] master.SplitLogManager: dead splitlog workers [slave1,16020,1407564512892]
2014-08-08 23:21:34,155 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-5] master.SplitLogManager: started splitting 51 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting]
2014-08-08 23:21:34,190 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133
2014-08-08 23:21:34,192 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:34,201 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564637414 acquired by slave1,16020,1407565289717
2014-08-08 23:21:34,219 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564613133, length=129806910
2014-08-08 23:21:34,219 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:34,224 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564613133
2014-08-08 23:21:34,225 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564613133 after 1ms
2014-08-08 23:21:34,315 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x74d81ae8, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:21:34,316 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x74d81ae8 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:21:34,316 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:21:34,317 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-08 23:21:34,320 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0012, negotiated timeout = 90000
2014-08-08 23:21:34,675 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 51 unassigned = 49 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564642087=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564637414=last_update = 1407565294338 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133=last_update = 1407565294225 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 0 error = 0}
2014-08-08 23:21:34,776 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026
2014-08-08 23:21:34,779 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:34,805 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564597026, length=128658735
2014-08-08 23:21:34,805 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:34,810 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564597026
2014-08-08 23:21:34,812 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564597026 after 2ms
2014-08-08 23:21:35,286 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564642087 acquired by slave1,16020,1407565289717
2014-08-08 23:21:36,927 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:36,928 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 18 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564613133 is corrupted = false progress failed = false
2014-08-08 23:21:36,932 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:36,932 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133 in 2741ms
2014-08-08 23:21:36,933 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:36,945 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564613133 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564613133
2014-08-08 23:21:36,946 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564613133
2014-08-08 23:21:36,957 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460
2014-08-08 23:21:36,960 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:36,985 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564601460, length=128345473
2014-08-08 23:21:36,985 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:36,997 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564601460
2014-08-08 23:21:36,999 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564601460 after 2ms
2014-08-08 23:21:37,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:37,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564597026 is corrupted = false progress failed = false
2014-08-08 23:21:37,457 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:37,457 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026 in 2680ms
2014-08-08 23:21:37,458 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:37,468 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564597026 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564597026
2014-08-08 23:21:37,469 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564597026
2014-08-08 23:21:37,834 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002
2014-08-08 23:21:37,835 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:37,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564823002, length=128811979
2014-08-08 23:21:37,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:37,874 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564823002
2014-08-08 23:21:37,875 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564823002 after 1ms
2014-08-08 23:21:38,555 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564637414 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:38,562 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564637414 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564637414
2014-08-08 23:21:38,563 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564637414
2014-08-08 23:21:38,582 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785 acquired by slave1,16020,1407565289717
2014-08-08 23:21:38,891 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564642087 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:38,900 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564642087 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564642087
2014-08-08 23:21:38,901 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564642087
2014-08-08 23:21:39,049 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:39,085 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 16 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564601460 is corrupted = false progress failed = false
2014-08-08 23:21:39,094 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:39,094 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460 in 2136ms
2014-08-08 23:21:39,096 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:39,106 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564601460 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564601460
2014-08-08 23:21:39,108 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564601460
2014-08-08 23:21:39,134 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222
2014-08-08 23:21:39,137 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:39,162 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564624222, length=129466950
2014-08-08 23:21:39,162 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:39,166 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564624222
2014-08-08 23:21:39,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564624222 after 2ms
2014-08-08 23:21:39,239 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177 acquired by slave1,16020,1407565289717
2014-08-08 23:21:39,698 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 46 unassigned = 42 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785=last_update = 1407565298621 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222=last_update = 1407565299167 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002=last_update = 1407565297876 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177=last_update = 1407565299323 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 5 error = 0}
2014-08-08 23:21:41,981 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:41,983 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 18 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564624222 is corrupted = false progress failed = false
2014-08-08 23:21:41,990 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:41,990 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222 in 2855ms
2014-08-08 23:21:41,993 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:42,003 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564624222 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564624222
2014-08-08 23:21:42,004 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564624222
2014-08-08 23:21:42,032 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391
2014-08-08 23:21:42,033 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:42,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564797391, length=128588582
2014-08-08 23:21:42,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:42,076 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564797391
2014-08-08 23:21:42,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564797391 after 1ms
2014-08-08 23:21:44,838 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 45 unassigned = 41 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391=last_update = 1407565302077 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785=last_update = 1407565298621 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002=last_update = 1407565297876 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177=last_update = 1407565299323 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 6 error = 0}
2014-08-08 23:21:45,297 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:45,298 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 147 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564823002 is corrupted = false progress failed = false
2014-08-08 23:21:45,304 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:45,304 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002 in 7470ms
2014-08-08 23:21:45,305 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:45,314 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564823002 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564823002
2014-08-08 23:21:45,315 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564823002
2014-08-08 23:21:45,338 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539
2014-08-08 23:21:45,340 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:45,375 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564589539, length=129820527
2014-08-08 23:21:45,375 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:45,380 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564589539
2014-08-08 23:21:45,381 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564589539 after 1ms
2014-08-08 23:21:46,584 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:46,594 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564785785 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564785785
2014-08-08 23:21:46,596 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564785785
2014-08-08 23:21:46,614 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110 acquired by slave1,16020,1407565289717
2014-08-08 23:21:47,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:47,650 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 22 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564589539 is corrupted = false progress failed = false
2014-08-08 23:21:47,654 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:47,654 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539 in 2315ms
2014-08-08 23:21:47,658 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:47,666 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564589539 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564589539
2014-08-08 23:21:47,667 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564589539
2014-08-08 23:21:47,687 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961
2014-08-08 23:21:47,694 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:47,715 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564758961, length=128745841
2014-08-08 23:21:47,715 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:47,718 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564758961
2014-08-08 23:21:47,719 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564758961 after 1ms
2014-08-08 23:21:47,755 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:47,762 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564774177 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564774177
2014-08-08 23:21:47,763 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564774177
2014-08-08 23:21:47,785 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320 acquired by slave1,16020,1407565289717
2014-08-08 23:21:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2404, hits=2396, hitRatio=99.67%, , cachingAccesses=2400, cachingHits=2392, cachingHitsRatio=99.67%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:21:48,804 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:48,805 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 120 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564797391 is corrupted = false progress failed = false
2014-08-08 23:21:48,811 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:48,811 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391 in 6779ms
2014-08-08 23:21:48,812 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:48,823 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564797391 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564797391
2014-08-08 23:21:48,824 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564797391
2014-08-08 23:21:48,835 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249
2014-08-08 23:21:48,837 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:48,885 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564652249, length=128617548
2014-08-08 23:21:48,885 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:48,893 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564652249
2014-08-08 23:21:48,895 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564652249 after 1ms
2014-08-08 23:21:49,029 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:49,038 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564699110 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564699110
2014-08-08 23:21:49,039 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564699110
2014-08-08 23:21:49,065 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232 acquired by slave1,16020,1407565289717
2014-08-08 23:21:49,838 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 39 unassigned = 35 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232=last_update = 1407565309132 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320=last_update = 1407565307830 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249=last_update = 1407565308894 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961=last_update = 1407565307720 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 12 error = 0}
2014-08-08 23:21:50,539 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:50,540 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564652249 is corrupted = false progress failed = false
2014-08-08 23:21:50,548 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:50,548 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249 in 1712ms
2014-08-08 23:21:50,549 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:50,558 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564652249 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564652249
2014-08-08 23:21:50,559 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564652249
2014-08-08 23:21:50,570 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171
2014-08-08 23:21:50,574 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:50,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564680171, length=132788296
2014-08-08 23:21:50,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:50,600 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564680171
2014-08-08 23:21:50,601 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564680171 after 1ms
2014-08-08 23:21:51,265 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:51,273 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564714320 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564714320
2014-08-08 23:21:51,274 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564714320
2014-08-08 23:21:51,302 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883 acquired by slave1,16020,1407565289717
2014-08-08 23:21:51,813 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:51,821 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564616232 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564616232
2014-08-08 23:21:51,822 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564616232
2014-08-08 23:21:52,037 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846 acquired by slave1,16020,1407565289717
2014-08-08 23:21:52,555 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:52,559 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 19 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564680171 is corrupted = false progress failed = false
2014-08-08 23:21:52,564 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:52,565 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171 in 1994ms
2014-08-08 23:21:52,567 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:52,575 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564680171 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564680171
2014-08-08 23:21:52,576 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564680171
2014-08-08 23:21:52,592 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294
2014-08-08 23:21:52,593 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:52,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564605294, length=129133720
2014-08-08 23:21:52,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:52,621 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564605294
2014-08-08 23:21:52,622 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564605294 after 1ms
2014-08-08 23:21:53,151 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:53,151 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 86 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564758961 is corrupted = false progress failed = false
2014-08-08 23:21:53,156 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:53,156 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961 in 5468ms
2014-08-08 23:21:53,157 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:53,166 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564758961 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564758961
2014-08-08 23:21:53,167 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564758961
2014-08-08 23:21:53,381 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183
2014-08-08 23:21:53,383 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:53,409 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564620183, length=129688526
2014-08-08 23:21:53,410 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:53,414 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564620183
2014-08-08 23:21:53,415 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564620183 after 1ms
2014-08-08 23:21:54,599 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:54,608 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564632846 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564632846
2014-08-08 23:21:54,609 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564632846
2014-08-08 23:21:54,618 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081 acquired by slave1,16020,1407565289717
2014-08-08 23:21:54,644 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:54,646 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 19 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564605294 is corrupted = false progress failed = false
2014-08-08 23:21:54,651 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:54,651 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294 in 2058ms
2014-08-08 23:21:54,651 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:54,664 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564605294 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564605294
2014-08-08 23:21:54,665 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564605294
2014-08-08 23:21:54,673 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884
2014-08-08 23:21:54,675 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:54,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564689884, length=131301349
2014-08-08 23:21:54,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:54,705 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564689884
2014-08-08 23:21:54,706 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564689884 after 1ms
2014-08-08 23:21:55,368 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:55,369 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 16 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564620183 is corrupted = false progress failed = false
2014-08-08 23:21:55,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:55,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183 in 1992ms
2014-08-08 23:21:55,376 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:55,386 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564620183 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564620183
2014-08-08 23:21:55,387 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564620183
2014-08-08 23:21:55,412 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321
2014-08-08 23:21:55,413 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:55,439 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564719321, length=128434402
2014-08-08 23:21:55,439 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:55,446 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564719321
2014-08-08 23:21:55,447 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564719321 after 1ms
2014-08-08 23:21:55,839 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 31 unassigned = 27 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884=last_update = 1407565314706 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081=last_update = 1407565314668 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883=last_update = 1407565311334 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321=last_update = 1407565315447 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 20 error = 0}
2014-08-08 23:21:56,220 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:56,229 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564781883 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564781883
2014-08-08 23:21:56,230 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564781883
2014-08-08 23:21:56,260 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054 acquired by slave1,16020,1407565289717
2014-08-08 23:21:56,574 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:56,575 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564689884 is corrupted = false progress failed = false
2014-08-08 23:21:56,579 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:56,579 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884 in 1905ms
2014-08-08 23:21:56,580 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:56,587 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564689884 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564689884
2014-08-08 23:21:56,588 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564689884
2014-08-08 23:21:56,595 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329
2014-08-08 23:21:56,596 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:56,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564656329, length=130578460
2014-08-08 23:21:56,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:56,625 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564656329
2014-08-08 23:21:56,626 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564656329 after 1ms
2014-08-08 23:21:57,222 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:21:57,235 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564666081 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564666081
2014-08-08 23:21:57,236 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564666081
2014-08-08 23:21:57,248 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412 acquired by slave1,16020,1407565289717
2014-08-08 23:21:59,093 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:59,095 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564656329 is corrupted = false progress failed = false
2014-08-08 23:21:59,102 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:59,102 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329 in 2506ms
2014-08-08 23:21:59,103 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:59,113 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564656329 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564656329
2014-08-08 23:21:59,114 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564656329
2014-08-08 23:21:59,122 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833
2014-08-08 23:21:59,123 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:59,147 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564646833, length=129236999
2014-08-08 23:21:59,147 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:21:59,152 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564646833
2014-08-08 23:21:59,153 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564646833 after 1ms
2014-08-08 23:21:59,752 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:21:59,752 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 52 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564719321 is corrupted = false progress failed = false
2014-08-08 23:21:59,758 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:59,759 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321 in 4346ms
2014-08-08 23:21:59,762 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:21:59,772 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564719321 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564719321
2014-08-08 23:21:59,773 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564719321
2014-08-08 23:22:00,067 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244
2014-08-08 23:22:00,068 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:00,093 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564515244, length=132749155
2014-08-08 23:22:00,094 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:00,100 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564515244
2014-08-08 23:22:00,101 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564515244 after 1ms
2014-08-08 23:22:00,839 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 26 unassigned = 22 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412=last_update = 1407565317323 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244=last_update = 1407565320102 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054=last_update = 1407565316316 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833=last_update = 1407565319158 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 25 error = 0}
2014-08-08 23:22:00,919 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:00,932 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564936412 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564936412
2014-08-08 23:22:00,933 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564936412
2014-08-08 23:22:00,948 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095 acquired by slave1,16020,1407565289717
2014-08-08 23:22:01,122 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:01,123 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 20 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564646833 is corrupted = false progress failed = false
2014-08-08 23:22:01,128 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:01,128 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833 in 2006ms
2014-08-08 23:22:01,129 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:01,138 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564646833 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564646833
2014-08-08 23:22:01,139 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564646833
2014-08-08 23:22:01,152 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179
2014-08-08 23:22:01,153 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:01,176 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564830179, length=17702536
2014-08-08 23:22:01,176 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:01,180 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564830179
2014-08-08 23:22:01,180 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:01,181 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564830179 after 1ms
2014-08-08 23:22:01,187 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564734054 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564734054
2014-08-08 23:22:01,188 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564734054
2014-08-08 23:22:01,295 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:01,295 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 12 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564515244 is corrupted = false progress failed = false
2014-08-08 23:22:01,299 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:01,299 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244 in 1231ms
2014-08-08 23:22:01,301 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:01,309 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564515244 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564515244
2014-08-08 23:22:01,310 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564515244
2014-08-08 23:22:01,472 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712 acquired by slave1,16020,1407565289717
2014-08-08 23:22:02,101 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888
2014-08-08 23:22:02,102 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:02,126 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564818888, length=130315517
2014-08-08 23:22:02,126 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:02,132 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564818888
2014-08-08 23:22:02,133 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564818888 after 1ms
2014-08-08 23:22:02,770 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:02,779 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564685095 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564685095
2014-08-08 23:22:02,780 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564685095
2014-08-08 23:22:02,818 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142 acquired by slave1,16020,1407565289717
2014-08-08 23:22:02,955 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:02,956 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 21 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564830179 is corrupted = false progress failed = false
2014-08-08 23:22:02,964 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:02,964 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179 in 1812ms
2014-08-08 23:22:02,969 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:02,977 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564830179 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564830179
2014-08-08 23:22:02,978 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564830179
2014-08-08 23:22:02,994 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297
2014-08-08 23:22:02,994 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:03,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564803297, length=137002004
2014-08-08 23:22:03,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:03,084 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564803297
2014-08-08 23:22:03,086 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564803297 after 2ms
2014-08-08 23:22:05,887 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 20 unassigned = 16 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = 1407565322133 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = 1407565322866 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = 1407565323085 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712=last_update = 1407565321547 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 31 error = 0}
2014-08-08 23:22:08,238 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:08,247 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564738712 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564738712
2014-08-08 23:22:08,248 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564738712
2014-08-08 23:22:08,264 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808 acquired by slave1,16020,1407565289717
2014-08-08 23:22:10,889 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 19 unassigned = 15 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888=last_update = 1407565322133 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142=last_update = 1407565322866 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297=last_update = 1407565323085 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808=last_update = 1407565328387 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 32 error = 0}
2014-08-08 23:22:12,081 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:12,083 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 137 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564818888 is corrupted = false progress failed = false
2014-08-08 23:22:12,495 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:12,495 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888 in 10394ms
2014-08-08 23:22:12,556 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:12,851 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:12,866 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x147b9605d6e0012
2014-08-08 23:22:12,895 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564818888 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564818888
2014-08-08 23:22:12,897 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564818888
2014-08-08 23:22:12,897 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:13,097 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] zookeeper.ZooKeeper: Session: 0x147b9605d6e0012 closed
2014-08-08 23:22:13,097 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-08 23:22:13,197 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 144 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564803297 is corrupted = false progress failed = false
2014-08-08 23:22:13,253 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564754142 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564754142
2014-08-08 23:22:13,255 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564754142
2014-08-08 23:22:13,386 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:13,427 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297 in 10433ms
2014-08-08 23:22:13,432 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:13,519 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564803297 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564803297
2014-08-08 23:22:13,520 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564803297
2014-08-08 23:22:13,734 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444
2014-08-08 23:22:13,761 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564661444, length=130274690
2014-08-08 23:22:13,761 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:13,967 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963 acquired by slave1,16020,1407565289717
2014-08-08 23:22:13,967 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:13,968 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564661444
2014-08-08 23:22:13,970 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564661444 after 2ms
2014-08-08 23:22:14,056 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x37610c4d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-08 23:22:14,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x37610c4d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-08 23:22:14,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-08 23:22:14,058 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-08 23:22:14,063 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b9606079000f, negotiated timeout = 90000
2014-08-08 23:22:14,183 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:14,191 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564592808 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564592808
2014-08-08 23:22:14,192 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564592808
2014-08-08 23:22:14,437 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606 acquired by slave1,16020,1407565289717
2014-08-08 23:22:14,500 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502
2014-08-08 23:22:14,500 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:14,524 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564724502, length=128180062
2014-08-08 23:22:14,524 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:14,527 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564724502
2014-08-08 23:22:14,528 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564724502 after 1ms
2014-08-08 23:22:15,890 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 15 unassigned = 11 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = 1407565333968 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = 1407565334528 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606=last_update = 1407565334473 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444=last_update = 1407565333969 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 36 error = 0}
2014-08-08 23:22:16,897 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:16,897 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564661444 is corrupted = false progress failed = false
2014-08-08 23:22:16,906 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:16,906 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444 in 3172ms
2014-08-08 23:22:16,908 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:16,917 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564661444 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564661444
2014-08-08 23:22:16,918 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564661444
2014-08-08 23:22:16,933 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434
2014-08-08 23:22:16,934 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:16,963 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564694434, length=131004380
2014-08-08 23:22:16,963 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:16,968 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564694434
2014-08-08 23:22:17,010 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564694434 after 40ms
2014-08-08 23:22:17,538 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:17,546 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564670606 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564670606
2014-08-08 23:22:17,548 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564670606
2014-08-08 23:22:17,562 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357 acquired by slave1,16020,1407565289717
2014-08-08 23:22:18,796 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:18,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 16 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564694434 is corrupted = false progress failed = false
2014-08-08 23:22:18,804 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:18,804 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434 in 1870ms
2014-08-08 23:22:18,805 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:18,814 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564694434 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564694434
2014-08-08 23:22:18,815 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564694434
2014-08-08 23:22:18,829 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527
2014-08-08 23:22:18,829 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:18,854 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564675527, length=128521578
2014-08-08 23:22:18,854 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:18,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564675527
2014-08-08 23:22:18,861 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564675527 after 1ms
2014-08-08 23:22:20,892 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 12 unassigned = 8 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527=last_update = 1407565338862 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963=last_update = 1407565333968 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502=last_update = 1407565334528 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = 1407565337654 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 39 error = 0}
2014-08-08 23:22:20,965 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:20,965 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 50 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564724502 is corrupted = false progress failed = false
2014-08-08 23:22:20,971 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:20,971 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502 in 6471ms
2014-08-08 23:22:20,978 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:20,990 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564724502 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564724502
2014-08-08 23:22:20,990 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564724502
2014-08-08 23:22:21,022 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642
2014-08-08 23:22:21,024 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:21,062 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564704642, length=130962148
2014-08-08 23:22:21,062 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:21,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564704642
2014-08-08 23:22:21,069 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564704642 after 1ms
2014-08-08 23:22:21,310 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:21,317 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564748963 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564748963
2014-08-08 23:22:21,318 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564748963
2014-08-08 23:22:21,330 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737 acquired by slave1,16020,1407565289717
2014-08-08 23:22:21,459 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:21,459 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564675527 is corrupted = false progress failed = false
2014-08-08 23:22:21,462 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:21,462 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527 in 2633ms
2014-08-08 23:22:21,463 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:21,470 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564675527 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564675527
2014-08-08 23:22:21,471 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564675527
2014-08-08 23:22:21,615 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586
2014-08-08 23:22:21,616 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:21,650 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564628586, length=128729312
2014-08-08 23:22:21,650 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:21,654 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564628586
2014-08-08 23:22:21,655 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564628586 after 1ms
2014-08-08 23:22:23,729 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:23,730 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 19 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564704642 is corrupted = false progress failed = false
2014-08-08 23:22:23,736 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:23,736 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642 in 2713ms
2014-08-08 23:22:23,738 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:23,746 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564704642 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564704642
2014-08-08 23:22:23,747 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564704642
2014-08-08 23:22:23,765 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050
2014-08-08 23:22:23,766 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:23,810 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564609050, length=130552840
2014-08-08 23:22:23,810 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:23,815 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564609050
2014-08-08 23:22:23,816 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564609050 after 1ms
2014-08-08 23:22:24,593 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:24,594 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564628586 is corrupted = false progress failed = false
2014-08-08 23:22:24,599 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:24,599 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586 in 2984ms
2014-08-08 23:22:24,602 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:24,613 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564628586 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564628586
2014-08-08 23:22:24,614 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564628586
2014-08-08 23:22:24,633 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567
2014-08-08 23:22:24,635 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:24,658 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564709567, length=129504417
2014-08-08 23:22:24,658 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:24,663 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564709567
2014-08-08 23:22:24,664 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564709567 after 1ms
2014-08-08 23:22:25,893 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 7 unassigned = 3 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567=last_update = 1407565344663 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050=last_update = 1407565343816 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737=last_update = 1407565341460 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357=last_update = 1407565337654 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 44 error = 0}
2014-08-08 23:22:26,041 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:26,049 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564791357 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564791357
2014-08-08 23:22:26,050 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564791357
2014-08-08 23:22:26,067 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821 acquired by slave1,16020,1407565289717
2014-08-08 23:22:26,543 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:26,545 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564609050 is corrupted = false progress failed = false
2014-08-08 23:22:26,553 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:26,553 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050 in 2787ms
2014-08-08 23:22:26,556 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:26,563 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564609050 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564609050
2014-08-08 23:22:26,563 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564609050
2014-08-08 23:22:26,603 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736
2014-08-08 23:22:26,604 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:26,627 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564743736, length=130521908
2014-08-08 23:22:26,627 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:26,633 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564743736
2014-08-08 23:22:26,634 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564743736 after 1ms
2014-08-08 23:22:27,423 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:27,424 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 25 edits across 2 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564709567 is corrupted = false progress failed = false
2014-08-08 23:22:27,428 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:27,428 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567 in 2794ms
2014-08-08 23:22:27,428 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:27,436 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564709567 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564709567
2014-08-08 23:22:27,437 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564709567
2014-08-08 23:22:27,451 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344
2014-08-08 23:22:27,452 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:27,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564729344, length=128374043
2014-08-08 23:22:27,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-08 23:22:27,478 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564729344
2014-08-08 23:22:27,479 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564729344 after 1ms
2014-08-08 23:22:28,911 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:28,920 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564768737 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564768737
2014-08-08 23:22:28,921 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564768737
2014-08-08 23:22:30,895 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 3 unassigned = 0 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344=last_update = 1407565347481 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 48 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821=last_update = 1407565346111 last_version = 2 cur_worker_name = slave1,16020,1407565289717 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 48 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736=last_update = 1407565346634 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 51 done = 48 error = 0}
2014-08-08 23:22:33,674 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821 entered state: DONE slave1,16020,1407565289717
2014-08-08 23:22:33,682 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564763821 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564763821
2014-08-08 23:22:33,683 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564763821
2014-08-08 23:22:34,367 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:34,369 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 53 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564729344 is corrupted = false progress failed = false
2014-08-08 23:22:34,381 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:34,381 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344 in 6930ms
2014-08-08 23:22:34,382 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:34,389 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564729344 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564729344
2014-08-08 23:22:34,390 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564729344
2014-08-08 23:22:34,655 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-08 23:22:34,659 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47b9606079000f
2014-08-08 23:22:34,667 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] zookeeper.ZooKeeper: Session: 0x47b9606079000f closed
2014-08-08 23:22:34,667 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-08 23:22:34,767 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 79 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564743736 is corrupted = false progress failed = false
2014-08-08 23:22:34,867 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:34,867 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736 in 8263ms
2014-08-08 23:22:34,868 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:22:34,874 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting/slave1%2C16020%2C1407564512892.1407564743736 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407564512892.1407564743736
2014-08-08 23:22:34,875 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407564512892-splitting%2Fslave1%252C16020%252C1407564512892.1407564743736
2014-08-08 23:22:34,938 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-5] master.SplitLogManager: finished splitting (more than or equal to) 6446753471 bytes in 51 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407564512892-splitting] in 60783ms
2014-08-08 23:22:34,938 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-5] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407564512892
2014-08-08 23:26:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2709, hits=2701, hitRatio=99.70%, , cachingAccesses=2705, cachingHits=2697, cachingHitsRatio=99.70%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:27:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:27:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565036255, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565636175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:27:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:27:16,191 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:27:16,197 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:27:16,197 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:27:16,198 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:27:16,198 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565636175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565636198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:27:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:27:16,202 INFO  [AM.-pool1-t8] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:27:16,202 INFO  [AM.-pool1-t8] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565636198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565636202, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:27:16,202 INFO  [AM.-pool1-t8] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:27:16,205 INFO  [AM.-pool1-t8] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:27:16,216 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:27:16,217 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:27:16,239 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:27:16,240 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:27:16,240 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565636202, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565636240, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:27:16,241 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:31:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2711, hits=2703, hitRatio=99.70%, , cachingAccesses=2707, cachingHits=2699, cachingHitsRatio=99.70%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:32:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:32:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565636240, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565936172, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:32:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:32:16,228 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:32:16,232 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:32:16,233 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:32:16,234 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:32:16,234 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407565936172, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565936234, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:32:16,234 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:32:16,237 INFO  [AM.-pool1-t9] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:32:16,237 INFO  [AM.-pool1-t9] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407565936234, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565936237, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:32:16,238 INFO  [AM.-pool1-t9] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:32:16,240 INFO  [AM.-pool1-t9] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:32:16,255 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:32:16,256 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:32:16,277 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:32:16,278 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:32:16,278 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407565936237, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565936278, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:32:16,278 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:36:48,202 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2713, hits=2705, hitRatio=99.71%, , cachingAccesses=2709, cachingHits=2701, cachingHitsRatio=99.70%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:37:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:37:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407565936278, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566236173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:37:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:37:16,181 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:37:16,189 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:37:16,189 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:37:16,190 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:37:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566236173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566236191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:37:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:37:16,196 INFO  [AM.-pool1-t10] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:37:16,196 INFO  [AM.-pool1-t10] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566236191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566236196, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:37:16,196 INFO  [AM.-pool1-t10] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:37:16,200 INFO  [AM.-pool1-t10] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:37:16,215 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:37:16,216 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:37:16,238 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:37:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:37:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566236196, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566236239, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:37:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:41:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2715, hits=2707, hitRatio=99.71%, , cachingAccesses=2711, cachingHits=2703, cachingHitsRatio=99.70%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:42:16,215 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:42:16,216 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566236239, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566536215, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:42:16,216 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:42:16,250 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:42:16,256 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:42:16,256 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:42:16,256 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:42:16,257 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566536215, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566536257, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:42:16,257 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:42:16,260 INFO  [AM.-pool1-t11] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:42:16,261 INFO  [AM.-pool1-t11] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566536257, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566536261, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:42:16,261 INFO  [AM.-pool1-t11] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:42:16,263 INFO  [AM.-pool1-t11] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:42:16,276 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:42:16,277 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:42:16,301 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:42:16,301 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:42:16,302 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566536261, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566536302, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:42:16,302 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:46:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=2721, hits=2713, hitRatio=99.71%, , cachingAccesses=2717, cachingHits=2709, cachingHitsRatio=99.71%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:47:16,180 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:47:16,181 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566536302, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566836181, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:47:16,181 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:47:16,186 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:47:16,192 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:47:16,193 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:47:16,193 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:47:16,194 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407566836181, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566836193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:47:16,194 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:47:16,198 INFO  [AM.-pool1-t12] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:47:16,198 INFO  [AM.-pool1-t12] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407566836193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566836198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:47:16,198 INFO  [AM.-pool1-t12] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:47:16,202 INFO  [AM.-pool1-t12] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:47:16,213 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:47:16,214 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:47:16,239 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:47:16,240 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:47:16,240 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407566836198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566836240, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:47:16,240 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:48:48,803 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sceplus-vm49.almaden.ibm.com,16020,1407565289717]
2014-08-08 23:48:48,803 WARN  [main-EventThread] zookeeper.RegionServerTracker: sceplus-vm49.almaden.ibm.com,16020,1407565289717 is not online or isn't known to the master.The latter could be caused by a DNS misconfiguration.
2014-08-08 23:48:48,806 INFO  [main-EventThread] replication.ReplicationTrackerZKImpl: /hbase/rs/sceplus-vm49.almaden.ibm.com,16020,1407565289717 znode expired, triggering replicatorRemoved event
2014-08-08 23:48:52,019 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm49.almaden.ibm.com,16020,1407565289717's hlogs to my queue
2014-08-08 23:48:52,023 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm49.almaden.ibm.com,16020,1407565289717/lock
2014-08-08 23:49:52,702 ERROR [defaultRpcServer.handler=23,queue=3,port=16020] master.MasterRpcServices: Region server slave1,16020,1407565289717 reported a fatal error:
ABORTING region server slave1,16020,1407565289717: regionserver:16020-0x147b9605d6e000f, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:16020-0x147b9605d6e000f received expired from ZooKeeper, aborting
Cause:
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:409)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:320)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)

2014-08-08 23:51:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=3395, hits=3387, hitRatio=99.76%, , cachingAccesses=3391, cachingHits=3383, cachingHitsRatio=99.76%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:52:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:52:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407566836240, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567136174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:52:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:52:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:52:16,186 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:52:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:52:16,187 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:52:16,188 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567136174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567136188, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:52:16,188 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:52:16,193 INFO  [AM.-pool1-t13] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:52:16,193 INFO  [AM.-pool1-t13] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567136188, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567136193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:52:16,193 INFO  [AM.-pool1-t13] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:52:16,197 INFO  [AM.-pool1-t13] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:52:16,209 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:52:16,210 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:52:16,235 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:52:16,235 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:52:16,236 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567136193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567136236, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:52:16,236 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:56:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=4302, hits=4294, hitRatio=99.81%, , cachingAccesses=4298, cachingHits=4290, cachingHitsRatio=99.81%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-08 23:57:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:57:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567136236, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567436174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:57:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-08 23:57:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:57:16,185 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-08 23:57:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:57:16,186 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-08 23:57:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567436174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567436186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:57:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-08 23:57:16,191 INFO  [AM.-pool1-t14] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-08 23:57:16,191 INFO  [AM.-pool1-t14] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567436186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567436191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:57:16,191 INFO  [AM.-pool1-t14] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-08 23:57:16,196 INFO  [AM.-pool1-t14] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:57:16,205 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-08 23:57:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-08 23:57:16,229 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-08 23:57:16,229 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-08 23:57:16,230 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567436191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567436230, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-08 23:57:16,230 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:01:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=5489, hits=5481, hitRatio=99.85%, , cachingAccesses=5485, cachingHits=5477, cachingHitsRatio=99.85%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:02:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:02:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567436230, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567736174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:02:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:02:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:02:16,187 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:02:16,188 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:02:16,189 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:02:16,189 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407567736174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567736189, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:02:16,189 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:02:16,194 INFO  [AM.-pool1-t15] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:02:16,194 INFO  [AM.-pool1-t15] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407567736189, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567736194, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:02:16,194 INFO  [AM.-pool1-t15] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:02:16,197 INFO  [AM.-pool1-t15] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:02:16,209 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:02:16,210 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:02:16,251 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:02:16,251 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:02:16,252 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407567736194, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567736252, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:02:16,252 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:06:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.01 MB, freeSize=4.75 GB, max=4.76 GB, accesses=6432, hits=6424, hitRatio=99.88%, , cachingAccesses=6428, cachingHits=6420, cachingHitsRatio=99.88%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:07:11,425 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407564430942 with entries=0, filesize=17 B; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407568031363
2014-08-09 00:07:15,734 INFO  [RS_OPEN_META-sceplus-vm48:16020-0-MetaLogRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407564435624.meta with entries=147, filesize=38.01 KB; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407568035683.meta
2014-08-09 00:07:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:07:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407567736252, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568036173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:07:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:07:16,177 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:07:16,181 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:07:16,181 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:07:16,181 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:07:16,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568036173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568036182, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:07:16,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:07:16,184 INFO  [AM.-pool1-t16] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:07:16,184 INFO  [AM.-pool1-t16] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568036182, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568036184, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:07:16,184 INFO  [AM.-pool1-t16] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:07:16,186 INFO  [AM.-pool1-t16] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:07:16,194 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:07:16,195 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:07:16,214 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:07:16,214 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:07:16,215 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568036184, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568036215, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:07:16,215 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:07:21,356 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher] regionserver.HRegionServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher requesting flush for region hbase:meta,,1.1588230740 after a delay of 11451
2014-08-09 00:07:31,357 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher] regionserver.HRegionServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher requesting flush for region hbase:meta,,1.1588230740 after a delay of 5121
2014-08-09 00:07:32,872 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3718, memsize=71.3 K, hasBloomFilter=false, into tmp file hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp/b8264adb120a4cada4a8e15313633178
2014-08-09 00:07:32,886 INFO  [MemStoreFlusher.1] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for b8264adb120a4cada4a8e15313633178
2014-08-09 00:07:32,904 INFO  [MemStoreFlusher.1] regionserver.StoreFile$Reader: Loaded Delete Family Bloom (CompoundBloomFilter) metadata for b8264adb120a4cada4a8e15313633178
2014-08-09 00:07:32,904 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/b8264adb120a4cada4a8e15313633178, entries=191, sequenceid=3718, filesize=26.4 K
2014-08-09 00:07:32,907 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~71.27 KB/72976, currentsize=0 B/0 for region hbase:meta,,1.1588230740 in 91ms, sequenceid=3718, compaction requested=false
2014-08-09 00:11:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=9089, hits=9078, hitRatio=99.88%, , cachingAccesses=9085, cachingHits=9074, cachingHitsRatio=99.88%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:12:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:12:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568036215, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568336174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:12:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:12:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:12:16,190 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:12:16,190 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:12:16,191 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:12:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568336174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568336191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:12:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:12:16,196 INFO  [AM.-pool1-t17] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:12:16,196 INFO  [AM.-pool1-t17] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568336191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568336196, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:12:16,196 INFO  [AM.-pool1-t17] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:12:16,200 INFO  [AM.-pool1-t17] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:12:16,212 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:12:16,213 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:12:16,242 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:12:16,243 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:12:16,243 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568336196, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568336243, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:12:16,244 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:09,225 INFO  [defaultRpcServer.handler=47,queue=2,port=16020] master.ServerManager: Registering server=slave1,16020,1407568505847
2014-08-09 00:15:09,225 INFO  [defaultRpcServer.handler=47,queue=2,port=16020] master.ServerManager: Triggering server recovery; existingServer slave1,16020,1407565289717 looks stale, new server:slave1,16020,1407568505847
2014-08-09 00:15:09,228 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407565289717 before assignment.
2014-08-09 00:15:09,228 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-09 00:15:09,345 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407565293908, server=slave1,16020,1407565289717} to {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407568509345, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,345 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OFFLINE
2014-08-09 00:15:09,350 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407565293865, server=slave1,16020,1407565289717} to {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407568509350, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,350 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OFFLINE
2014-08-09 00:15:09,353 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407565293932, server=slave1,16020,1407565289717} to {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407568509353, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,353 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OFFLINE
2014-08-09 00:15:09,355 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407565293972, server=slave1,16020,1407565289717} to {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407568509355, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,356 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OFFLINE
2014-08-09 00:15:09,358 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407565293964, server=slave1,16020,1407565289717} to {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407568509358, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,358 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OFFLINE
2014-08-09 00:15:09,360 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407565293997, server=slave1,16020,1407565289717} to {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407568509360, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,360 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OFFLINE
2014-08-09 00:15:09,362 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407565294052, server=slave1,16020,1407565289717} to {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407568509362, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,363 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OFFLINE
2014-08-09 00:15:09,365 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407565294083, server=slave1,16020,1407565289717} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407568509365, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,365 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OFFLINE
2014-08-09 00:15:09,367 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407565294110, server=slave1,16020,1407565289717} to {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407568509367, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,367 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OFFLINE
2014-08-09 00:15:09,369 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407565294134, server=slave1,16020,1407565289717} to {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407568509369, server=slave1,16020,1407565289717}
2014-08-09 00:15:09,369 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OFFLINE
2014-08-09 00:15:09,372 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] handler.ServerShutdownHandler: Reassigning 10 region(s) that slave1,16020,1407565289717 was carrying (and 0 regions(s) that were opening on this server)
2014-08-09 00:15:09,598 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407568505847
2014-08-09 00:15:09,599 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407568509345, server=slave1,16020,1407565289717} to {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407568509599, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,599 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,601 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407568509350, server=slave1,16020,1407565289717} to {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407568509601, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,601 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,603 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407568509353, server=slave1,16020,1407565289717} to {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407568509603, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,603 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,605 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407568509355, server=slave1,16020,1407565289717} to {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407568509605, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,605 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,607 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407568509358, server=slave1,16020,1407565289717} to {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407568509607, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,607 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,608 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407568509360, server=slave1,16020,1407565289717} to {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407568509608, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,609 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,610 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407568509362, server=slave1,16020,1407565289717} to {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407568509610, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,610 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,612 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407568509365, server=slave1,16020,1407565289717} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407568509612, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,612 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,614 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407568509367, server=slave1,16020,1407565289717} to {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407568509614, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,614 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:09,615 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407568509369, server=slave1,16020,1407565289717} to {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407568509615, server=slave1,16020,1407568505847}
2014-08-09 00:15:09,615 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=PENDING_OPEN&sn=slave1,16020,1407568505847
2014-08-09 00:15:10,191 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 4e94700075ca0745a008d12f3ba7a525 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:10,436 INFO  [defaultRpcServer.handler=44,queue=4,port=16020] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407568509601, server=slave1,16020,1407568505847} to {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407568510436, server=slave1,16020,1407568505847}
2014-08-09 00:15:10,436 INFO  [defaultRpcServer.handler=44,queue=4,port=16020] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407568505847
2014-08-09 00:15:10,441 INFO  [defaultRpcServer.handler=44,queue=4,port=16020] master.RegionStates: Onlined f65c0619898a2814f37e9033f4c8d361 on slave1,16020,1407568505847
2014-08-09 00:15:10,893 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407568509605, server=slave1,16020,1407568505847} to {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407568510893, server=slave1,16020,1407568505847}
2014-08-09 00:15:10,893 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OPEN&openSeqNum=80010464&server=slave1,16020,1407568505847
2014-08-09 00:15:10,896 INFO  [defaultRpcServer.handler=2,queue=2,port=16020] master.RegionStates: Onlined 14035e464a191fb59338e10668a4ffbe on slave1,16020,1407568505847
2014-08-09 00:15:10,898 INFO  [defaultRpcServer.handler=28,queue=3,port=16020] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407568509603, server=slave1,16020,1407568505847} to {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407568510898, server=slave1,16020,1407568505847}
2014-08-09 00:15:10,898 INFO  [defaultRpcServer.handler=28,queue=3,port=16020] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OPEN&openSeqNum=80009863&server=slave1,16020,1407568505847
2014-08-09 00:15:10,900 INFO  [defaultRpcServer.handler=28,queue=3,port=16020] master.RegionStates: Onlined 9c1292974692f9c4f73114174c03102b on slave1,16020,1407568505847
2014-08-09 00:15:10,962 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407568509599, server=slave1,16020,1407568505847} to {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407568510962, server=slave1,16020,1407568505847}
2014-08-09 00:15:10,963 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OPEN&openSeqNum=80010144&server=slave1,16020,1407568505847
2014-08-09 00:15:10,965 INFO  [defaultRpcServer.handler=7,queue=2,port=16020] master.RegionStates: Onlined 4e94700075ca0745a008d12f3ba7a525 on slave1,16020,1407568505847
2014-08-09 00:15:10,965 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 543e4d92ab5371cb8d68689c118e60f9 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,207 INFO  [defaultRpcServer.handler=49,queue=4,port=16020] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407568509607, server=slave1,16020,1407568505847} to {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407568511207, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,207 INFO  [defaultRpcServer.handler=49,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OPEN&openSeqNum=80009613&server=slave1,16020,1407568505847
2014-08-09 00:15:11,210 INFO  [defaultRpcServer.handler=49,queue=4,port=16020] master.RegionStates: Onlined 543e4d92ab5371cb8d68689c118e60f9 on slave1,16020,1407568505847
2014-08-09 00:15:11,210 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 6e260e9bb95ca1cadb358a9041447ccd to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,278 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407568509608, server=slave1,16020,1407568505847} to {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407568511278, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,278 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OPEN&openSeqNum=80010144&server=slave1,16020,1407568505847
2014-08-09 00:15:11,280 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Onlined 6e260e9bb95ca1cadb358a9041447ccd on slave1,16020,1407568505847
2014-08-09 00:15:11,280 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 4369dfa620a1c88558f70b80359f6710 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,416 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407568509610, server=slave1,16020,1407568505847} to {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407568511416, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,416 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OPEN&openSeqNum=80010144&server=slave1,16020,1407568505847
2014-08-09 00:15:11,418 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.RegionStates: Onlined 4369dfa620a1c88558f70b80359f6710 on slave1,16020,1407568505847
2014-08-09 00:15:11,418 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for e93cd3088d438d7ce5cbf7674dc6e8b5 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,494 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407568509612, server=slave1,16020,1407568505847} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407568511494, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,494 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OPEN&openSeqNum=80009885&server=slave1,16020,1407568505847
2014-08-09 00:15:11,496 INFO  [defaultRpcServer.handler=12,queue=2,port=16020] master.RegionStates: Onlined e93cd3088d438d7ce5cbf7674dc6e8b5 on slave1,16020,1407568505847
2014-08-09 00:15:11,496 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 84ff456bfbe98c0f165826a0f12b688f to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,591 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407568509614, server=slave1,16020,1407568505847} to {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407568511591, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,591 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OPEN&openSeqNum=80009717&server=slave1,16020,1407568505847
2014-08-09 00:15:11,593 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStates: Onlined 84ff456bfbe98c0f165826a0f12b688f on slave1,16020,1407568505847
2014-08-09 00:15:11,593 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-2] master.AssignmentManager: Waiting for 71ff2fc858512aa6069c415dc539a5d1 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 00:15:11,746 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407568509615, server=slave1,16020,1407568505847} to {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407568511746, server=slave1,16020,1407568505847}
2014-08-09 00:15:11,746 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OPEN&openSeqNum=80009860&server=slave1,16020,1407568505847
2014-08-09 00:15:11,748 INFO  [defaultRpcServer.handler=4,queue=4,port=16020] master.RegionStates: Onlined 71ff2fc858512aa6069c415dc539a5d1 on slave1,16020,1407568505847
2014-08-09 00:15:11,754 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-6] master.SplitLogManager: dead splitlog workers [slave1,16020,1407565289717]
2014-08-09 00:15:11,763 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-6] master.SplitLogManager: started splitting 47 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting]
2014-08-09 00:15:11,805 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307
2014-08-09 00:15:11,807 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:11,814 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566606885 acquired by slave1,16020,1407568505847
2014-08-09 00:15:11,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566568307, length=127782017
2014-08-09 00:15:11,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:11,838 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566568307
2014-08-09 00:15:11,839 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566568307 after 1ms
2014-08-09 00:15:12,342 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 47 unassigned = 45 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566606885=last_update = 1407568511869 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307=last_update = 1407568511839 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566721593=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566595972=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 0 error = 0}
2014-08-09 00:15:12,477 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998
2014-08-09 00:15:12,478 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:12,503 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566517998, length=128614354
2014-08-09 00:15:12,503 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:12,508 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566517998
2014-08-09 00:15:12,509 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566517998 after 1ms
2014-08-09 00:15:12,770 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566595972 acquired by slave1,16020,1407568505847
2014-08-09 00:15:13,230 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:13,230 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566568307 is corrupted = false progress failed = false
2014-08-09 00:15:13,236 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:13,236 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307 in 1430ms
2014-08-09 00:15:13,239 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:13,248 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566568307 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566568307
2014-08-09 00:15:13,249 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566568307
2014-08-09 00:15:13,282 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336
2014-08-09 00:15:13,283 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:13,308 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566840336, length=131238821
2014-08-09 00:15:13,308 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:13,323 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566840336
2014-08-09 00:15:13,325 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566840336 after 2ms
2014-08-09 00:15:13,373 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x2f22a63d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-09 00:15:13,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x2f22a63d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-09 00:15:13,374 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-09 00:15:13,375 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-09 00:15:13,397 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e0017, negotiated timeout = 90000
2014-08-09 00:15:14,142 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:14,143 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566517998 is corrupted = false progress failed = false
2014-08-09 00:15:14,146 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:14,146 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998 in 1668ms
2014-08-09 00:15:14,148 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:14,158 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566517998 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566517998
2014-08-09 00:15:14,159 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566517998
2014-08-09 00:15:14,209 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457
2014-08-09 00:15:14,210 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:14,248 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566624457, length=127578226
2014-08-09 00:15:14,248 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:14,254 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566624457
2014-08-09 00:15:14,256 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566624457 after 2ms
2014-08-09 00:15:15,276 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566606885 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:15,287 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566606885 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566606885
2014-08-09 00:15:15,288 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566606885
2014-08-09 00:15:15,302 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:15,303 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566624457 is corrupted = false progress failed = false
2014-08-09 00:15:15,308 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:15,308 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457 in 1098ms
2014-08-09 00:15:15,309 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:15,319 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566624457 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566624457
2014-08-09 00:15:15,320 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566624457
2014-08-09 00:15:15,331 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648 acquired by slave1,16020,1407568505847
2014-08-09 00:15:15,333 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396
2014-08-09 00:15:15,334 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:15,358 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566686396, length=139760422
2014-08-09 00:15:15,358 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:15,363 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566686396
2014-08-09 00:15:15,365 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566686396 after 1ms
2014-08-09 00:15:15,650 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566595972 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:15,657 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566595972 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566595972
2014-08-09 00:15:15,658 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566595972
2014-08-09 00:15:15,987 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566721593 acquired by slave1,16020,1407568505847
2014-08-09 00:15:17,363 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 42 unassigned = 38 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336=last_update = 1407568513324 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566721593=last_update = 1407568516131 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648=last_update = 1407568515374 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396=last_update = 1407568515364 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 5 error = 0}
2014-08-09 00:15:18,859 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:18,860 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 12 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566686396 is corrupted = false progress failed = false
2014-08-09 00:15:18,865 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:18,865 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396 in 3532ms
2014-08-09 00:15:18,867 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:18,877 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566686396 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566686396
2014-08-09 00:15:18,879 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566686396
2014-08-09 00:15:18,889 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340
2014-08-09 00:15:18,890 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:18,925 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566536340, length=129394218
2014-08-09 00:15:18,926 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:18,934 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566536340
2014-08-09 00:15:18,936 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566536340 after 2ms
2014-08-09 00:15:21,263 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:21,264 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566536340 is corrupted = false progress failed = false
2014-08-09 00:15:21,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:21,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340 in 2378ms
2014-08-09 00:15:21,269 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:21,277 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566536340 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566536340
2014-08-09 00:15:21,278 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566536340
2014-08-09 00:15:21,288 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136
2014-08-09 00:15:21,289 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:21,317 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566804136, length=131292255
2014-08-09 00:15:21,317 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:21,321 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566804136
2014-08-09 00:15:21,322 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566804136 after 1ms
2014-08-09 00:15:21,844 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:21,845 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 152 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566840336 is corrupted = false progress failed = false
2014-08-09 00:15:21,888 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:21,888 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336 in 8605ms
2014-08-09 00:15:21,890 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:21,897 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566840336 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566840336
2014-08-09 00:15:21,898 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566840336
2014-08-09 00:15:22,125 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566721593 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:22,134 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566721593 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566721593
2014-08-09 00:15:22,136 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566721593
2014-08-09 00:15:22,148 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869 acquired by slave1,16020,1407568505847
2014-08-09 00:15:22,276 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575
2014-08-09 00:15:22,278 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:22,302 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566843575, length=136409783
2014-08-09 00:15:22,302 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:22,310 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566843575
2014-08-09 00:15:22,312 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566843575 after 2ms
2014-08-09 00:15:22,365 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 38 unassigned = 34 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869=last_update = 1407568522265 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575=last_update = 1407568522311 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136=last_update = 1407568521322 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648=last_update = 1407568515374 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 9 error = 0}
2014-08-09 00:15:22,548 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:22,556 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566815648 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566815648
2014-08-09 00:15:22,557 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566815648
2014-08-09 00:15:22,737 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208 acquired by slave1,16020,1407568505847
2014-08-09 00:15:25,727 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:25,737 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566612869 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566612869
2014-08-09 00:15:25,738 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566612869
2014-08-09 00:15:25,795 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477 acquired by slave1,16020,1407568505847
2014-08-09 00:15:27,467 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:27,477 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566523477 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566523477
2014-08-09 00:15:27,478 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566523477
2014-08-09 00:15:27,506 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940 acquired by slave1,16020,1407568505847
2014-08-09 00:15:28,366 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 35 unassigned = 31 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940=last_update = 1407568527548 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208=last_update = 1407568522760 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575=last_update = 1407568522311 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136=last_update = 1407568521322 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 12 error = 0}
2014-08-09 00:15:29,516 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:29,527 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566717208 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566717208
2014-08-09 00:15:29,528 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566717208
2014-08-09 00:15:29,582 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119 acquired by slave1,16020,1407568505847
2014-08-09 00:15:30,597 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:30,606 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566680119 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566680119
2014-08-09 00:15:30,607 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566680119
2014-08-09 00:15:30,622 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130 acquired by slave1,16020,1407568505847
2014-08-09 00:15:30,764 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:30,765 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 123 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566804136 is corrupted = false progress failed = false
2014-08-09 00:15:30,770 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:30,771 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136 in 9483ms
2014-08-09 00:15:30,771 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:30,778 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566804136 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566804136
2014-08-09 00:15:30,779 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566804136
2014-08-09 00:15:30,791 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591
2014-08-09 00:15:30,791 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:30,819 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566807591, length=134006799
2014-08-09 00:15:30,819 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:30,823 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566807591
2014-08-09 00:15:30,824 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566807591 after 1ms
2014-08-09 00:15:31,128 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:31,136 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566713940 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566713940
2014-08-09 00:15:31,137 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566713940
2014-08-09 00:15:31,447 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599 acquired by slave1,16020,1407568505847
2014-08-09 00:15:32,356 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:32,357 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 160 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566843575 is corrupted = false progress failed = false
2014-08-09 00:15:32,367 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:32,367 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575 in 10090ms
2014-08-09 00:15:32,368 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:32,380 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566843575 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566843575
2014-08-09 00:15:32,382 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566843575
2014-08-09 00:15:32,390 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951
2014-08-09 00:15:32,391 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:32,414 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566756951, length=132545350
2014-08-09 00:15:32,415 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:32,420 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566756951
2014-08-09 00:15:32,422 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566756951 after 2ms
2014-08-09 00:15:34,366 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 30 unassigned = 26 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = 1407568530662 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = 1407568532421 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = 1407568531488 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = 1407568530824 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0}
2014-08-09 00:15:39,368 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 30 unassigned = 26 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130=last_update = 1407568530662 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951=last_update = 1407568532421 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599=last_update = 1407568531488 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591=last_update = 1407568530824 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 17 error = 0}
2014-08-09 00:15:39,647 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:39,648 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 125 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566807591 is corrupted = false progress failed = false
2014-08-09 00:15:39,652 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:39,652 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591 in 8861ms
2014-08-09 00:15:39,653 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:39,664 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566807591 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566807591
2014-08-09 00:15:39,666 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566807591
2014-08-09 00:15:39,674 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524
2014-08-09 00:15:39,675 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:39,701 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566601524, length=128136638
2014-08-09 00:15:39,701 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:39,710 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566601524
2014-08-09 00:15:39,711 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566601524 after 1ms
2014-08-09 00:15:39,789 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:39,796 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566775130 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566775130
2014-08-09 00:15:39,797 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566775130
2014-08-09 00:15:39,813 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367 acquired by slave1,16020,1407568505847
2014-08-09 00:15:40,087 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:40,098 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566763599 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566763599
2014-08-09 00:15:40,099 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566763599
2014-08-09 00:15:40,393 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591 acquired by slave1,16020,1407568505847
2014-08-09 00:15:40,979 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:40,980 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566601524 is corrupted = false progress failed = false
2014-08-09 00:15:40,985 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:40,985 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524 in 1311ms
2014-08-09 00:15:40,987 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:40,997 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566601524 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566601524
2014-08-09 00:15:40,999 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566601524
2014-08-09 00:15:41,018 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978
2014-08-09 00:15:41,019 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:41,047 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566668978, length=130938987
2014-08-09 00:15:41,047 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:41,052 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566668978
2014-08-09 00:15:41,054 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566668978 after 2ms
2014-08-09 00:15:41,233 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:41,239 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x147b9605d6e0017
2014-08-09 00:15:41,248 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x147b9605d6e0017 closed
2014-08-09 00:15:41,248 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-09 00:15:41,348 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 75 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566756951 is corrupted = false progress failed = false
2014-08-09 00:15:41,352 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:41,352 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951 in 8962ms
2014-08-09 00:15:41,352 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:41,361 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566756951 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566756951
2014-08-09 00:15:41,362 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566756951
2014-08-09 00:15:41,560 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:41,567 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566584367 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566584367
2014-08-09 00:15:41,568 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566584367
2014-08-09 00:15:41,577 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351 acquired by slave1,16020,1407568505847
2014-08-09 00:15:41,712 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319
2014-08-09 00:15:41,713 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:41,751 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566639319, length=132538233
2014-08-09 00:15:41,751 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:41,757 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566639319
2014-08-09 00:15:41,758 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566639319 after 1ms
2014-08-09 00:15:42,243 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:42,253 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566618591 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566618591
2014-08-09 00:15:42,254 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566618591
2014-08-09 00:15:42,265 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237 acquired by slave1,16020,1407568505847
2014-08-09 00:15:42,544 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:42,544 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566668978 is corrupted = false progress failed = false
2014-08-09 00:15:42,550 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:42,550 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978 in 1532ms
2014-08-09 00:15:42,552 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:42,564 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566668978 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566668978
2014-08-09 00:15:42,566 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566668978
2014-08-09 00:15:42,647 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065
2014-08-09 00:15:42,648 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:42,676 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566672065, length=128861684
2014-08-09 00:15:42,676 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:42,679 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566672065
2014-08-09 00:15:42,680 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566672065 after 1ms
2014-08-09 00:15:43,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:43,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566639319 is corrupted = false progress failed = false
2014-08-09 00:15:44,206 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:44,207 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319 in 2495ms
2014-08-09 00:15:44,208 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:44,219 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566639319 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566639319
2014-08-09 00:15:44,220 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319
2014-08-09 00:15:44,369 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 22 unassigned = 18 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351=last_update = 1407568541613 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566639319=last_update = 1407568544207 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = success incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065=last_update = 1407568542680 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237=last_update = 1407568542337 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 26 error = 0}
2014-08-09 00:15:44,430 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:44,430 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566672065 is corrupted = false progress failed = false
2014-08-09 00:15:44,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:44,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065 in 2002ms
2014-08-09 00:15:44,650 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:44,661 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566672065 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566672065
2014-08-09 00:15:44,662 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566672065
2014-08-09 00:15:44,806 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762
2014-08-09 00:15:44,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566573762, length=128080268
2014-08-09 00:15:44,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:44,976 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:45,128 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566573762
2014-08-09 00:15:45,129 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566573762 after 1ms
2014-08-09 00:15:45,670 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631
2014-08-09 00:15:45,670 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:45,695 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566693631, length=131483206
2014-08-09 00:15:45,695 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:45,700 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566693631
2014-08-09 00:15:45,702 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566693631 after 2ms
2014-08-09 00:15:45,782 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x54a48245, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-09 00:15:45,783 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x54a48245 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-09 00:15:45,784 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-09 00:15:45,785 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-08-09 00:15:45,792 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x147b9605d6e001a, negotiated timeout = 90000
2014-08-09 00:15:46,066 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:46,082 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566590237 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566590237
2014-08-09 00:15:46,083 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566590237
2014-08-09 00:15:46,094 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840 acquired by slave1,16020,1407568505847
2014-08-09 00:15:46,131 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:46,139 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566724351 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566724351
2014-08-09 00:15:46,140 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566724351
2014-08-09 00:15:46,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:46,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566573762 is corrupted = false progress failed = false
2014-08-09 00:15:46,625 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:46,625 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762 in 1819ms
2014-08-09 00:15:46,626 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:46,636 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566573762 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566573762
2014-08-09 00:15:46,637 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566573762
2014-08-09 00:15:46,648 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638
2014-08-09 00:15:46,649 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:46,673 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566759638, length=134190536
2014-08-09 00:15:46,674 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:46,679 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566759638
2014-08-09 00:15:46,680 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566759638 after 1ms
2014-08-09 00:15:46,796 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383 acquired by slave1,16020,1407568505847
2014-08-09 00:15:48,272 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:48,282 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566991840 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566991840
2014-08-09 00:15:48,284 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566991840
2014-08-09 00:15:48,293 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100 acquired by slave1,16020,1407568505847
2014-08-09 00:15:48,349 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:48,349 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566693631 is corrupted = false progress failed = false
2014-08-09 00:15:48,358 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:48,358 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631 in 2688ms
2014-08-09 00:15:48,358 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:48,368 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566693631 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566693631
2014-08-09 00:15:48,369 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566693631
2014-08-09 00:15:48,410 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847
2014-08-09 00:15:48,411 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:48,438 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566665847, length=130432844
2014-08-09 00:15:48,438 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:48,442 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566665847
2014-08-09 00:15:48,444 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566665847 after 2ms
2014-08-09 00:15:49,370 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 15 unassigned = 11 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383=last_update = 1407568546877 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638=last_update = 1407568546680 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100=last_update = 1407568548379 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847=last_update = 1407568548443 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 32 error = 0}
2014-08-09 00:15:49,771 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:49,772 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566665847 is corrupted = false progress failed = false
2014-08-09 00:15:49,776 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:49,776 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847 in 1365ms
2014-08-09 00:15:49,778 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:49,789 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566665847 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566665847
2014-08-09 00:15:49,790 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566665847
2014-08-09 00:15:49,805 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798
2014-08-09 00:15:49,806 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:49,838 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566683798, length=130515420
2014-08-09 00:15:49,838 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:49,844 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566683798
2014-08-09 00:15:49,845 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566683798 after 1ms
2014-08-09 00:15:50,357 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:50,367 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566579100 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566579100
2014-08-09 00:15:50,369 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566579100
2014-08-09 00:15:50,377 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455 acquired by slave1,16020,1407568505847
2014-08-09 00:15:51,147 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:51,148 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566683798 is corrupted = false progress failed = false
2014-08-09 00:15:51,153 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:51,153 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798 in 1348ms
2014-08-09 00:15:51,154 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:51,162 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566683798 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566683798
2014-08-09 00:15:51,163 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566683798
2014-08-09 00:15:51,192 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055
2014-08-09 00:15:51,194 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:51,217 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566690055, length=130739404
2014-08-09 00:15:51,217 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:51,219 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566690055
2014-08-09 00:15:51,221 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566690055 after 1ms
2014-08-09 00:15:52,115 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:52,123 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566630455 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566630455
2014-08-09 00:15:52,124 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566630455
2014-08-09 00:15:52,136 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963 acquired by slave1,16020,1407568505847
2014-08-09 00:15:53,709 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:53,719 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566562963 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566562963
2014-08-09 00:15:53,721 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566562963
2014-08-09 00:15:53,739 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241 acquired by slave1,16020,1407568505847
2014-08-09 00:15:54,662 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:54,662 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 18 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566690055 is corrupted = false progress failed = false
2014-08-09 00:15:54,665 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:54,666 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055 in 3474ms
2014-08-09 00:15:54,666 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:54,677 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566690055 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566690055
2014-08-09 00:15:54,678 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566690055
2014-08-09 00:15:54,686 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539
2014-08-09 00:15:54,686 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:54,715 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566529539, length=128337745
2014-08-09 00:15:54,716 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:54,718 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566529539
2014-08-09 00:15:54,719 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566529539 after 0ms
2014-08-09 00:15:55,013 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:55,014 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x147b9605d6e001a
2014-08-09 00:15:55,024 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x147b9605d6e001a closed
2014-08-09 00:15:55,024 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-09 00:15:55,124 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 90 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566759638 is corrupted = false progress failed = false
2014-08-09 00:15:55,128 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:55,129 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638 in 8480ms
2014-08-09 00:15:55,130 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:55,140 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566759638 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566759638
2014-08-09 00:15:55,141 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566759638
2014-08-09 00:15:55,141 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:55,147 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566800383 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566800383
2014-08-09 00:15:55,148 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566800383
2014-08-09 00:15:55,150 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512 acquired by slave1,16020,1407568505847
2014-08-09 00:15:55,267 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238
2014-08-09 00:15:55,269 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:55,278 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:15:55,283 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566542241 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566542241
2014-08-09 00:15:55,284 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566542241
2014-08-09 00:15:55,291 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566777238, length=130471704
2014-08-09 00:15:55,291 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:55,294 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566777238
2014-08-09 00:15:55,295 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566777238 after 1ms
2014-08-09 00:15:55,323 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x1c3e2b6f, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-09 00:15:55,324 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x1c3e2b6f connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-09 00:15:55,324 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-09 00:15:55,325 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-09 00:15:55,330 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790014, negotiated timeout = 90000
2014-08-09 00:15:55,370 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 6 unassigned = 3 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = 1407568555295 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539=last_update = 1407568554718 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = 1407568555187 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 41 error = 0}
2014-08-09 00:15:56,118 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293 acquired by slave1,16020,1407568505847
2014-08-09 00:15:56,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:15:56,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566529539 is corrupted = false progress failed = false
2014-08-09 00:15:56,448 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:56,448 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539 in 1762ms
2014-08-09 00:15:56,456 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:56,464 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566529539 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566529539
2014-08-09 00:15:56,465 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566529539
2014-08-09 00:15:56,490 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658
2014-08-09 00:15:56,494 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:15:56,515 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566727658, length=134001849
2014-08-09 00:15:56,515 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 00:15:56,520 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566727658
2014-08-09 00:15:56,522 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566727658 after 2ms
2014-08-09 00:16:01,371 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 5 unassigned = 1 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = 1407568556173 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = 1407568555295 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = 1407568556521 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = 1407568555187 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0}
2014-08-09 00:16:06,372 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 5 unassigned = 1 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = 1407568556173 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = 1407568555295 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = 1407568556521 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = 1407568555187 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0}
2014-08-09 00:16:11,374 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 5 unassigned = 1 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293=last_update = 1407568556173 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238=last_update = 1407568555295 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658=last_update = 1407568556521 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512=last_update = 1407568555187 last_version = 2 cur_worker_name = slave1,16020,1407568505847 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 47 done = 42 error = 0}
2014-08-09 00:16:12,723 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:16:12,735 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566849512 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566849512
2014-08-09 00:16:12,736 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566849512
2014-08-09 00:16:12,957 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 00:16:12,958 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 64 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566727658 is corrupted = false progress failed = false
2014-08-09 00:16:12,959 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 00:16:12,962 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47b96060790014
2014-08-09 00:16:13,134 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47b96060790014 closed
2014-08-09 00:16:13,134 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:16:13,135 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658 in 16644ms
2014-08-09 00:16:13,135 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-09 00:16:13,136 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:16:13,144 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566727658 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566727658
2014-08-09 00:16:13,145 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566727658
2014-08-09 00:16:13,235 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 104 edits across 6 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566777238 is corrupted = false progress failed = false
2014-08-09 00:16:13,257 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:16:13,257 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238 in 17989ms
2014-08-09 00:16:13,258 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963 acquired by slave1,16020,1407568505847
2014-08-09 00:16:13,258 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:16:13,268 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566777238 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566777238
2014-08-09 00:16:13,269 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566777238
2014-08-09 00:16:13,704 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:16:13,714 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566811293 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566811293
2014-08-09 00:16:13,716 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566811293
2014-08-09 00:16:14,427 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963 entered state: DONE slave1,16020,1407568505847
2014-08-09 00:16:14,437 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting/slave1%2C16020%2C1407565289717.1407566675963 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407565289717.1407566675963
2014-08-09 00:16:14,438 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407565289717-splitting%2Fslave1%252C16020%252C1407565289717.1407566675963
2014-08-09 00:16:14,504 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-6] master.SplitLogManager: finished splitting (more than or equal to) 6054548255 bytes in 47 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407565289717-splitting] in 62741ms
2014-08-09 00:16:14,504 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-6] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407565289717
2014-08-09 00:16:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11238, hits=11227, hitRatio=99.90%, , cachingAccesses=11234, cachingHits=11223, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:17:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568336243, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568636174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:17:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:17:16,185 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:17:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:17:16,186 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:17:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568636174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568636186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:17:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:17:16,191 INFO  [AM.-pool1-t18] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:17:16,191 INFO  [AM.-pool1-t18] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568636186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568636191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:17:16,192 INFO  [AM.-pool1-t18] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:17:16,194 INFO  [AM.-pool1-t18] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:17:16,209 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:17:16,210 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:17:16,231 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:17:16,232 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:17:16,233 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568636191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568636233, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:17:16,233 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:21:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11243, hits=11232, hitRatio=99.90%, , cachingAccesses=11239, cachingHits=11228, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:22:16,172 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:22:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568636233, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568936173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:22:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:22:16,189 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:22:16,191 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:22:16,192 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:22:16,192 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:22:16,192 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407568936173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568936192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:22:16,192 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:22:16,195 INFO  [AM.-pool1-t19] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:22:16,195 INFO  [AM.-pool1-t19] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407568936192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568936195, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:22:16,195 INFO  [AM.-pool1-t19] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:22:16,196 INFO  [AM.-pool1-t19] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:22:16,208 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:22:16,211 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:22:16,240 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:22:16,241 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:22:16,241 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407568936195, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568936241, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:22:16,241 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:26:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11248, hits=11237, hitRatio=99.90%, , cachingAccesses=11244, cachingHits=11233, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:27:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:27:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407568936241, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569236173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:27:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:27:16,187 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:27:16,190 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:27:16,190 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:27:16,190 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:27:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569236173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569236191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:27:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:27:16,193 INFO  [AM.-pool1-t20] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:27:16,193 INFO  [AM.-pool1-t20] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569236191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569236193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:27:16,193 INFO  [AM.-pool1-t20] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:27:16,195 INFO  [AM.-pool1-t20] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:27:16,202 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:27:16,202 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:27:16,216 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:27:16,218 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:27:16,218 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569236193, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569236218, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:27:16,218 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:31:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11253, hits=11242, hitRatio=99.90%, , cachingAccesses=11249, cachingHits=11238, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:32:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:32:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569236218, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569536173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:32:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:32:16,192 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:32:16,200 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:32:16,201 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:32:16,201 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:32:16,201 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569536173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569536201, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:32:16,201 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:32:16,204 INFO  [AM.-pool1-t21] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:32:16,204 INFO  [AM.-pool1-t21] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569536201, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569536204, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:32:16,204 INFO  [AM.-pool1-t21] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:32:16,206 INFO  [AM.-pool1-t21] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:32:16,215 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:32:16,216 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:32:16,238 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:32:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:32:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569536204, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569536239, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:32:16,239 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:36:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11258, hits=11247, hitRatio=99.90%, , cachingAccesses=11254, cachingHits=11243, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569536239, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569836174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:37:16,192 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:37:16,197 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:37:16,197 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:37:16,197 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:37:16,198 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407569836174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569836198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:37:16,198 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:37:16,200 INFO  [AM.-pool1-t22] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:37:16,200 INFO  [AM.-pool1-t22] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407569836198, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569836200, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:37:16,200 INFO  [AM.-pool1-t22] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:37:16,201 INFO  [AM.-pool1-t22] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:37:16,209 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:37:16,210 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:37:16,230 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:37:16,231 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:37:16,231 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407569836200, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569836231, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:37:16,232 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:41:15,962 INFO  [main-EventThread] zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sceplus-vm49.almaden.ibm.com,16020,1407568505847]
2014-08-09 00:41:15,962 WARN  [main-EventThread] zookeeper.RegionServerTracker: sceplus-vm49.almaden.ibm.com,16020,1407568505847 is not online or isn't known to the master.The latter could be caused by a DNS misconfiguration.
2014-08-09 00:41:15,965 INFO  [main-EventThread] replication.ReplicationTrackerZKImpl: /hbase/rs/sceplus-vm49.almaden.ibm.com,16020,1407568505847 znode expired, triggering replicatorRemoved event
2014-08-09 00:41:19,719 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm49.almaden.ibm.com,16020,1407568505847's hlogs to my queue
2014-08-09 00:41:19,729 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm49.almaden.ibm.com,16020,1407568505847/lock
2014-08-09 00:41:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=11358, hits=11347, hitRatio=99.90%, , cachingAccesses=11354, cachingHits=11343, cachingHitsRatio=99.90%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:42:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:42:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407569836231, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570136173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:42:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:42:16,193 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:42:16,198 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:42:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:42:16,199 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:42:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570136173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570136199, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:42:16,199 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:42:16,203 INFO  [AM.-pool1-t23] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:42:16,203 INFO  [AM.-pool1-t23] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570136199, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570136203, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:42:16,203 INFO  [AM.-pool1-t23] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:42:16,206 INFO  [AM.-pool1-t23] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:42:16,217 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:42:16,218 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:42:16,241 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:42:16,242 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:42:16,243 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570136203, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570136242, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:42:16,243 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:43:21,820 ERROR [defaultRpcServer.handler=17,queue=2,port=16020] master.MasterRpcServices: Region server slave1,16020,1407568505847 reported a fatal error:
ABORTING region server slave1,16020,1407568505847: regionserver:16020-0x147b9605d6e0016, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:16020-0x147b9605d6e0016 received expired from ZooKeeper, aborting
Cause:
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:409)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:320)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)

2014-08-09 00:46:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=13831, hits=13820, hitRatio=99.92%, , cachingAccesses=13827, cachingHits=13816, cachingHitsRatio=99.92%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:47:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:47:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570136242, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570436175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:47:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:47:16,183 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:47:16,191 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:47:16,191 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:47:16,191 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:47:16,192 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570436175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570436192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:47:16,192 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:47:16,196 INFO  [AM.-pool1-t24] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:47:16,197 INFO  [AM.-pool1-t24] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570436192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570436197, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:47:16,197 INFO  [AM.-pool1-t24] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:47:16,200 INFO  [AM.-pool1-t24] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:47:16,210 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:47:16,211 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:47:16,233 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:47:16,234 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:47:16,235 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570436197, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570436235, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:47:16,235 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:51:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=15354, hits=15343, hitRatio=99.93%, , cachingAccesses=15350, cachingHits=15339, cachingHitsRatio=99.93%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:52:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:52:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570436235, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570736174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:52:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:52:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:52:16,185 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:52:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:52:16,186 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:52:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407570736174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570736186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:52:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:52:16,190 INFO  [AM.-pool1-t25] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:52:16,191 INFO  [AM.-pool1-t25] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407570736186, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570736190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:52:16,191 INFO  [AM.-pool1-t25] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:52:16,194 INFO  [AM.-pool1-t25] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:52:16,205 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:52:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:52:16,224 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:52:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:52:16,225 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407570736190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570736225, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:52:16,225 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:56:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=17508, hits=17497, hitRatio=99.94%, , cachingAccesses=17504, cachingHits=17493, cachingHitsRatio=99.94%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 00:57:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:57:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407570736225, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571036174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:57:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 00:57:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:57:16,184 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 00:57:16,185 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:57:16,185 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 00:57:16,185 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571036174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571036185, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:57:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 00:57:16,190 INFO  [AM.-pool1-t26] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 00:57:16,190 INFO  [AM.-pool1-t26] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571036185, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571036190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:57:16,190 INFO  [AM.-pool1-t26] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 00:57:16,193 INFO  [AM.-pool1-t26] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:57:16,204 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 00:57:16,205 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 00:57:16,225 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 00:57:16,225 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 00:57:16,226 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571036190, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571036226, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 00:57:16,226 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:01:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=19374, hits=19363, hitRatio=99.94%, , cachingAccesses=19370, cachingHits=19359, cachingHitsRatio=99.94%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 01:02:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:02:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571036226, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571336173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:02:16,173 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:02:16,177 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:02:16,181 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:02:16,181 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:02:16,181 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:02:16,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571336173, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571336182, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:02:16,182 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:02:16,184 INFO  [AM.-pool1-t27] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:02:16,184 INFO  [AM.-pool1-t27] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571336182, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571336184, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:02:16,184 INFO  [AM.-pool1-t27] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:02:16,186 INFO  [AM.-pool1-t27] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:02:16,198 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:02:16,199 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:02:16,223 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:02:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:02:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571336184, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571336224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:02:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:06:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=21260, hits=21249, hitRatio=99.95%, , cachingAccesses=21256, cachingHits=21245, cachingHitsRatio=99.95%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 01:07:11,555 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407568031363 with entries=0, filesize=17 B; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407571631491
2014-08-09 01:07:15,820 INFO  [RS_OPEN_META-sceplus-vm48:16020-0-MetaLogRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407568035683.meta with entries=80, filesize=19.79 KB; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,16020,1407564407993/sceplus-vm48.almaden.ibm.com%2C16020%2C1407564407993.1407571635789.meta
2014-08-09 01:07:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:07:16,175 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571336224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571636175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:07:16,176 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:07:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:07:16,186 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:07:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:07:16,187 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:07:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571636175, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571636187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:07:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:07:16,191 INFO  [AM.-pool1-t28] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:07:16,191 INFO  [AM.-pool1-t28] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571636187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571636191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:07:16,191 INFO  [AM.-pool1-t28] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:07:16,194 INFO  [AM.-pool1-t28] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:07:16,207 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:07:16,208 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:07:16,229 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:07:16,230 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:07:16,231 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571636191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571636231, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:07:16,231 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:11:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=23519, hits=23508, hitRatio=99.95%, , cachingAccesses=23515, cachingHits=23504, cachingHitsRatio=99.95%, evictions=0, evicted=6, evictedPerRun=Infinity
2014-08-09 01:12:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:12:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571636231, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571936174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:12:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:12:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:12:16,183 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:12:16,184 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:12:16,184 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:12:16,185 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407571936174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571936185, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:12:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:12:16,191 INFO  [AM.-pool1-t29] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:12:16,191 INFO  [AM.-pool1-t29] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407571936185, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571936191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:12:16,191 INFO  [AM.-pool1-t29] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:12:16,194 INFO  [AM.-pool1-t29] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:12:16,205 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:12:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:12:16,223 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:12:16,223 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:12:16,223 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407571936191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571936223, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:12:16,223 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:12:21,409 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher] regionserver.HRegionServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher requesting flush for region hbase:meta,,1.1588230740 after a delay of 11480
2014-08-09 01:12:31,408 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher] regionserver.HRegionServer: master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020.periodicFlusher requesting flush for region hbase:meta,,1.1588230740 after a delay of 3254
2014-08-09 01:12:32,933 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3885, memsize=37.5 K, hasBloomFilter=false, into tmp file hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp/c17d2a2a003f4678b620a30a5a15f4d3
2014-08-09 01:12:32,964 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/hbase/meta/1588230740/info/c17d2a2a003f4678b620a30a5a15f4d3, entries=110, sequenceid=3885, filesize=17.0 K
2014-08-09 01:12:32,967 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~37.48 KB/38376, currentsize=0 B/0 for region hbase:meta,,1.1588230740 in 78ms, sequenceid=3885, compaction requested=true
2014-08-09 01:12:32,971 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-longCompactions-1407571952970] regionserver.HRegion: Starting compaction on info in region hbase:meta,,1.1588230740
2014-08-09 01:12:32,971 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-longCompactions-1407571952970] regionserver.HStore: Starting compaction of 3 file(s) in info of hbase:meta,,1.1588230740 into tmpdir=hdfs://master:54310/hbase/data/hbase/meta/1588230740/.tmp, totalSize=61.4 K
2014-08-09 01:12:32,975 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-longCompactions-1407571952970] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:12:33,058 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-longCompactions-1407571952970] regionserver.HStore: Completed compaction of 3 (all) file(s) in info of hbase:meta,,1.1588230740 into f798e1fc63714eae8dfb157a1c0c799d(size=33.6 K), total size for store is 33.6 K. This selection was in queue for 0sec, and took 0sec to execute.
2014-08-09 01:12:33,058 INFO  [master/sceplus-vm48.almaden.ibm.com/9.1.143.58:16020-longCompactions-1407571952970] regionserver.CompactSplitThread: Completed compaction: Request = regionName=hbase:meta,,1.1588230740, storeName=info, fileCount=3, fileSize=61.4 K, priority=19997, time=1230360334191772; duration=0sec
2014-08-09 01:13:11,863 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-HeapMemoryTunerChore] regionserver.HeapMemoryManager: Setting block cache heap size to 5108610048 and memstore heap size to 5108610048
2014-08-09 01:16:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=24730, hits=24713, hitRatio=99.93%, , cachingAccesses=24719, cachingHits=24704, cachingHitsRatio=99.94%, evictions=0, evicted=11, evictedPerRun=Infinity
2014-08-09 01:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407571936223, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572236174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:17:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:17:16,179 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:17:16,186 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:17:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:17:16,187 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:17:16,188 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572236174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572236188, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:17:16,188 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:17:16,192 INFO  [AM.-pool1-t30] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:17:16,192 INFO  [AM.-pool1-t30] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572236188, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572236192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:17:16,192 INFO  [AM.-pool1-t30] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:17:16,196 INFO  [AM.-pool1-t30] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:17:16,207 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:17:16,207 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:17:16,229 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:17:16,229 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:17:16,230 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572236192, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572236230, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:17:16,230 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:21:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=26214, hits=26197, hitRatio=99.94%, , cachingAccesses=26203, cachingHits=26188, cachingHitsRatio=99.94%, evictions=0, evicted=11, evictedPerRun=Infinity
2014-08-09 01:22:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:22:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572236230, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572536174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:22:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:22:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:22:16,186 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:22:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:22:16,186 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:22:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572536174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572536187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:22:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:22:16,191 INFO  [AM.-pool1-t31] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:22:16,191 INFO  [AM.-pool1-t31] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572536187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572536191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:22:16,192 INFO  [AM.-pool1-t31] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:22:16,197 INFO  [AM.-pool1-t31] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:22:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:22:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:22:16,223 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:22:16,223 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:22:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572536191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572536224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:22:16,224 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:26:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=27433, hits=27416, hitRatio=99.94%, , cachingAccesses=27422, cachingHits=27407, cachingHitsRatio=99.95%, evictions=0, evicted=11, evictedPerRun=Infinity
2014-08-09 01:27:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572536224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572836174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:27:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:27:16,178 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:16,185 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:27:16,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:27:16,186 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-2] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:27:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407572836174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572836187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:27:16,187 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-2] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:27:16,191 INFO  [AM.-pool1-t32] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:16,191 INFO  [AM.-pool1-t32] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407572836187, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572836191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:27:16,191 INFO  [AM.-pool1-t32] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:27:16,195 INFO  [AM.-pool1-t32] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:27:16,206 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:27:16,207 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:27:16,235 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:27:16,235 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:27:16,236 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407572836191, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572836236, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:27:16,236 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:40,159 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.ServerManager: Registering server=slave1,16020,1407572856643
2014-08-09 01:27:40,159 INFO  [defaultRpcServer.handler=35,queue=0,port=16020] master.ServerManager: Triggering server recovery; existingServer slave1,16020,1407568505847 looks stale, new server:slave1,16020,1407572856643
2014-08-09 01:27:40,173 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Splitting logs for slave1,16020,1407568505847 before assignment.
2014-08-09 01:27:40,173 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Mark regions in recovery before assignment.
2014-08-09 01:27:40,367 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407568510962, server=slave1,16020,1407568505847} to {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407572860367, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,367 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OFFLINE
2014-08-09 01:27:40,373 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407568510436, server=slave1,16020,1407568505847} to {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407572860373, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,373 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OFFLINE
2014-08-09 01:27:40,374 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407568510898, server=slave1,16020,1407568505847} to {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407572860374, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,374 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OFFLINE
2014-08-09 01:27:40,376 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407568510893, server=slave1,16020,1407568505847} to {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407572860376, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,376 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OFFLINE
2014-08-09 01:27:40,377 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407568511207, server=slave1,16020,1407568505847} to {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407572860377, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,377 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OFFLINE
2014-08-09 01:27:40,379 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407568511278, server=slave1,16020,1407568505847} to {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407572860379, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,379 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OFFLINE
2014-08-09 01:27:40,385 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407568511416, server=slave1,16020,1407568505847} to {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407572860385, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,385 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OFFLINE
2014-08-09 01:27:40,386 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407568511494, server=slave1,16020,1407568505847} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407572860386, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,386 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OFFLINE
2014-08-09 01:27:40,388 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407568511591, server=slave1,16020,1407568505847} to {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407572860388, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,388 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OFFLINE
2014-08-09 01:27:40,390 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407568511746, server=slave1,16020,1407568505847} to {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407572860390, server=slave1,16020,1407568505847}
2014-08-09 01:27:40,390 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OFFLINE
2014-08-09 01:27:40,395 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] handler.ServerShutdownHandler: Reassigning 10 region(s) that slave1,16020,1407568505847 was carrying (and 0 regions(s) that were opening on this server)
2014-08-09 01:27:40,646 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Assigning 10 region(s) to slave1,16020,1407572856643
2014-08-09 01:27:40,646 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=OFFLINE, ts=1407572860367, server=slave1,16020,1407568505847} to {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407572860646, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,646 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,649 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=OFFLINE, ts=1407572860373, server=slave1,16020,1407568505847} to {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407572860649, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,649 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,650 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=OFFLINE, ts=1407572860374, server=slave1,16020,1407568505847} to {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407572860650, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,650 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,652 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=OFFLINE, ts=1407572860376, server=slave1,16020,1407568505847} to {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407572860652, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,652 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,654 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=OFFLINE, ts=1407572860377, server=slave1,16020,1407568505847} to {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407572860653, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,654 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,655 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=OFFLINE, ts=1407572860379, server=slave1,16020,1407568505847} to {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407572860655, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,655 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,657 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=OFFLINE, ts=1407572860385, server=slave1,16020,1407568505847} to {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407572860657, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,657 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,658 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OFFLINE, ts=1407572860386, server=slave1,16020,1407568505847} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407572860658, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,658 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,659 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=OFFLINE, ts=1407572860388, server=slave1,16020,1407568505847} to {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407572860659, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,660 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:40,661 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=OFFLINE, ts=1407572860390, server=slave1,16020,1407568505847} to {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407572860661, server=slave1,16020,1407572856643}
2014-08-09 01:27:40,661 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=PENDING_OPEN&sn=slave1,16020,1407572856643
2014-08-09 01:27:41,165 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 4e94700075ca0745a008d12f3ba7a525 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 01:27:41,369 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Transition {f65c0619898a2814f37e9033f4c8d361 state=PENDING_OPEN, ts=1407572860649, server=slave1,16020,1407572856643} to {f65c0619898a2814f37e9033f4c8d361 state=OPEN, ts=1407572861369, server=slave1,16020,1407572856643}
2014-08-09 01:27:41,369 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStateStore: Updating row usertable,,1407564565236.f65c0619898a2814f37e9033f4c8d361. with state=OPEN&openSeqNum=40000002&server=slave1,16020,1407572856643
2014-08-09 01:27:41,371 INFO  [defaultRpcServer.handler=16,queue=1,port=16020] master.RegionStates: Onlined f65c0619898a2814f37e9033f4c8d361 on slave1,16020,1407572856643
2014-08-09 01:27:41,760 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStates: Transition {14035e464a191fb59338e10668a4ffbe state=PENDING_OPEN, ts=1407572860652, server=slave1,16020,1407572856643} to {14035e464a191fb59338e10668a4ffbe state=OPEN, ts=1407572861760, server=slave1,16020,1407572856643}
2014-08-09 01:27:41,760 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user5,1407564565236.14035e464a191fb59338e10668a4ffbe. with state=OPEN&openSeqNum=120016839&server=slave1,16020,1407572856643
2014-08-09 01:27:41,763 INFO  [defaultRpcServer.handler=29,queue=4,port=16020] master.RegionStates: Onlined 14035e464a191fb59338e10668a4ffbe on slave1,16020,1407572856643
2014-08-09 01:27:41,849 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStates: Transition {9c1292974692f9c4f73114174c03102b state=PENDING_OPEN, ts=1407572860650, server=slave1,16020,1407572856643} to {9c1292974692f9c4f73114174c03102b state=OPEN, ts=1407572861849, server=slave1,16020,1407572856643}
2014-08-09 01:27:41,849 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user3,1407564565236.9c1292974692f9c4f73114174c03102b. with state=OPEN&openSeqNum=120016091&server=slave1,16020,1407572856643
2014-08-09 01:27:41,851 INFO  [defaultRpcServer.handler=40,queue=0,port=16020] master.RegionStates: Onlined 9c1292974692f9c4f73114174c03102b on slave1,16020,1407572856643
2014-08-09 01:27:41,890 INFO  [defaultRpcServer.handler=21,queue=1,port=16020] master.RegionStates: Transition {4e94700075ca0745a008d12f3ba7a525 state=PENDING_OPEN, ts=1407572860646, server=slave1,16020,1407572856643} to {4e94700075ca0745a008d12f3ba7a525 state=OPEN, ts=1407572861890, server=slave1,16020,1407572856643}
2014-08-09 01:27:41,890 INFO  [defaultRpcServer.handler=21,queue=1,port=16020] master.RegionStateStore: Updating row usertable,user4,1407564565236.4e94700075ca0745a008d12f3ba7a525. with state=OPEN&openSeqNum=120017101&server=slave1,16020,1407572856643
2014-08-09 01:27:41,892 INFO  [defaultRpcServer.handler=21,queue=1,port=16020] master.RegionStates: Onlined 4e94700075ca0745a008d12f3ba7a525 on slave1,16020,1407572856643
2014-08-09 01:27:41,892 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 543e4d92ab5371cb8d68689c118e60f9 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 01:27:42,178 INFO  [defaultRpcServer.handler=45,queue=0,port=16020] master.RegionStates: Transition {543e4d92ab5371cb8d68689c118e60f9 state=PENDING_OPEN, ts=1407572860653, server=slave1,16020,1407572856643} to {543e4d92ab5371cb8d68689c118e60f9 state=OPEN, ts=1407572862178, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,179 INFO  [defaultRpcServer.handler=45,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user9,1407564565236.543e4d92ab5371cb8d68689c118e60f9. with state=OPEN&openSeqNum=120015321&server=slave1,16020,1407572856643
2014-08-09 01:27:42,180 INFO  [defaultRpcServer.handler=45,queue=0,port=16020] master.RegionStates: Onlined 543e4d92ab5371cb8d68689c118e60f9 on slave1,16020,1407572856643
2014-08-09 01:27:42,180 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 6e260e9bb95ca1cadb358a9041447ccd to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 01:27:42,359 INFO  [defaultRpcServer.handler=0,queue=0,port=16020] master.RegionStates: Transition {4369dfa620a1c88558f70b80359f6710 state=PENDING_OPEN, ts=1407572860657, server=slave1,16020,1407572856643} to {4369dfa620a1c88558f70b80359f6710 state=OPEN, ts=1407572862359, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,360 INFO  [defaultRpcServer.handler=0,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user6,1407564565236.4369dfa620a1c88558f70b80359f6710. with state=OPEN&openSeqNum=120016877&server=slave1,16020,1407572856643
2014-08-09 01:27:42,362 INFO  [defaultRpcServer.handler=0,queue=0,port=16020] master.RegionStates: Onlined 4369dfa620a1c88558f70b80359f6710 on slave1,16020,1407572856643
2014-08-09 01:27:42,432 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Transition {6e260e9bb95ca1cadb358a9041447ccd state=PENDING_OPEN, ts=1407572860655, server=slave1,16020,1407572856643} to {6e260e9bb95ca1cadb358a9041447ccd state=OPEN, ts=1407572862432, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,433 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStateStore: Updating row usertable,user8,1407564565236.6e260e9bb95ca1cadb358a9041447ccd. with state=OPEN&openSeqNum=120016091&server=slave1,16020,1407572856643
2014-08-09 01:27:42,435 INFO  [defaultRpcServer.handler=5,queue=0,port=16020] master.RegionStates: Onlined 6e260e9bb95ca1cadb358a9041447ccd on slave1,16020,1407572856643
2014-08-09 01:27:42,435 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for e93cd3088d438d7ce5cbf7674dc6e8b5 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 01:27:42,601 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStates: Transition {84ff456bfbe98c0f165826a0f12b688f state=PENDING_OPEN, ts=1407572860659, server=slave1,16020,1407572856643} to {84ff456bfbe98c0f165826a0f12b688f state=OPEN, ts=1407572862600, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,601 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user1,1407564565236.84ff456bfbe98c0f165826a0f12b688f. with state=OPEN&openSeqNum=120015441&server=slave1,16020,1407572856643
2014-08-09 01:27:42,603 INFO  [defaultRpcServer.handler=34,queue=4,port=16020] master.RegionStates: Onlined 84ff456bfbe98c0f165826a0f12b688f on slave1,16020,1407572856643
2014-08-09 01:27:42,636 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Transition {e93cd3088d438d7ce5cbf7674dc6e8b5 state=PENDING_OPEN, ts=1407572860658, server=slave1,16020,1407572856643} to {e93cd3088d438d7ce5cbf7674dc6e8b5 state=OPEN, ts=1407572862635, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,636 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStateStore: Updating row usertable,user2,1407564565236.e93cd3088d438d7ce5cbf7674dc6e8b5. with state=OPEN&openSeqNum=120016207&server=slave1,16020,1407572856643
2014-08-09 01:27:42,638 INFO  [defaultRpcServer.handler=22,queue=2,port=16020] master.RegionStates: Onlined e93cd3088d438d7ce5cbf7674dc6e8b5 on slave1,16020,1407572856643
2014-08-09 01:27:42,638 INFO  [MASTER_SERVER_OPERATIONS-sceplus-vm48:16020-3] master.AssignmentManager: Waiting for 71ff2fc858512aa6069c415dc539a5d1 to leave regions-in-transition, timeOut=15000 ms.
2014-08-09 01:27:42,679 INFO  [defaultRpcServer.handler=39,queue=4,port=16020] master.RegionStates: Transition {71ff2fc858512aa6069c415dc539a5d1 state=PENDING_OPEN, ts=1407572860661, server=slave1,16020,1407572856643} to {71ff2fc858512aa6069c415dc539a5d1 state=OPEN, ts=1407572862679, server=slave1,16020,1407572856643}
2014-08-09 01:27:42,679 INFO  [defaultRpcServer.handler=39,queue=4,port=16020] master.RegionStateStore: Updating row usertable,user7,1407564565236.71ff2fc858512aa6069c415dc539a5d1. with state=OPEN&openSeqNum=120017244&server=slave1,16020,1407572856643
2014-08-09 01:27:42,681 INFO  [defaultRpcServer.handler=39,queue=4,port=16020] master.RegionStates: Onlined 71ff2fc858512aa6069c415dc539a5d1 on slave1,16020,1407572856643
2014-08-09 01:27:42,686 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-7] master.SplitLogManager: dead splitlog workers [slave1,16020,1407568505847]
2014-08-09 01:27:42,692 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-7] master.SplitLogManager: started splitting 64 logs in [hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting]
2014-08-09 01:27:42,736 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569505722 acquired by slave1,16020,1407572856643
2014-08-09 01:27:42,739 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440
2014-08-09 01:27:42,739 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:42,763 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569820440, length=129496725
2014-08-09 01:27:42,763 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:27:42,772 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569820440
2014-08-09 01:27:42,774 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569820440 after 2ms
2014-08-09 01:27:42,833 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x7e2334b7, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-09 01:27:42,834 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7e2334b7 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-09 01:27:42,835 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-09 01:27:42,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-09 01:27:42,840 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790017, negotiated timeout = 90000
2014-08-09 01:27:42,974 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 64 unassigned = 62 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569568697=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569505722=last_update = 1407572862791 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569873125=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440=last_update = 1407572862773 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 0 error = 0}
2014-08-09 01:27:43,582 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041
2014-08-09 01:27:43,583 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:43,645 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569751041, length=153976436
2014-08-09 01:27:43,645 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:27:43,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569751041
2014-08-09 01:27:43,650 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569751041 after 1ms
2014-08-09 01:27:43,754 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569873125 acquired by slave1,16020,1407572856643
2014-08-09 01:27:43,884 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569505722 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:43,922 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569505722 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569505722
2014-08-09 01:27:43,923 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569505722
2014-08-09 01:27:44,743 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569568697 acquired by slave1,16020,1407572856643
2014-08-09 01:27:46,433 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569568697 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:46,441 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569568697 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569568697
2014-08-09 01:27:46,443 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569568697
2014-08-09 01:27:46,469 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801 acquired by slave1,16020,1407572856643
2014-08-09 01:27:47,975 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 62 unassigned = 58 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801=last_update = 1407572866557 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041=last_update = 1407572863650 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569873125=last_update = 1407572863804 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440=last_update = 1407572862773 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 2 error = 0}
2014-08-09 01:27:51,679 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569873125 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:51,690 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569873125 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569873125
2014-08-09 01:27:51,691 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569873125
2014-08-09 01:27:51,706 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497 acquired by slave1,16020,1407572856643
2014-08-09 01:27:51,948 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:27:51,950 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 106 edits across 6 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569820440 is corrupted = false progress failed = false
2014-08-09 01:27:51,953 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:51,953 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440 in 9214ms
2014-08-09 01:27:51,954 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:51,962 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569820440 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569820440
2014-08-09 01:27:51,964 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569820440
2014-08-09 01:27:51,973 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166
2014-08-09 01:27:51,974 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:52,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569573166, length=134098627
2014-08-09 01:27:52,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:27:52,006 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569573166
2014-08-09 01:27:52,007 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569573166 after 1ms
2014-08-09 01:27:52,087 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:27:52,091 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47b96060790017
2014-08-09 01:27:52,105 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47b96060790017 closed
2014-08-09 01:27:52,105 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-09 01:27:52,205 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 109 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569751041 is corrupted = false progress failed = false
2014-08-09 01:27:52,214 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:52,215 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041 in 8632ms
2014-08-09 01:27:52,217 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:52,223 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569751041 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569751041
2014-08-09 01:27:52,224 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569751041
2014-08-09 01:27:52,976 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 59 unassigned = 56 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166=last_update = 1407572872007 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801=last_update = 1407572866557 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497=last_update = 1407572871757 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 5 error = 0}
2014-08-09 01:27:53,045 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413
2014-08-09 01:27:53,046 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:53,047 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:53,053 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569686801 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569686801
2014-08-09 01:27:53,054 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569686801
2014-08-09 01:27:53,068 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569800413, length=134821732
2014-08-09 01:27:53,069 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:27:53,071 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569800413
2014-08-09 01:27:53,071 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623 acquired by slave1,16020,1407572856643
2014-08-09 01:27:53,072 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569800413 after 1ms
2014-08-09 01:27:53,108 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x70a2d894, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-08-09 01:27:53,109 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x70a2d894 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-08-09 01:27:53,109 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-08-09 01:27:53,110 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-08-09 01:27:53,115 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x47b96060790018, negotiated timeout = 90000
2014-08-09 01:27:53,312 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:27:53,313 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569573166 is corrupted = false progress failed = false
2014-08-09 01:27:53,321 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:53,321 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166 in 1347ms
2014-08-09 01:27:53,322 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:53,330 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569573166 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569573166
2014-08-09 01:27:53,330 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569573166
2014-08-09 01:27:53,828 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075
2014-08-09 01:27:53,829 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:27:53,853 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569984075, length=131367840
2014-08-09 01:27:53,853 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:27:53,856 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569984075
2014-08-09 01:27:53,857 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569984075 after 1ms
2014-08-09 01:27:54,606 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:54,615 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569551623 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569551623
2014-08-09 01:27:54,616 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569551623
2014-08-09 01:27:54,629 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404 acquired by slave1,16020,1407572856643
2014-08-09 01:27:55,903 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:27:55,911 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569509404 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569509404
2014-08-09 01:27:55,912 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569509404
2014-08-09 01:27:55,928 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705 acquired by slave1,16020,1407572856643
2014-08-09 01:27:58,021 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 55 unassigned = 51 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497=last_update = 1407572871757 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413=last_update = 1407572873071 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075=last_update = 1407572873857 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705=last_update = 1407572876018 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 9 error = 0}
2014-08-09 01:28:00,412 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:00,424 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569723497 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569723497
2014-08-09 01:28:00,426 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569723497
2014-08-09 01:28:00,443 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888 acquired by slave1,16020,1407572856643
2014-08-09 01:28:02,896 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:02,897 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 101 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569800413 is corrupted = false progress failed = false
2014-08-09 01:28:02,901 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:02,901 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413 in 9856ms
2014-08-09 01:28:02,902 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:02,910 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569800413 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569800413
2014-08-09 01:28:02,911 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569800413
2014-08-09 01:28:02,921 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177
2014-08-09 01:28:02,922 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:02,946 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569927177, length=129932224
2014-08-09 01:28:02,946 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:02,955 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569927177
2014-08-09 01:28:02,956 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569927177 after 1ms
2014-08-09 01:28:03,022 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 53 unassigned = 49 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888=last_update = 1407572880549 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = 1407572882956 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075=last_update = 1407572873857 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705=last_update = 1407572876018 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 11 error = 0}
2014-08-09 01:28:04,319 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:04,319 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 145 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569984075 is corrupted = false progress failed = false
2014-08-09 01:28:04,473 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:04,473 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:04,474 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075 in 10646ms
2014-08-09 01:28:04,670 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570004888 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407570004888
2014-08-09 01:28:04,671 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570004888
2014-08-09 01:28:04,671 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:04,833 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569984075 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569984075
2014-08-09 01:28:04,834 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569984075
2014-08-09 01:28:05,328 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942 acquired by slave1,16020,1407572856643
2014-08-09 01:28:05,388 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574
2014-08-09 01:28:05,389 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:05,412 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569631574, length=128406501
2014-08-09 01:28:05,412 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:05,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569631574
2014-08-09 01:28:05,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569631574 after 1ms
2014-08-09 01:28:05,928 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:05,936 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569929705 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569929705
2014-08-09 01:28:05,937 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569929705
2014-08-09 01:28:06,011 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420 acquired by slave1,16020,1407572856643
2014-08-09 01:28:09,015 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:09,015 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569631574 is corrupted = false progress failed = false
2014-08-09 01:28:09,019 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:09,019 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574 in 3631ms
2014-08-09 01:28:09,021 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:09,022 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 50 unassigned = 46 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177=last_update = 1407572882956 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420=last_update = 1407572886057 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = 1407572885389 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574=last_update = 1407572889019 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 14 error = 0}
2014-08-09 01:28:09,028 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569631574 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569631574
2014-08-09 01:28:09,029 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569631574
2014-08-09 01:28:09,056 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:09,056 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163
2014-08-09 01:28:09,083 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569628163, length=128574014
2014-08-09 01:28:09,083 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:09,087 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569628163
2014-08-09 01:28:09,088 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569628163 after 1ms
2014-08-09 01:28:10,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:10,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 121 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569927177 is corrupted = false progress failed = false
2014-08-09 01:28:10,172 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:10,172 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177 in 7250ms
2014-08-09 01:28:10,172 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:10,179 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569927177 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569927177
2014-08-09 01:28:10,180 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569927177
2014-08-09 01:28:10,190 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663
2014-08-09 01:28:10,193 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:10,214 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569991663, length=130178155
2014-08-09 01:28:10,214 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:10,221 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569991663
2014-08-09 01:28:10,222 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569991663 after 1ms
2014-08-09 01:28:11,839 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:11,847 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569679420 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569679420
2014-08-09 01:28:11,848 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569679420
2014-08-09 01:28:11,911 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696 acquired by slave1,16020,1407572856643
2014-08-09 01:28:12,298 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:12,299 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 16 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569628163 is corrupted = false progress failed = false
2014-08-09 01:28:12,304 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:12,304 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163 in 3246ms
2014-08-09 01:28:12,305 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:12,313 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569628163 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569628163
2014-08-09 01:28:12,313 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569628163
2014-08-09 01:28:12,327 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110
2014-08-09 01:28:12,328 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:12,350 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569579110, length=133932756
2014-08-09 01:28:12,350 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:12,355 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569579110
2014-08-09 01:28:12,356 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569579110 after 1ms
2014-08-09 01:28:13,335 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:13,335 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569579110 is corrupted = false progress failed = false
2014-08-09 01:28:13,339 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:13,339 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110 in 1012ms
2014-08-09 01:28:13,340 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:13,348 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569579110 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569579110
2014-08-09 01:28:13,349 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569579110
2014-08-09 01:28:13,361 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761
2014-08-09 01:28:13,363 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:13,385 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569620761, length=132726837
2014-08-09 01:28:13,385 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:13,389 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569620761
2014-08-09 01:28:13,390 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569620761 after 1ms
2014-08-09 01:28:14,025 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 45 unassigned = 41 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761=last_update = 1407572893390 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = 1407572890222 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = 1407572891954 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942=last_update = 1407572885389 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 19 error = 0}
2014-08-09 01:28:17,860 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:17,867 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569814942 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569814942
2014-08-09 01:28:17,867 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569814942
2014-08-09 01:28:17,886 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388 acquired by slave1,16020,1407572856643
2014-08-09 01:28:18,060 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:18,062 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 19 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569620761 is corrupted = false progress failed = false
2014-08-09 01:28:18,072 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:18,072 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761 in 4710ms
2014-08-09 01:28:18,074 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:18,081 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569620761 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569620761
2014-08-09 01:28:18,082 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569620761
2014-08-09 01:28:18,091 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577
2014-08-09 01:28:18,092 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:18,116 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569754577, length=132408556
2014-08-09 01:28:18,116 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:18,120 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569754577
2014-08-09 01:28:18,121 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569754577 after 1ms
2014-08-09 01:28:19,028 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 43 unassigned = 39 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = 1407572898121 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663=last_update = 1407572890222 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696=last_update = 1407572891954 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = 1407572898022 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 21 error = 0}
2014-08-09 01:28:20,983 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:20,989 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569661696 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569661696
2014-08-09 01:28:20,990 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569661696
2014-08-09 01:28:21,006 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159 acquired by slave1,16020,1407572856643
2014-08-09 01:28:21,452 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:21,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 158 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569991663 is corrupted = false progress failed = false
2014-08-09 01:28:21,465 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:21,465 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663 in 11275ms
2014-08-09 01:28:21,465 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:21,473 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569991663 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569991663
2014-08-09 01:28:21,474 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569991663
2014-08-09 01:28:21,483 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594
2014-08-09 01:28:21,484 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:21,506 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569876594, length=130342303
2014-08-09 01:28:21,506 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:21,509 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569876594
2014-08-09 01:28:21,511 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569876594 after 2ms
2014-08-09 01:28:24,030 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 41 unassigned = 37 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = 1407572901511 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577=last_update = 1407572898121 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159=last_update = 1407572901149 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388=last_update = 1407572898022 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 23 error = 0}
2014-08-09 01:28:26,083 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:26,092 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569587159 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569587159
2014-08-09 01:28:26,093 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569587159
2014-08-09 01:28:26,108 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607 acquired by slave1,16020,1407572856643
2014-08-09 01:28:27,489 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:27,496 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569817388 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569817388
2014-08-09 01:28:27,497 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569817388
2014-08-09 01:28:27,542 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491 acquired by slave1,16020,1407572856643
2014-08-09 01:28:27,617 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:27,618 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 87 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569754577 is corrupted = false progress failed = false
2014-08-09 01:28:27,623 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:27,623 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577 in 9532ms
2014-08-09 01:28:27,627 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:27,633 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569754577 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569754577
2014-08-09 01:28:27,634 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569754577
2014-08-09 01:28:27,650 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155
2014-08-09 01:28:27,650 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:27,673 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569745155, length=134235963
2014-08-09 01:28:27,673 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:27,676 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569745155
2014-08-09 01:28:27,677 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569745155 after 1ms
2014-08-09 01:28:29,030 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 38 unassigned = 34 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594=last_update = 1407572901511 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = 1407572907671 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = 1407572906199 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = 1407572907677 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 26 error = 0}
2014-08-09 01:28:32,474 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:32,474 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 121 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569876594 is corrupted = false progress failed = false
2014-08-09 01:28:32,697 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:32,707 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594 in 11224ms
2014-08-09 01:28:32,698 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:32,918 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569876594 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569876594
2014-08-09 01:28:32,919 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569876594
2014-08-09 01:28:33,699 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912
2014-08-09 01:28:33,701 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:33,723 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569727912, length=131888645
2014-08-09 01:28:33,723 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:34,001 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569727912
2014-08-09 01:28:34,003 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569727912 after 1ms
2014-08-09 01:28:34,031 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 37 unassigned = 33 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491=last_update = 1407572907671 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607=last_update = 1407572906199 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = 1407572914006 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155=last_update = 1407572907677 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 27 error = 0}
2014-08-09 01:28:34,451 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:34,603 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569690607 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569690607
2014-08-09 01:28:34,605 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569690607
2014-08-09 01:28:34,934 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421 acquired by slave1,16020,1407572856643
2014-08-09 01:28:36,631 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:36,632 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 91 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569745155 is corrupted = false progress failed = false
2014-08-09 01:28:36,861 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:36,861 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155 in 9211ms
2014-08-09 01:28:37,150 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:37,158 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569745155 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569745155
2014-08-09 01:28:37,159 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569745155
2014-08-09 01:28:37,160 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:37,172 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569869491 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569869491
2014-08-09 01:28:37,173 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569869491
2014-08-09 01:28:37,684 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692 acquired by slave1,16020,1407572856643
2014-08-09 01:28:37,705 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158
2014-08-09 01:28:37,706 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:37,728 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569988158, length=128353743
2014-08-09 01:28:37,728 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:37,935 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569988158
2014-08-09 01:28:37,937 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569988158 after 2ms
2014-08-09 01:28:39,058 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 34 unassigned = 30 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = 1407572917936 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = 1407572915224 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912=last_update = 1407572914006 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = 1407572917936 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 30 error = 0}
2014-08-09 01:28:43,532 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:43,533 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 70 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569727912 is corrupted = false progress failed = false
2014-08-09 01:28:43,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:43,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912 in 9838ms
2014-08-09 01:28:43,538 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:43,546 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569727912 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569727912
2014-08-09 01:28:43,548 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569727912
2014-08-09 01:28:43,556 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724
2014-08-09 01:28:43,556 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:43,584 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569763724, length=131502924
2014-08-09 01:28:43,584 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:43,588 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569763724
2014-08-09 01:28:43,589 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569763724 after 1ms
2014-08-09 01:28:44,058 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 33 unassigned = 29 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692=last_update = 1407572917936 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421=last_update = 1407572915224 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = 1407572923589 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158=last_update = 1407572917936 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 31 error = 0}
2014-08-09 01:28:49,148 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:49,156 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569747421 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569747421
2014-08-09 01:28:49,157 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569747421
2014-08-09 01:28:49,169 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253 acquired by slave1,16020,1407572856643
2014-08-09 01:28:49,743 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:49,744 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 156 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569988158 is corrupted = false progress failed = false
2014-08-09 01:28:49,748 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:49,748 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158 in 12042ms
2014-08-09 01:28:49,749 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:49,756 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569988158 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569988158
2014-08-09 01:28:49,756 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569988158
2014-08-09 01:28:49,763 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315
2014-08-09 01:28:49,764 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:49,788 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569516315, length=130446823
2014-08-09 01:28:49,788 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:49,793 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569516315
2014-08-09 01:28:49,794 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569516315 after 1ms
2014-08-09 01:28:49,805 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:49,811 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569937692 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569937692
2014-08-09 01:28:49,811 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569937692
2014-08-09 01:28:50,059 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 30 unassigned = 27 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315=last_update = 1407572929794 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724=last_update = 1407572923589 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253=last_update = 1407572929208 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 34 error = 0}
2014-08-09 01:28:50,105 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844 acquired by slave1,16020,1407572856643
2014-08-09 01:28:51,157 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:51,158 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569516315 is corrupted = false progress failed = false
2014-08-09 01:28:51,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:51,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315 in 1404ms
2014-08-09 01:28:51,168 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:51,176 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569516315 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569516315
2014-08-09 01:28:51,177 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569516315
2014-08-09 01:28:51,189 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977
2014-08-09 01:28:51,190 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:51,215 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569866977, length=132917329
2014-08-09 01:28:51,215 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:51,218 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569866977
2014-08-09 01:28:51,219 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569866977 after 1ms
2014-08-09 01:28:52,575 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:52,575 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 87 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569763724 is corrupted = false progress failed = false
2014-08-09 01:28:52,578 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:52,579 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724 in 9023ms
2014-08-09 01:28:52,579 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:52,588 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569763724 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569763724
2014-08-09 01:28:52,589 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569763724
2014-08-09 01:28:52,594 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822
2014-08-09 01:28:52,595 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:52,618 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569519822, length=130023635
2014-08-09 01:28:52,618 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:52,620 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569519822
2014-08-09 01:28:52,621 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569519822 after 1ms
2014-08-09 01:28:53,887 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:28:53,887 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569519822 is corrupted = false progress failed = false
2014-08-09 01:28:53,890 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:53,890 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822 in 1295ms
2014-08-09 01:28:53,891 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:53,899 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569519822 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569519822
2014-08-09 01:28:53,900 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569519822
2014-08-09 01:28:53,908 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297
2014-08-09 01:28:53,908 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:28:53,933 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570001297, length=131965555
2014-08-09 01:28:53,933 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:28:53,942 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570001297
2014-08-09 01:28:53,944 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570001297 after 1ms
2014-08-09 01:28:54,287 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:54,295 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569635253 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569635253
2014-08-09 01:28:54,296 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569635253
2014-08-09 01:28:54,308 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049 acquired by slave1,16020,1407572856643
2014-08-09 01:28:55,060 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 26 unassigned = 22 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = 1407572933944 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = 1407572931219 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049=last_update = 1407572934488 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = 1407572930160 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 38 error = 0}
2014-08-09 01:28:59,791 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:28:59,799 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569583049 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569583049
2014-08-09 01:28:59,800 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569583049
2014-08-09 01:29:00,061 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 25 unassigned = 22 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = 1407572933944 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977=last_update = 1407572931219 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844=last_update = 1407572930160 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 39 error = 0}
2014-08-09 01:29:00,739 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067 acquired by slave1,16020,1407572856643
2014-08-09 01:29:02,177 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:02,185 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569997844 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569997844
2014-08-09 01:29:02,186 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569997844
2014-08-09 01:29:02,197 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918 acquired by slave1,16020,1407572856643
2014-08-09 01:29:04,384 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:04,590 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:04,592 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 117 edits across 7 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569866977 is corrupted = false progress failed = false
2014-08-09 01:29:04,694 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569512918 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569512918
2014-08-09 01:29:04,695 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569512918
2014-08-09 01:29:04,734 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:04,734 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977 in 13545ms
2014-08-09 01:29:04,996 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:05,131 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569866977 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569866977
2014-08-09 01:29:05,132 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414
2014-08-09 01:29:05,171 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569866977
2014-08-09 01:29:05,194 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569624414, length=132349467
2014-08-09 01:29:05,194 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:05,194 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:05,195 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037 acquired by slave1,16020,1407572856643
2014-08-09 01:29:05,452 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569624414
2014-08-09 01:29:05,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569624414 after 1ms
2014-08-09 01:29:06,061 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 22 unassigned = 18 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414=last_update = 1407572945481 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = 1407572945481 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297=last_update = 1407572933944 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067=last_update = 1407572941124 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 42 error = 0}
2014-08-09 01:29:07,404 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:07,404 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 158 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570001297 is corrupted = false progress failed = false
2014-08-09 01:29:07,408 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:07,408 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297 in 13500ms
2014-08-09 01:29:07,410 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:07,418 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570001297 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407570001297
2014-08-09 01:29:07,419 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570001297
2014-08-09 01:29:07,432 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395
2014-08-09 01:29:07,432 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:07,455 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569665395, length=128547092
2014-08-09 01:29:07,455 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:07,457 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569665395
2014-08-09 01:29:07,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569665395 after 1ms
2014-08-09 01:29:09,534 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:09,535 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 17 edits across 1 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569624414 is corrupted = false progress failed = false
2014-08-09 01:29:09,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:09,538 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414 in 4367ms
2014-08-09 01:29:09,540 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:09,548 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569624414 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569624414
2014-08-09 01:29:09,549 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569624414
2014-08-09 01:29:09,561 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495
2014-08-09 01:29:09,562 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:09,585 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569948495, length=129832683
2014-08-09 01:29:09,585 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:09,587 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569948495
2014-08-09 01:29:09,588 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569948495 after 1ms
2014-08-09 01:29:09,754 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:09,761 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569683067 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569683067
2014-08-09 01:29:09,762 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569683067
2014-08-09 01:29:09,781 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941 acquired by slave1,16020,1407572856643
2014-08-09 01:29:11,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 19 unassigned = 15 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941=last_update = 1407572949886 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = 1407572949587 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037=last_update = 1407572945481 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395=last_update = 1407572947457 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 45 error = 0}
2014-08-09 01:29:11,459 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:11,467 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569547941 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569547941
2014-08-09 01:29:11,468 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569547941
2014-08-09 01:29:11,489 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134 acquired by slave1,16020,1407572856643
2014-08-09 01:29:12,642 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:12,650 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569669037 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569669037
2014-08-09 01:29:12,651 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569669037
2014-08-09 01:29:12,663 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975 acquired by slave1,16020,1407572856643
2014-08-09 01:29:13,144 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:13,153 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569555134 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569555134
2014-08-09 01:29:13,153 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569555134
2014-08-09 01:29:13,412 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:13,413 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 52 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569665395 is corrupted = false progress failed = false
2014-08-09 01:29:13,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:13,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395 in 5985ms
2014-08-09 01:29:13,417 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:13,422 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569665395 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569665395
2014-08-09 01:29:13,423 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569665395
2014-08-09 01:29:13,434 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467
2014-08-09 01:29:13,435 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:13,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569675467, length=138997795
2014-08-09 01:29:13,458 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:13,468 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569675467
2014-08-09 01:29:13,468 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569675467 after 0ms
2014-08-09 01:29:13,532 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654 acquired by slave1,16020,1407572856643
2014-08-09 01:29:17,122 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 15 unassigned = 11 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467=last_update = 1407572953469 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975=last_update = 1407572952704 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495=last_update = 1407572949587 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = 1407572953571 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 49 error = 0}
2014-08-09 01:29:18,854 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:18,893 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569658975 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569658975
2014-08-09 01:29:18,894 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569658975
2014-08-09 01:29:18,910 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377 acquired by slave1,16020,1407572856643
2014-08-09 01:29:19,166 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:19,167 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 138 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569948495 is corrupted = false progress failed = false
2014-08-09 01:29:19,170 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:19,170 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495 in 9608ms
2014-08-09 01:29:19,171 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:19,178 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569948495 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569948495
2014-08-09 01:29:19,179 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569948495
2014-08-09 01:29:19,186 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975
2014-08-09 01:29:19,187 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:19,210 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569671975, length=132342656
2014-08-09 01:29:19,210 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:19,217 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569671975
2014-08-09 01:29:19,218 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569671975 after 1ms
2014-08-09 01:29:22,186 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:22,186 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 58 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569675467 is corrupted = false progress failed = false
2014-08-09 01:29:22,189 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:22,189 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467 in 8754ms
2014-08-09 01:29:22,190 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:22,198 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569675467 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569675467
2014-08-09 01:29:22,198 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569675467
2014-08-09 01:29:22,204 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050
2014-08-09 01:29:22,205 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:22,228 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569824050, length=150012278
2014-08-09 01:29:22,228 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:22,230 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569824050
2014-08-09 01:29:22,231 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569824050 after 1ms
2014-08-09 01:29:23,124 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 12 unassigned = 8 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = 1407572959009 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = 1407572959220 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654=last_update = 1407572953571 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 52 error = 0}
2014-08-09 01:29:26,707 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:26,717 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569944654 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569944654
2014-08-09 01:29:26,717 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569944654
2014-08-09 01:29:26,796 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117 acquired by slave1,16020,1407572856643
2014-08-09 01:29:28,126 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 11 unassigned = 7 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = 1407572959009 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975=last_update = 1407572959220 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 53 error = 0}
2014-08-09 01:29:30,645 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:30,646 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 52 edits across 3 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569671975 is corrupted = false progress failed = false
2014-08-09 01:29:30,652 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:30,652 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975 in 11466ms
2014-08-09 01:29:30,653 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:30,660 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569671975 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569671975
2014-08-09 01:29:30,661 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569671975
2014-08-09 01:29:30,667 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065
2014-08-09 01:29:30,672 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:30,692 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569565065, length=152255531
2014-08-09 01:29:30,692 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:30,714 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569565065
2014-08-09 01:29:30,716 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569565065 after 2ms
2014-08-09 01:29:32,261 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:32,262 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569565065 is corrupted = false progress failed = false
2014-08-09 01:29:32,265 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:32,265 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065 in 1597ms
2014-08-09 01:29:32,265 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:32,274 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569565065 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569565065
2014-08-09 01:29:32,275 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569565065
2014-08-09 01:29:32,282 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351
2014-08-09 01:29:32,283 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:32,311 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569994351, length=128589522
2014-08-09 01:29:32,311 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:32,328 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569994351
2014-08-09 01:29:32,330 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569994351 after 2ms
2014-08-09 01:29:33,168 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 9 unassigned = 5 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377=last_update = 1407572959009 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 55 error = 0}
2014-08-09 01:29:33,914 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:33,923 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569923377 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569923377
2014-08-09 01:29:33,923 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569923377
2014-08-09 01:29:33,941 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273 acquired by slave1,16020,1407572856643
2014-08-09 01:29:39,157 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 8 unassigned = 4 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = 1407572974108 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0}
2014-08-09 01:29:44,158 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 8 unassigned = 4 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = 1407572974108 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0}
2014-08-09 01:29:49,160 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 8 unassigned = 4 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = 1407572974108 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0}
2014-08-09 01:29:55,161 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 8 unassigned = 4 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050=last_update = 1407572962232 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = 1407572974108 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117=last_update = 1407572966841 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 56 error = 0}
2014-08-09 01:29:55,417 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:29:55,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 124 edits across 6 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569824050 is corrupted = false progress failed = false
2014-08-09 01:29:55,422 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:55,422 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050 in 33217ms
2014-08-09 01:29:55,424 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:55,442 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569824050 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569824050
2014-08-09 01:29:55,444 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569824050
2014-08-09 01:29:55,459 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911
2014-08-09 01:29:55,461 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:29:55,489 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569940911, length=128793103
2014-08-09 01:29:55,489 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:29:55,495 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569940911
2014-08-09 01:29:55,496 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569940911 after 1ms
2014-08-09 01:29:58,459 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:29:58,467 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569829117 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569829117
2014-08-09 01:29:58,467 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569829117
2014-08-09 01:29:58,493 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057 acquired by slave1,16020,1407572856643
2014-08-09 01:30:00,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 6 unassigned = 2 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273=last_update = 1407572974108 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = 1407572998543 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911=last_update = 1407572995498 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351=last_update = 1407572972329 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = -1 last_version = -1 cur_worker_name = null status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 58 error = 0}
2014-08-09 01:30:02,986 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:30:02,986 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 137 edits across 8 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569940911 is corrupted = false progress failed = false
2014-08-09 01:30:02,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:02,989 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911 in 7529ms
2014-08-09 01:30:02,991 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:02,999 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569940911 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569940911
2014-08-09 01:30:03,000 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569940911
2014-08-09 01:30:03,009 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,16020,1407564407993] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 acquired task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277
2014-08-09 01:30:03,010 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277 acquired by sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:03,034 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569861277, length=136151105
2014-08-09 01:30:03,034 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: DistributedLogReplay = true
2014-08-09 01:30:03,043 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569861277
2014-08-09 01:30:03,044 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569861277 after 1ms
2014-08-09 01:30:03,312 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Split writers finished
2014-08-09 01:30:03,313 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] wal.HLogSplitter: Processed 155 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569994351 is corrupted = false progress failed = false
2014-08-09 01:30:03,316 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:03,316 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351 in 31034ms
2014-08-09 01:30:03,317 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:03,326 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569994351 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569994351
2014-08-09 01:30:03,327 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569994351
2014-08-09 01:30:03,427 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:30:03,436 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569879273 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569879273
2014-08-09 01:30:03,437 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569879273
2014-08-09 01:30:03,456 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873 acquired by slave1,16020,1407572856643
2014-08-09 01:30:06,162 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993.splitLogManagerTimeoutMonitor] master.SplitLogManager: total tasks = 3 unassigned = 0 tasks={/hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277=last_update = 1407573003044 last_version = 2 cur_worker_name = sceplus-vm48.almaden.ibm.com,16020,1407564407993 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 61 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057=last_update = 1407572998543 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 61 error = 0, /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873=last_update = 1407573003525 last_version = 2 cur_worker_name = slave1,16020,1407572856643 status = in_progress incarnation = 0 resubmits = 0 batch = installed = 64 done = 61 error = 0}
2014-08-09 01:30:06,614 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:30:06,622 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407570200057 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407570200057
2014-08-09 01:30:06,623 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407570200057
2014-08-09 01:30:09,279 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Split writers finished
2014-08-09 01:30:09,286 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] client.ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x47b96060790018
2014-08-09 01:30:09,405 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] zookeeper.ZooKeeper: Session: 0x47b96060790018 closed
2014-08-09 01:30:09,405 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0-Writer-2-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-08-09 01:30:09,506 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] wal.HLogSplitter: Processed 126 edits across 6 regions; log file=hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569861277 is corrupted = false progress failed = false
2014-08-09 01:30:09,552 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277 to final state DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:09,552 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:16020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,16020,1407564407993 done with task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277 in 6543ms
2014-08-09 01:30:09,563 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277 entered state: DONE sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:30:09,569 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569861277 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569861277
2014-08-09 01:30:09,570 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569861277
2014-08-09 01:30:10,222 INFO  [main-EventThread] master.SplitLogManager: task /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873 entered state: DONE slave1,16020,1407572856643
2014-08-09 01:30:10,227 INFO  [main-EventThread] wal.HLogSplitter: Archived processed log hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting/slave1%2C16020%2C1407568505847.1407569759873 to hdfs://master:54310/hbase/oldWALs/slave1%2C16020%2C1407568505847.1407569759873
2014-08-09 01:30:10,228 INFO  [main-EventThread] master.SplitLogManager: Done splitting /hbase/splitWAL/WALs%2Fslave1%2C16020%2C1407568505847-splitting%2Fslave1%252C16020%252C1407568505847.1407569759873
2014-08-09 01:30:10,296 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-7] master.SplitLogManager: finished splitting (more than or equal to) 8314158129 bytes in 64 log files in [hdfs://master:54310/hbase/WALs/slave1,16020,1407568505847-splitting] in 147604ms
2014-08-09 01:30:10,296 INFO  [M_LOG_REPLAY_OPS-sceplus-vm48:16020-7] handler.LogReplayHandler: Finished processing shutdown of slave1,16020,1407568505847
2014-08-09 01:31:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=29149, hits=29132, hitRatio=99.94%, , cachingAccesses=29138, cachingHits=29123, cachingHitsRatio=99.95%, evictions=0, evicted=11, evictedPerRun=Infinity
2014-08-09 01:32:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:32:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407572836236, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407573136174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:32:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:32:16,205 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:32:16,210 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:32:16,210 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:32:16,210 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-0] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:32:16,211 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407573136174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407573136211, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:32:16,211 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-0] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:32:16,214 INFO  [AM.-pool1-t33] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:32:16,214 INFO  [AM.-pool1-t33] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407573136211, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407573136214, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:32:16,214 INFO  [AM.-pool1-t33] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:32:16,216 INFO  [AM.-pool1-t33] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:32:16,225 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:32:16,226 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:32:16,246 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:32:16,247 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:32:16,247 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407573136214, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407573136247, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:32:16,248 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:36:48,201 INFO  [LruBlockCacheStatsExecutor] hfile.LruBlockCache: totalSize=5.03 MB, freeSize=4.75 GB, max=4.76 GB, accesses=29153, hits=29136, hitRatio=99.94%, , cachingAccesses=29142, cachingHits=29127, cachingHitsRatio=99.95%, evictions=0, evicted=11, evictedPerRun=Infinity
2014-08-09 01:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.HMaster: balance hri=hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a., src=sceplus-vm48.almaden.ibm.com,16020,1407563646474, dest=sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407573136247, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407573436174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:37:16,174 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_CLOSE
2014-08-09 01:37:16,215 INFO  [sceplus-vm48.almaden.ibm.com,16020,1407564407993-BalancerChore] regionserver.RSRpcServices: Close ac22fb07770c81080da256230256da5a, moving to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:37:16,220 INFO  [StoreCloserThread-hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.-1] regionserver.HStore: Closed info
2014-08-09 01:37:16,220 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegion: Closed hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:37:16,221 WARN  [RS_CLOSE_REGION-sceplus-vm48:16020-1] regionserver.HRegionServer: Not adding moved region record: ac22fb07770c81080da256230256da5a to self.
2014-08-09 01:37:16,221 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_CLOSE, ts=1407573436174, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407573436221, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:37:16,221 INFO  [RS_CLOSE_REGION-sceplus-vm48:16020-1] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=CLOSED
2014-08-09 01:37:16,224 INFO  [AM.-pool1-t34] master.AssignmentManager: Assigning hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. to sceplus-vm48.almaden.ibm.com,16020,1407564407993
2014-08-09 01:37:16,224 INFO  [AM.-pool1-t34] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=CLOSED, ts=1407573436221, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407573436224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:37:16,224 INFO  [AM.-pool1-t34] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=PENDING_OPEN
2014-08-09 01:37:16,229 INFO  [AM.-pool1-t34] regionserver.RSRpcServices: Open hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:37:16,239 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] hfile.CacheConfig: blockCache=org.apache.hadoop.hbase.io.hfile.LruBlockCache@711c664, cacheDataOnRead=true, cacheDataOnWrite=false, cacheIndexesOnWrite=false, cacheBloomsOnWrite=false, cacheEvictOnClose=false, cacheCompressed=false, prefetchOnOpen=false
2014-08-09 01:37:16,240 INFO  [StoreOpener-ac22fb07770c81080da256230256da5a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2560; major period 604800000, major jitter 0.500000
2014-08-09 01:37:16,268 INFO  [RS_OPEN_REGION-sceplus-vm48:16020-2] regionserver.HRegion: Onlined ac22fb07770c81080da256230256da5a; next sequenceid=8
2014-08-09 01:37:16,269 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] regionserver.HRegionServer: Post open deploy tasks for hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a.
2014-08-09 01:37:16,269 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStates: Transition {ac22fb07770c81080da256230256da5a state=PENDING_OPEN, ts=1407573436224, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993} to {ac22fb07770c81080da256230256da5a state=OPEN, ts=1407573436269, server=sceplus-vm48.almaden.ibm.com,16020,1407564407993}
2014-08-09 01:37:16,269 INFO  [PostOpenDeployTasks:ac22fb07770c81080da256230256da5a] master.RegionStateStore: Updating row hbase:namespace,,1407484099191.ac22fb07770c81080da256230256da5a. with state=OPEN&openSeqNum=8&server=sceplus-vm48.almaden.ibm.com,16020,1407564407993
