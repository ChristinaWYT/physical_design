Thu Jul 10 23:55:44 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-10 23:55:44,598 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-10 23:55:44,599 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-10 23:55:44,599 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 41815 22
2014-07-10 23:55:44,844 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-10 23:55:44,845 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-10 23:55:44,846 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 41815 9.1.143.59 22
2014-07-10 23:55:44,846 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-10 23:55:44,846 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-10 23:55:44,846 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-10 23:55:44,848 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=180
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-10 23:55:44,849 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-10 23:55:44,851 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-10 23:55:44,852 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-10 23:55:45,083 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-10 23:55:45,502 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-10 23:55:45,604 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-10 23:55:45,617 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-10 23:55:45,680 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-10 23:55:45,681 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-10 23:55:45,681 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-10 23:55:45,687 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-10 23:55:45,691 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-10 23:55:45,775 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-10 23:55:45,775 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-10 23:55:45,779 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-10 23:55:45,782 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-10 23:55:45,864 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-10 23:55:45,920 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-10 23:55:45,929 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-10 23:55:45,931 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-10 23:55:45,931 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-10 23:55:45,931 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-10 23:55:46,261 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-10 23:55:46,308 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-10 23:55:46,308 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-10 23:55:46,308 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-10 23:55:46,308 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-10 23:55:46,309 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-10 23:55:46,310 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 23:55:46,334 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 23:55:46,339 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 23:55:46,346 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-10 23:55:46,362 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472434cb3c0000, negotiated timeout = 90000
2014-07-10 23:56:18,451 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0xba6ceb, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 23:56:18,453 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0xba6ceb connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 23:56:18,454 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 23:56:18,454 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-10 23:56:18,469 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x472434cc6e0004, negotiated timeout = 90000
2014-07-10 23:56:18,744 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@175327f4
2014-07-10 23:56:18,748 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-10 23:56:18,753 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-10 23:56:18,767 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-10 23:56:18,805 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-10 23:56:18,812 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-10 23:56:18,816 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-10 23:56:18,837 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405061743981 with port=60020, startcode=1405061745702
2014-07-10 23:56:19,158 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-10 23:56:19,159 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-10 23:56:19,159 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-10 23:56:19,189 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-10 23:56:19,198 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702
2014-07-10 23:56:19,243 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-10 23:56:19,255 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-10 23:56:19,360 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405061779266
2014-07-10 23:56:19,378 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-10 23:56:19,383 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-10 23:56:19,387 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-10 23:56:19,391 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-10 23:56:19,394 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-10 23:56:19,394 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-10 23:56:19,394 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-10 23:56:19,394 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-10 23:56:19,395 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-10 23:56:19,401 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1405061745702, sceplus-vm48.almaden.ibm.com,60020,1405061745836] other RSs: [slave1,60020,1405061745702, sceplus-vm48.almaden.ibm.com,60020,1405061745836]
2014-07-10 23:56:19,425 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-10 23:56:19,427 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x19dcef1a, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 23:56:19,428 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x19dcef1a connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 23:56:19,428 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 23:56:19,429 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-10 23:56:19,431 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1472434cb3c0003, negotiated timeout = 90000
2014-07-10 23:56:19,437 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-10 23:56:19,437 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-10 23:56:19,481 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405061745702, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x1472434cb3c0000
2014-07-10 23:56:19,482 INFO  [SplitLogWorker-slave1,60020,1405061745702] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405061745702 starting
2014-07-10 23:56:19,482 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-10 23:56:19,482 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405061745702
2014-07-10 23:56:19,482 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405061745702'
2014-07-10 23:56:19,482 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-10 23:56:19,483 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-10 23:56:19,484 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-10 23:56:24,102 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:24,243 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:24,243 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d72d54e8219504183264cf4b80647ff from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,244 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 23:56:24,245 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1415efc4609fde0f73340d69f18de237 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,273 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:24,273 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,274 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:24,274 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d72d54e8219504183264cf4b80647ff from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,274 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:24,274 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1415efc4609fde0f73340d69f18de237 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,279 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,299 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 3d72d54e8219504183264cf4b80647ff, NAME => 'usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-10 23:56:24,299 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-10 23:56:24,300 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 1415efc4609fde0f73340d69f18de237, NAME => 'usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-10 23:56:24,328 INFO  [RS_OPEN_REGION-slave1:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-10 23:56:24,328 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3d72d54e8219504183264cf4b80647ff
2014-07-10 23:56:24,329 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:24,329 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-10 23:56:24,329 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 1415efc4609fde0f73340d69f18de237
2014-07-10 23:56:24,329 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 23:56:24,330 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:24,341 INFO  [RS_OPEN_REGION-slave1:60020-0] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-10 23:56:24,343 INFO  [RS_OPEN_REGION-slave1:60020-0] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-10 23:56:24,345 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-10 23:56:24,345 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-10 23:56:24,408 INFO  [StoreOpener-3d72d54e8219504183264cf4b80647ff-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-10 23:56:24,409 INFO  [StoreOpener-1415efc4609fde0f73340d69f18de237-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-10 23:56:24,408 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 23:56:24,445 INFO  [StoreOpener-3d72d54e8219504183264cf4b80647ff-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-10 23:56:24,522 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3d72d54e8219504183264cf4b80647ff
2014-07-10 23:56:24,534 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-10 23:56:24,546 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/02688c70b48b4e7db6e1aa6781ecb2a1, isReference=false, isBulkLoadResult=false, seqid=2371, majorCompaction=true
2014-07-10 23:56:24,567 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-10 23:56:24,572 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 3d72d54e8219504183264cf4b80647ff; next sequenceid=1
2014-07-10 23:56:24,573 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3d72d54e8219504183264cf4b80647ff
2014-07-10 23:56:24,575 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-10 23:56:24,577 INFO  [PostOpenDeployTasks:3d72d54e8219504183264cf4b80647ff] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:24,579 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-10 23:56:24,579 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-10 23:56:24,581 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 23:56:24,582 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/03ee192cb26141b6bb2b3a705511e421, isReference=false, isBulkLoadResult=false, seqid=3278, majorCompaction=false
2014-07-10 23:56:24,602 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/4a852a325e9e4309a3732a1bdde7fb04, isReference=false, isBulkLoadResult=false, seqid=3082, majorCompaction=false
2014-07-10 23:56:24,629 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/5fe7281f15834b53970ea4d3a6f6caf7, isReference=false, isBulkLoadResult=false, seqid=2913, majorCompaction=false
2014-07-10 23:56:24,662 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/601ca73c598d4245b46c3619924f6d3d, isReference=false, isBulkLoadResult=false, seqid=2746, majorCompaction=false
2014-07-10 23:56:24,706 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/76fffc8085a7490091e70555bad4b11f, isReference=false, isBulkLoadResult=false, seqid=2577, majorCompaction=false
2014-07-10 23:56:24,718 DEBUG [StoreOpener-1415efc4609fde0f73340d69f18de237-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/95c8597bd483494d98f185bcf3f8b0c1, isReference=false, isBulkLoadResult=false, seqid=3253, majorCompaction=false
2014-07-10 23:56:24,719 INFO  [PostOpenDeployTasks:3d72d54e8219504183264cf4b80647ff] catalog.MetaEditor: Updated row usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff. with server=slave1,60020,1405061745702
2014-07-10 23:56:24,719 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405061745702
2014-07-10 23:56:24,720 INFO  [PostOpenDeployTasks:3d72d54e8219504183264cf4b80647ff] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:24,721 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 23:56:24,721 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d72d54e8219504183264cf4b80647ff from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,721 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,722 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237
2014-07-10 23:56:24,724 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 1415efc4609fde0f73340d69f18de237; next sequenceid=3279
2014-07-10 23:56:24,724 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1415efc4609fde0f73340d69f18de237
2014-07-10 23:56:24,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d72d54e8219504183264cf4b80647ff from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 3d72d54e8219504183264cf4b80647ff to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:24,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff. on slave1,60020,1405061745702
2014-07-10 23:56:24,725 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b0645baeeaeff5e861eee6d3147eabd0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,726 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:24,726 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405061745702
2014-07-10 23:56:24,726 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 283e6ac382eb78b85dc3bc8a3376765d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,727 INFO  [PostOpenDeployTasks:1415efc4609fde0f73340d69f18de237] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:24,729 DEBUG [PostOpenDeployTasks:1415efc4609fde0f73340d69f18de237] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-10 23:56:24,730 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b0645baeeaeff5e861eee6d3147eabd0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,730 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-10 23:56:24,730 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => b0645baeeaeff5e861eee6d3147eabd0, NAME => 'usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-10 23:56:24,731 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 283e6ac382eb78b85dc3bc8a3376765d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,731 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b0645baeeaeff5e861eee6d3147eabd0
2014-07-10 23:56:24,731 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:24,731 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 283e6ac382eb78b85dc3bc8a3376765d, NAME => 'usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-10 23:56:24,732 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 283e6ac382eb78b85dc3bc8a3376765d
2014-07-10 23:56:24,732 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 375589430 starting at candidate #1 after considering 15 permutations with 10 in ratio
2014-07-10 23:56:24,732 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:24,734 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.HStore: 1415efc4609fde0f73340d69f18de237 - family: Initiating minor compaction
2014-07-10 23:56:24,734 INFO  [regionserver60020-smallCompactions-1405061784729] regionserver.HRegion: Starting compaction on family in region usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:24,734 INFO  [regionserver60020-smallCompactions-1405061784729] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/.tmp, totalSize=358.2m
2014-07-10 23:56:24,735 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/76fffc8085a7490091e70555bad4b11f, keycount=115919, bloomtype=ROW, size=82.5m, encoding=NONE, seqNum=2577
2014-07-10 23:56:24,735 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/601ca73c598d4245b46c3619924f6d3d, keycount=95063, bloomtype=ROW, size=67.7m, encoding=NONE, seqNum=2746
2014-07-10 23:56:24,736 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/5fe7281f15834b53970ea4d3a6f6caf7, keycount=93285, bloomtype=ROW, size=66.4m, encoding=NONE, seqNum=2913
2014-07-10 23:56:24,736 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/4a852a325e9e4309a3732a1bdde7fb04, keycount=93524, bloomtype=ROW, size=66.6m, encoding=NONE, seqNum=3082
2014-07-10 23:56:24,736 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/95c8597bd483494d98f185bcf3f8b0c1, keycount=93392, bloomtype=ROW, size=66.5m, encoding=NONE, seqNum=3253
2014-07-10 23:56:24,736 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1415efc4609fde0f73340d69f18de237/family/03ee192cb26141b6bb2b3a705511e421, keycount=11969, bloomtype=ROW, size=8.5m, encoding=NONE, seqNum=3278
2014-07-10 23:56:24,737 INFO  [PostOpenDeployTasks:1415efc4609fde0f73340d69f18de237] catalog.MetaEditor: Updated row usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237. with server=slave1,60020,1405061745702
2014-07-10 23:56:24,738 INFO  [PostOpenDeployTasks:1415efc4609fde0f73340d69f18de237] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:24,739 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1415efc4609fde0f73340d69f18de237 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,740 INFO  [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-10 23:56:24,741 INFO  [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-10 23:56:24,741 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1415efc4609fde0f73340d69f18de237 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,742 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 1415efc4609fde0f73340d69f18de237 to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:24,742 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237. on slave1,60020,1405061745702
2014-07-10 23:56:24,743 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d7b363608a43906e9988c93f25f91d4f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,746 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d7b363608a43906e9988c93f25f91d4f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 23:56:24,747 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => d7b363608a43906e9988c93f25f91d4f, NAME => 'usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-10 23:56:24,747 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable d7b363608a43906e9988c93f25f91d4f
2014-07-10 23:56:24,748 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:24,757 INFO  [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-10 23:56:24,777 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/06f4bc771b1944a1a329c8ce6d8ec475, isReference=false, isBulkLoadResult=false, seqid=2962, majorCompaction=false
2014-07-10 23:56:24,798 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/11ef889e1da549c685ea21340966ee8d, isReference=false, isBulkLoadResult=false, seqid=575, majorCompaction=false
2014-07-10 23:56:24,808 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/072bd0794dfd4a53a3cac1ed5a70fbdc, isReference=false, isBulkLoadResult=false, seqid=3274, majorCompaction=false
2014-07-10 23:56:24,811 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/23a54069e0c14a2caefe4d3cfe0ad734, isReference=false, isBulkLoadResult=false, seqid=1543, majorCompaction=false
2014-07-10 23:56:24,825 INFO  [regionserver60020-smallCompactions-1405061784729] compress.CodecPool: Got brand-new decompressor
2014-07-10 23:56:24,825 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/0a7ad520562545f8b1c520ae75fd73ab, isReference=false, isBulkLoadResult=false, seqid=3131, majorCompaction=false
2014-07-10 23:56:24,836 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/430d3b78525d4064a5d432e31108faa0, isReference=false, isBulkLoadResult=false, seqid=1992, majorCompaction=false
2014-07-10 23:56:24,840 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/2500da8f034f44af9e63d9b3d863552d, isReference=false, isBulkLoadResult=false, seqid=2323, majorCompaction=false
2014-07-10 23:56:24,847 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/3005f52009834528a2c779f4ab3b0e12, isReference=false, isBulkLoadResult=false, seqid=1684, majorCompaction=false
2014-07-10 23:56:24,857 DEBUG [regionserver60020-smallCompactions-1405061784729] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 23:56:24,862 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/4840f430f61b458a8321c3485b8edc75, isReference=false, isBulkLoadResult=false, seqid=2840, majorCompaction=false
2014-07-10 23:56:24,868 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/822a0bd00c79491491781a5e27143b12, isReference=false, isBulkLoadResult=false, seqid=1125, majorCompaction=false
2014-07-10 23:56:24,870 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/502377778ebe4f3e8ad954f1ada57904, isReference=false, isBulkLoadResult=false, seqid=1408, majorCompaction=true
2014-07-10 23:56:24,885 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/5d37df24755444d1a09ff2e7713540f7, isReference=false, isBulkLoadResult=false, seqid=2491, majorCompaction=false
2014-07-10 23:56:24,886 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/b422a527c4384525b9946a307cd0bf54, isReference=false, isBulkLoadResult=false, seqid=2794, majorCompaction=false
2014-07-10 23:56:24,894 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/be82b7bcafd841b38406b35014b46105, isReference=false, isBulkLoadResult=false, seqid=2153, majorCompaction=false
2014-07-10 23:56:24,943 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/d2a7acc88387401ab70bde0ecf75e909, isReference=false, isBulkLoadResult=false, seqid=2625, majorCompaction=false
2014-07-10 23:56:24,944 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/de197ac2ceef4c9fb0ebd0c517468a93, isReference=false, isBulkLoadResult=false, seqid=3260, majorCompaction=false
2014-07-10 23:56:24,944 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/8d0d734911504a919f0947844be3636b, isReference=false, isBulkLoadResult=false, seqid=1208, majorCompaction=true
2014-07-10 23:56:24,964 DEBUG [StoreOpener-283e6ac382eb78b85dc3bc8a3376765d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d/family/ec09b0f8f4ad4b2eb3c3f90bfc799755, isReference=false, isBulkLoadResult=false, seqid=2703, majorCompaction=false
2014-07-10 23:56:24,968 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/9e58203dbc70474f8315b6d8e6108b10, isReference=false, isBulkLoadResult=false, seqid=3273, majorCompaction=false
2014-07-10 23:56:24,968 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/283e6ac382eb78b85dc3bc8a3376765d
2014-07-10 23:56:24,977 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/d82bfa9e0b3841e28f67c3b76744ee72, isReference=false, isBulkLoadResult=false, seqid=2203, majorCompaction=false
2014-07-10 23:56:24,978 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 283e6ac382eb78b85dc3bc8a3376765d; next sequenceid=3261
2014-07-10 23:56:24,978 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 283e6ac382eb78b85dc3bc8a3376765d
2014-07-10 23:56:24,981 INFO  [PostOpenDeployTasks:283e6ac382eb78b85dc3bc8a3376765d] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:24,982 DEBUG [PostOpenDeployTasks:283e6ac382eb78b85dc3bc8a3376765d] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 23:56:24,992 INFO  [PostOpenDeployTasks:283e6ac382eb78b85dc3bc8a3376765d] catalog.MetaEditor: Updated row usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d. with server=slave1,60020,1405061745702
2014-07-10 23:56:24,992 INFO  [PostOpenDeployTasks:283e6ac382eb78b85dc3bc8a3376765d] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:24,994 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 283e6ac382eb78b85dc3bc8a3376765d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,996 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/a453efd9335745d699d2fe3ac8def899, isReference=false, isBulkLoadResult=false, seqid=2071, majorCompaction=false
2014-07-10 23:56:24,999 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 283e6ac382eb78b85dc3bc8a3376765d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:24,999 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 283e6ac382eb78b85dc3bc8a3376765d to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:24,999 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d. on slave1,60020,1405061745702
2014-07-10 23:56:25,018 DEBUG [StoreOpener-d7b363608a43906e9988c93f25f91d4f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f/family/fdbd577989bc416abd30e1ab4631a068, isReference=false, isBulkLoadResult=false, seqid=2457, majorCompaction=false
2014-07-10 23:56:25,033 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/d7b363608a43906e9988c93f25f91d4f
2014-07-10 23:56:25,038 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/e28bee025cd345048643a2b595e3faba, isReference=false, isBulkLoadResult=false, seqid=3007, majorCompaction=false
2014-07-10 23:56:25,041 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined d7b363608a43906e9988c93f25f91d4f; next sequenceid=3275
2014-07-10 23:56:25,041 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node d7b363608a43906e9988c93f25f91d4f
2014-07-10 23:56:25,043 INFO  [PostOpenDeployTasks:d7b363608a43906e9988c93f25f91d4f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:25,043 DEBUG [PostOpenDeployTasks:d7b363608a43906e9988c93f25f91d4f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-10 23:56:25,051 INFO  [PostOpenDeployTasks:d7b363608a43906e9988c93f25f91d4f] catalog.MetaEditor: Updated row usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f. with server=slave1,60020,1405061745702
2014-07-10 23:56:25,051 INFO  [PostOpenDeployTasks:d7b363608a43906e9988c93f25f91d4f] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:25,052 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d7b363608a43906e9988c93f25f91d4f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:25,055 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d7b363608a43906e9988c93f25f91d4f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:25,055 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned d7b363608a43906e9988c93f25f91d4f to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:25,055 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f. on slave1,60020,1405061745702
2014-07-10 23:56:25,104 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/e53671c5610843bc87902347e330f8af, isReference=false, isBulkLoadResult=false, seqid=1376, majorCompaction=false
2014-07-10 23:56:25,116 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/eb38687f909c4019a8fdf4cd9683efa7, isReference=false, isBulkLoadResult=false, seqid=3177, majorCompaction=false
2014-07-10 23:56:25,134 DEBUG [StoreOpener-b0645baeeaeff5e861eee6d3147eabd0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0/family/fe80efad523345b1b98c19fcd5ef7241, isReference=false, isBulkLoadResult=false, seqid=2672, majorCompaction=false
2014-07-10 23:56:25,136 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b0645baeeaeff5e861eee6d3147eabd0
2014-07-10 23:56:25,138 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined b0645baeeaeff5e861eee6d3147eabd0; next sequenceid=3274
2014-07-10 23:56:25,138 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b0645baeeaeff5e861eee6d3147eabd0
2014-07-10 23:56:25,140 INFO  [PostOpenDeployTasks:b0645baeeaeff5e861eee6d3147eabd0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:25,140 DEBUG [PostOpenDeployTasks:b0645baeeaeff5e861eee6d3147eabd0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-10 23:56:25,148 INFO  [PostOpenDeployTasks:b0645baeeaeff5e861eee6d3147eabd0] catalog.MetaEditor: Updated row usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0. with server=slave1,60020,1405061745702
2014-07-10 23:56:25,148 INFO  [PostOpenDeployTasks:b0645baeeaeff5e861eee6d3147eabd0] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:25,149 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b0645baeeaeff5e861eee6d3147eabd0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:25,152 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b0645baeeaeff5e861eee6d3147eabd0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 23:56:25,152 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned b0645baeeaeff5e861eee6d3147eabd0 to OPENED in zk on slave1,60020,1405061745702
2014-07-10 23:56:25,152 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0. on slave1,60020,1405061745702
2014-07-10 23:56:29,398 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-10 23:56:29,398 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-10 23:56:29,399 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-10 23:56:52,414 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close 3d72d54e8219504183264cf4b80647ff, via zk=yes, znode version=0, on null
2014-07-10 23:56:52,415 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close d7b363608a43906e9988c93f25f91d4f, via zk=yes, znode version=0, on null
2014-07-10 23:56:52,416 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close b0645baeeaeff5e861eee6d3147eabd0, via zk=yes, znode version=0, on null
2014-07-10 23:56:52,417 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 1415efc4609fde0f73340d69f18de237, via zk=yes, znode version=0, on null
2014-07-10 23:56:52,418 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:52,419 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close 283e6ac382eb78b85dc3bc8a3376765d, via zk=yes, znode version=0, on null
2014-07-10 23:56:52,419 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:52,420 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:52,421 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.: disabling compactions & flushes
2014-07-10 23:56:52,422 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.: disabling compactions & flushes
2014-07-10 23:56:52,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.: disabling compactions & flushes
2014-07-10 23:56:52,423 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:52,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:52,423 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:52,425 INFO  [StoreCloserThread-usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.-1] regionserver.HStore: Closed family
2014-07-10 23:56:52,429 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:52,429 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d72d54e8219504183264cf4b80647ff from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,437 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d72d54e8219504183264cf4b80647ff from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,437 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff. on slave1,60020,1405061745702
2014-07-10 23:56:52,437 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,,1405061178629.3d72d54e8219504183264cf4b80647ff.
2014-07-10 23:56:52,438 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:52,439 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.: disabling compactions & flushes
2014-07-10 23:56:52,439 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:52,460 INFO  [StoreCloserThread-usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.-1] regionserver.HStore: Closed family
2014-07-10 23:56:52,460 INFO  [StoreCloserThread-usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.-1] regionserver.HStore: Closed family
2014-07-10 23:56:52,460 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:52,461 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d7b363608a43906e9988c93f25f91d4f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,461 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:52,461 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b0645baeeaeff5e861eee6d3147eabd0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d7b363608a43906e9988c93f25f91d4f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f. on slave1,60020,1405061745702
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f.
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b0645baeeaeff5e861eee6d3147eabd0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0. on slave1,60020,1405061745702
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:52,464 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0.
2014-07-10 23:56:52,466 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.: disabling compactions & flushes
2014-07-10 23:56:52,466 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:52,468 INFO  [StoreCloserThread-usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.-1] regionserver.HStore: Closed family
2014-07-10 23:56:52,468 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:52,468 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 283e6ac382eb78b85dc3bc8a3376765d from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,472 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 283e6ac382eb78b85dc3bc8a3376765d from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,472 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d. on slave1,60020,1405061745702
2014-07-10 23:56:52,472 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d.
2014-07-10 23:56:52,607 INFO  [regionserver60020-smallCompactions-1405061784729] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 23:56:52,608 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:52,613 INFO  [StoreCloserThread-usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.-1] regionserver.HStore: Closed family
2014-07-10 23:56:52,614 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-10 23:56:52,615 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1415efc4609fde0f73340d69f18de237 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,616 INFO  [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237., storeName=family, fileCount=6, fileSize=358.2m, priority=13, time=15692160668590; duration=27sec
2014-07-10 23:56:52,616 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-10 23:56:52,617 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d. because compaction request was cancelled
2014-07-10 23:56:52,617 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405061178629.283e6ac382eb78b85dc3bc8a3376765d. because compaction request was cancelled
2014-07-10 23:56:52,617 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f. because compaction request was cancelled
2014-07-10 23:56:52,617 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0. because compaction request was cancelled
2014-07-10 23:56:52,618 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405061178629.b0645baeeaeff5e861eee6d3147eabd0. because compaction request was cancelled
2014-07-10 23:56:52,618 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405061178629.d7b363608a43906e9988c93f25f91d4f. because compaction request was cancelled
2014-07-10 23:56:52,620 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1415efc4609fde0f73340d69f18de237 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 23:56:52,620 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237. on slave1,60020,1405061745702
2014-07-10 23:56:52,620 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user5,1405061178629.1415efc4609fde0f73340d69f18de237.
2014-07-11 00:00:45,791 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=7062, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-11 00:02:02,518 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:02,530 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:02,530 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b5cb06e5da6171e7af34144523ae71d0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,531 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:02:02,532 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3e650a1993bb1a49e183adcbb9770b96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,532 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:02:02,533 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1231dfaa320930fef79c95c3c97a0c48 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,533 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Open usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:02:02,552 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b5cb06e5da6171e7af34144523ae71d0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,552 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3e650a1993bb1a49e183adcbb9770b96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,552 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1231dfaa320930fef79c95c3c97a0c48 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,552 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => b5cb06e5da6171e7af34144523ae71d0, NAME => 'usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-11 00:02:02,553 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 3e650a1993bb1a49e183adcbb9770b96, NAME => 'usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-11 00:02:02,553 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 1231dfaa320930fef79c95c3c97a0c48, NAME => 'usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b5cb06e5da6171e7af34144523ae71d0
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3e650a1993bb1a49e183adcbb9770b96
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 1231dfaa320930fef79c95c3c97a0c48
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:02,554 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:02:02,577 INFO  [StoreOpener-b5cb06e5da6171e7af34144523ae71d0-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:02:02,579 INFO  [StoreOpener-3e650a1993bb1a49e183adcbb9770b96-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:02:02,579 INFO  [StoreOpener-1231dfaa320930fef79c95c3c97a0c48-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:02:02,582 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0
2014-07-11 00:02:02,583 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96
2014-07-11 00:02:02,584 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48
2014-07-11 00:02:02,586 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined b5cb06e5da6171e7af34144523ae71d0; next sequenceid=1
2014-07-11 00:02:02,586 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b5cb06e5da6171e7af34144523ae71d0
2014-07-11 00:02:02,586 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 3e650a1993bb1a49e183adcbb9770b96; next sequenceid=1
2014-07-11 00:02:02,586 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3e650a1993bb1a49e183adcbb9770b96
2014-07-11 00:02:02,588 INFO  [PostOpenDeployTasks:b5cb06e5da6171e7af34144523ae71d0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:02,588 INFO  [PostOpenDeployTasks:3e650a1993bb1a49e183adcbb9770b96] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:02,601 INFO  [PostOpenDeployTasks:3e650a1993bb1a49e183adcbb9770b96] catalog.MetaEditor: Updated row usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. with server=slave1,60020,1405061745702
2014-07-11 00:02:02,601 INFO  [PostOpenDeployTasks:3e650a1993bb1a49e183adcbb9770b96] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:02,603 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3e650a1993bb1a49e183adcbb9770b96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,603 INFO  [PostOpenDeployTasks:b5cb06e5da6171e7af34144523ae71d0] catalog.MetaEditor: Updated row usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. with server=slave1,60020,1405061745702
2014-07-11 00:02:02,603 INFO  [PostOpenDeployTasks:b5cb06e5da6171e7af34144523ae71d0] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:02,605 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b5cb06e5da6171e7af34144523ae71d0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,607 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3e650a1993bb1a49e183adcbb9770b96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,607 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 3e650a1993bb1a49e183adcbb9770b96 to OPENED in zk on slave1,60020,1405061745702
2014-07-11 00:02:02,607 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. on slave1,60020,1405061745702
2014-07-11 00:02:02,608 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a6b9b2a8688211770d19e0dceb93c590 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,609 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b5cb06e5da6171e7af34144523ae71d0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,609 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned b5cb06e5da6171e7af34144523ae71d0 to OPENED in zk on slave1,60020,1405061745702
2014-07-11 00:02:02,609 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. on slave1,60020,1405061745702
2014-07-11 00:02:02,610 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d3859e2dcf154a4a686686041e068bf5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,612 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a6b9b2a8688211770d19e0dceb93c590 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,613 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => a6b9b2a8688211770d19e0dceb93c590, NAME => 'usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-11 00:02:02,614 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d3859e2dcf154a4a686686041e068bf5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-11 00:02:02,614 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => d3859e2dcf154a4a686686041e068bf5, NAME => 'usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-11 00:02:02,614 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:02:02,614 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:02:02,615 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable d3859e2dcf154a4a686686041e068bf5
2014-07-11 00:02:02,615 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:02:02,617 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 1231dfaa320930fef79c95c3c97a0c48; next sequenceid=1
2014-07-11 00:02:02,617 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1231dfaa320930fef79c95c3c97a0c48
2014-07-11 00:02:02,619 INFO  [PostOpenDeployTasks:1231dfaa320930fef79c95c3c97a0c48] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:02:02,627 INFO  [StoreOpener-d3859e2dcf154a4a686686041e068bf5-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:02:02,627 INFO  [PostOpenDeployTasks:1231dfaa320930fef79c95c3c97a0c48] catalog.MetaEditor: Updated row usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. with server=slave1,60020,1405061745702
2014-07-11 00:02:02,628 INFO  [PostOpenDeployTasks:1231dfaa320930fef79c95c3c97a0c48] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:02:02,629 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1231dfaa320930fef79c95c3c97a0c48 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,632 INFO  [StoreOpener-a6b9b2a8688211770d19e0dceb93c590-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-11 00:02:02,636 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1231dfaa320930fef79c95c3c97a0c48 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,636 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 1231dfaa320930fef79c95c3c97a0c48 to OPENED in zk on slave1,60020,1405061745702
2014-07-11 00:02:02,636 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. on slave1,60020,1405061745702
2014-07-11 00:02:02,639 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5
2014-07-11 00:02:02,640 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:02:02,644 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined d3859e2dcf154a4a686686041e068bf5; next sequenceid=1
2014-07-11 00:02:02,644 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node d3859e2dcf154a4a686686041e068bf5
2014-07-11 00:02:02,646 INFO  [PostOpenDeployTasks:d3859e2dcf154a4a686686041e068bf5] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:02:02,656 INFO  [PostOpenDeployTasks:d3859e2dcf154a4a686686041e068bf5] catalog.MetaEditor: Updated row usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. with server=slave1,60020,1405061745702
2014-07-11 00:02:02,656 INFO  [PostOpenDeployTasks:d3859e2dcf154a4a686686041e068bf5] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:02:02,657 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning d3859e2dcf154a4a686686041e068bf5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,662 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node d3859e2dcf154a4a686686041e068bf5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,662 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned d3859e2dcf154a4a686686041e068bf5 to OPENED in zk on slave1,60020,1405061745702
2014-07-11 00:02:02,662 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. on slave1,60020,1405061745702
2014-07-11 00:02:02,681 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined a6b9b2a8688211770d19e0dceb93c590; next sequenceid=1
2014-07-11 00:02:02,681 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:02:02,683 INFO  [PostOpenDeployTasks:a6b9b2a8688211770d19e0dceb93c590] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:02:02,691 INFO  [PostOpenDeployTasks:a6b9b2a8688211770d19e0dceb93c590] catalog.MetaEditor: Updated row usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. with server=slave1,60020,1405061745702
2014-07-11 00:02:02,691 INFO  [PostOpenDeployTasks:a6b9b2a8688211770d19e0dceb93c590] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:02:02,693 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a6b9b2a8688211770d19e0dceb93c590 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,698 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x1472434cb3c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a6b9b2a8688211770d19e0dceb93c590 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-11 00:02:02,698 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned a6b9b2a8688211770d19e0dceb93c590 to OPENED in zk on slave1,60020,1405061745702
2014-07-11 00:02:02,698 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. on slave1,60020,1405061745702
2014-07-11 00:02:21,621 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:21,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92 synced till here 86
2014-07-11 00:02:21,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405061779266 with entries=92, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062141621
2014-07-11 00:02:23,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:23,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 199 synced till here 185
2014-07-11 00:02:23,892 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062141621 with entries=107, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062143500
2014-07-11 00:02:25,416 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:25,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 322 synced till here 297
2014-07-11 00:02:25,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062143500 with entries=123, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062145417
2014-07-11 00:02:27,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:27,815 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 439 synced till here 410
2014-07-11 00:02:28,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062145417 with entries=117, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062147657
2014-07-11 00:02:30,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:30,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 548 synced till here 530
2014-07-11 00:02:31,168 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062147657 with entries=109, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062150400
2014-07-11 00:02:33,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:33,640 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 679 synced till here 653
2014-07-11 00:02:33,949 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062150400 with entries=131, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062153473
2014-07-11 00:02:35,062 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:35,077 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 256.7m
2014-07-11 00:02:36,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:36,415 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 811 synced till here 779
2014-07-11 00:02:36,819 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:36,820 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 256.1m
2014-07-11 00:02:36,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062153473 with entries=132, filesize=99.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062156212
2014-07-11 00:02:37,079 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:37,636 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:02:38,389 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:38,393 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-11 00:02:38,393 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-11 00:02:38,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:38,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 953 synced till here 918
2014-07-11 00:02:39,075 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:02:39,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062156212 with entries=142, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062158421
2014-07-11 00:02:41,691 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:41,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1075 synced till here 1053
2014-07-11 00:02:42,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062158421 with entries=122, filesize=95.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062161691
2014-07-11 00:02:43,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:43,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1179 synced till here 1177
2014-07-11 00:02:43,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062161691 with entries=104, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062163153
2014-07-11 00:02:44,137 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=219, memsize=65.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/35ed37915eff4500b8c9a7a317c7b94b
2014-07-11 00:02:44,153 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/35ed37915eff4500b8c9a7a317c7b94b as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/35ed37915eff4500b8c9a7a317c7b94b
2014-07-11 00:02:44,176 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/35ed37915eff4500b8c9a7a317c7b94b, entries=239270, sequenceid=219, filesize=17.0m
2014-07-11 00:02:44,177 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~263.0m/275827040, currentsize=106.3m/111481840 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 9099ms, sequenceid=219, compaction requested=false
2014-07-11 00:02:44,179 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 363.0m
2014-07-11 00:02:44,837 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:44,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1271 synced till here 1267
2014-07-11 00:02:45,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062163153 with entries=92, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062164837
2014-07-11 00:02:45,034 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:45,173 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=172, memsize=52.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/56d74a0627ee4cb4a8ad18c05686feec
2014-07-11 00:02:45,189 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/56d74a0627ee4cb4a8ad18c05686feec as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/56d74a0627ee4cb4a8ad18c05686feec
2014-07-11 00:02:45,205 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/56d74a0627ee4cb4a8ad18c05686feec, entries=191210, sequenceid=172, filesize=13.6m
2014-07-11 00:02:45,205 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.6m/275352400, currentsize=116.0m/121635440 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 8385ms, sequenceid=172, compaction requested=false
2014-07-11 00:02:45,206 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 372.8m
2014-07-11 00:02:46,111 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:46,489 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:46,512 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1362 synced till here 1360
2014-07-11 00:02:46,679 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062164837 with entries=91, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062166490
2014-07-11 00:02:48,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:48,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1442 synced till here 1437
2014-07-11 00:02:48,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062166490 with entries=80, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062168279
2014-07-11 00:02:50,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:50,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1543 synced till here 1530
2014-07-11 00:02:50,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062168279 with entries=101, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062170258
2014-07-11 00:02:52,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:52,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1642 synced till here 1629
2014-07-11 00:02:52,527 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062170258 with entries=99, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062172161
2014-07-11 00:02:53,829 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:02:54,012 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=240, memsize=52.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/57c667d25fd14a56828662ca5d35bf0b
2014-07-11 00:02:54,128 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/57c667d25fd14a56828662ca5d35bf0b as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/57c667d25fd14a56828662ca5d35bf0b
2014-07-11 00:02:54,140 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/57c667d25fd14a56828662ca5d35bf0b, entries=190370, sequenceid=240, filesize=13.6m
2014-07-11 00:02:54,141 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~366.2m/383950400, currentsize=143.6m/150573040 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 9961ms, sequenceid=240, compaction requested=false
2014-07-11 00:02:54,141 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 258.2m
2014-07-11 00:02:54,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:54,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1743 synced till here 1724
2014-07-11 00:02:55,123 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062172161 with entries=101, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062174471
2014-07-11 00:02:55,486 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:55,720 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:02:56,045 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=247, memsize=52.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/f742d21ff4384a40a4ab9c79986ae71b
2014-07-11 00:02:56,149 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/f742d21ff4384a40a4ab9c79986ae71b as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/f742d21ff4384a40a4ab9c79986ae71b
2014-07-11 00:02:56,164 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/f742d21ff4384a40a4ab9c79986ae71b, entries=189850, sequenceid=247, filesize=13.5m
2014-07-11 00:02:56,164 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~374.4m/392609520, currentsize=159.6m/167316880 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 10958ms, sequenceid=247, compaction requested=false
2014-07-11 00:02:56,165 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 260.9m
2014-07-11 00:02:56,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:02:57,091 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:02:59,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1967 synced till here 1956
2014-07-11 00:02:59,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062174471 with entries=224, filesize=164.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062176639
2014-07-11 00:03:00,947 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:03:01,917 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:01,936 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2062 synced till here 2043
2014-07-11 00:03:02,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062176639 with entries=95, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062181918
2014-07-11 00:03:16,910 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:03:16,972 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16682,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180229,"queuetimems":0,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-11 00:03:16,972 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181960,"queuetimems":0,"class":"HRegionServer","responsesize":68,"method":"Multi"}
2014-07-11 00:03:16,972 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14962,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181948,"queuetimems":0,"class":"HRegionServer","responsesize":207,"method":"Multi"}
2014-07-11 00:03:16,972 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062179384,"queuetimems":0,"class":"HRegionServer","responsesize":14999,"method":"Multi"}
2014-07-11 00:03:16,973 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 887 service: ClientService methodName: Multi size: 13.8k connection: 9.1.143.53:44796: output error
2014-07-11 00:03:16,974 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:16,974 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 819 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:16,990 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:16,990 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 885 service: ClientService methodName: Multi size: 42.5k connection: 9.1.143.53:44796: output error
2014-07-11 00:03:16,990 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:16,990 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 834 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:16,990 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:19,577 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=421, memsize=101.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/ec2446140294416495472fa05a0d923c
2014-07-11 00:03:19,589 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/ec2446140294416495472fa05a0d923c as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/ec2446140294416495472fa05a0d923c
2014-07-11 00:03:19,592 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=346, memsize=103.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/af17a17dd872451f9fb7d949e7cf3eec
2014-07-11 00:03:19,601 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/ec2446140294416495472fa05a0d923c, entries=370360, sequenceid=421, filesize=26.4m
2014-07-11 00:03:19,602 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.4m/275146320, currentsize=54.3m/56887840 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 23437ms, sequenceid=421, compaction requested=false
2014-07-11 00:03:19,603 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 278.2m
2014-07-11 00:03:19,603 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/af17a17dd872451f9fb7d949e7cf3eec as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/af17a17dd872451f9fb7d949e7cf3eec
2014-07-11 00:03:19,613 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/af17a17dd872451f9fb7d949e7cf3eec, entries=375920, sequenceid=346, filesize=26.8m
2014-07-11 00:03:19,613 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~278.3m/291797280, currentsize=111.2m/116652320 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 25472ms, sequenceid=346, compaction requested=false
2014-07-11 00:03:19,613 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 261.7m
2014-07-11 00:03:19,663 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062179625,"queuetimems":0,"class":"HRegionServer","responsesize":20551,"method":"Multi"}
2014-07-11 00:03:19,663 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 816 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:19,663 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:19,673 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062179835,"queuetimems":0,"class":"HRegionServer","responsesize":13367,"method":"Multi"}
2014-07-11 00:03:19,674 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 813 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:19,674 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,019 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062179875,"queuetimems":1,"class":"HRegionServer","responsesize":16940,"method":"Multi"}
2014-07-11 00:03:20,020 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 825 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,020 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,025 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19611,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180414,"queuetimems":0,"class":"HRegionServer","responsesize":19196,"method":"Multi"}
2014-07-11 00:03:20,025 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19990,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180035,"queuetimems":1,"class":"HRegionServer","responsesize":19980,"method":"Multi"}
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 839 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 824 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19645,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180380,"queuetimems":0,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 835 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,025 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180199,"queuetimems":0,"class":"HRegionServer","responsesize":20351,"method":"Multi"}
2014-07-11 00:03:20,027 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 827 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,027 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,026 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,189 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:20,409 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:20,415 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:20,417 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19825,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180591,"queuetimems":1,"class":"HRegionServer","responsesize":20318,"method":"Multi"}
2014-07-11 00:03:20,418 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 845 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,418 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2186 synced till here 2146
2014-07-11 00:03:20,727 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180775,"queuetimems":0,"class":"HRegionServer","responsesize":16916,"method":"Multi"}
2014-07-11 00:03:20,727 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 851 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,727 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,969 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181147,"queuetimems":1,"class":"HRegionServer","responsesize":17178,"method":"Multi"}
2014-07-11 00:03:20,969 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181118,"queuetimems":1,"class":"HRegionServer","responsesize":16985,"method":"Multi"}
2014-07-11 00:03:20,969 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 863 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,969 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,969 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 860 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,970 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:20,978 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19789,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181183,"queuetimems":1,"class":"HRegionServer","responsesize":19237,"method":"Multi"}
2014-07-11 00:03:20,978 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 864 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:20,978 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:21,109 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062181918 with entries=124, filesize=98.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062200415
2014-07-11 00:03:21,892 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20963,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062180928,"queuetimems":1,"class":"HRegionServer","responsesize":19870,"method":"Multi"}
2014-07-11 00:03:21,892 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 856 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:21,892 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181786,"queuetimems":0,"class":"HRegionServer","responsesize":13367,"method":"Multi"}
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181570,"queuetimems":0,"class":"HRegionServer","responsesize":13231,"method":"Multi"}
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 875 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 874 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:21,893 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:21,913 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19576,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062182322,"queuetimems":0,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-11 00:03:21,913 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183775,"queuetimems":1,"class":"HRegionServer","responsesize":12850,"method":"Multi"}
2014-07-11 00:03:21,914 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 894 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:21,914 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:21,914 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 898 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:21,914 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:22,543 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183530,"queuetimems":0,"class":"HRegionServer","responsesize":20387,"method":"Multi"}
2014-07-11 00:03:22,543 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20188,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062182355,"queuetimems":0,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-11 00:03:22,544 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183459,"queuetimems":0,"class":"HRegionServer","responsesize":16209,"method":"Multi"}
2014-07-11 00:03:22,544 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 895 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,544 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,544 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181766,"queuetimems":0,"class":"HRegionServer","responsesize":19980,"method":"Multi"}
2014-07-11 00:03:22,544 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 902 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,545 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,546 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 901 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,546 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,546 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 876 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,546 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,547 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18824,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183722,"queuetimems":1,"class":"HRegionServer","responsesize":17136,"method":"Multi"}
2014-07-11 00:03:22,547 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 900 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,547 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,558 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19129,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183428,"queuetimems":0,"class":"HRegionServer","responsesize":14999,"method":"Multi"}
2014-07-11 00:03:22,558 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062182180,"queuetimems":0,"class":"HRegionServer","responsesize":20351,"method":"Multi"}
2014-07-11 00:03:22,558 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062181546,"queuetimems":0,"class":"HRegionServer","responsesize":16867,"method":"Multi"}
2014-07-11 00:03:22,558 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 897 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,559 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,559 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 873 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,559 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,559 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 889 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:22,560 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:22,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2289 synced till here 2267
2014-07-11 00:03:22,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062200415 with entries=103, filesize=87.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062202541
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183983,"queuetimems":0,"class":"HRegionServer","responsesize":16940,"method":"Multi"}
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062182553,"queuetimems":1,"class":"HRegionServer","responsesize":20381,"method":"Multi"}
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 903 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 896 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19742,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44796","starttimems":1405062183751,"queuetimems":1,"class":"HRegionServer","responsesize":20551,"method":"Multi"}
2014-07-11 00:03:23,493 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:23,494 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 899 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44796: output error
2014-07-11 00:03:23,494 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:03:24,935 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:24,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2395 synced till here 2385
2014-07-11 00:03:25,143 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062202541 with entries=106, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062204935
2014-07-11 00:03:27,218 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:03:27,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:27,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2483 synced till here 2479
2014-07-11 00:03:27,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062204935 with entries=88, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062207343
2014-07-11 00:03:28,568 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=407, memsize=112.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/607a4286dbdf4495ade473f786a43442
2014-07-11 00:03:28,660 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/607a4286dbdf4495ade473f786a43442 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/607a4286dbdf4495ade473f786a43442
2014-07-11 00:03:28,681 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=411, memsize=124.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7b329b2437ad429e9e3cdbafbbbcdaa6
2014-07-11 00:03:28,755 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/607a4286dbdf4495ade473f786a43442, entries=408750, sequenceid=407, filesize=29.1m
2014-07-11 00:03:28,756 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~265.6m/278451200, currentsize=155.0m/162541760 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 9143ms, sequenceid=407, compaction requested=false
2014-07-11 00:03:28,756 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 267.0m
2014-07-11 00:03:28,760 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7b329b2437ad429e9e3cdbafbbbcdaa6 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7b329b2437ad429e9e3cdbafbbbcdaa6
2014-07-11 00:03:28,794 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7b329b2437ad429e9e3cdbafbbbcdaa6, entries=453080, sequenceid=411, filesize=32.3m
2014-07-11 00:03:28,794 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~287.5m/301475200, currentsize=152.7m/160083760 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 9192ms, sequenceid=411, compaction requested=false
2014-07-11 00:03:29,050 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:32,149 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:32,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2579 synced till here 2561
2014-07-11 00:03:32,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062207343 with entries=96, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062212150
2014-07-11 00:03:35,000 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=498, memsize=80.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/ae1568b5c8cf4f76a10d50d63a4eefcf
2014-07-11 00:03:35,018 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/ae1568b5c8cf4f76a10d50d63a4eefcf as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/ae1568b5c8cf4f76a10d50d63a4eefcf
2014-07-11 00:03:35,029 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/ae1568b5c8cf4f76a10d50d63a4eefcf, entries=291690, sequenceid=498, filesize=20.8m
2014-07-11 00:03:35,029 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~267.0m/279958800, currentsize=27.9m/29277600 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 6273ms, sequenceid=498, compaction requested=true
2014-07-11 00:03:35,030 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:03:35,030 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 00:03:35,030 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 00:03:35,030 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:03:35,030 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:03:35,030 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:03:37,766 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:37,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2700 synced till here 2674
2014-07-11 00:03:38,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062212150 with entries=121, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062217767
2014-07-11 00:03:40,726 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:03:40,727 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 257.5m
2014-07-11 00:03:41,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:41,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2827 synced till here 2800
2014-07-11 00:03:41,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062217767 with entries=127, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062221207
2014-07-11 00:03:41,705 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:03:41,706 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 257.1m
2014-07-11 00:03:41,801 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:03:42,123 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:42,314 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:43,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:43,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2919 synced till here 2907
2014-07-11 00:03:43,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062221207 with entries=92, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062223110
2014-07-11 00:03:45,059 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:03:45,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:45,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3017 synced till here 3013
2014-07-11 00:03:45,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062223110 with entries=98, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062225113
2014-07-11 00:03:47,087 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=532, memsize=50.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/5dba4461ef524e468bb868403fb6b8b8
2014-07-11 00:03:47,103 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/5dba4461ef524e468bb868403fb6b8b8 as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/5dba4461ef524e468bb868403fb6b8b8
2014-07-11 00:03:47,117 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/5dba4461ef524e468bb868403fb6b8b8, entries=183430, sequenceid=532, filesize=13.1m
2014-07-11 00:03:47,118 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~263.7m/276541440, currentsize=33.2m/34841360 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 6391ms, sequenceid=532, compaction requested=false
2014-07-11 00:03:47,119 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 344.0m
2014-07-11 00:03:47,123 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:47,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3100 synced till here 3091
2014-07-11 00:03:47,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062225113 with entries=83, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062227123
2014-07-11 00:03:47,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405061779266
2014-07-11 00:03:47,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062141621
2014-07-11 00:03:47,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062143500
2014-07-11 00:03:47,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062145417
2014-07-11 00:03:47,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062147657
2014-07-11 00:03:47,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062150400
2014-07-11 00:03:47,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062153473
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062156212
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062158421
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062161691
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062163153
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062164837
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062166490
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062168279
2014-07-11 00:03:47,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062170258
2014-07-11 00:03:47,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062172161
2014-07-11 00:03:48,477 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:50,109 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=550, memsize=87.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/675bcfea6b7c46459ec790fb80728dcc
2014-07-11 00:03:50,123 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/675bcfea6b7c46459ec790fb80728dcc as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/675bcfea6b7c46459ec790fb80728dcc
2014-07-11 00:03:50,141 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/675bcfea6b7c46459ec790fb80728dcc, entries=318210, sequenceid=550, filesize=22.7m
2014-07-11 00:03:50,141 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.1m/269599520, currentsize=110.1m/115398720 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 8436ms, sequenceid=550, compaction requested=true
2014-07-11 00:03:50,141 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:03:50,141 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 00:03:50,141 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 00:03:50,142 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:03:50,142 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 309.9m
2014-07-11 00:03:50,142 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:03:50,142 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:03:50,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:50,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3218 synced till here 3196
2014-07-11 00:03:51,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062227123 with entries=118, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062230315
2014-07-11 00:03:51,433 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:51,737 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:03:52,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:52,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3313 synced till here 3300
2014-07-11 00:03:53,324 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062230315 with entries=95, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062232848
2014-07-11 00:03:56,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:56,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3424 synced till here 3393
2014-07-11 00:03:56,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062232848 with entries=111, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062236158
2014-07-11 00:03:57,047 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=598, memsize=99.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/57e2e64761cd470880f1e9dc7adcd343
2014-07-11 00:03:57,083 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/57e2e64761cd470880f1e9dc7adcd343 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/57e2e64761cd470880f1e9dc7adcd343
2014-07-11 00:03:57,107 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/57e2e64761cd470880f1e9dc7adcd343, entries=362750, sequenceid=598, filesize=25.9m
2014-07-11 00:03:57,107 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~350.1m/367145280, currentsize=112.3m/117802640 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 9988ms, sequenceid=598, compaction requested=true
2014-07-11 00:03:57,108 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:03:57,108 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 00:03:57,108 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 00:03:57,108 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:03:57,108 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:03:57,108 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 324.3m
2014-07-11 00:03:57,108 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:03:58,861 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:03:59,705 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:03:59,792 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3509 synced till here 3497
2014-07-11 00:04:00,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062236158 with entries=85, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062239705
2014-07-11 00:04:00,292 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=773, memsize=136.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/657b4f0af450468c819ee6e5d29deb80
2014-07-11 00:04:00,318 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/657b4f0af450468c819ee6e5d29deb80 as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/657b4f0af450468c819ee6e5d29deb80
2014-07-11 00:04:00,336 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/657b4f0af450468c819ee6e5d29deb80, entries=495570, sequenceid=773, filesize=35.3m
2014-07-11 00:04:00,336 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~321.2m/336752320, currentsize=53.6m/56175680 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 10194ms, sequenceid=773, compaction requested=true
2014-07-11 00:04:00,337 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:00,337 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 00:04:00,337 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 00:04:00,337 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:00,337 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:00,337 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:04:00,700 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:04:00,705 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 261.0m
2014-07-11 00:04:01,359 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:01,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:01,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062239705 with entries=89, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062241624
2014-07-11 00:04:01,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062174471
2014-07-11 00:04:01,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062176639
2014-07-11 00:04:01,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062181918
2014-07-11 00:04:01,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062200415
2014-07-11 00:04:01,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062202541
2014-07-11 00:04:01,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062204935
2014-07-11 00:04:03,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:04,035 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3680 synced till here 3671
2014-07-11 00:04:04,308 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=680, memsize=85.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/9f9436733a43450b900f5a34a6b14015
2014-07-11 00:04:04,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062241624 with entries=82, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062243927
2014-07-11 00:04:04,351 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/9f9436733a43450b900f5a34a6b14015 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/9f9436733a43450b900f5a34a6b14015
2014-07-11 00:04:04,378 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/9f9436733a43450b900f5a34a6b14015, entries=310800, sequenceid=680, filesize=22.2m
2014-07-11 00:04:04,378 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~343.2m/359865680, currentsize=72.1m/75582400 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 7270ms, sequenceid=680, compaction requested=true
2014-07-11 00:04:04,379 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:04,379 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 00:04:04,379 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 00:04:04,379 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:04,379 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:04,379 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:04:06,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:06,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3756 synced till here 3752
2014-07-11 00:04:06,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062243927 with entries=76, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062246338
2014-07-11 00:04:06,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062207343
2014-07-11 00:04:06,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062212150
2014-07-11 00:04:06,703 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=696, memsize=85.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/1c3599bf9acf4cf789c197c595c707a5
2014-07-11 00:04:06,726 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/1c3599bf9acf4cf789c197c595c707a5 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/1c3599bf9acf4cf789c197c595c707a5
2014-07-11 00:04:06,739 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/1c3599bf9acf4cf789c197c595c707a5, entries=311530, sequenceid=696, filesize=22.2m
2014-07-11 00:04:06,739 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~276.0m/289362720, currentsize=70.6m/74057680 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 6034ms, sequenceid=696, compaction requested=true
2014-07-11 00:04:06,740 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:06,740 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 00:04:06,740 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 00:04:06,740 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:06,740 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:06,740 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:04:07,648 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:04:07,649 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 257.4m
2014-07-11 00:04:08,433 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:08,951 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:08,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3834 synced till here 3825
2014-07-11 00:04:09,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062246338 with entries=78, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062248951
2014-07-11 00:04:11,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:12,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3938 synced till here 3937
2014-07-11 00:04:12,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062248951 with entries=104, filesize=87.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062251372
2014-07-11 00:04:14,363 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=736, memsize=98.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/503924f26e4e45f6be739c693eb32b3d
2014-07-11 00:04:14,408 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/503924f26e4e45f6be739c693eb32b3d as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/503924f26e4e45f6be739c693eb32b3d
2014-07-11 00:04:14,428 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/503924f26e4e45f6be739c693eb32b3d, entries=359450, sequenceid=736, filesize=25.6m
2014-07-11 00:04:14,429 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.8m/274539360, currentsize=74.7m/78293840 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 6780ms, sequenceid=736, compaction requested=true
2014-07-11 00:04:14,430 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:14,430 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 00:04:14,430 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 00:04:14,430 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:14,431 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:14,431 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:04:15,276 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:15,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062251372 with entries=101, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062255277
2014-07-11 00:04:20,644 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:20,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4116 synced till here 4109
2014-07-11 00:04:21,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062255277 with entries=77, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062260645
2014-07-11 00:04:23,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:23,593 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:04:23,593 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 257.3m
2014-07-11 00:04:23,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4200 synced till here 4198
2014-07-11 00:04:23,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062260645 with entries=84, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062263460
2014-07-11 00:04:24,101 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:26,346 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:04:26,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:26,346 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 257.1m
2014-07-11 00:04:26,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4278 synced till here 4277
2014-07-11 00:04:26,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062263460 with entries=78, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062266346
2014-07-11 00:04:26,558 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:30,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:30,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4388 synced till here 4383
2014-07-11 00:04:30,332 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:04:30,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062266346 with entries=110, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062270193
2014-07-11 00:04:32,458 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=823, memsize=192.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/e7e8cba3bd5f4866bf12a31de5d25e85
2014-07-11 00:04:32,476 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/e7e8cba3bd5f4866bf12a31de5d25e85 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/e7e8cba3bd5f4866bf12a31de5d25e85
2014-07-11 00:04:32,586 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/e7e8cba3bd5f4866bf12a31de5d25e85, entries=701080, sequenceid=823, filesize=50.0m
2014-07-11 00:04:32,587 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~262.3m/275014080, currentsize=78.4m/82256720 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 8994ms, sequenceid=823, compaction requested=true
2014-07-11 00:04:32,587 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:32,587 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 00:04:32,588 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 00:04:32,588 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 272.1m
2014-07-11 00:04:32,588 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:32,588 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:32,588 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:04:33,383 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:33,499 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:33,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4479 synced till here 4458
2014-07-11 00:04:33,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062270193 with entries=91, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062273383
2014-07-11 00:04:35,500 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=840, memsize=211.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/065316e8ca8e41c98c78603bd11aec5e
2014-07-11 00:04:35,524 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/065316e8ca8e41c98c78603bd11aec5e as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/065316e8ca8e41c98c78603bd11aec5e
2014-07-11 00:04:35,537 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/065316e8ca8e41c98c78603bd11aec5e, entries=768550, sequenceid=840, filesize=54.8m
2014-07-11 00:04:35,538 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.6m/275376640, currentsize=78.8m/82656800 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 9192ms, sequenceid=840, compaction requested=true
2014-07-11 00:04:35,538 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:35,538 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 00:04:35,538 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 00:04:35,538 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:35,538 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:35,539 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:04:38,791 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:04:38,798 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 256.3m
2014-07-11 00:04:39,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:39,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4597 synced till here 4584
2014-07-11 00:04:40,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062273383 with entries=118, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062279765
2014-07-11 00:04:40,355 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:42,076 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:42,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4685 synced till here 4674
2014-07-11 00:04:42,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062279765 with entries=88, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062282077
2014-07-11 00:04:44,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:44,620 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4795 synced till here 4780
2014-07-11 00:04:44,822 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=987, memsize=213.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/568dba2d04634ecf82e07af1fcaaeafa
2014-07-11 00:04:44,962 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/568dba2d04634ecf82e07af1fcaaeafa as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/568dba2d04634ecf82e07af1fcaaeafa
2014-07-11 00:04:44,988 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/568dba2d04634ecf82e07af1fcaaeafa, entries=776180, sequenceid=987, filesize=55.3m
2014-07-11 00:04:44,988 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~281.8m/295474800, currentsize=109.5m/114828800 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 12400ms, sequenceid=987, compaction requested=true
2014-07-11 00:04:44,989 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:44,989 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 00:04:44,989 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 00:04:44,989 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:44,989 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:44,989 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:04:45,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062282077 with entries=110, filesize=80.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062284599
2014-07-11 00:04:47,630 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:47,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4954 synced till here 4916
2014-07-11 00:04:48,599 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062284599 with entries=159, filesize=100.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062287630
2014-07-11 00:04:49,652 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:04:49,653 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 260.0m
2014-07-11 00:04:50,618 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:04:50,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:50,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5063 synced till here 5046
2014-07-11 00:04:51,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062287630 with entries=109, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062290790
2014-07-11 00:04:52,685 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:52,686 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:04:52,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5166 synced till here 5148
2014-07-11 00:04:52,971 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062290790 with entries=103, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062292685
2014-07-11 00:04:55,123 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:55,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5291 synced till here 5260
2014-07-11 00:04:55,575 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:04:56,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062292685 with entries=125, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062295124
2014-07-11 00:04:57,420 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:04:58,126 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:04:58,908 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=893, memsize=216.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/086557e2fffa41c290456639d757d624
2014-07-11 00:04:58,927 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/086557e2fffa41c290456639d757d624 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/086557e2fffa41c290456639d757d624
2014-07-11 00:04:58,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5474 synced till here 5441
2014-07-11 00:04:59,010 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/086557e2fffa41c290456639d757d624, entries=789560, sequenceid=893, filesize=56.3m
2014-07-11 00:04:59,010 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.1m/273744080, currentsize=288.3m/302272880 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 20212ms, sequenceid=893, compaction requested=true
2014-07-11 00:04:59,011 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:04:59,011 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 00:04:59,011 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 00:04:59,011 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 337.0m
2014-07-11 00:04:59,011 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:04:59,011 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:04:59,012 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:04:59,245 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:04:59,357 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062295124 with entries=183, filesize=138.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062298126
2014-07-11 00:05:00,418 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:01,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:01,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5617 synced till here 5578
2014-07-11 00:05:01,665 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=988, memsize=129.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/da36da6df34647399ffe62283514bded
2014-07-11 00:05:01,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062298126 with entries=143, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062301346
2014-07-11 00:05:01,682 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/da36da6df34647399ffe62283514bded as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/da36da6df34647399ffe62283514bded
2014-07-11 00:05:01,699 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/da36da6df34647399ffe62283514bded, entries=472780, sequenceid=988, filesize=33.7m
2014-07-11 00:05:01,699 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~273.7m/286979680, currentsize=189.6m/198774800 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 12046ms, sequenceid=988, compaction requested=true
2014-07-11 00:05:01,700 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:01,700 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 00:05:01,700 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 00:05:01,700 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:01,700 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 292.8m
2014-07-11 00:05:01,700 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:01,700 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:05:02,178 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:03,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:03,221 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5727 synced till here 5722
2014-07-11 00:05:03,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062301346 with entries=110, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062303198
2014-07-11 00:05:05,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:05,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062303198 with entries=93, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062305337
2014-07-11 00:05:06,772 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:05:07,446 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:07,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5918 synced till here 5912
2014-07-11 00:05:07,496 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062305337 with entries=98, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062307446
2014-07-11 00:05:09,338 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1063, memsize=108.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/6809699dfcb142c9a5c175d4460a4ab8
2014-07-11 00:05:09,372 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/6809699dfcb142c9a5c175d4460a4ab8 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/6809699dfcb142c9a5c175d4460a4ab8
2014-07-11 00:05:09,398 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/6809699dfcb142c9a5c175d4460a4ab8, entries=394050, sequenceid=1063, filesize=28.1m
2014-07-11 00:05:09,398 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~374.6m/392786400, currentsize=161.8m/169617920 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 10387ms, sequenceid=1063, compaction requested=true
2014-07-11 00:05:09,399 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:09,399 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 00:05:09,399 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 00:05:09,399 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:09,399 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 384.3m
2014-07-11 00:05:09,399 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:09,399 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:05:09,642 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:09,692 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5999 synced till here 5991
2014-07-11 00:05:09,855 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062307446 with entries=81, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062309642
2014-07-11 00:05:09,978 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1074, memsize=105.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/8ea4052b1b7144129c4e836a7c619b92
2014-07-11 00:05:09,995 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/8ea4052b1b7144129c4e836a7c619b92 as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/8ea4052b1b7144129c4e836a7c619b92
2014-07-11 00:05:10,008 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/8ea4052b1b7144129c4e836a7c619b92, entries=384070, sequenceid=1074, filesize=27.4m
2014-07-11 00:05:10,008 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~292.8m/307040560, currentsize=40.7m/42628320 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 8308ms, sequenceid=1074, compaction requested=false
2014-07-11 00:05:10,009 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 461.2m
2014-07-11 00:05:10,433 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:10,806 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:12,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:13,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6123 synced till here 6110
2014-07-11 00:05:13,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062309642 with entries=124, filesize=103.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062312151
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062217767
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062221207
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062223110
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062225113
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062227123
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062230315
2014-07-11 00:05:13,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062232848
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062236158
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062239705
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062241624
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062243927
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062246338
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062248951
2014-07-11 00:05:13,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062251372
2014-07-11 00:05:13,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062255277
2014-07-11 00:05:13,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062260645
2014-07-11 00:05:13,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062263460
2014-07-11 00:05:13,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062266346
2014-07-11 00:05:15,809 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:15,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6227 synced till here 6204
2014-07-11 00:05:16,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062312151 with entries=104, filesize=87.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062315815
2014-07-11 00:05:18,237 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:05:18,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:19,025 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6347 synced till here 6321
2014-07-11 00:05:19,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062315815 with entries=120, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062318715
2014-07-11 00:05:21,172 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:21,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6470 synced till here 6432
2014-07-11 00:05:21,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062318715 with entries=123, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062321172
2014-07-11 00:05:23,778 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:23,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6587 synced till here 6565
2014-07-11 00:05:24,538 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062321172 with entries=117, filesize=97.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062323778
2014-07-11 00:05:26,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:26,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6695 synced till here 6661
2014-07-11 00:05:27,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062323778 with entries=108, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062326379
2014-07-11 00:05:28,201 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1150, memsize=134.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7bdbb61a645c49d49b5ac4da0a60c1f3
2014-07-11 00:05:28,238 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7bdbb61a645c49d49b5ac4da0a60c1f3 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7bdbb61a645c49d49b5ac4da0a60c1f3
2014-07-11 00:05:28,255 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7bdbb61a645c49d49b5ac4da0a60c1f3, entries=488290, sequenceid=1150, filesize=34.8m
2014-07-11 00:05:28,266 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~463.4m/485859360, currentsize=303.7m/318439280 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 18257ms, sequenceid=1150, compaction requested=true
2014-07-11 00:05:28,277 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:28,277 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 00:05:28,277 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 00:05:28,277 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 617.8m
2014-07-11 00:05:28,277 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:28,277 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:28,277 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:05:28,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:28,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6776 synced till here 6765
2014-07-11 00:05:28,727 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:05:28,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062326379 with entries=81, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062328574
2014-07-11 00:05:28,821 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1440, memsize=169.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/ed9a4b0760374f6885b306e53491c637
2014-07-11 00:05:28,950 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/ed9a4b0760374f6885b306e53491c637 as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/ed9a4b0760374f6885b306e53491c637
2014-07-11 00:05:28,962 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/ed9a4b0760374f6885b306e53491c637, entries=615430, sequenceid=1440, filesize=43.9m
2014-07-11 00:05:28,963 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~389.2m/408147120, currentsize=119.8m/125625200 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 19564ms, sequenceid=1440, compaction requested=true
2014-07-11 00:05:28,963 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 00:05:28,963 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 00:05:28,963 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:28,963 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:28,963 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:05:28,964 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:28,964 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 480.5m
2014-07-11 00:05:29,814 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:29,977 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:30,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:32,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6920 synced till here 6901
2014-07-11 00:05:33,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062328574 with entries=144, filesize=116.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062330534
2014-07-11 00:05:33,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062270193
2014-07-11 00:05:33,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062273383
2014-07-11 00:05:33,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062279765
2014-07-11 00:05:33,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062282077
2014-07-11 00:05:33,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062284599
2014-07-11 00:05:34,777 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:34,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062330534 with entries=87, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062334777
2014-07-11 00:05:37,065 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:37,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7088 synced till here 7082
2014-07-11 00:05:37,275 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062334777 with entries=81, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062337066
2014-07-11 00:05:39,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:39,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7165 synced till here 7160
2014-07-11 00:05:39,233 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062337066 with entries=77, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062339062
2014-07-11 00:05:40,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:40,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7252 synced till here 7249
2014-07-11 00:05:40,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062339062 with entries=87, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062340756
2014-07-11 00:05:42,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:42,663 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7347 synced till here 7342
2014-07-11 00:05:42,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062340756 with entries=95, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062342644
2014-07-11 00:05:44,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:44,549 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7444 synced till here 7439
2014-07-11 00:05:44,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062342644 with entries=97, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062344527
2014-07-11 00:05:44,906 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:05:45,794 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=7062, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-11 00:05:46,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:46,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7555 synced till here 7546
2014-07-11 00:05:46,986 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062344527 with entries=111, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062346699
2014-07-11 00:05:48,689 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1309, memsize=198.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/f8ce8bd8c75c4751a39ecbabc38b964f
2014-07-11 00:05:48,709 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/f8ce8bd8c75c4751a39ecbabc38b964f as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/f8ce8bd8c75c4751a39ecbabc38b964f
2014-07-11 00:05:48,710 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:48,831 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/f8ce8bd8c75c4751a39ecbabc38b964f, entries=723660, sequenceid=1309, filesize=51.6m
2014-07-11 00:05:48,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7637 synced till here 7630
2014-07-11 00:05:48,832 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~498.5m/522734320, currentsize=285.2m/299050800 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 19868ms, sequenceid=1309, compaction requested=true
2014-07-11 00:05:48,832 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:48,832 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:05:48,833 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 00:05:48,833 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:48,833 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 615.0m
2014-07-11 00:05:48,833 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:48,833 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:05:48,898 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:05:48,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062346699 with entries=82, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062348710
2014-07-11 00:05:50,952 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:51,059 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1310, memsize=197.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/60a80f01b0dd4caa94e0e8c6d3aeb014
2014-07-11 00:05:51,264 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/60a80f01b0dd4caa94e0e8c6d3aeb014 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/60a80f01b0dd4caa94e0e8c6d3aeb014
2014-07-11 00:05:51,288 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/60a80f01b0dd4caa94e0e8c6d3aeb014, entries=718710, sequenceid=1310, filesize=51.2m
2014-07-11 00:05:51,289 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~627.8m/658273600, currentsize=332.9m/349090960 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 23012ms, sequenceid=1310, compaction requested=true
2014-07-11 00:05:51,289 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:05:51,289 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:05:51,289 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 314.5m
2014-07-11 00:05:51,290 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 00:05:51,290 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:05:51,290 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:05:51,292 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:05:51,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:51,825 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:05:52,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7752 synced till here 7722
2014-07-11 00:05:52,633 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:05:52,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062348710 with entries=115, filesize=97.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062351825
2014-07-11 00:05:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062287630
2014-07-11 00:05:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062290790
2014-07-11 00:05:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062292685
2014-07-11 00:05:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062295124
2014-07-11 00:05:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062298126
2014-07-11 00:05:54,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:54,563 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7850 synced till here 7836
2014-07-11 00:05:54,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062351825 with entries=98, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062354375
2014-07-11 00:05:55,865 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:05:56,638 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:56,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7950 synced till here 7939
2014-07-11 00:05:57,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062354375 with entries=100, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062356638
2014-07-11 00:05:59,688 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:05:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8019 synced till here 8011
2014-07-11 00:06:00,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062356638 with entries=69, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062359689
2014-07-11 00:06:01,877 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:01,893 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8095 synced till here 8089
2014-07-11 00:06:01,971 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062359689 with entries=76, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062361877
2014-07-11 00:06:05,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:05,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8198 synced till here 8176
2014-07-11 00:06:05,879 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062361877 with entries=103, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062365198
2014-07-11 00:06:08,243 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:09,233 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8337 synced till here 8334
2014-07-11 00:06:09,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062365198 with entries=139, filesize=120.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062368244
2014-07-11 00:06:10,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:10,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8418 synced till here 8416
2014-07-11 00:06:10,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062368244 with entries=81, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062370775
2014-07-11 00:06:11,224 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1915, memsize=238.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/1a408f1148bb437f88a98254c4a385dc
2014-07-11 00:06:11,243 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/1a408f1148bb437f88a98254c4a385dc as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/1a408f1148bb437f88a98254c4a385dc
2014-07-11 00:06:11,260 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/1a408f1148bb437f88a98254c4a385dc, entries=868960, sequenceid=1915, filesize=61.9m
2014-07-11 00:06:11,261 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~316.2m/331564560, currentsize=46.0m/48241040 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 19972ms, sequenceid=1915, compaction requested=true
2014-07-11 00:06:11,261 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:11,262 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 00:06:11,262 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 00:06:11,262 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:11,262 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:11,262 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:06:11,262 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 646.3m
2014-07-11 00:06:12,824 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:12,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:12,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8516 synced till here 8508
2014-07-11 00:06:13,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062370775 with entries=98, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062372865
2014-07-11 00:06:13,444 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1448, memsize=223.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/3d20d6a18eed4ab3aa6ba79ba54373df
2014-07-11 00:06:13,566 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/3d20d6a18eed4ab3aa6ba79ba54373df as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/3d20d6a18eed4ab3aa6ba79ba54373df
2014-07-11 00:06:13,588 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/3d20d6a18eed4ab3aa6ba79ba54373df, entries=812870, sequenceid=1448, filesize=57.9m
2014-07-11 00:06:13,589 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~619.8m/649920480, currentsize=377.6m/395967440 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 24756ms, sequenceid=1448, compaction requested=true
2014-07-11 00:06:13,590 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:13,590 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:06:13,590 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 00:06:13,591 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:13,591 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 667.8m
2014-07-11 00:06:13,591 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:13,591 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:06:13,628 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:06:14,824 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:15,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:15,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8588 synced till here 8582
2014-07-11 00:06:15,350 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062372865 with entries=72, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062375174
2014-07-11 00:06:17,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:17,885 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8691 synced till here 8671
2014-07-11 00:06:18,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062375174 with entries=103, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062377729
2014-07-11 00:06:20,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:20,751 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8801 synced till here 8771
2014-07-11 00:06:21,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062377729 with entries=110, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062380629
2014-07-11 00:06:24,491 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:24,665 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8928 synced till here 8899
2014-07-11 00:06:25,004 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062380629 with entries=127, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062384491
2014-07-11 00:06:25,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:06:27,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:28,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9051 synced till here 9049
2014-07-11 00:06:28,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062384491 with entries=123, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062387822
2014-07-11 00:06:28,385 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:06:30,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:30,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9153 synced till here 9143
2014-07-11 00:06:30,306 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062387822 with entries=102, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062390059
2014-07-11 00:06:30,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:06:31,875 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1612, memsize=233.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/72466359f7a045929bf527b23d7d8a92
2014-07-11 00:06:31,896 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/72466359f7a045929bf527b23d7d8a92 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/72466359f7a045929bf527b23d7d8a92
2014-07-11 00:06:31,911 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/72466359f7a045929bf527b23d7d8a92, entries=850310, sequenceid=1612, filesize=60.6m
2014-07-11 00:06:31,911 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~646.3m/677737840, currentsize=301.2m/315870800 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 20649ms, sequenceid=1612, compaction requested=true
2014-07-11 00:06:31,912 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:31,912 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 00:06:31,912 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 422.6m
2014-07-11 00:06:31,912 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 00:06:31,912 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:31,912 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:31,912 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:06:31,965 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:06:32,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:32,423 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9241 synced till here 9239
2014-07-11 00:06:32,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062390059 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062392300
2014-07-11 00:06:32,997 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:34,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:34,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9330 synced till here 9323
2014-07-11 00:06:34,541 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062392300 with entries=89, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062394199
2014-07-11 00:06:35,951 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1630, memsize=259.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/3905d0cbd1544a369dd9582ab6c79bb6
2014-07-11 00:06:35,979 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/3905d0cbd1544a369dd9582ab6c79bb6 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/3905d0cbd1544a369dd9582ab6c79bb6
2014-07-11 00:06:35,997 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/3905d0cbd1544a369dd9582ab6c79bb6, entries=943510, sequenceid=1630, filesize=67.2m
2014-07-11 00:06:35,998 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~677.8m/710732000, currentsize=332.0m/348146240 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 22408ms, sequenceid=1630, compaction requested=true
2014-07-11 00:06:35,998 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:35,998 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 00:06:35,999 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 715.8m
2014-07-11 00:06:35,999 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 00:06:35,999 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:35,999 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:35,999 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:06:36,151 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:06:36,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:36,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062394199 with entries=88, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062396335
2014-07-11 00:06:37,692 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:38,666 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:38,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9518 synced till here 9504
2014-07-11 00:06:39,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062396335 with entries=100, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062398667
2014-07-11 00:06:40,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:40,928 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9611 synced till here 9604
2014-07-11 00:06:41,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062398667 with entries=93, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062400911
2014-07-11 00:06:41,695 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:06:43,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:46,010 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1753, memsize=134.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/216b821181424810996c5849e58f9a64
2014-07-11 00:06:46,036 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/216b821181424810996c5849e58f9a64 as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/216b821181424810996c5849e58f9a64
2014-07-11 00:06:46,064 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/216b821181424810996c5849e58f9a64, entries=490300, sequenceid=1753, filesize=34.9m
2014-07-11 00:06:46,065 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~422.6m/443135760, currentsize=58.0m/60782560 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 14153ms, sequenceid=1753, compaction requested=true
2014-07-11 00:06:46,065 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:46,066 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 495.0m
2014-07-11 00:06:46,066 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-11 00:06:46,066 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-11 00:06:46,066 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:46,066 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:46,067 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. because compaction request was cancelled
2014-07-11 00:06:46,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9821 synced till here 9813
2014-07-11 00:06:46,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062400911 with entries=210, filesize=161.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062403446
2014-07-11 00:06:46,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062301346
2014-07-11 00:06:46,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062303198
2014-07-11 00:06:46,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062305337
2014-07-11 00:06:46,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062307446
2014-07-11 00:06:46,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062309642
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062312151
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062315815
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062318715
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062321172
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062323778
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062326379
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062328574
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062330534
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062334777
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062337066
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062339062
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062340756
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062342644
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062344527
2014-07-11 00:06:46,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062346699
2014-07-11 00:06:47,542 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:48,064 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:48,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9894 synced till here 9889
2014-07-11 00:06:48,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062403446 with entries=73, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062408064
2014-07-11 00:06:49,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:49,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9968 synced till here 9967
2014-07-11 00:06:49,849 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062408064 with entries=74, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062409739
2014-07-11 00:06:51,819 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:51,955 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10051 synced till here 10046
2014-07-11 00:06:52,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062409739 with entries=83, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062411819
2014-07-11 00:06:54,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:54,225 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10150 synced till here 10135
2014-07-11 00:06:54,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062411819 with entries=99, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062414055
2014-07-11 00:06:56,392 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:56,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10269 synced till here 10237
2014-07-11 00:06:56,873 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1793, memsize=185.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7e20e07a0c844a499ff502043d2ca7bd
2014-07-11 00:06:56,891 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/7e20e07a0c844a499ff502043d2ca7bd as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7e20e07a0c844a499ff502043d2ca7bd
2014-07-11 00:06:56,907 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/7e20e07a0c844a499ff502043d2ca7bd, entries=676540, sequenceid=1793, filesize=48.2m
2014-07-11 00:06:56,907 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~718.1m/753004800, currentsize=297.9m/312353840 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 20908ms, sequenceid=1793, compaction requested=true
2014-07-11 00:06:56,908 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:06:56,908 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 00:06:56,908 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 00:06:56,908 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 649.4m
2014-07-11 00:06:56,908 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:06:56,908 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:06:56,908 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:06:56,954 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:06:57,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062414055 with entries=119, filesize=89.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062416393
2014-07-11 00:06:58,809 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:06:58,951 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:06:58,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10367 synced till here 10351
2014-07-11 00:06:59,182 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062416393 with entries=98, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062418809
2014-07-11 00:07:01,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:01,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10459 synced till here 10451
2014-07-11 00:07:01,504 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062418809 with entries=92, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062421234
2014-07-11 00:07:03,799 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:04,388 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10598 synced till here 10580
2014-07-11 00:07:04,828 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062421234 with entries=139, filesize=104.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062423800
2014-07-11 00:07:06,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:06,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10705 synced till here 10696
2014-07-11 00:07:06,743 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1860, memsize=208.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/e4ec0c2a626e48c88604d7fbb3aaf203
2014-07-11 00:07:06,750 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062423800 with entries=107, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062426556
2014-07-11 00:07:06,766 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/e4ec0c2a626e48c88604d7fbb3aaf203 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/e4ec0c2a626e48c88604d7fbb3aaf203
2014-07-11 00:07:06,780 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/e4ec0c2a626e48c88604d7fbb3aaf203, entries=757820, sequenceid=1860, filesize=54.0m
2014-07-11 00:07:06,780 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~511.3m/536117280, currentsize=327.7m/343572400 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 20714ms, sequenceid=1860, compaction requested=true
2014-07-11 00:07:06,781 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:07:06,781 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 00:07:06,781 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 461.3m
2014-07-11 00:07:06,781 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 00:07:06,781 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:07:06,781 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:07:06,781 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:07:06,801 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:07:08,004 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:07:08,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:08,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10794 synced till here 10775
2014-07-11 00:07:09,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062426556 with entries=89, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062428850
2014-07-11 00:07:10,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:11,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10902 synced till here 10890
2014-07-11 00:07:11,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062428850 with entries=108, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062430972
2014-07-11 00:07:13,172 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:13,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10990 synced till here 10985
2014-07-11 00:07:13,663 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062430972 with entries=88, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062433173
2014-07-11 00:07:15,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:16,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11112 synced till here 11108
2014-07-11 00:07:16,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062433173 with entries=122, filesize=106.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062435370
2014-07-11 00:07:18,447 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:18,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11203 synced till here 11196
2014-07-11 00:07:18,814 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062435370 with entries=91, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062438447
2014-07-11 00:07:20,596 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:20,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11298 synced till here 11278
2014-07-11 00:07:20,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062438447 with entries=95, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062440597
2014-07-11 00:07:21,146 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1948, memsize=234.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/071c6019848b41d4a9a5ddec9037bcb7
2014-07-11 00:07:21,161 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/071c6019848b41d4a9a5ddec9037bcb7 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/071c6019848b41d4a9a5ddec9037bcb7
2014-07-11 00:07:21,308 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/071c6019848b41d4a9a5ddec9037bcb7, entries=854850, sequenceid=1948, filesize=60.9m
2014-07-11 00:07:21,310 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~661.3m/693403280, currentsize=386.2m/404958080 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 24402ms, sequenceid=1948, compaction requested=true
2014-07-11 00:07:21,311 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:07:21,311 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 00:07:21,311 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 704.0m
2014-07-11 00:07:21,311 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 00:07:21,311 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:07:21,311 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:07:21,311 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:07:21,476 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:07:22,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:22,503 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11388 synced till here 11382
2014-07-11 00:07:22,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062440597 with entries=90, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062442365
2014-07-11 00:07:23,053 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:07:36,067 WARN  [regionserver60020] util.Sleeper: We slept 14069ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-11 00:07:36,088 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 12709ms
GC pool 'ParNew' had collection(s): count=3 time=165ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=12751ms
2014-07-11 00:07:36,375 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062442730,"queuetimems":0,"class":"HRegionServer","responsesize":20212,"method":"Multi"}
2014-07-11 00:07:36,375 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062442888,"queuetimems":0,"class":"HRegionServer","responsesize":15519,"method":"Multi"}
2014-07-11 00:07:36,376 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7330 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:36,544 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:36,544 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7333 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:36,544 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:36,950 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062443101,"queuetimems":1,"class":"HRegionServer","responsesize":20542,"method":"Multi"}
2014-07-11 00:07:36,950 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062443244,"queuetimems":0,"class":"HRegionServer","responsesize":20376,"method":"Multi"}
2014-07-11 00:07:36,951 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7344 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:36,951 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:36,951 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7341 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:36,951 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:37,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:37,592 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7336 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:37,592 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062443293,"queuetimems":0,"class":"HRegionServer","responsesize":13348,"method":"Multi"}
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44798","starttimems":1405062443050,"queuetimems":0,"class":"HRegionServer","responsesize":20011,"method":"Multi"}
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7338 service: ClientService methodName: Multi size: 3.6m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7339 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:37,596 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:37,597 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7334 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.53:44798: output error
2014-07-11 00:07:37,597 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-11 00:07:37,623 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062442365 with entries=72, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062457533
2014-07-11 00:07:40,477 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:40,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11549 synced till here 11532
2014-07-11 00:07:40,579 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:07:40,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062457533 with entries=89, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062460477
2014-07-11 00:07:41,898 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:41,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062460477 with entries=85, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062461898
2014-07-11 00:07:44,295 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:44,789 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2678, memsize=333.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/830a55321fc34fc3b5eb1d866b83dd20
2014-07-11 00:07:44,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11779 synced till here 11777
2014-07-11 00:07:44,868 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/830a55321fc34fc3b5eb1d866b83dd20 as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/830a55321fc34fc3b5eb1d866b83dd20
2014-07-11 00:07:44,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062461898 with entries=145, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062464296
2014-07-11 00:07:44,964 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/830a55321fc34fc3b5eb1d866b83dd20, entries=1213990, sequenceid=2678, filesize=86.4m
2014-07-11 00:07:44,964 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~462.8m/485285200, currentsize=211.7m/221958880 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 38183ms, sequenceid=2678, compaction requested=true
2014-07-11 00:07:44,965 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:07:44,965 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 00:07:44,965 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:07:44,965 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:07:44,965 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:07:44,965 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:07:44,966 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 727.4m
2014-07-11 00:07:45,685 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:07:46,405 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:46,427 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11865 synced till here 11860
2014-07-11 00:07:46,492 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062464296 with entries=86, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062466406
2014-07-11 00:07:46,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062348710
2014-07-11 00:07:46,493 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062351825
2014-07-11 00:07:46,493 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062354375
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062356638
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062359689
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062361877
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062365198
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062368244
2014-07-11 00:07:46,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062370775
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062372865
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062375174
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062377729
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062380629
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062384491
2014-07-11 00:07:46,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062387822
2014-07-11 00:07:48,394 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:07:48,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:48,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11941 synced till here 11934
2014-07-11 00:07:48,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062466406 with entries=76, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062468423
2014-07-11 00:07:49,944 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:49,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12018 synced till here 12011
2014-07-11 00:07:50,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062468423 with entries=77, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062469944
2014-07-11 00:07:51,703 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:51,732 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12092 synced till here 12090
2014-07-11 00:07:51,813 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062469944 with entries=74, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062471704
2014-07-11 00:07:52,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:52,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12170 synced till here 12157
2014-07-11 00:07:53,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062471704 with entries=78, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062472676
2014-07-11 00:07:55,304 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2133, memsize=319.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/1b8d99a8b0724529a2baebc603c1b9a4
2014-07-11 00:07:55,326 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/1b8d99a8b0724529a2baebc603c1b9a4 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/1b8d99a8b0724529a2baebc603c1b9a4
2014-07-11 00:07:55,339 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/1b8d99a8b0724529a2baebc603c1b9a4, entries=1161430, sequenceid=2133, filesize=82.7m
2014-07-11 00:07:55,339 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~714.8m/749519200, currentsize=324.9m/340726800 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 34028ms, sequenceid=2133, compaction requested=true
2014-07-11 00:07:55,340 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:07:55,340 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 00:07:55,340 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 721.4m
2014-07-11 00:07:55,340 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 00:07:55,340 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:07:55,341 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:07:55,341 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:07:55,468 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:55,470 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:07:55,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12319 synced till here 12275
2014-07-11 00:07:55,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062472676 with entries=149, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062475468
2014-07-11 00:07:56,287 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:07:57,433 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:57,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12491 synced till here 12475
2014-07-11 00:07:57,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062475468 with entries=172, filesize=97.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062477433
2014-07-11 00:07:57,872 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:07:59,530 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:07:59,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062477433 with entries=76, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062479530
2014-07-11 00:07:59,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:08:01,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:01,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12640 synced till here 12632
2014-07-11 00:08:01,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062479530 with entries=73, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062481010
2014-07-11 00:08:01,236 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:08:02,037 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:02,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12718 synced till here 12716
2014-07-11 00:08:02,646 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062481010 with entries=78, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062482038
2014-07-11 00:08:02,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:08:03,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:04,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12802 synced till here 12797
2014-07-11 00:08:04,556 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062482038 with entries=84, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062483515
2014-07-11 00:08:04,556 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:08:05,432 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:06,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12889 synced till here 12888
2014-07-11 00:08:06,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062483515 with entries=87, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062485433
2014-07-11 00:08:06,255 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:08:06,510 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2218, memsize=350.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/ae3ccf3ee3e8442fb76b2f04bc8c0a5f
2014-07-11 00:08:06,530 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/ae3ccf3ee3e8442fb76b2f04bc8c0a5f as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/ae3ccf3ee3e8442fb76b2f04bc8c0a5f
2014-07-11 00:08:06,545 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/ae3ccf3ee3e8442fb76b2f04bc8c0a5f, entries=1277530, sequenceid=2218, filesize=91.0m
2014-07-11 00:08:06,546 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~729.9m/765329200, currentsize=402.5m/422096000 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 21580ms, sequenceid=2218, compaction requested=true
2014-07-11 00:08:06,546 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:08:06,547 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 00:08:06,547 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 401.2m
2014-07-11 00:08:06,547 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 00:08:06,547 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:08:06,547 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:08:06,547 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:08:06,553 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:08:07,134 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:08:07,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:07,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12970 synced till here 12967
2014-07-11 00:08:07,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062485433 with entries=81, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062487267
2014-07-11 00:08:08,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:09,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13089 synced till here 13054
2014-07-11 00:08:10,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062487267 with entries=119, filesize=89.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062488626
2014-07-11 00:08:11,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:12,036 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13200 synced till here 13180
2014-07-11 00:08:12,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062488626 with entries=111, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062491991
2014-07-11 00:08:13,801 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:14,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13338 synced till here 13322
2014-07-11 00:08:14,192 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062491991 with entries=138, filesize=96.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062493802
2014-07-11 00:08:15,770 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:15,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13422 synced till here 13416
2014-07-11 00:08:15,921 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062493802 with entries=84, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062495770
2014-07-11 00:08:17,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:18,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13533 synced till here 13521
2014-07-11 00:08:18,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062495770 with entries=111, filesize=109.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062497729
2014-07-11 00:08:19,820 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:19,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13645 synced till here 13630
2014-07-11 00:08:19,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062497729 with entries=112, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062499820
2014-07-11 00:08:21,693 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:21,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13741 synced till here 13719
2014-07-11 00:08:21,878 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2411, memsize=204.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/32cdd3ea55de45a7bdaaa15d77d81afe
2014-07-11 00:08:21,895 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/32cdd3ea55de45a7bdaaa15d77d81afe as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/32cdd3ea55de45a7bdaaa15d77d81afe
2014-07-11 00:08:21,933 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/32cdd3ea55de45a7bdaaa15d77d81afe, entries=745320, sequenceid=2411, filesize=53.1m
2014-07-11 00:08:21,949 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~401.8m/421365760, currentsize=89.1m/93407040 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 15402ms, sequenceid=2411, compaction requested=true
2014-07-11 00:08:21,952 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-11 00:08:21,952 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-11 00:08:21,952 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:08:21,952 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:08:21,952 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. because compaction request was cancelled
2014-07-11 00:08:21,953 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:08:21,953 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 654.7m
2014-07-11 00:08:21,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062499820 with entries=96, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062501693
2014-07-11 00:08:21,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062390059
2014-07-11 00:08:21,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062392300
2014-07-11 00:08:21,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062394199
2014-07-11 00:08:21,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062396335
2014-07-11 00:08:21,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062398667
2014-07-11 00:08:21,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062400911
2014-07-11 00:08:21,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062403446
2014-07-11 00:08:21,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062408064
2014-07-11 00:08:21,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062409739
2014-07-11 00:08:21,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062411819
2014-07-11 00:08:21,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062414055
2014-07-11 00:08:23,674 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:23,718 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:08:23,720 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13834 synced till here 13828
2014-07-11 00:08:23,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062501693 with entries=93, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062503675
2014-07-11 00:08:24,289 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2320, memsize=410.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/7e20f92fc64d47169ad9001948896b68
2014-07-11 00:08:24,325 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/7e20f92fc64d47169ad9001948896b68 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/7e20f92fc64d47169ad9001948896b68
2014-07-11 00:08:24,357 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/7e20f92fc64d47169ad9001948896b68, entries=1494670, sequenceid=2320, filesize=106.4m
2014-07-11 00:08:24,357 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~735.2m/770879200, currentsize=588.6m/617180160 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 29017ms, sequenceid=2320, compaction requested=true
2014-07-11 00:08:25,203 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 00:08:25,203 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 00:08:25,203 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:08:25,203 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:08:25,203 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:08:25,206 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:08:25,206 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 924.8m
2014-07-11 00:08:25,266 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:08:25,492 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:25,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13926 synced till here 13919
2014-07-11 00:08:25,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062503675 with entries=92, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062505493
2014-07-11 00:08:25,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062416393
2014-07-11 00:08:25,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062418809
2014-07-11 00:08:25,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062421234
2014-07-11 00:08:25,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062423800
2014-07-11 00:08:26,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:26,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13999 synced till here 13996
2014-07-11 00:08:26,834 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062505493 with entries=73, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062506774
2014-07-11 00:08:26,859 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:08:27,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:27,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14065 synced till here 14062
2014-07-11 00:08:27,751 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062506774 with entries=66, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062507714
2014-07-11 00:08:28,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:28,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14129 synced till here 14128
2014-07-11 00:08:28,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062507714 with entries=64, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062508943
2014-07-11 00:08:31,017 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:31,052 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14206 synced till here 14204
2014-07-11 00:08:31,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062508943 with entries=77, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062511018
2014-07-11 00:08:33,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:33,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14294 synced till here 14280
2014-07-11 00:08:33,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062511018 with entries=88, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062513157
2014-07-11 00:08:34,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:34,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14385 synced till here 14379
2014-07-11 00:08:34,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062513157 with entries=91, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062514824
2014-07-11 00:08:36,276 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:36,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14474 synced till here 14463
2014-07-11 00:08:36,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062514824 with entries=89, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062516276
2014-07-11 00:08:37,806 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,807 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,809 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,811 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,811 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,811 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,813 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,813 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,814 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,816 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,839 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,845 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,848 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,857 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,866 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,904 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:37,937 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:38,006 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,006 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,007 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,007 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,007 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,008 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,009 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,009 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,010 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,011 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,012 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,016 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,022 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14574 synced till here 14571
2014-07-11 00:08:38,026 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,029 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,030 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062516276 with entries=100, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062518004
2014-07-11 00:08:38,047 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,087 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,129 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,162 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,186 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,199 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,201 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,232 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,232 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,255 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,256 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,297 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,339 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,358 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,371 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,372 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,372 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:38,373 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:08:42,807 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,809 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-11 00:08:42,810 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,811 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,811 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,812 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,813 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,814 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,814 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,816 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,840 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,846 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,848 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,857 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,866 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:42,905 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:42,937 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,006 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,006 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,007 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,008 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,008 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,009 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,009 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,010 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,011 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,012 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,012 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,016 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,026 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,030 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,030 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,047 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,088 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,129 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,163 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,186 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,200 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,202 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,232 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,233 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,255 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,256 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,297 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,339 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,359 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,371 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,372 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:08:43,373 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:43,374 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:08:47,808 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,810 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-11 00:08:47,811 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-11 00:08:47,811 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:47,812 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,813 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,816 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,817 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,818 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-11 00:08:47,819 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-11 00:08:47,840 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,852 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-11 00:08:47,852 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10007ms
2014-07-11 00:08:47,857 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,866 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:47,905 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:47,938 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,007 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,007 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,007 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,008 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,009 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,010 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-11 00:08:48,010 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,011 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-11 00:08:48,011 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,012 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,012 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,016 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,026 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,030 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,031 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,048 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,088 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,130 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,163 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,187 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,200 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,202 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,233 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,234 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-11 00:08:48,256 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,256 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,298 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,340 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-11 00:08:48,359 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,372 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,373 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,373 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,374 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-11 00:08:48,619 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3501, memsize=513.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/7121f4c02947425ab100f64535aadd0d
2014-07-11 00:08:48,636 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/7121f4c02947425ab100f64535aadd0d as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/7121f4c02947425ab100f64535aadd0d
2014-07-11 00:08:48,655 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/7121f4c02947425ab100f64535aadd0d, entries=1870920, sequenceid=3501, filesize=133.2m
2014-07-11 00:08:48,655 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~657.9m/689889280, currentsize=174.5m/183009280 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 26702ms, sequenceid=3501, compaction requested=true
2014-07-11 00:08:48,655 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:08:48,655 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-11 00:08:48,656 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-11 00:08:48,656 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10283ms
2014-07-11 00:08:48,656 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:08:48,656 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,656 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 1.0g
2014-07-11 00:08:48,656 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10284ms
2014-07-11 00:08:48,656 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,656 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:08:48,657 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:08:48,661 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10289ms
2014-07-11 00:08:48,661 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,661 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10290ms
2014-07-11 00:08:48,661 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,661 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10303ms
2014-07-11 00:08:48,661 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,667 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10328ms
2014-07-11 00:08:48,667 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,667 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10370ms
2014-07-11 00:08:48,667 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,667 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10411ms
2014-07-11 00:08:48,667 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,667 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10412ms
2014-07-11 00:08:48,668 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,669 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10437ms
2014-07-11 00:08:48,669 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,673 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10441ms
2014-07-11 00:08:48,673 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,673 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10472ms
2014-07-11 00:08:48,673 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,673 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10474ms
2014-07-11 00:08:48,673 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,673 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10487ms
2014-07-11 00:08:48,673 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,681 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10309,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518372,"queuetimems":1,"class":"HRegionServer","responsesize":631,"method":"Multi"}
2014-07-11 00:08:48,681 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10519ms
2014-07-11 00:08:48,681 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,682 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10553ms
2014-07-11 00:08:48,682 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,682 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10595ms
2014-07-11 00:08:48,682 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,682 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10635ms
2014-07-11 00:08:48,682 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,682 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10652ms
2014-07-11 00:08:48,682 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,683 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10654ms
2014-07-11 00:08:48,683 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,693 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10667ms
2014-07-11 00:08:48,693 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,693 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10677ms
2014-07-11 00:08:48,693 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,693 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10682ms
2014-07-11 00:08:48,693 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,693 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10682ms
2014-07-11 00:08:48,693 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,698 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10688ms
2014-07-11 00:08:48,698 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,698 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10689ms
2014-07-11 00:08:48,699 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,699 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10690ms
2014-07-11 00:08:48,699 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,701 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10693ms
2014-07-11 00:08:48,701 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,701 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10694ms
2014-07-11 00:08:48,701 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,709 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10702ms
2014-07-11 00:08:48,709 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,709 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10702ms
2014-07-11 00:08:48,709 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,709 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10703ms
2014-07-11 00:08:48,709 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,709 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10703ms
2014-07-11 00:08:48,709 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,710 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10774ms
2014-07-11 00:08:48,710 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,712 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10808ms
2014-07-11 00:08:48,712 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,712 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10846ms
2014-07-11 00:08:48,712 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,713 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10857ms
2014-07-11 00:08:48,713 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,713 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10868ms
2014-07-11 00:08:48,713 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,713 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10865ms
2014-07-11 00:08:48,713 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,713 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10874ms
2014-07-11 00:08:48,713 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,718 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10905ms
2014-07-11 00:08:48,718 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,718 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10904ms
2014-07-11 00:08:48,718 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,718 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10902ms
2014-07-11 00:08:48,718 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,719 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10906ms
2014-07-11 00:08:48,719 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,726 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518371,"queuetimems":0,"class":"HRegionServer","responsesize":2653,"method":"Multi"}
2014-07-11 00:08:48,726 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518372,"queuetimems":0,"class":"HRegionServer","responsesize":26,"method":"Multi"}
2014-07-11 00:08:48,729 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10918ms
2014-07-11 00:08:48,729 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,729 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10918ms
2014-07-11 00:08:48,729 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,741 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10930ms
2014-07-11 00:08:48,741 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,741 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10932ms
2014-07-11 00:08:48,741 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,742 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10935ms
2014-07-11 00:08:48,742 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,742 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10936ms
2014-07-11 00:08:48,742 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:08:48,850 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12453,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516396,"queuetimems":1,"class":"HRegionServer","responsesize":18731,"method":"Multi"}
2014-07-11 00:08:48,955 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516438,"queuetimems":1,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-11 00:08:48,956 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516475,"queuetimems":1,"class":"HRegionServer","responsesize":17670,"method":"Multi"}
2014-07-11 00:08:48,956 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516122,"queuetimems":0,"class":"HRegionServer","responsesize":20693,"method":"Multi"}
2014-07-11 00:08:48,957 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516646,"queuetimems":0,"class":"HRegionServer","responsesize":20213,"method":"Multi"}
2014-07-11 00:08:48,961 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12608,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516353,"queuetimems":0,"class":"HRegionServer","responsesize":18869,"method":"Multi"}
2014-07-11 00:08:48,962 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516272,"queuetimems":0,"class":"HRegionServer","responsesize":13075,"method":"Multi"}
2014-07-11 00:08:48,963 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12412,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516550,"queuetimems":0,"class":"HRegionServer","responsesize":13318,"method":"Multi"}
2014-07-11 00:08:48,963 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12451,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516511,"queuetimems":0,"class":"HRegionServer","responsesize":17127,"method":"Multi"}
2014-07-11 00:08:48,955 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12720,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516234,"queuetimems":0,"class":"HRegionServer","responsesize":20621,"method":"Multi"}
2014-07-11 00:08:48,966 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12379,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516587,"queuetimems":0,"class":"HRegionServer","responsesize":8007,"method":"Multi"}
2014-07-11 00:08:48,961 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12649,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516312,"queuetimems":1,"class":"HRegionServer","responsesize":17026,"method":"Multi"}
2014-07-11 00:08:48,957 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10591,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518366,"queuetimems":0,"class":"HRegionServer","responsesize":5051,"method":"Multi"}
2014-07-11 00:08:48,969 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10715,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518254,"queuetimems":0,"class":"HRegionServer","responsesize":464,"method":"Multi"}
2014-07-11 00:08:48,973 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10772,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518200,"queuetimems":0,"class":"HRegionServer","responsesize":1743,"method":"Multi"}
2014-07-11 00:08:48,973 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11127,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517845,"queuetimems":0,"class":"HRegionServer","responsesize":14,"method":"Multi"}
2014-07-11 00:08:48,973 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10742,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518231,"queuetimems":0,"class":"HRegionServer","responsesize":1414,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517854,"queuetimems":0,"class":"HRegionServer","responsesize":5454,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11244,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517863,"queuetimems":0,"class":"HRegionServer","responsesize":5085,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11301,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517806,"queuetimems":21,"class":"HRegionServer","responsesize":1635,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516572,"queuetimems":0,"class":"HRegionServer","responsesize":10740,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517805,"queuetimems":22,"class":"HRegionServer","responsesize":569,"method":"Multi"}
2014-07-11 00:08:49,108 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517845,"queuetimems":1,"class":"HRegionServer","responsesize":4840,"method":"Multi"}
2014-07-11 00:08:50,523 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:50,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14712 synced till here 14677
2014-07-11 00:08:50,707 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:08:50,815 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517741,"queuetimems":0,"class":"HRegionServer","responsesize":20488,"method":"Multi"}
2014-07-11 00:08:50,821 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517781,"queuetimems":1,"class":"HRegionServer","responsesize":20327,"method":"Multi"}
2014-07-11 00:08:50,882 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12525,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518356,"queuetimems":0,"class":"HRegionServer","responsesize":10581,"method":"Multi"}
2014-07-11 00:08:50,882 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517562,"queuetimems":0,"class":"HRegionServer","responsesize":11427,"method":"Multi"}
2014-07-11 00:08:50,896 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062518004 with entries=138, filesize=102.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062530524
2014-07-11 00:08:50,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062426556
2014-07-11 00:08:50,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062428850
2014-07-11 00:08:50,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062430972
2014-07-11 00:08:50,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062433173
2014-07-11 00:08:50,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062435370
2014-07-11 00:08:50,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062438447
2014-07-11 00:08:51,149 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12812,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518336,"queuetimems":0,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-11 00:08:51,149 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517608,"queuetimems":0,"class":"HRegionServer","responsesize":19130,"method":"Multi"}
2014-07-11 00:08:51,149 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13653,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517495,"queuetimems":1,"class":"HRegionServer","responsesize":20488,"method":"Multi"}
2014-07-11 00:08:51,149 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12918,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518230,"queuetimems":0,"class":"HRegionServer","responsesize":13318,"method":"Multi"}
2014-07-11 00:08:51,190 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2609, memsize=459.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/22c39a4a6e544dc7ab2b70d6b2b23d74
2014-07-11 00:08:51,205 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/22c39a4a6e544dc7ab2b70d6b2b23d74 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/22c39a4a6e544dc7ab2b70d6b2b23d74
2014-07-11 00:08:51,216 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/22c39a4a6e544dc7ab2b70d6b2b23d74, entries=1671550, sequenceid=2609, filesize=119.1m
2014-07-11 00:08:51,216 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~935.5m/980969200, currentsize=309.6m/324679840 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 26010ms, sequenceid=2609, compaction requested=true
2014-07-11 00:08:51,217 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:08:51,217 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 00:08:51,217 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 935.1m
2014-07-11 00:08:51,217 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 00:08:51,217 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:08:51,218 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:08:51,218 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:08:51,425 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:08:52,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:52,793 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518254,"queuetimems":0,"class":"HRegionServer","responsesize":11427,"method":"Multi"}
2014-07-11 00:08:52,798 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517836,"queuetimems":1,"class":"HRegionServer","responsesize":20449,"method":"Multi"}
2014-07-11 00:08:52,804 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14620,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518184,"queuetimems":0,"class":"HRegionServer","responsesize":10740,"method":"Multi"}
2014-07-11 00:08:52,805 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518127,"queuetimems":1,"class":"HRegionServer","responsesize":16339,"method":"Multi"}
2014-07-11 00:08:52,817 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14811,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518006,"queuetimems":2,"class":"HRegionServer","responsesize":18633,"method":"Multi"}
2014-07-11 00:08:52,818 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517900,"queuetimems":0,"class":"HRegionServer","responsesize":19130,"method":"Multi"}
2014-07-11 00:08:52,823 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14529,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518294,"queuetimems":1,"class":"HRegionServer","responsesize":20213,"method":"Multi"}
2014-07-11 00:08:52,823 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518198,"queuetimems":0,"class":"HRegionServer","responsesize":8007,"method":"Multi"}
2014-07-11 00:08:52,824 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14738,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518085,"queuetimems":0,"class":"HRegionServer","responsesize":13075,"method":"Multi"}
2014-07-11 00:08:52,824 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518044,"queuetimems":0,"class":"HRegionServer","responsesize":17026,"method":"Multi"}
2014-07-11 00:08:52,824 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14818,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518005,"queuetimems":38,"class":"HRegionServer","responsesize":17670,"method":"Multi"}
2014-07-11 00:08:52,824 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517934,"queuetimems":1,"class":"HRegionServer","responsesize":17215,"method":"Multi"}
2014-07-11 00:08:52,824 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15129,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517695,"queuetimems":0,"class":"HRegionServer","responsesize":17172,"method":"Multi"}
2014-07-11 00:08:52,825 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062516688,"queuetimems":1,"class":"HRegionServer","responsesize":20327,"method":"Multi"}
2014-07-11 00:08:52,825 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14664,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062518160,"queuetimems":0,"class":"HRegionServer","responsesize":17127,"method":"Multi"}
2014-07-11 00:08:52,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14832 synced till here 14816
2014-07-11 00:08:52,938 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15429,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:44801","starttimems":1405062517508,"queuetimems":0,"class":"HRegionServer","responsesize":17172,"method":"Multi"}
2014-07-11 00:08:52,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062530524 with entries=120, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062532792
2014-07-11 00:08:52,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062440597
2014-07-11 00:08:52,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062442365
2014-07-11 00:08:52,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062457533
2014-07-11 00:08:52,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062460477
2014-07-11 00:08:52,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062461898
2014-07-11 00:08:53,257 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:08:54,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:54,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14961 synced till here 14945
2014-07-11 00:08:54,901 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:08:54,973 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062532792 with entries=129, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062534711
2014-07-11 00:08:56,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:56,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15064 synced till here 15032
2014-07-11 00:08:56,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062534711 with entries=103, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062536534
2014-07-11 00:08:57,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:57,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15189 synced till here 15150
2014-07-11 00:08:58,577 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062536534 with entries=125, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062537791
2014-07-11 00:08:59,019 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:08:59,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:08:59,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062537791 with entries=109, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062539299
2014-07-11 00:09:01,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:01,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15387 synced till here 15386
2014-07-11 00:09:01,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062539299 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062541279
2014-07-11 00:09:03,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:05,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15562 synced till here 15556
2014-07-11 00:09:05,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062541279 with entries=175, filesize=153.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062543117
2014-07-11 00:09:06,378 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:06,396 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15628 synced till here 15626
2014-07-11 00:09:06,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062543117 with entries=66, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062546379
2014-07-11 00:09:08,071 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,104 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,106 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,110 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,115 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,121 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,138 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,167 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,183 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:08,199 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,220 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,231 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062546379 with entries=64, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062548195
2014-07-11 00:09:08,280 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,281 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,339 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,351 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,364 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,414 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,458 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,520 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,521 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,522 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,524 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,628 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,682 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,725 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,775 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,809 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,961 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:08,968 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,041 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,050 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,051 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,076 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,144 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,144 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,144 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,144 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,145 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,157 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,162 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,171 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,172 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,175 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,224 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,253 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:09,264 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:10,342 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:10,395 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:10,410 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:09:13,761 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5036ms
2014-07-11 00:09:13,762 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5410ms
2014-07-11 00:09:13,762 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5563ms
2014-07-11 00:09:13,762 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5641ms
2014-07-11 00:09:13,763 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5399ms
2014-07-11 00:09:13,763 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5349ms
2014-07-11 00:09:13,763 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-07-11 00:09:13,764 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5244ms
2014-07-11 00:09:13,765 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5242ms
2014-07-11 00:09:13,765 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5083ms
2014-07-11 00:09:13,765 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5241ms
2014-07-11 00:09:13,766 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5246ms
2014-07-11 00:09:13,766 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5138ms
2014-07-11 00:09:13,766 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5695ms
2014-07-11 00:09:13,766 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5662ms
2014-07-11 00:09:13,766 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5660ms
2014-07-11 00:09:13,767 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5657ms
2014-07-11 00:09:13,767 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5652ms
2014-07-11 00:09:13,767 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5629ms
2014-07-11 00:09:13,768 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5601ms
2014-07-11 00:09:13,769 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5585ms
2014-07-11 00:09:13,769 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5549ms
2014-07-11 00:09:13,769 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5490ms
2014-07-11 00:09:13,769 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5538ms
2014-07-11 00:09:13,770 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5489ms
2014-07-11 00:09:13,770 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5432ms
2014-07-11 00:09:13,775 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:13,810 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:13,962 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:13,969 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,041 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,050 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,051 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,077 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,144 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,144 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,145 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,146 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,146 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,158 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,163 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,172 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,173 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:09:14,176 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,193 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2782, memsize=373.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/a3b1706e609e4f18b8dee70f91c12c6d
2014-07-11 00:09:14,214 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/a3b1706e609e4f18b8dee70f91c12c6d as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/a3b1706e609e4f18b8dee70f91c12c6d
2014-07-11 00:09:14,224 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:09:14,232 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/a3b1706e609e4f18b8dee70f91c12c6d, entries=1358040, sequenceid=2782, filesize=96.7m
2014-07-11 00:09:14,232 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~940.3m/985988400, currentsize=302.8m/317505840 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 23015ms, sequenceid=2782, compaction requested=true
2014-07-11 00:09:14,233 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:09:14,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 00:09:14,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 00:09:14,234 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-11 00:09:14,234 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,234 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 646.4m
2014-07-11 00:09:14,234 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5059ms
2014-07-11 00:09:14,234 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,234 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:09:14,234 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5062ms
2014-07-11 00:09:14,235 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:09:14,235 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,235 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:09:14,235 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5064ms
2014-07-11 00:09:14,235 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,235 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5073ms
2014-07-11 00:09:14,235 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,236 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5079ms
2014-07-11 00:09:14,236 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,236 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5091ms
2014-07-11 00:09:14,236 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,236 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5092ms
2014-07-11 00:09:14,236 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,236 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5092ms
2014-07-11 00:09:14,240 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,240 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5097ms
2014-07-11 00:09:14,240 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,240 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5097ms
2014-07-11 00:09:14,240 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,240 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5164ms
2014-07-11 00:09:14,240 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,245 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5195ms
2014-07-11 00:09:14,245 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,245 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5195ms
2014-07-11 00:09:14,245 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,245 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5204ms
2014-07-11 00:09:14,245 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,245 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5277ms
2014-07-11 00:09:14,245 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,245 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5284ms
2014-07-11 00:09:14,246 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,246 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5437ms
2014-07-11 00:09:14,246 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,246 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5471ms
2014-07-11 00:09:14,246 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,246 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5908ms
2014-07-11 00:09:14,247 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,247 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5966ms
2014-07-11 00:09:14,247 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,247 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6016ms
2014-07-11 00:09:14,247 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,248 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5969ms
2014-07-11 00:09:14,248 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,250 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6030ms
2014-07-11 00:09:14,250 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,251 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6067ms
2014-07-11 00:09:14,251 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,251 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6084ms
2014-07-11 00:09:14,251 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,251 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6113ms
2014-07-11 00:09:14,251 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,251 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6136ms
2014-07-11 00:09:14,251 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,251 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6141ms
2014-07-11 00:09:14,251 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,253 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6147ms
2014-07-11 00:09:14,253 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,257 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-11 00:09:14,257 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,258 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6154ms
2014-07-11 00:09:14,258 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,258 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6187ms
2014-07-11 00:09:14,258 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,258 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5630ms
2014-07-11 00:09:14,258 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,258 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5738ms
2014-07-11 00:09:14,258 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,258 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5734ms
2014-07-11 00:09:14,258 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,261 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5579ms
2014-07-11 00:09:14,261 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,261 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5739ms
2014-07-11 00:09:14,261 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,262 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5742ms
2014-07-11 00:09:14,262 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,262 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5804ms
2014-07-11 00:09:14,262 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,262 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5848ms
2014-07-11 00:09:14,262 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,269 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-11 00:09:14,269 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,269 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5905ms
2014-07-11 00:09:14,269 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,270 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6149ms
2014-07-11 00:09:14,270 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,317 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6118ms
2014-07-11 00:09:14,317 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,317 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5966ms
2014-07-11 00:09:14,317 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,317 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5592ms
2014-07-11 00:09:14,317 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,324 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3914ms
2014-07-11 00:09:14,324 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,325 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3930ms
2014-07-11 00:09:14,325 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,326 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3983ms
2014-07-11 00:09:14,326 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:09:14,503 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:09:14,874 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:09:15,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:16,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15831 synced till here 15805
2014-07-11 00:09:16,250 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062548195 with entries=139, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062555263
2014-07-11 00:09:17,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:18,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16032 synced till here 16019
2014-07-11 00:09:18,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062555263 with entries=201, filesize=124.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062557169
2014-07-11 00:09:18,547 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2733, memsize=487.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/23904dcf3a0943c684bd75acbb8d56b5
2014-07-11 00:09:18,560 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/23904dcf3a0943c684bd75acbb8d56b5 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/23904dcf3a0943c684bd75acbb8d56b5
2014-07-11 00:09:18,571 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/23904dcf3a0943c684bd75acbb8d56b5, entries=1775070, sequenceid=2733, filesize=126.4m
2014-07-11 00:09:18,572 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1084191680, currentsize=499.0m/523188880 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 29916ms, sequenceid=2733, compaction requested=true
2014-07-11 00:09:18,573 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:09:18,573 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 00:09:18,573 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 517.5m
2014-07-11 00:09:18,573 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 00:09:18,573 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:09:18,574 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:09:18,574 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:09:18,672 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:09:19,148 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:09:20,080 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:20,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16118 synced till here 16104
2014-07-11 00:09:20,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062557169 with entries=86, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062560080
2014-07-11 00:09:20,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062464296
2014-07-11 00:09:20,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062466406
2014-07-11 00:09:20,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062468423
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062469944
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062471704
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062472676
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062475468
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062477433
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062479530
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062481010
2014-07-11 00:09:20,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062482038
2014-07-11 00:09:20,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062483515
2014-07-11 00:09:21,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:21,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16196 synced till here 16193
2014-07-11 00:09:21,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062560080 with entries=78, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062561559
2014-07-11 00:09:22,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:22,926 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16267 synced till here 16266
2014-07-11 00:09:22,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062561559 with entries=71, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062562900
2014-07-11 00:09:22,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:23,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16335 synced till here 16332
2014-07-11 00:09:24,002 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062562900 with entries=68, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062563948
2014-07-11 00:09:24,003 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:25,351 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:25,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16406 synced till here 16401
2014-07-11 00:09:25,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062563948 with entries=71, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062565352
2014-07-11 00:09:25,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:27,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:27,670 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16509 synced till here 16508
2014-07-11 00:09:27,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062565352 with entries=103, filesize=85.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062567273
2014-07-11 00:09:27,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:29,114 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:30,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16616 synced till here 16611
2014-07-11 00:09:30,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062567273 with entries=107, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062569114
2014-07-11 00:09:30,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:31,448 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:31,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16708 synced till here 16705
2014-07-11 00:09:31,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062569114 with entries=92, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062571448
2014-07-11 00:09:31,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:09:32,741 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2933, memsize=293.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/696327b8c8fa41e7b2028df89c2b74a7
2014-07-11 00:09:32,830 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/696327b8c8fa41e7b2028df89c2b74a7 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/696327b8c8fa41e7b2028df89c2b74a7
2014-07-11 00:09:32,850 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/696327b8c8fa41e7b2028df89c2b74a7, entries=1067300, sequenceid=2933, filesize=76.0m
2014-07-11 00:09:32,850 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~646.4m/677784960, currentsize=363.1m/380733680 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 18616ms, sequenceid=2933, compaction requested=true
2014-07-11 00:09:32,851 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:09:32,851 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 00:09:32,851 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 00:09:32,851 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 414.8m
2014-07-11 00:09:32,851 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:09:32,852 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:09:32,852 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:09:32,975 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:09:33,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:33,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16788 synced till here 16781
2014-07-11 00:09:33,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062571448 with entries=80, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062573282
2014-07-11 00:09:34,506 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:09:35,078 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4176, memsize=265.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/dfae33ad4ce14b26acdb298e475e8593
2014-07-11 00:09:35,092 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/dfae33ad4ce14b26acdb298e475e8593 as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/dfae33ad4ce14b26acdb298e475e8593
2014-07-11 00:09:35,109 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/dfae33ad4ce14b26acdb298e475e8593, entries=965630, sequenceid=4176, filesize=68.8m
2014-07-11 00:09:35,110 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~523.7m/549107040, currentsize=254.8m/267178640 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 16537ms, sequenceid=4176, compaction requested=true
2014-07-11 00:09:35,110 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:09:35,110 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-11 00:09:35,110 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-11 00:09:35,110 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 718.1m
2014-07-11 00:09:35,110 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:09:35,110 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:09:35,111 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:09:35,207 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:09:35,207 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:35,245 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16902 synced till here 16884
2014-07-11 00:09:36,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062573282 with entries=114, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062575208
2014-07-11 00:09:36,736 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:09:37,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:37,316 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17004 synced till here 16988
2014-07-11 00:09:38,394 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062575208 with entries=102, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062577282
2014-07-11 00:09:39,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:39,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17090 synced till here 17076
2014-07-11 00:09:40,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062577282 with entries=86, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062579282
2014-07-11 00:09:41,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:41,603 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17180 synced till here 17176
2014-07-11 00:09:42,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062579282 with entries=90, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062581219
2014-07-11 00:09:43,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:43,498 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17271 synced till here 17265
2014-07-11 00:09:43,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062581219 with entries=91, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062583480
2014-07-11 00:09:45,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:45,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17356 synced till here 17351
2014-07-11 00:09:45,241 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062583480 with entries=85, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062585132
2014-07-11 00:09:46,391 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3096, memsize=181.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/afa6455d4354485dbdfedcff477ebc82
2014-07-11 00:09:46,415 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/afa6455d4354485dbdfedcff477ebc82 as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/afa6455d4354485dbdfedcff477ebc82
2014-07-11 00:09:46,429 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/afa6455d4354485dbdfedcff477ebc82, entries=661470, sequenceid=3096, filesize=47.1m
2014-07-11 00:09:46,430 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~416.6m/436838080, currentsize=75.7m/79337840 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 13579ms, sequenceid=3096, compaction requested=true
2014-07-11 00:09:46,430 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:09:46,430 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-11 00:09:46,430 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-11 00:09:46,431 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:09:46,431 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 1017.8m
2014-07-11 00:09:46,431 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:09:46,431 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. because compaction request was cancelled
2014-07-11 00:09:46,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:46,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17433 synced till here 17428
2014-07-11 00:09:46,816 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062585132 with entries=77, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062586737
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062485433
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062487267
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062488626
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062491991
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062493802
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062495770
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062497729
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062499820
2014-07-11 00:09:46,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062501693
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062503675
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062505493
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062506774
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062507714
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062508943
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062511018
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062513157
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062514824
2014-07-11 00:09:46,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062516276
2014-07-11 00:09:48,286 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:09:48,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:48,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17524 synced till here 17517
2014-07-11 00:09:48,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062586737 with entries=91, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062588466
2014-07-11 00:09:50,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:50,632 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17606 synced till here 17601
2014-07-11 00:09:50,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062588466 with entries=82, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062590607
2014-07-11 00:09:52,402 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:52,425 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17700 synced till here 17690
2014-07-11 00:09:52,504 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062590607 with entries=94, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062592403
2014-07-11 00:09:54,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:54,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17800 synced till here 17784
2014-07-11 00:09:54,483 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062592403 with entries=100, filesize=80.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062594314
2014-07-11 00:09:56,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:56,516 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17908 synced till here 17887
2014-07-11 00:09:56,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062594314 with entries=108, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062596460
2014-07-11 00:09:58,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:09:58,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18008 synced till here 17989
2014-07-11 00:09:59,146 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062596460 with entries=100, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062598832
2014-07-11 00:10:01,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:01,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18107 synced till here 18093
2014-07-11 00:10:01,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062598832 with entries=99, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062601124
2014-07-11 00:10:01,483 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,484 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,484 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,484 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,484 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,486 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,486 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,512 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,513 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,574 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,576 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,584 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,616 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,626 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,659 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,664 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:01,700 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,496 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,508 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,536 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,571 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,571 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,577 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,587 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,603 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,629 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,659 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,682 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:02,695 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,643 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,644 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,647 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,647 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,648 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,648 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:03,717 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:04,081 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:04,271 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3139, memsize=439.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/597f42214a4942d990a41c96b8b646cf
2014-07-11 00:10:04,286 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/597f42214a4942d990a41c96b8b646cf as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/597f42214a4942d990a41c96b8b646cf
2014-07-11 00:10:04,297 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/597f42214a4942d990a41c96b8b646cf, entries=1601220, sequenceid=3139, filesize=114.0m
2014-07-11 00:10:04,297 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~726.5m/761748640, currentsize=488.6m/512323440 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 29187ms, sequenceid=3139, compaction requested=true
2014-07-11 00:10:04,298 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:10:04,298 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 00:10:04,298 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 00:10:04,298 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 217ms
2014-07-11 00:10:04,298 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:10:04,298 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 901.9m
2014-07-11 00:10:04,298 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,298 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:10:04,299 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 582ms
2014-07-11 00:10:04,299 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,299 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:10:04,299 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 651ms
2014-07-11 00:10:04,299 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,299 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 652ms
2014-07-11 00:10:04,300 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,300 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 653ms
2014-07-11 00:10:04,300 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,301 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 654ms
2014-07-11 00:10:04,301 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,302 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 658ms
2014-07-11 00:10:04,302 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,302 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 659ms
2014-07-11 00:10:04,302 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,303 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1608ms
2014-07-11 00:10:04,303 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,303 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1621ms
2014-07-11 00:10:04,303 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,303 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1644ms
2014-07-11 00:10:04,304 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,304 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1675ms
2014-07-11 00:10:04,304 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,304 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1701ms
2014-07-11 00:10:04,304 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,315 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1728ms
2014-07-11 00:10:04,316 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,316 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1739ms
2014-07-11 00:10:04,316 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,316 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1745ms
2014-07-11 00:10:04,316 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,325 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1754ms
2014-07-11 00:10:04,325 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,325 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1789ms
2014-07-11 00:10:04,325 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,331 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1824ms
2014-07-11 00:10:04,331 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,332 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1835ms
2014-07-11 00:10:04,332 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,332 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2632ms
2014-07-11 00:10:04,332 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,335 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-11 00:10:04,335 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,336 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2676ms
2014-07-11 00:10:04,336 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,336 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2710ms
2014-07-11 00:10:04,336 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,337 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2721ms
2014-07-11 00:10:04,337 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,337 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2753ms
2014-07-11 00:10:04,337 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,337 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2761ms
2014-07-11 00:10:04,337 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,341 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2767ms
2014-07-11 00:10:04,341 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,341 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2828ms
2014-07-11 00:10:04,341 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,378 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2865ms
2014-07-11 00:10:04,378 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,378 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2892ms
2014-07-11 00:10:04,378 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,378 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2892ms
2014-07-11 00:10:04,378 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,379 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2895ms
2014-07-11 00:10:04,379 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,379 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2895ms
2014-07-11 00:10:04,379 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,379 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2895ms
2014-07-11 00:10:04,379 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,379 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2896ms
2014-07-11 00:10:04,379 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,380 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2897ms
2014-07-11 00:10:04,380 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:04,612 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:10:05,142 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:05,219 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18222 synced till here 18201
2014-07-11 00:10:06,337 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062601124 with entries=115, filesize=81.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062605142
2014-07-11 00:10:06,413 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:10:07,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:09,133 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1323ms
GC pool 'ParNew' had collection(s): count=1 time=1471ms
2014-07-11 00:10:09,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062605142 with entries=142, filesize=99.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062607320
2014-07-11 00:10:10,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:10,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18430 synced till here 18423
2014-07-11 00:10:11,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062607320 with entries=66, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062610567
2014-07-11 00:10:12,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:12,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18535 synced till here 18520
2014-07-11 00:10:13,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062610567 with entries=105, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062612599
2014-07-11 00:10:14,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:14,543 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18619 synced till here 18614
2014-07-11 00:10:14,624 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062612599 with entries=84, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062614519
2014-07-11 00:10:15,455 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,457 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,457 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,465 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,480 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,525 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,596 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,601 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,610 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,626 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,667 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,674 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,683 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,745 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,798 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,836 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,895 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,919 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,970 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:15,971 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,035 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,097 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,164 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,203 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,221 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,222 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,310 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,315 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,315 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,320 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,328 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,331 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,333 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,402 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,485 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,488 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,490 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,516 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,526 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,625 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,730 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,830 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:16,888 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,668 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,710 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,757 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,812 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,863 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,908 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:17,950 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:20,624 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5014ms
2014-07-11 00:10:20,624 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-07-11 00:10:20,624 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-07-11 00:10:20,624 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-07-11 00:10:20,625 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-07-11 00:10:20,625 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5145ms
2014-07-11 00:10:20,625 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5030ms
2014-07-11 00:10:20,625 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5100ms
2014-07-11 00:10:20,625 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-07-11 00:10:20,626 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:10:20,667 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:10:20,674 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:10:20,684 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:10:20,745 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:10:20,778 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3235, memsize=495.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/02a1ff8b95e946e68582c6f09c974323
2014-07-11 00:10:20,792 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/02a1ff8b95e946e68582c6f09c974323 as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/02a1ff8b95e946e68582c6f09c974323
2014-07-11 00:10:20,799 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:10:20,803 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/02a1ff8b95e946e68582c6f09c974323, entries=1802500, sequenceid=3235, filesize=128.4m
2014-07-11 00:10:20,804 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1022.3m/1071999600, currentsize=476.1m/499237600 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 34373ms, sequenceid=3235, compaction requested=true
2014-07-11 00:10:20,804 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:10:20,804 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 00:10:20,804 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-07-11 00:10:20,805 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 563.6m
2014-07-11 00:10:20,805 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 00:10:20,805 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,805 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:10:20,805 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5061ms
2014-07-11 00:10:20,805 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,805 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:10:20,805 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5122ms
2014-07-11 00:10:20,806 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,806 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:10:20,817 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5144ms
2014-07-11 00:10:20,817 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,817 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5150ms
2014-07-11 00:10:20,817 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,817 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5191ms
2014-07-11 00:10:20,817 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,817 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5216ms
2014-07-11 00:10:20,817 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,818 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5293ms
2014-07-11 00:10:20,818 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,818 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5223ms
2014-07-11 00:10:20,818 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,819 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5339ms
2014-07-11 00:10:20,819 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,819 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5354ms
2014-07-11 00:10:20,819 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,819 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5362ms
2014-07-11 00:10:20,819 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,819 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5364ms
2014-07-11 00:10:20,819 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,836 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-11 00:10:20,836 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,845 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5388ms
2014-07-11 00:10:20,845 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,845 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5235ms
2014-07-11 00:10:20,846 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,847 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2897ms
2014-07-11 00:10:20,847 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,847 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2939ms
2014-07-11 00:10:20,847 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,847 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2984ms
2014-07-11 00:10:20,847 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,848 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3036ms
2014-07-11 00:10:20,848 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,848 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3091ms
2014-07-11 00:10:20,848 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,848 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3138ms
2014-07-11 00:10:20,848 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,848 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3180ms
2014-07-11 00:10:20,848 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,850 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3961ms
2014-07-11 00:10:20,850 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,857 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4027ms
2014-07-11 00:10:20,857 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,857 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4127ms
2014-07-11 00:10:20,857 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,857 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4232ms
2014-07-11 00:10:20,857 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,865 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4339ms
2014-07-11 00:10:20,865 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,865 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4349ms
2014-07-11 00:10:20,865 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,866 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4376ms
2014-07-11 00:10:20,866 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,866 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4378ms
2014-07-11 00:10:20,866 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,867 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4382ms
2014-07-11 00:10:20,867 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,871 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4469ms
2014-07-11 00:10:20,871 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,872 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4539ms
2014-07-11 00:10:20,872 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,872 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4541ms
2014-07-11 00:10:20,872 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,873 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4545ms
2014-07-11 00:10:20,873 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,877 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4557ms
2014-07-11 00:10:20,878 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,878 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4563ms
2014-07-11 00:10:20,878 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,878 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4563ms
2014-07-11 00:10:20,878 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,881 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4571ms
2014-07-11 00:10:20,881 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,881 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4659ms
2014-07-11 00:10:20,881 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,881 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4660ms
2014-07-11 00:10:20,881 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,881 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4679ms
2014-07-11 00:10:20,881 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,882 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4717ms
2014-07-11 00:10:20,882 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,890 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4793ms
2014-07-11 00:10:20,890 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,891 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4856ms
2014-07-11 00:10:20,891 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,896 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-11 00:10:20,896 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,897 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4926ms
2014-07-11 00:10:20,897 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,897 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4927ms
2014-07-11 00:10:20,897 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:20,897 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4978ms
2014-07-11 00:10:20,897 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:21,168 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:10:21,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:21,533 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:10:21,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18730 synced till here 18711
2014-07-11 00:10:21,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062614519 with entries=111, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062621465
2014-07-11 00:10:21,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062518004
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062530524
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062532792
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062534711
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062536534
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062537791
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062539299
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062541279
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062543117
2014-07-11 00:10:21,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062546379
2014-07-11 00:10:23,878 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:23,961 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18837 synced till here 18804
2014-07-11 00:10:24,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062621465 with entries=107, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062623879
2014-07-11 00:10:25,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:25,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19003 synced till here 18999
2014-07-11 00:10:25,973 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062623879 with entries=166, filesize=101.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062625521
2014-07-11 00:10:26,118 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:10:27,386 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:27,472 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19081 synced till here 19072
2014-07-11 00:10:27,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062625521 with entries=78, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062627387
2014-07-11 00:10:29,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:29,188 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19161 synced till here 19158
2014-07-11 00:10:29,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062627387 with entries=80, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062629153
2014-07-11 00:10:30,830 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:32,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19337 synced till here 19323
2014-07-11 00:10:32,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062629153 with entries=176, filesize=143.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062630831
2014-07-11 00:10:33,257 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3371, memsize=462.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/65cebe432d9845bcbfb5d8aa6ffffdb7
2014-07-11 00:10:33,289 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/65cebe432d9845bcbfb5d8aa6ffffdb7 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/65cebe432d9845bcbfb5d8aa6ffffdb7
2014-07-11 00:10:33,306 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/65cebe432d9845bcbfb5d8aa6ffffdb7, entries=1685460, sequenceid=3371, filesize=120.0m
2014-07-11 00:10:33,314 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~901.9m/945696160, currentsize=459.4m/481667760 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 29016ms, sequenceid=3371, compaction requested=true
2014-07-11 00:10:33,314 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 00:10:33,315 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 00:10:33,315 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:10:33,315 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:10:33,315 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:10:33,315 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:10:33,315 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 943.3m
2014-07-11 00:10:33,338 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:10:34,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:34,690 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19430 synced till here 19423
2014-07-11 00:10:34,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062630831 with entries=93, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062634657
2014-07-11 00:10:34,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062548195
2014-07-11 00:10:34,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062555263
2014-07-11 00:10:35,090 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:10:35,961 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:35,989 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19501 synced till here 19500
2014-07-11 00:10:36,017 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062634657 with entries=71, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062635961
2014-07-11 00:10:37,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:37,109 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19569 synced till here 19568
2014-07-11 00:10:37,124 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062635961 with entries=68, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062637088
2014-07-11 00:10:38,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:38,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062637088 with entries=68, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062638290
2014-07-11 00:10:40,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:40,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062638290 with entries=87, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062640187
2014-07-11 00:10:42,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:42,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19794 synced till here 19790
2014-07-11 00:10:42,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062640187 with entries=70, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062642034
2014-07-11 00:10:43,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:44,502 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19902 synced till here 19896
2014-07-11 00:10:44,623 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062642034 with entries=108, filesize=100.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062643457
2014-07-11 00:10:44,984 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:44,985 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:44,988 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,031 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,046 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,047 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,057 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,093 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,114 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,120 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,200 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,208 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,221 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,236 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,317 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,321 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,325 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,411 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:45,531 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4826, memsize=481.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/3856320b3c48411fb0760b90fdbc0c11
2014-07-11 00:10:45,554 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/3856320b3c48411fb0760b90fdbc0c11 as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/3856320b3c48411fb0760b90fdbc0c11
2014-07-11 00:10:45,568 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/3856320b3c48411fb0760b90fdbc0c11, entries=1752880, sequenceid=4826, filesize=124.8m
2014-07-11 00:10:45,569 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~563.6m/591017040, currentsize=322.8m/338438960 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 24763ms, sequenceid=4826, compaction requested=true
2014-07-11 00:10:45,569 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:10:45,569 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-11 00:10:45,569 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 158ms
2014-07-11 00:10:45,569 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,570 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 961.1m
2014-07-11 00:10:45,570 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 245ms
2014-07-11 00:10:45,570 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,569 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-11 00:10:45,570 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 249ms
2014-07-11 00:10:45,570 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,570 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:10:45,570 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 253ms
2014-07-11 00:10:45,570 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,570 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:10:45,570 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:10:45,573 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 337ms
2014-07-11 00:10:45,573 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,573 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 352ms
2014-07-11 00:10:45,573 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,589 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 382ms
2014-07-11 00:10:45,589 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,597 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 397ms
2014-07-11 00:10:45,597 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,597 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 477ms
2014-07-11 00:10:45,597 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,597 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 483ms
2014-07-11 00:10:45,597 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,597 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 504ms
2014-07-11 00:10:45,597 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,597 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 540ms
2014-07-11 00:10:45,598 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,601 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 554ms
2014-07-11 00:10:45,601 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,601 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 555ms
2014-07-11 00:10:45,601 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,601 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-07-11 00:10:45,601 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,605 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 617ms
2014-07-11 00:10:45,605 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,605 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 620ms
2014-07-11 00:10:45,605 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,605 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 621ms
2014-07-11 00:10:45,605 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:45,649 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:10:45,789 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=7062, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-11 00:10:46,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:46,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19984 synced till here 19975
2014-07-11 00:10:47,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062643457 with entries=82, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062646901
2014-07-11 00:10:47,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062557169
2014-07-11 00:10:47,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062560080
2014-07-11 00:10:47,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062561559
2014-07-11 00:10:47,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062562900
2014-07-11 00:10:47,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062563948
2014-07-11 00:10:47,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062565352
2014-07-11 00:10:47,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062567273
2014-07-11 00:10:47,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062569114
2014-07-11 00:10:47,048 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:10:47,212 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:10:47,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:47,597 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20093 synced till here 20078
2014-07-11 00:10:47,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062646901 with entries=109, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062647576
2014-07-11 00:10:47,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:10:48,839 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:49,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20180 synced till here 20177
2014-07-11 00:10:49,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062647576 with entries=87, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062648840
2014-07-11 00:10:49,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:10:50,354 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:50,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20259 synced till here 20253
2014-07-11 00:10:50,440 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062648840 with entries=79, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062650354
2014-07-11 00:10:50,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): a6b9b2a8688211770d19e0dceb93c590
2014-07-11 00:10:51,360 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,368 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,375 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,376 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,408 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,412 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,420 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,453 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,489 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,499 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,645 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,654 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,663 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,810 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:51,853 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,012 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,013 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,207 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,312 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,393 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,397 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,496 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,574 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,746 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,871 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:52,877 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,019 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,058 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,062 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,190 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,336 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,343 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,344 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,361 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,406 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,454 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,538 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,581 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,644 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,698 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,764 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,813 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,828 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,883 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,891 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,893 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,916 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,946 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:53,997 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:54,105 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:10:55,955 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3616, memsize=404.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/adcca7eae5b04e79a4d115ff38feb438
2014-07-11 00:10:55,974 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/adcca7eae5b04e79a4d115ff38feb438 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/adcca7eae5b04e79a4d115ff38feb438
2014-07-11 00:10:55,990 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/adcca7eae5b04e79a4d115ff38feb438, entries=1473760, sequenceid=3616, filesize=105.0m
2014-07-11 00:10:55,990 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~949.3m/995408160, currentsize=356.3m/373635440 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 22675ms, sequenceid=3616, compaction requested=true
2014-07-11 00:10:55,991 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:10:55,991 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 00:10:55,992 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 00:10:55,992 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:10:55,992 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:10:55,992 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:10:55,992 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 408.7m
2014-07-11 00:10:55,993 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1887ms
2014-07-11 00:10:55,993 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:55,993 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1996ms
2014-07-11 00:10:55,993 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:55,997 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2051ms
2014-07-11 00:10:55,997 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:55,997 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2081ms
2014-07-11 00:10:55,997 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:55,997 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2104ms
2014-07-11 00:10:55,997 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:55,998 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2107ms
2014-07-11 00:10:55,998 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,004 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2121ms
2014-07-11 00:10:56,004 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,004 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2176ms
2014-07-11 00:10:56,005 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,005 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2192ms
2014-07-11 00:10:56,005 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,009 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2246ms
2014-07-11 00:10:56,009 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,009 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2311ms
2014-07-11 00:10:56,009 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,010 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2366ms
2014-07-11 00:10:56,011 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,021 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2440ms
2014-07-11 00:10:56,021 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,021 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2483ms
2014-07-11 00:10:56,021 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,033 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2579ms
2014-07-11 00:10:56,033 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,034 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2627ms
2014-07-11 00:10:56,034 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,034 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2673ms
2014-07-11 00:10:56,034 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,034 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2690ms
2014-07-11 00:10:56,034 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,035 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2691ms
2014-07-11 00:10:56,035 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,035 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2699ms
2014-07-11 00:10:56,035 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,036 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2846ms
2014-07-11 00:10:56,036 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,036 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2974ms
2014-07-11 00:10:56,036 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,037 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2979ms
2014-07-11 00:10:56,037 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,039 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3020ms
2014-07-11 00:10:56,039 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,039 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3162ms
2014-07-11 00:10:56,039 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,039 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3168ms
2014-07-11 00:10:56,039 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,044 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3298ms
2014-07-11 00:10:56,044 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,049 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3475ms
2014-07-11 00:10:56,050 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,050 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3554ms
2014-07-11 00:10:56,050 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,055 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3657ms
2014-07-11 00:10:56,055 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,056 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3662ms
2014-07-11 00:10:56,056 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,057 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3744ms
2014-07-11 00:10:56,057 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,058 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3850ms
2014-07-11 00:10:56,058 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,069 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4056ms
2014-07-11 00:10:56,069 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,069 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4057ms
2014-07-11 00:10:56,069 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,076 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4223ms
2014-07-11 00:10:56,076 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,079 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4269ms
2014-07-11 00:10:56,079 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,080 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4416ms
2014-07-11 00:10:56,080 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,081 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4427ms
2014-07-11 00:10:56,081 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,084 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4438ms
2014-07-11 00:10:56,084 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,084 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4586ms
2014-07-11 00:10:56,084 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,084 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4595ms
2014-07-11 00:10:56,084 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,084 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4631ms
2014-07-11 00:10:56,084 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,084 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4664ms
2014-07-11 00:10:56,085 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,086 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4673ms
2014-07-11 00:10:56,086 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,088 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4680ms
2014-07-11 00:10:56,088 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,098 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4721ms
2014-07-11 00:10:56,098 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,105 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4730ms
2014-07-11 00:10:56,105 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,113 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4745ms
2014-07-11 00:10:56,113 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,117 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4757ms
2014-07-11 00:10:56,117 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:10:56,314 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:10:56,576 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:10:56,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:57,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20382 synced till here 20381
2014-07-11 00:10:58,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062650354 with entries=123, filesize=105.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062656738
2014-07-11 00:10:59,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:10:59,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20478 synced till here 20445
2014-07-11 00:11:00,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062656738 with entries=96, filesize=97.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062659132
2014-07-11 00:11:01,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:02,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20598 synced till here 20567
2014-07-11 00:11:03,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062659132 with entries=120, filesize=95.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062661732
2014-07-11 00:11:05,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:05,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20706 synced till here 20676
2014-07-11 00:11:05,663 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062661732 with entries=108, filesize=100.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062665077
2014-07-11 00:11:07,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:07,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20821 synced till here 20795
2014-07-11 00:11:07,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062665077 with entries=115, filesize=95.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062667607
2014-07-11 00:11:09,469 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1275ms
GC pool 'ParNew' had collection(s): count=1 time=1237ms
2014-07-11 00:11:10,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:10,197 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,202 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,202 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,203 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,204 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,205 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,207 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,209 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,212 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,212 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,213 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,214 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,280 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,421 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,423 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,447 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,448 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,449 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,449 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,450 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,458 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,458 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,462 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,484 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,484 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,563 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,563 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,564 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,564 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,565 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,565 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,565 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,566 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,566 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,567 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,567 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,567 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,567 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,570 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,587 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,589 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,589 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,664 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:10,706 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:11,607 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:11,618 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:13,464 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:13,464 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:13,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062667607 with entries=130, filesize=115.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062670148
2014-07-11 00:11:13,468 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:13,471 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:14,180 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3764, memsize=206.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/6e5043d6c7454689a150bfd8c936aad7
2014-07-11 00:11:14,198 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/6e5043d6c7454689a150bfd8c936aad7 as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/6e5043d6c7454689a150bfd8c936aad7
2014-07-11 00:11:14,214 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/6e5043d6c7454689a150bfd8c936aad7, entries=752020, sequenceid=3764, filesize=53.6m
2014-07-11 00:11:14,215 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~408.7m/428508320, currentsize=59.7m/62584160 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 18223ms, sequenceid=3764, compaction requested=true
2014-07-11 00:11:14,215 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:11:14,215 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-11 00:11:14,215 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-11 00:11:14,215 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 744ms
2014-07-11 00:11:14,216 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,216 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:11:14,216 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 748ms
2014-07-11 00:11:14,216 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,216 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 1.1g
2014-07-11 00:11:14,216 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:11:14,216 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. because compaction request was cancelled
2014-07-11 00:11:14,229 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 765ms
2014-07-11 00:11:14,229 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,229 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 765ms
2014-07-11 00:11:14,229 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,229 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2612ms
2014-07-11 00:11:14,229 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,229 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2622ms
2014-07-11 00:11:14,229 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,230 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3524ms
2014-07-11 00:11:14,230 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,230 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3566ms
2014-07-11 00:11:14,230 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,230 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3641ms
2014-07-11 00:11:14,230 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,230 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3642ms
2014-07-11 00:11:14,231 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,231 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3644ms
2014-07-11 00:11:14,231 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,231 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3661ms
2014-07-11 00:11:14,231 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,231 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3665ms
2014-07-11 00:11:14,231 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,234 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3668ms
2014-07-11 00:11:14,234 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,238 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3671ms
2014-07-11 00:11:14,238 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,252 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3672ms
2014-07-11 00:11:14,252 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,253 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3686ms
2014-07-11 00:11:14,253 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,253 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3687ms
2014-07-11 00:11:14,253 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,253 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3688ms
2014-07-11 00:11:14,253 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,261 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3696ms
2014-07-11 00:11:14,261 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,261 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-11 00:11:14,261 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,261 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-11 00:11:14,261 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,261 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-11 00:11:14,262 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,262 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3699ms
2014-07-11 00:11:14,262 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,278 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3710ms
2014-07-11 00:11:14,278 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,278 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3794ms
2014-07-11 00:11:14,278 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,278 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3794ms
2014-07-11 00:11:14,279 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,279 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3817ms
2014-07-11 00:11:14,279 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,279 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3821ms
2014-07-11 00:11:14,279 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,280 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3821ms
2014-07-11 00:11:14,280 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,283 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3833ms
2014-07-11 00:11:14,283 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,284 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3835ms
2014-07-11 00:11:14,284 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,284 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3835ms
2014-07-11 00:11:14,284 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,284 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3836ms
2014-07-11 00:11:14,284 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,285 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3838ms
2014-07-11 00:11:14,285 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,285 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3862ms
2014-07-11 00:11:14,285 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,285 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3864ms
2014-07-11 00:11:14,286 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,297 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4017ms
2014-07-11 00:11:14,297 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,297 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4084ms
2014-07-11 00:11:14,297 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,297 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4084ms
2014-07-11 00:11:14,297 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,298 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4089ms
2014-07-11 00:11:14,298 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,309 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4097ms
2014-07-11 00:11:14,309 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,310 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4100ms
2014-07-11 00:11:14,310 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,310 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4103ms
2014-07-11 00:11:14,310 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,321 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4116ms
2014-07-11 00:11:14,321 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,321 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4117ms
2014-07-11 00:11:14,321 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,321 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4118ms
2014-07-11 00:11:14,321 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,329 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4127ms
2014-07-11 00:11:14,329 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,329 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4127ms
2014-07-11 00:11:14,329 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:14,329 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4132ms
2014-07-11 00:11:14,329 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:15,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:16,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21064 synced till here 21030
2014-07-11 00:11:16,136 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:11:16,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062670148 with entries=113, filesize=86.4m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062675219
2014-07-11 00:11:16,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062571448
2014-07-11 00:11:16,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062573282
2014-07-11 00:11:16,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062575208
2014-07-11 00:11:16,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062577282
2014-07-11 00:11:16,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062579282
2014-07-11 00:11:16,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062581219
2014-07-11 00:11:16,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062583480
2014-07-11 00:11:17,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:17,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062675219 with entries=85, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062677154
2014-07-11 00:11:18,755 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,791 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,847 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,848 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,874 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,909 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,944 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,975 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,975 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,976 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,976 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,977 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,978 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,978 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,979 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,980 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,981 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,981 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,988 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:18,999 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,006 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,023 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,034 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,037 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,045 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,053 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,053 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,059 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,060 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,069 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,069 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,081 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,081 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,081 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,081 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,082 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,082 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,094 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,095 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,095 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,120 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,123 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,157 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,349 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,471 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:19,574 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:20,192 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3715, memsize=478.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/2f36e342a1064876b98565e66e6b49dd
2014-07-11 00:11:20,214 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/2f36e342a1064876b98565e66e6b49dd as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/2f36e342a1064876b98565e66e6b49dd
2014-07-11 00:11:20,232 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/2f36e342a1064876b98565e66e6b49dd, entries=1741690, sequenceid=3715, filesize=124.1m
2014-07-11 00:11:20,232 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~961.1m/1007751280, currentsize=473.5m/496527840 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 34662ms, sequenceid=3715, compaction requested=true
2014-07-11 00:11:20,233 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:11:20,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 00:11:20,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 00:11:20,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:11:20,233 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:11:20,233 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 659ms
2014-07-11 00:11:20,233 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:11:20,234 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,234 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 695.5m
2014-07-11 00:11:20,234 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 763ms
2014-07-11 00:11:20,234 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,234 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 885ms
2014-07-11 00:11:20,234 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,235 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1078ms
2014-07-11 00:11:20,235 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,235 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1112ms
2014-07-11 00:11:20,237 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,241 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1120ms
2014-07-11 00:11:20,241 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,241 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1146ms
2014-07-11 00:11:20,241 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,242 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1147ms
2014-07-11 00:11:20,242 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,242 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1148ms
2014-07-11 00:11:20,242 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,242 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1160ms
2014-07-11 00:11:20,242 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,243 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1162ms
2014-07-11 00:11:20,243 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,244 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1163ms
2014-07-11 00:11:20,244 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,245 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1163ms
2014-07-11 00:11:20,245 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,246 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1164ms
2014-07-11 00:11:20,246 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,246 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1165ms
2014-07-11 00:11:20,246 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,247 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1178ms
2014-07-11 00:11:20,247 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,248 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1180ms
2014-07-11 00:11:20,248 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,248 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1188ms
2014-07-11 00:11:20,248 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,248 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1189ms
2014-07-11 00:11:20,248 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,248 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1195ms
2014-07-11 00:11:20,249 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,257 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1204ms
2014-07-11 00:11:20,257 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,257 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1212ms
2014-07-11 00:11:20,257 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,257 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1220ms
2014-07-11 00:11:20,258 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,258 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1224ms
2014-07-11 00:11:20,258 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,258 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1235ms
2014-07-11 00:11:20,258 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,261 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1256ms
2014-07-11 00:11:20,261 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,261 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1262ms
2014-07-11 00:11:20,261 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,262 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1273ms
2014-07-11 00:11:20,262 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,262 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1281ms
2014-07-11 00:11:20,262 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,265 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1284ms
2014-07-11 00:11:20,265 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,265 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1285ms
2014-07-11 00:11:20,265 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,265 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1286ms
2014-07-11 00:11:20,265 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,273 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1295ms
2014-07-11 00:11:20,273 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,273 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1295ms
2014-07-11 00:11:20,273 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,274 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1296ms
2014-07-11 00:11:20,274 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,282 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1305ms
2014-07-11 00:11:20,282 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,282 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1306ms
2014-07-11 00:11:20,282 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,282 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1307ms
2014-07-11 00:11:20,282 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,289 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1314ms
2014-07-11 00:11:20,289 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,289 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1345ms
2014-07-11 00:11:20,289 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,290 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1382ms
2014-07-11 00:11:20,291 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,291 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1417ms
2014-07-11 00:11:20,291 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,291 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1443ms
2014-07-11 00:11:20,291 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,301 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1453ms
2014-07-11 00:11:20,301 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,301 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1510ms
2014-07-11 00:11:20,302 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,302 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1547ms
2014-07-11 00:11:20,302 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:20,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:20,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21249 synced till here 21234
2014-07-11 00:11:21,610 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:11:21,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062677154 with entries=100, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062680420
2014-07-11 00:11:21,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062585132
2014-07-11 00:11:21,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062586737
2014-07-11 00:11:21,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062588466
2014-07-11 00:11:21,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062590607
2014-07-11 00:11:21,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062592403
2014-07-11 00:11:21,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062594314
2014-07-11 00:11:21,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062596460
2014-07-11 00:11:21,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062598832
2014-07-11 00:11:21,926 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:11:22,380 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:22,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062680420 with entries=125, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062682380
2014-07-11 00:11:24,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:24,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062682380 with entries=159, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062684681
2014-07-11 00:11:26,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:26,968 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062684681 with entries=66, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062686942
2014-07-11 00:11:29,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:30,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062686942 with entries=87, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062689290
2014-07-11 00:11:32,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:32,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21765 synced till here 21757
2014-07-11 00:11:32,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062689290 with entries=79, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062692374
2014-07-11 00:11:34,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:34,297 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21865 synced till here 21853
2014-07-11 00:11:34,339 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062692374 with entries=100, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062694278
2014-07-11 00:11:35,864 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:35,890 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21946 synced till here 21944
2014-07-11 00:11:35,914 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062694278 with entries=81, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062695864
2014-07-11 00:11:36,559 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,597 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,618 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,632 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,675 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,762 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,956 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:36,966 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,108 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,156 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,171 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,261 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,373 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:37,450 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:38,724 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:38,770 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1405061745702: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-11 00:11:38,937 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5460, memsize=429.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/4d3ae873b6a14ac297bb4f96da27652b
2014-07-11 00:11:38,968 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/4d3ae873b6a14ac297bb4f96da27652b as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/4d3ae873b6a14ac297bb4f96da27652b
2014-07-11 00:11:38,984 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/4d3ae873b6a14ac297bb4f96da27652b, entries=1562150, sequenceid=5460, filesize=111.2m
2014-07-11 00:11:38,984 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~695.5m/729323360, currentsize=191.7m/200992480 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 18750ms, sequenceid=5460, compaction requested=true
2014-07-11 00:11:38,985 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:11:38,985 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-11 00:11:38,985 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 215ms
2014-07-11 00:11:38,985 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-11 00:11:38,985 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 909.6m
2014-07-11 00:11:38,985 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:11:38,985 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,985 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:11:38,985 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 261ms
2014-07-11 00:11:38,985 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,985 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:11:38,985 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1536ms
2014-07-11 00:11:38,986 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,986 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1613ms
2014-07-11 00:11:38,986 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,986 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1725ms
2014-07-11 00:11:38,986 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,989 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1818ms
2014-07-11 00:11:38,989 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,993 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1837ms
2014-07-11 00:11:38,993 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,996 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1888ms
2014-07-11 00:11:38,996 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,996 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2030ms
2014-07-11 00:11:38,996 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,996 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2040ms
2014-07-11 00:11:38,996 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:38,996 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2234ms
2014-07-11 00:11:38,996 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,005 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2330ms
2014-07-11 00:11:39,005 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,005 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2373ms
2014-07-11 00:11:39,005 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,005 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2387ms
2014-07-11 00:11:39,005 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,009 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2408ms
2014-07-11 00:11:39,009 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,010 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2452ms
2014-07-11 00:11:39,010 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1405061745702
2014-07-11 00:11:39,902 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:40,136 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22045 synced till here 22043
2014-07-11 00:11:40,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062695864 with entries=99, filesize=81.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062699903
2014-07-11 00:11:40,296 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:11:40,535 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3916, memsize=545.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/30cfa5993c134a0cb8fb70cf575462c9
2014-07-11 00:11:40,556 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/30cfa5993c134a0cb8fb70cf575462c9 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/30cfa5993c134a0cb8fb70cf575462c9
2014-07-11 00:11:40,566 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/30cfa5993c134a0cb8fb70cf575462c9, entries=1986790, sequenceid=3916, filesize=141.4m
2014-07-11 00:11:40,567 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1132857280, currentsize=339.5m/356007840 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 26351ms, sequenceid=3916, compaction requested=true
2014-07-11 00:11:40,567 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:11:40,567 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-11 00:11:40,567 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-11 00:11:40,567 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:11:40,568 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 730.8m
2014-07-11 00:11:40,568 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:11:40,568 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:11:40,603 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5.
2014-07-11 00:11:41,432 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:11:42,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:42,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062699903 with entries=78, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062702153
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062601124
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062605142
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062607320
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062610567
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062612599
2014-07-11 00:11:42,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062614519
2014-07-11 00:11:42,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062621465
2014-07-11 00:11:42,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062623879
2014-07-11 00:11:42,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062625521
2014-07-11 00:11:42,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062627387
2014-07-11 00:11:42,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062629153
2014-07-11 00:11:44,682 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:44,700 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22203 synced till here 22195
2014-07-11 00:11:44,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062702153 with entries=80, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062704683
2014-07-11 00:11:45,333 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96.
2014-07-11 00:11:46,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:46,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22282 synced till here 22275
2014-07-11 00:11:46,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062704683 with entries=79, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062706091
2014-07-11 00:11:47,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:47,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22350 synced till here 22346
2014-07-11 00:11:47,603 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062706091 with entries=68, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062707500
2014-07-11 00:11:50,078 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:50,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062707500 with entries=89, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062710078
2014-07-11 00:11:54,839 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4112, memsize=341.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/91c82e7f276e44a2b0042d811299becb
2014-07-11 00:11:54,850 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/91c82e7f276e44a2b0042d811299becb as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/91c82e7f276e44a2b0042d811299becb
2014-07-11 00:11:54,864 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/91c82e7f276e44a2b0042d811299becb, entries=1245010, sequenceid=4112, filesize=88.7m
2014-07-11 00:11:54,865 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~735.6m/771330800, currentsize=152.1m/159511440 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 14298ms, sequenceid=4112, compaction requested=true
2014-07-11 00:11:54,865 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:11:54,865 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 00:11:54,865 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 00:11:54,865 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5., current region memstore size 492.1m
2014-07-11 00:11:54,866 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:11:54,866 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:11:54,866 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:11:55,209 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:11:56,332 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:56,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22562 synced till here 22561
2014-07-11 00:11:57,245 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062710078 with entries=123, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062716332
2014-07-11 00:11:58,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:11:58,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22652 synced till here 22647
2014-07-11 00:11:59,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062716332 with entries=90, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062718941
2014-07-11 00:12:00,301 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4092, memsize=520.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/3001d866c54d48d2876e2b5ec40f54b2
2014-07-11 00:12:00,315 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/3001d866c54d48d2876e2b5ec40f54b2 as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/3001d866c54d48d2876e2b5ec40f54b2
2014-07-11 00:12:00,325 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/3001d866c54d48d2876e2b5ec40f54b2, entries=1896110, sequenceid=4092, filesize=135.0m
2014-07-11 00:12:00,326 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~909.6m/953749280, currentsize=254.5m/266834880 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 21341ms, sequenceid=4092, compaction requested=true
2014-07-11 00:12:00,326 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:00,326 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 00:12:00,327 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 00:12:00,327 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96., current region memstore size 386.3m
2014-07-11 00:12:00,327 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:00,327 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:00,327 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:12:00,563 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:12:00,701 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0.
2014-07-11 00:12:00,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:00,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22730 synced till here 22728
2014-07-11 00:12:00,992 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062718941 with entries=78, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062720941
2014-07-11 00:12:00,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062630831
2014-07-11 00:12:00,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062634657
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062635961
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062637088
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062638290
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062640187
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062642034
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062643457
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062646901
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062647576
2014-07-11 00:12:00,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062648840
2014-07-11 00:12:02,705 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:02,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062720941 with entries=76, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062722705
2014-07-11 00:12:02,853 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48.
2014-07-11 00:12:02,867 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590.
2014-07-11 00:12:04,300 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:04,575 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22891 synced till here 22889
2014-07-11 00:12:04,599 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062722705 with entries=85, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062724301
2014-07-11 00:12:06,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:06,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22961 synced till here 22960
2014-07-11 00:12:06,076 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062724301 with entries=70, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062726052
2014-07-11 00:12:07,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:07,900 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23034 synced till here 23032
2014-07-11 00:12:07,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062726052 with entries=73, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062727883
2014-07-11 00:12:08,380 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4189, memsize=353.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/9ad5125befc24eeba063d77e7b113ed7
2014-07-11 00:12:08,397 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/.tmp/9ad5125befc24eeba063d77e7b113ed7 as hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/9ad5125befc24eeba063d77e7b113ed7
2014-07-11 00:12:08,407 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/d3859e2dcf154a4a686686041e068bf5/family/9ad5125befc24eeba063d77e7b113ed7, entries=1286630, sequenceid=4189, filesize=91.6m
2014-07-11 00:12:08,408 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~492.1m/515996000, currentsize=194.5m/203973920 for region usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. in 13542ms, sequenceid=4189, compaction requested=true
2014-07-11 00:12:08,408 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:08,408 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-11 00:12:08,408 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0., current region memstore size 382.5m
2014-07-11 00:12:08,408 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-11 00:12:08,409 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:08,409 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:08,409 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user5,1405062122530.d3859e2dcf154a4a686686041e068bf5. because compaction request was cancelled
2014-07-11 00:12:08,699 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:12:09,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-11 00:12:10,400 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062727883 with entries=91, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1405061745702/slave1%2C60020%2C1405061745702.1405062729599
2014-07-11 00:12:12,479 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5827, memsize=363.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/e931f60df75c434c9939891a50d870fd
2014-07-11 00:12:12,501 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/.tmp/e931f60df75c434c9939891a50d870fd as hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/e931f60df75c434c9939891a50d870fd
2014-07-11 00:12:12,517 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e650a1993bb1a49e183adcbb9770b96/family/e931f60df75c434c9939891a50d870fd, entries=1322600, sequenceid=5827, filesize=94.1m
2014-07-11 00:12:12,517 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~386.3m/405018000, currentsize=165.9m/173950720 for region usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. in 12190ms, sequenceid=5827, compaction requested=true
2014-07-11 00:12:12,518 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:12,518 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-11 00:12:12,518 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-11 00:12:12,518 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:12,518 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48., current region memstore size 382.5m
2014-07-11 00:12:12,518 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:12,518 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user1,1405062122529.3e650a1993bb1a49e183adcbb9770b96. because compaction request was cancelled
2014-07-11 00:12:12,750 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:12:19,614 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4310, memsize=373.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/80c29d7d9d6441eda267283ff5b1afeb
2014-07-11 00:12:19,634 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/.tmp/80c29d7d9d6441eda267283ff5b1afeb as hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/80c29d7d9d6441eda267283ff5b1afeb
2014-07-11 00:12:19,652 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b5cb06e5da6171e7af34144523ae71d0/family/80c29d7d9d6441eda267283ff5b1afeb, entries=1359160, sequenceid=4310, filesize=96.8m
2014-07-11 00:12:19,653 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~383.8m/402412480, currentsize=30.9m/32424880 for region usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. in 11244ms, sequenceid=4310, compaction requested=true
2014-07-11 00:12:19,653 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:19,653 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 00:12:19,653 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 00:12:19,654 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590., current region memstore size 295.2m
2014-07-11 00:12:19,654 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:19,654 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:19,654 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user4,1405062122530.b5cb06e5da6171e7af34144523ae71d0. because compaction request was cancelled
2014-07-11 00:12:19,860 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-11 00:12:23,773 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4332, memsize=382.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/3b071e3e372e457e85009700a54fba4a
2014-07-11 00:12:23,791 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/.tmp/3b071e3e372e457e85009700a54fba4a as hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/3b071e3e372e457e85009700a54fba4a
2014-07-11 00:12:23,806 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1231dfaa320930fef79c95c3c97a0c48/family/3b071e3e372e457e85009700a54fba4a, entries=1392860, sequenceid=4332, filesize=99.2m
2014-07-11 00:12:23,806 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~382.5m/401132640, currentsize=0.0/0 for region usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. in 11288ms, sequenceid=4332, compaction requested=true
2014-07-11 00:12:23,807 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:23,807 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-11 00:12:23,807 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-11 00:12:23,807 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:23,807 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:23,807 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user6,1405062122530.1231dfaa320930fef79c95c3c97a0c48. because compaction request was cancelled
2014-07-11 00:12:25,484 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4308, memsize=175.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/db7263aa9076498bab600f0c93ee010b
2014-07-11 00:12:25,500 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/.tmp/db7263aa9076498bab600f0c93ee010b as hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/db7263aa9076498bab600f0c93ee010b
2014-07-11 00:12:25,513 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a6b9b2a8688211770d19e0dceb93c590/family/db7263aa9076498bab600f0c93ee010b, entries=639700, sequenceid=4308, filesize=45.6m
2014-07-11 00:12:25,514 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~295.2m/309542160, currentsize=0.0/0 for region usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. in 5861ms, sequenceid=4308, compaction requested=true
2014-07-11 00:12:25,514 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-11 00:12:25,514 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-11 00:12:25,514 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-11 00:12:25,514 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-11 00:12:25,514 DEBUG [regionserver60020-smallCompactions-1405061784729] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-11 00:12:25,514 DEBUG [regionserver60020-smallCompactions-1405061784729] regionserver.CompactSplitThread: Not compacting usertable,user9,1405062122530.a6b9b2a8688211770d19e0dceb93c590. because compaction request was cancelled
