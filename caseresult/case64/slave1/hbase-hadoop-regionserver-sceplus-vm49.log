Thu Jul 10 15:33:58 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-10 15:33:58,594 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-10 15:33:58,595 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-10 15:33:58,595 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-10 15:33:58,834 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-10 15:33:58,834 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-10 15:33:58,834 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-10 15:33:58,834 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-10 15:33:58,834 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 51332 22
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-10 15:33:58,835 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 51332 9.1.143.59 22
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-10 15:33:58,836 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-10 15:33:58,838 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-10 15:33:58,838 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=225
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-10 15:33:58,839 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-10 15:33:58,840 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-10 15:33:58,842 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-10 15:33:58,842 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-10 15:33:59,066 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-10 15:33:59,490 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-10 15:33:59,593 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-10 15:33:59,605 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-10 15:33:59,668 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-10 15:33:59,669 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-10 15:33:59,669 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-10 15:33:59,675 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-10 15:33:59,679 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-10 15:33:59,770 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-10 15:33:59,770 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-10 15:33:59,774 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-10 15:33:59,776 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-10 15:33:59,852 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-10 15:33:59,910 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-10 15:33:59,919 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-10 15:33:59,921 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-10 15:33:59,921 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-10 15:33:59,921 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-10 15:34:00,240 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-10 15:34:00,284 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-10 15:34:00,290 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-10 15:34:00,291 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-10 15:34:00,291 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-10 15:34:00,292 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 15:34:00,315 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 15:34:00,330 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:34:00,335 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-10 15:34:00,353 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x4722696eb90000, negotiated timeout = 90000
2014-07-10 15:34:33,158 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x71856b6b, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 15:34:33,159 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x71856b6b connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 15:34:33,160 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:34:33,160 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-10 15:34:33,165 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x14722696b230002, negotiated timeout = 90000
2014-07-10 15:34:33,443 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@56adba61
2014-07-10 15:34:33,449 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-10 15:34:33,455 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-10 15:34:33,472 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-10 15:34:33,535 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-10 15:34:33,546 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-10 15:34:33,551 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-10 15:34:33,572 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1405031639072 with port=60020, startcode=1405031639695
2014-07-10 15:34:33,861 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-10 15:34:33,861 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-10 15:34:33,861 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-10 15:34:33,897 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-10 15:34:33,912 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695
2014-07-10 15:34:33,959 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-10 15:34:33,971 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-10 15:34:34,080 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405031673977
2014-07-10 15:34:34,098 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-10 15:34:34,103 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-10 15:34:34,107 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-10 15:34:34,111 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-10 15:34:34,114 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-10 15:34:34,114 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-10 15:34:34,114 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-10 15:34:34,114 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-10 15:34:34,115 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-10 15:34:34,124 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1405031640512, slave1,60020,1405031639695] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1405031640512, slave1,60020,1405031639695]
2014-07-10 15:34:34,151 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-10 15:34:34,153 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0xecb57e0, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-10 15:34:34,154 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0xecb57e0 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-10 15:34:34,155 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:34:34,156 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-10 15:34:34,160 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x4722696eb90004, negotiated timeout = 90000
2014-07-10 15:34:34,166 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-10 15:34:34,166 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-10 15:34:34,222 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1405031639695, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x4722696eb90000
2014-07-10 15:34:34,222 INFO  [SplitLogWorker-slave1,60020,1405031639695] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405031639695 starting
2014-07-10 15:34:34,222 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-10 15:34:34,223 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1405031639695
2014-07-10 15:34:34,223 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1405031639695'
2014-07-10 15:34:34,223 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-10 15:34:34,224 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-10 15:34:34,225 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-10 15:34:38,854 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:34:39,019 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b2219710a6a33bc63194598235eef963 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,040 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:34:39,041 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:34:39,042 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d8c99231d7b65ab261a447265aa324a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,044 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:34:39,045 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 44724ac6e2ab8557d62f0b94602fa104 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,046 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:34:39,046 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:34:39,058 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:34:39,059 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:34:39,059 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:34:39,059 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:34:39,059 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:34:39,073 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d8c99231d7b65ab261a447265aa324a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,074 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 44724ac6e2ab8557d62f0b94602fa104 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,074 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b2219710a6a33bc63194598235eef963 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,093 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 3d8c99231d7b65ab261a447265aa324a, NAME => 'usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-10 15:34:39,093 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => b2219710a6a33bc63194598235eef963, NAME => 'usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-10 15:34:39,093 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 44724ac6e2ab8557d62f0b94602fa104, NAME => 'usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-10 15:34:39,130 INFO  [RS_OPEN_REGION-slave1:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-10 15:34:39,131 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 44724ac6e2ab8557d62f0b94602fa104
2014-07-10 15:34:39,131 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b2219710a6a33bc63194598235eef963
2014-07-10 15:34:39,131 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3d8c99231d7b65ab261a447265aa324a
2014-07-10 15:34:39,132 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:34:39,132 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:34:39,132 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:34:39,140 INFO  [RS_OPEN_REGION-slave1:60020-0] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-10 15:34:39,142 INFO  [RS_OPEN_REGION-slave1:60020-0] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-10 15:34:39,144 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-10 15:34:39,144 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-10 15:34:39,146 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-10 15:34:39,221 INFO  [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,221 INFO  [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,233 INFO  [StoreOpener-b2219710a6a33bc63194598235eef963-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,269 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-10 15:34:39,347 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-10 15:34:39,347 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-10 15:34:39,348 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-10 15:34:39,360 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/11ce08a1278e4c33acf532c3447883b7, isReference=false, isBulkLoadResult=false, seqid=609, majorCompaction=false
2014-07-10 15:34:39,360 DEBUG [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104/family/352d4591e6bd440986fb011664228521, isReference=false, isBulkLoadResult=false, seqid=343, majorCompaction=false
2014-07-10 15:34:39,360 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/59b15a4106734b5d9e61a8d0e7282ff7, isReference=false, isBulkLoadResult=false, seqid=772, majorCompaction=false
2014-07-10 15:34:39,381 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/64643752dfdf4543b2d728a1a14842f1, isReference=false, isBulkLoadResult=false, seqid=288, majorCompaction=false
2014-07-10 15:34:39,387 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/3d5021ba9ad2416faba0001a834d0cdb, isReference=false, isBulkLoadResult=false, seqid=157, majorCompaction=false
2014-07-10 15:34:39,401 DEBUG [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104/family/9e10944a35ee4b4d9392ccbffaff4c02, isReference=false, isBulkLoadResult=false, seqid=2141, majorCompaction=false
2014-07-10 15:34:39,405 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/6473932f6e464bf3a3eae15e05100662, isReference=false, isBulkLoadResult=false, seqid=1074, majorCompaction=false
2014-07-10 15:34:39,410 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/4b05888398a14f069f9a6c54766845a5, isReference=false, isBulkLoadResult=false, seqid=1427, majorCompaction=false
2014-07-10 15:34:39,415 DEBUG [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104/family/a90942cd4f1a409baec96c34ce79a559, isReference=false, isBulkLoadResult=false, seqid=1215, majorCompaction=false
2014-07-10 15:34:39,423 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/73161f4d3d4848bf956070a04ec0e085, isReference=false, isBulkLoadResult=false, seqid=1124, majorCompaction=false
2014-07-10 15:34:39,424 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/bad7bbb363fa402d842faa80c505eeef, isReference=false, isBulkLoadResult=false, seqid=1472, majorCompaction=false
2014-07-10 15:34:39,432 DEBUG [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104/family/aad38725683c4b5bbf0131ee0b66c853, isReference=false, isBulkLoadResult=false, seqid=2704, majorCompaction=false
2014-07-10 15:34:39,436 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/c3837208c0f74f7f8a4103c6b398b23d, isReference=false, isBulkLoadResult=false, seqid=1482, majorCompaction=false
2014-07-10 15:34:39,436 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/7da53d69d7924178a71e780026ac739d, isReference=false, isBulkLoadResult=false, seqid=1492, majorCompaction=false
2014-07-10 15:34:39,450 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/8835b39eab874ee2854678c12acd0695, isReference=false, isBulkLoadResult=false, seqid=825, majorCompaction=false
2014-07-10 15:34:39,468 DEBUG [StoreOpener-b2219710a6a33bc63194598235eef963-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963/family/d4bba2aae9a840c3a2bc3b1f41355fc8, isReference=false, isBulkLoadResult=false, seqid=1188, majorCompaction=false
2014-07-10 15:34:39,477 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/9346d03e3cb343b786e314c7339fd800, isReference=false, isBulkLoadResult=false, seqid=1197, majorCompaction=false
2014-07-10 15:34:39,489 DEBUG [StoreOpener-44724ac6e2ab8557d62f0b94602fa104-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104/family/e4c014cbbbd34cba941f1d19cda53ff8, isReference=false, isBulkLoadResult=false, seqid=2374, majorCompaction=false
2014-07-10 15:34:39,495 DEBUG [StoreOpener-3d8c99231d7b65ab261a447265aa324a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/fa413dc6950543ea825411f759c37fc6, isReference=false, isBulkLoadResult=false, seqid=974, majorCompaction=false
2014-07-10 15:34:39,536 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/44724ac6e2ab8557d62f0b94602fa104
2014-07-10 15:34:39,537 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a
2014-07-10 15:34:39,539 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b2219710a6a33bc63194598235eef963
2014-07-10 15:34:39,542 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 3d8c99231d7b65ab261a447265aa324a; next sequenceid=1493
2014-07-10 15:34:39,542 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 44724ac6e2ab8557d62f0b94602fa104; next sequenceid=2705
2014-07-10 15:34:39,543 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 44724ac6e2ab8557d62f0b94602fa104
2014-07-10 15:34:39,543 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3d8c99231d7b65ab261a447265aa324a
2014-07-10 15:34:39,547 INFO  [PostOpenDeployTasks:44724ac6e2ab8557d62f0b94602fa104] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:34:39,547 INFO  [PostOpenDeployTasks:3d8c99231d7b65ab261a447265aa324a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:34:39,550 DEBUG [PostOpenDeployTasks:3d8c99231d7b65ab261a447265aa324a] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-10 15:34:39,575 DEBUG [PostOpenDeployTasks:44724ac6e2ab8557d62f0b94602fa104] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:34:39,586 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-10 15:34:39,588 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined b2219710a6a33bc63194598235eef963; next sequenceid=1483
2014-07-10 15:34:39,588 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b2219710a6a33bc63194598235eef963
2014-07-10 15:34:39,590 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 8 files of size 595170461 starting at candidate #0 after considering 21 permutations with 21 in ratio
2014-07-10 15:34:39,592 INFO  [PostOpenDeployTasks:b2219710a6a33bc63194598235eef963] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:34:39,592 DEBUG [PostOpenDeployTasks:b2219710a6a33bc63194598235eef963] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-10 15:34:39,592 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: 3d8c99231d7b65ab261a447265aa324a - family: Initiating major compaction
2014-07-10 15:34:39,592 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: Starting compaction on family in region usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:34:39,593 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Starting compaction of 8 file(s) in family of usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/.tmp, totalSize=567.6m
2014-07-10 15:34:39,594 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/3d5021ba9ad2416faba0001a834d0cdb, keycount=95008, bloomtype=ROW, size=67.7m, encoding=NONE, seqNum=157, earliestPutTs=1405030793557
2014-07-10 15:34:39,594 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/11ce08a1278e4c33acf532c3447883b7, keycount=263909, bloomtype=ROW, size=187.8m, encoding=NONE, seqNum=609, earliestPutTs=1405030832498
2014-07-10 15:34:39,594 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/8835b39eab874ee2854678c12acd0695, keycount=130247, bloomtype=ROW, size=92.7m, encoding=NONE, seqNum=825, earliestPutTs=1405030907115
2014-07-10 15:34:39,595 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/fa413dc6950543ea825411f759c37fc6, keycount=93219, bloomtype=ROW, size=66.4m, encoding=NONE, seqNum=974, earliestPutTs=1405031014147
2014-07-10 15:34:39,595 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/73161f4d3d4848bf956070a04ec0e085, keycount=93311, bloomtype=ROW, size=66.5m, encoding=NONE, seqNum=1124, earliestPutTs=1405031042932
2014-07-10 15:34:39,596 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/9346d03e3cb343b786e314c7339fd800, keycount=42803, bloomtype=ROW, size=30.5m, encoding=NONE, seqNum=1197, earliestPutTs=1405031096185
2014-07-10 15:34:39,596 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/4b05888398a14f069f9a6c54766845a5, keycount=54969, bloomtype=ROW, size=39.2m, encoding=NONE, seqNum=1427, earliestPutTs=1405031279785
2014-07-10 15:34:39,597 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3d8c99231d7b65ab261a447265aa324a/family/7da53d69d7924178a71e780026ac739d, keycount=23502, bloomtype=ROW, size=16.8m, encoding=NONE, seqNum=1492, earliestPutTs=1405031326732
2014-07-10 15:34:39,720 DEBUG [regionserver60020-smallCompactions-1405031679550] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:34:39,774 INFO  [PostOpenDeployTasks:3d8c99231d7b65ab261a447265aa324a] catalog.MetaEditor: Updated row usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a. with server=slave1,60020,1405031639695
2014-07-10 15:34:39,774 INFO  [PostOpenDeployTasks:44724ac6e2ab8557d62f0b94602fa104] catalog.MetaEditor: Updated row usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104. with server=slave1,60020,1405031639695
2014-07-10 15:34:39,774 INFO  [PostOpenDeployTasks:3d8c99231d7b65ab261a447265aa324a] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:34:39,774 INFO  [PostOpenDeployTasks:b2219710a6a33bc63194598235eef963] catalog.MetaEditor: Updated row usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963. with server=slave1,60020,1405031639695
2014-07-10 15:34:39,775 INFO  [PostOpenDeployTasks:b2219710a6a33bc63194598235eef963] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:34:39,775 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d8c99231d7b65ab261a447265aa324a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,774 INFO  [PostOpenDeployTasks:44724ac6e2ab8557d62f0b94602fa104] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:34:39,776 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b2219710a6a33bc63194598235eef963 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,776 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 44724ac6e2ab8557d62f0b94602fa104 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,783 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d8c99231d7b65ab261a447265aa324a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,783 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 3d8c99231d7b65ab261a447265aa324a to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:39,783 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a. on slave1,60020,1405031639695
2014-07-10 15:34:39,784 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9afd05e38e9125d4b5d7ec25b8a4c1b8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,784 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b2219710a6a33bc63194598235eef963 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,784 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned b2219710a6a33bc63194598235eef963 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:39,784 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963. on slave1,60020,1405031639695
2014-07-10 15:34:39,784 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3b827d483637558eba753d45430d8d72 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,785 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 44724ac6e2ab8557d62f0b94602fa104 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,785 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 44724ac6e2ab8557d62f0b94602fa104 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:39,785 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104. on slave1,60020,1405031639695
2014-07-10 15:34:39,785 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,790 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9afd05e38e9125d4b5d7ec25b8a4c1b8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,791 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 9afd05e38e9125d4b5d7ec25b8a4c1b8, NAME => 'usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-10 15:34:39,791 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3b827d483637558eba753d45430d8d72 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,791 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9afd05e38e9125d4b5d7ec25b8a4c1b8
2014-07-10 15:34:39,792 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:34:39,792 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 3b827d483637558eba753d45430d8d72, NAME => 'usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-10 15:34:39,792 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,792 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-10 15:34:39,792 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3b827d483637558eba753d45430d8d72
2014-07-10 15:34:39,793 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:34:39,793 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-10 15:34:39,793 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:34:39,801 INFO  [StoreOpener-9afd05e38e9125d4b5d7ec25b8a4c1b8-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,801 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,803 INFO  [StoreOpener-3b827d483637558eba753d45430d8d72-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,819 DEBUG [StoreOpener-9afd05e38e9125d4b5d7ec25b8a4c1b8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9afd05e38e9125d4b5d7ec25b8a4c1b8/family/1016a26c213d44f5b8eee08a40774cfd, isReference=false, isBulkLoadResult=false, seqid=2321, majorCompaction=false
2014-07-10 15:34:39,825 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-10 15:34:39,842 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-10 15:34:39,849 DEBUG [StoreOpener-9afd05e38e9125d4b5d7ec25b8a4c1b8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9afd05e38e9125d4b5d7ec25b8a4c1b8/family/2d01b204cd5d495eaf0d670d7714c4ea, isReference=false, isBulkLoadResult=false, seqid=2470, majorCompaction=false
2014-07-10 15:34:39,849 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-10 15:34:39,849 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-10 15:34:39,852 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:34:39,860 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1405031639695
2014-07-10 15:34:39,860 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:34:39,861 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,865 DEBUG [StoreOpener-9afd05e38e9125d4b5d7ec25b8a4c1b8-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9afd05e38e9125d4b5d7ec25b8a4c1b8/family/fbbad3729dfa4d198a28fd12d1ea159a, isReference=false, isBulkLoadResult=false, seqid=2165, majorCompaction=true
2014-07-10 15:34:39,868 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9afd05e38e9125d4b5d7ec25b8a4c1b8
2014-07-10 15:34:39,871 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 9afd05e38e9125d4b5d7ec25b8a4c1b8; next sequenceid=2471
2014-07-10 15:34:39,871 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9afd05e38e9125d4b5d7ec25b8a4c1b8
2014-07-10 15:34:39,893 DEBUG [StoreOpener-3b827d483637558eba753d45430d8d72-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72/family/0156ea4d4d1a42219cfc460aa8f8c189, isReference=false, isBulkLoadResult=false, seqid=522, majorCompaction=false
2014-07-10 15:34:39,910 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,910 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:39,910 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1405031639695
2014-07-10 15:34:39,911 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 540460f8e35135ac03b237532d7ea068 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,911 INFO  [PostOpenDeployTasks:9afd05e38e9125d4b5d7ec25b8a4c1b8] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:34:39,912 DEBUG [PostOpenDeployTasks:9afd05e38e9125d4b5d7ec25b8a4c1b8] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-10 15:34:39,919 INFO  [PostOpenDeployTasks:9afd05e38e9125d4b5d7ec25b8a4c1b8] catalog.MetaEditor: Updated row usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8. with server=slave1,60020,1405031639695
2014-07-10 15:34:39,920 INFO  [PostOpenDeployTasks:9afd05e38e9125d4b5d7ec25b8a4c1b8] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:34:39,920 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9afd05e38e9125d4b5d7ec25b8a4c1b8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,923 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 540460f8e35135ac03b237532d7ea068 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,924 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 540460f8e35135ac03b237532d7ea068, NAME => 'usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-10 15:34:39,924 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 540460f8e35135ac03b237532d7ea068
2014-07-10 15:34:39,925 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:34:39,933 INFO  [StoreOpener-540460f8e35135ac03b237532d7ea068-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,934 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9afd05e38e9125d4b5d7ec25b8a4c1b8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:39,934 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 9afd05e38e9125d4b5d7ec25b8a4c1b8 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:39,934 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8. on slave1,60020,1405031639695
2014-07-10 15:34:39,935 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0f26af6751fdbada6a0f28f4315708ee from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,942 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0f26af6751fdbada6a0f28f4315708ee from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:39,943 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 0f26af6751fdbada6a0f28f4315708ee, NAME => 'usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-10 15:34:39,945 DEBUG [StoreOpener-3b827d483637558eba753d45430d8d72-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72/family/41a01247064841d3b6ac669874618ca3, isReference=false, isBulkLoadResult=false, seqid=1052, majorCompaction=false
2014-07-10 15:34:39,945 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 0f26af6751fdbada6a0f28f4315708ee
2014-07-10 15:34:39,945 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:34:39,954 DEBUG [StoreOpener-540460f8e35135ac03b237532d7ea068-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068/family/180440bf7c064d7cb18df49a896786d5, isReference=false, isBulkLoadResult=false, seqid=1438, majorCompaction=false
2014-07-10 15:34:39,957 INFO  [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:39,959 DEBUG [StoreOpener-3b827d483637558eba753d45430d8d72-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72/family/5120af3c0a4d43a4a6943cef00df4664, isReference=false, isBulkLoadResult=false, seqid=2458, majorCompaction=false
2014-07-10 15:34:39,976 DEBUG [StoreOpener-540460f8e35135ac03b237532d7ea068-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068/family/474bcb3fc093498aa800483f95bf8552, isReference=false, isBulkLoadResult=false, seqid=1056, majorCompaction=false
2014-07-10 15:34:39,979 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/6a74f81eed124750a0419f356719f6d0, isReference=false, isBulkLoadResult=false, seqid=974, majorCompaction=false
2014-07-10 15:34:39,979 DEBUG [StoreOpener-3b827d483637558eba753d45430d8d72-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72/family/5195330cc90747e389f70973d4c7b892, isReference=false, isBulkLoadResult=false, seqid=1553, majorCompaction=false
2014-07-10 15:34:39,997 DEBUG [StoreOpener-540460f8e35135ac03b237532d7ea068-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068/family/709899715e2c474a80d75c43c9a194ae, isReference=false, isBulkLoadResult=false, seqid=1172, majorCompaction=false
2014-07-10 15:34:39,997 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/80758b35b31f4295b389cb28442cc24a, isReference=false, isBulkLoadResult=false, seqid=1124, majorCompaction=false
2014-07-10 15:34:40,008 DEBUG [StoreOpener-3b827d483637558eba753d45430d8d72-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72/family/bb23830f2103438a8ea3045d2e984939, isReference=false, isBulkLoadResult=false, seqid=2093, majorCompaction=false
2014-07-10 15:34:40,012 DEBUG [StoreOpener-540460f8e35135ac03b237532d7ea068-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068/family/9c9275a86775471486b9d240cb81e3ac, isReference=false, isBulkLoadResult=false, seqid=905, majorCompaction=false
2014-07-10 15:34:40,012 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3b827d483637558eba753d45430d8d72
2014-07-10 15:34:40,019 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 3b827d483637558eba753d45430d8d72; next sequenceid=2459
2014-07-10 15:34:40,020 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3b827d483637558eba753d45430d8d72
2014-07-10 15:34:40,020 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/a51ab4db39bf4823ae88a9f51f8d4ed3, isReference=false, isBulkLoadResult=false, seqid=1471, majorCompaction=false
2014-07-10 15:34:40,022 INFO  [PostOpenDeployTasks:3b827d483637558eba753d45430d8d72] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:34:40,022 DEBUG [PostOpenDeployTasks:3b827d483637558eba753d45430d8d72] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-10 15:34:40,025 DEBUG [StoreOpener-540460f8e35135ac03b237532d7ea068-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068/family/edb75b71183d4c3f95983ac593512adf, isReference=false, isBulkLoadResult=false, seqid=561, majorCompaction=false
2014-07-10 15:34:40,030 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/540460f8e35135ac03b237532d7ea068
2014-07-10 15:34:40,030 INFO  [PostOpenDeployTasks:3b827d483637558eba753d45430d8d72] catalog.MetaEditor: Updated row usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,030 INFO  [PostOpenDeployTasks:3b827d483637558eba753d45430d8d72] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:34:40,031 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/b4243a1fed3e46448082b29bbed8a0b8, isReference=false, isBulkLoadResult=false, seqid=1490, majorCompaction=false
2014-07-10 15:34:40,031 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3b827d483637558eba753d45430d8d72 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,033 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 540460f8e35135ac03b237532d7ea068; next sequenceid=1439
2014-07-10 15:34:40,034 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 540460f8e35135ac03b237532d7ea068
2014-07-10 15:34:40,036 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3b827d483637558eba753d45430d8d72 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,036 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 3b827d483637558eba753d45430d8d72 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,036 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72. on slave1,60020,1405031639695
2014-07-10 15:34:40,037 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd94382cb2f6d6b768117baeba5c4d44 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,038 INFO  [PostOpenDeployTasks:540460f8e35135ac03b237532d7ea068] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:34:40,038 DEBUG [PostOpenDeployTasks:540460f8e35135ac03b237532d7ea068] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-10 15:34:40,042 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd94382cb2f6d6b768117baeba5c4d44 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,042 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => fd94382cb2f6d6b768117baeba5c4d44, NAME => 'usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-10 15:34:40,043 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable fd94382cb2f6d6b768117baeba5c4d44
2014-07-10 15:34:40,043 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:34:40,045 INFO  [PostOpenDeployTasks:540460f8e35135ac03b237532d7ea068] catalog.MetaEditor: Updated row usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,046 INFO  [PostOpenDeployTasks:540460f8e35135ac03b237532d7ea068] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:34:40,046 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 540460f8e35135ac03b237532d7ea068 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,050 INFO  [StoreOpener-fd94382cb2f6d6b768117baeba5c4d44-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:40,051 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 540460f8e35135ac03b237532d7ea068 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,051 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 540460f8e35135ac03b237532d7ea068 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,051 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068. on slave1,60020,1405031639695
2014-07-10 15:34:40,052 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a70b09b7ac9db45246ab633a65d7d42b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,057 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a70b09b7ac9db45246ab633a65d7d42b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,057 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/ce0e68e37bfa44739f68b2385992abcf, isReference=false, isBulkLoadResult=false, seqid=304, majorCompaction=false
2014-07-10 15:34:40,058 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => a70b09b7ac9db45246ab633a65d7d42b, NAME => 'usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-10 15:34:40,058 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a70b09b7ac9db45246ab633a65d7d42b
2014-07-10 15:34:40,058 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:34:40,065 DEBUG [StoreOpener-fd94382cb2f6d6b768117baeba5c4d44-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd94382cb2f6d6b768117baeba5c4d44/family/4ef9d14cfd92419cae7f8cda9f39cabb, isReference=false, isBulkLoadResult=false, seqid=2474, majorCompaction=false
2014-07-10 15:34:40,073 INFO  [StoreOpener-a70b09b7ac9db45246ab633a65d7d42b-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:40,079 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/f74e18a57e484cef8f25ae4eab04da63, isReference=false, isBulkLoadResult=false, seqid=1195, majorCompaction=false
2014-07-10 15:34:40,080 DEBUG [StoreOpener-fd94382cb2f6d6b768117baeba5c4d44-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd94382cb2f6d6b768117baeba5c4d44/family/b2515f8edab74e2cbbf38bf20612a4ce, isReference=false, isBulkLoadResult=false, seqid=2264, majorCompaction=false
2014-07-10 15:34:40,085 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a70b09b7ac9db45246ab633a65d7d42b
2014-07-10 15:34:40,131 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined a70b09b7ac9db45246ab633a65d7d42b; next sequenceid=1
2014-07-10 15:34:40,131 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a70b09b7ac9db45246ab633a65d7d42b
2014-07-10 15:34:40,132 DEBUG [StoreOpener-0f26af6751fdbada6a0f28f4315708ee-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee/family/ffbc324b3df44dc4919aa1aa115b7a55, isReference=false, isBulkLoadResult=false, seqid=824, majorCompaction=false
2014-07-10 15:34:40,134 DEBUG [StoreOpener-fd94382cb2f6d6b768117baeba5c4d44-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/fd94382cb2f6d6b768117baeba5c4d44/family/c6f34388218b43d1be224f6c4a11c4bb, isReference=false, isBulkLoadResult=false, seqid=2104, majorCompaction=true
2014-07-10 15:34:40,134 INFO  [PostOpenDeployTasks:a70b09b7ac9db45246ab633a65d7d42b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:34:40,138 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/fd94382cb2f6d6b768117baeba5c4d44
2014-07-10 15:34:40,140 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/0f26af6751fdbada6a0f28f4315708ee
2014-07-10 15:34:40,143 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined fd94382cb2f6d6b768117baeba5c4d44; next sequenceid=2475
2014-07-10 15:34:40,144 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node fd94382cb2f6d6b768117baeba5c4d44
2014-07-10 15:34:40,144 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 0f26af6751fdbada6a0f28f4315708ee; next sequenceid=1491
2014-07-10 15:34:40,144 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 0f26af6751fdbada6a0f28f4315708ee
2014-07-10 15:34:40,146 INFO  [PostOpenDeployTasks:a70b09b7ac9db45246ab633a65d7d42b] catalog.MetaEditor: Updated row usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,146 INFO  [PostOpenDeployTasks:a70b09b7ac9db45246ab633a65d7d42b] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:34:40,147 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a70b09b7ac9db45246ab633a65d7d42b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,147 INFO  [PostOpenDeployTasks:fd94382cb2f6d6b768117baeba5c4d44] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:34:40,148 DEBUG [PostOpenDeployTasks:fd94382cb2f6d6b768117baeba5c4d44] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-10 15:34:40,148 INFO  [PostOpenDeployTasks:0f26af6751fdbada6a0f28f4315708ee] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:34:40,148 DEBUG [PostOpenDeployTasks:0f26af6751fdbada6a0f28f4315708ee] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-10 15:34:40,152 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a70b09b7ac9db45246ab633a65d7d42b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,152 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned a70b09b7ac9db45246ab633a65d7d42b to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,152 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b. on slave1,60020,1405031639695
2014-07-10 15:34:40,152 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 8d3a3257356cb68fbe239fec6d8f1e27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,154 INFO  [PostOpenDeployTasks:fd94382cb2f6d6b768117baeba5c4d44] catalog.MetaEditor: Updated row usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,154 INFO  [PostOpenDeployTasks:fd94382cb2f6d6b768117baeba5c4d44] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:34:40,154 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd94382cb2f6d6b768117baeba5c4d44 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,158 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 8d3a3257356cb68fbe239fec6d8f1e27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:34:40,158 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 8d3a3257356cb68fbe239fec6d8f1e27, NAME => 'usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-10 15:34:40,159 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 8d3a3257356cb68fbe239fec6d8f1e27
2014-07-10 15:34:40,159 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:34:40,159 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd94382cb2f6d6b768117baeba5c4d44 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,159 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned fd94382cb2f6d6b768117baeba5c4d44 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,159 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44. on slave1,60020,1405031639695
2014-07-10 15:34:40,165 INFO  [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:34:40,183 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/231e4243873f4c148c21bdf5d6a43fdc, isReference=false, isBulkLoadResult=false, seqid=2357, majorCompaction=false
2014-07-10 15:34:40,191 INFO  [PostOpenDeployTasks:0f26af6751fdbada6a0f28f4315708ee] catalog.MetaEditor: Updated row usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,191 INFO  [PostOpenDeployTasks:0f26af6751fdbada6a0f28f4315708ee] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:34:40,192 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0f26af6751fdbada6a0f28f4315708ee from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,201 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/35a9b39ea48340a8b901dc7b06ca251a, isReference=false, isBulkLoadResult=false, seqid=2164, majorCompaction=false
2014-07-10 15:34:40,211 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/68972ecd4fa24bd9b2ff7edda81d7b0a, isReference=false, isBulkLoadResult=false, seqid=2469, majorCompaction=false
2014-07-10 15:34:40,224 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0f26af6751fdbada6a0f28f4315708ee from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,224 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 0f26af6751fdbada6a0f28f4315708ee to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,224 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee. on slave1,60020,1405031639695
2014-07-10 15:34:40,251 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/7d56f48bae884ca7bd3e219725f7cc31, isReference=false, isBulkLoadResult=false, seqid=1814, majorCompaction=false
2014-07-10 15:34:40,287 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/af6850c4db3c419d979605ed5a05d698, isReference=false, isBulkLoadResult=false, seqid=2011, majorCompaction=false
2014-07-10 15:34:40,318 DEBUG [StoreOpener-8d3a3257356cb68fbe239fec6d8f1e27-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27/family/fa4617420c1f4b22ac0213bdff2c58fa, isReference=false, isBulkLoadResult=false, seqid=1657, majorCompaction=true
2014-07-10 15:34:40,323 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/8d3a3257356cb68fbe239fec6d8f1e27
2014-07-10 15:34:40,326 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 8d3a3257356cb68fbe239fec6d8f1e27; next sequenceid=2470
2014-07-10 15:34:40,326 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 8d3a3257356cb68fbe239fec6d8f1e27
2014-07-10 15:34:40,329 INFO  [PostOpenDeployTasks:8d3a3257356cb68fbe239fec6d8f1e27] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:34:40,329 DEBUG [PostOpenDeployTasks:8d3a3257356cb68fbe239fec6d8f1e27] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-10 15:34:40,336 INFO  [PostOpenDeployTasks:8d3a3257356cb68fbe239fec6d8f1e27] catalog.MetaEditor: Updated row usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27. with server=slave1,60020,1405031639695
2014-07-10 15:34:40,337 INFO  [PostOpenDeployTasks:8d3a3257356cb68fbe239fec6d8f1e27] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:34:40,338 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 8d3a3257356cb68fbe239fec6d8f1e27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,343 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 8d3a3257356cb68fbe239fec6d8f1e27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:34:40,343 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 8d3a3257356cb68fbe239fec6d8f1e27 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:34:40,343 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27. on slave1,60020,1405031639695
2014-07-10 15:34:44,118 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-10 15:34:44,119 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-10 15:34:44,119 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-10 15:34:44,119 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:12), split_queue=0, merge_queue=0
2014-07-10 15:34:44,120 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:13), split_queue=0, merge_queue=0
2014-07-10 15:34:44,120 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:14), split_queue=0, merge_queue=0
2014-07-10 15:34:44,120 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-10 15:34:44,120 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-10 15:35:05,708 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close a70b09b7ac9db45246ab633a65d7d42b, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,709 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close fd94382cb2f6d6b768117baeba5c4d44, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,709 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 44724ac6e2ab8557d62f0b94602fa104, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,709 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close 3b827d483637558eba753d45430d8d72, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,710 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close 9afd05e38e9125d4b5d7ec25b8a4c1b8, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,712 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:35:05,712 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:35:05,713 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Close 540460f8e35135ac03b237532d7ea068, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,713 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Close 3d8c99231d7b65ab261a447265aa324a, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,713 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:35:05,714 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 8d3a3257356cb68fbe239fec6d8f1e27, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,715 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Close 0f26af6751fdbada6a0f28f4315708ee, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,716 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close b2219710a6a33bc63194598235eef963, via zk=yes, znode version=0, on null
2014-07-10 15:35:05,717 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.: disabling compactions & flushes
2014-07-10 15:35:05,717 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.: disabling compactions & flushes
2014-07-10 15:35:05,717 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:35:05,717 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:35:05,720 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.: disabling compactions & flushes
2014-07-10 15:35:05,720 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:35:05,721 INFO  [StoreCloserThread-usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,724 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:35:05,724 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a70b09b7ac9db45246ab633a65d7d42b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,729 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a70b09b7ac9db45246ab633a65d7d42b from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,729 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b. on slave1,60020,1405031639695
2014-07-10 15:35:05,729 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,,1405030776194.a70b09b7ac9db45246ab633a65d7d42b.
2014-07-10 15:35:05,729 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:35:05,734 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.: disabling compactions & flushes
2014-07-10 15:35:05,734 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:35:05,758 INFO  [StoreCloserThread-usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,759 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:35:05,760 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9afd05e38e9125d4b5d7ec25b8a4c1b8 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,761 INFO  [StoreCloserThread-usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,762 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:35:05,762 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning fd94382cb2f6d6b768117baeba5c4d44 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,763 INFO  [StoreCloserThread-usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,764 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:35:05,764 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 44724ac6e2ab8557d62f0b94602fa104 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,771 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9afd05e38e9125d4b5d7ec25b8a4c1b8 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8. on slave1,60020,1405031639695
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node fd94382cb2f6d6b768117baeba5c4d44 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44. on slave1,60020,1405031639695
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8.
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 44724ac6e2ab8557d62f0b94602fa104 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104. on slave1,60020,1405031639695
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104.
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44.
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:35:05,772 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:35:05,775 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.: disabling compactions & flushes
2014-07-10 15:35:05,775 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:35:05,776 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.: disabling compactions & flushes
2014-07-10 15:35:05,776 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:35:05,776 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.: disabling compactions & flushes
2014-07-10 15:35:05,776 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:35:05,785 INFO  [StoreCloserThread-usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,786 INFO  [StoreCloserThread-usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,787 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:35:05,787 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 540460f8e35135ac03b237532d7ea068 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,787 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:35:05,787 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3b827d483637558eba753d45430d8d72 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 540460f8e35135ac03b237532d7ea068 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068. on slave1,60020,1405031639695
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068.
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3b827d483637558eba753d45430d8d72 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72. on slave1,60020,1405031639695
2014-07-10 15:35:05,792 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72.
2014-07-10 15:35:05,793 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:35:05,795 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.: disabling compactions & flushes
2014-07-10 15:35:05,795 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:35:05,796 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.: disabling compactions & flushes
2014-07-10 15:35:05,796 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:35:05,798 INFO  [StoreCloserThread-usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,799 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:35:05,799 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 0f26af6751fdbada6a0f28f4315708ee from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,800 INFO  [StoreCloserThread-usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,800 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:35:05,801 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 8d3a3257356cb68fbe239fec6d8f1e27 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,815 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 0f26af6751fdbada6a0f28f4315708ee from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,815 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee. on slave1,60020,1405031639695
2014-07-10 15:35:05,815 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee.
2014-07-10 15:35:05,816 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:35:05,816 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 8d3a3257356cb68fbe239fec6d8f1e27 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,816 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27. on slave1,60020,1405031639695
2014-07-10 15:35:05,816 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27.
2014-07-10 15:35:05,818 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.: disabling compactions & flushes
2014-07-10 15:35:05,818 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:35:05,820 INFO  [StoreCloserThread-usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.-1] regionserver.HStore: Closed family
2014-07-10 15:35:05,821 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:35:05,822 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b2219710a6a33bc63194598235eef963 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,827 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b2219710a6a33bc63194598235eef963 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:05,827 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963. on slave1,60020,1405031639695
2014-07-10 15:35:05,827 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963.
2014-07-10 15:35:06,316 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:35:06,317 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:35:06,323 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a., storeName=family, fileCount=8, fileSize=567.6m (66.4m, 66.5m, 30.5m, 39.2m, 16.8m), priority=12, time=38480977444744; duration=26sec
2014-07-10 15:35:06,323 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-10 15:35:06,326 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44. because compaction request was cancelled
2014-07-10 15:35:06,327 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8. because compaction request was cancelled
2014-07-10 15:35:06,327 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104. because compaction request was cancelled
2014-07-10 15:35:06,327 INFO  [StoreCloserThread-usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.-1] regionserver.HStore: Closed family
2014-07-10 15:35:06,327 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27. because compaction request was cancelled
2014-07-10 15:35:06,328 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963. because compaction request was cancelled
2014-07-10 15:35:06,328 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:35:06,328 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user1,1405030776194.44724ac6e2ab8557d62f0b94602fa104. because compaction request was cancelled
2014-07-10 15:35:06,328 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3d8c99231d7b65ab261a447265aa324a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:06,328 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user7,1405030776195.b2219710a6a33bc63194598235eef963. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user4,1405030776195.0f26af6751fdbada6a0f28f4315708ee. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user2,1405030776195.9afd05e38e9125d4b5d7ec25b8a4c1b8. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user8,1405030776195.540460f8e35135ac03b237532d7ea068. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user6,1405030776195.8d3a3257356cb68fbe239fec6d8f1e27. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user5,1405030776195.fd94382cb2f6d6b768117baeba5c4d44. because compaction request was cancelled
2014-07-10 15:35:06,329 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Not compacting usertable,user9,1405030776195.3b827d483637558eba753d45430d8d72. because compaction request was cancelled
2014-07-10 15:35:06,334 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3d8c99231d7b65ab261a447265aa324a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-10 15:35:06,334 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a. on slave1,60020,1405031639695
2014-07-10 15:35:06,334 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user3,1405030776195.3d8c99231d7b65ab261a447265aa324a.
2014-07-10 15:38:59,786 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=6684, hits=3, hitRatio=0.04%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-10 15:40:15,033 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:40:15,042 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:40:15,042 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ac30afe9571104f9c77d40f07e8fd90a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,043 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:40:15,043 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:15,044 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 23bf7ec8a173b0eb7e717fa868992012 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,044 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a7dcb4e2617e2f18516bffa9660ca2e3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,045 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:40:15,051 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ac30afe9571104f9c77d40f07e8fd90a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,052 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 23bf7ec8a173b0eb7e717fa868992012 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,052 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => ac30afe9571104f9c77d40f07e8fd90a, NAME => 'usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-10 15:40:15,052 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a7dcb4e2617e2f18516bffa9660ca2e3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,052 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => a7dcb4e2617e2f18516bffa9660ca2e3, NAME => 'usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-10 15:40:15,053 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 23bf7ec8a173b0eb7e717fa868992012, NAME => 'usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-10 15:40:15,053 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a7dcb4e2617e2f18516bffa9660ca2e3
2014-07-10 15:40:15,053 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ac30afe9571104f9c77d40f07e8fd90a
2014-07-10 15:40:15,053 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:40:15,053 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 23bf7ec8a173b0eb7e717fa868992012
2014-07-10 15:40:15,054 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:40:15,054 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:40:15,065 INFO  [StoreOpener-a7dcb4e2617e2f18516bffa9660ca2e3-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:40:15,065 INFO  [StoreOpener-ac30afe9571104f9c77d40f07e8fd90a-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:40:15,068 INFO  [StoreOpener-23bf7ec8a173b0eb7e717fa868992012-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:40:15,071 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3
2014-07-10 15:40:15,073 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ac30afe9571104f9c77d40f07e8fd90a
2014-07-10 15:40:15,074 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012
2014-07-10 15:40:15,074 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined a7dcb4e2617e2f18516bffa9660ca2e3; next sequenceid=1
2014-07-10 15:40:15,074 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a7dcb4e2617e2f18516bffa9660ca2e3
2014-07-10 15:40:15,075 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined ac30afe9571104f9c77d40f07e8fd90a; next sequenceid=1
2014-07-10 15:40:15,076 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ac30afe9571104f9c77d40f07e8fd90a
2014-07-10 15:40:15,077 INFO  [PostOpenDeployTasks:a7dcb4e2617e2f18516bffa9660ca2e3] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:40:15,079 INFO  [PostOpenDeployTasks:ac30afe9571104f9c77d40f07e8fd90a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:40:15,091 INFO  [PostOpenDeployTasks:a7dcb4e2617e2f18516bffa9660ca2e3] catalog.MetaEditor: Updated row usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. with server=slave1,60020,1405031639695
2014-07-10 15:40:15,091 INFO  [PostOpenDeployTasks:a7dcb4e2617e2f18516bffa9660ca2e3] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:40:15,091 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a7dcb4e2617e2f18516bffa9660ca2e3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,092 INFO  [PostOpenDeployTasks:ac30afe9571104f9c77d40f07e8fd90a] catalog.MetaEditor: Updated row usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a. with server=slave1,60020,1405031639695
2014-07-10 15:40:15,092 INFO  [PostOpenDeployTasks:ac30afe9571104f9c77d40f07e8fd90a] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:40:15,092 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ac30afe9571104f9c77d40f07e8fd90a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,098 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a7dcb4e2617e2f18516bffa9660ca2e3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,098 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned a7dcb4e2617e2f18516bffa9660ca2e3 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:40:15,098 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. on slave1,60020,1405031639695
2014-07-10 15:40:15,098 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3b0f1f9aafa5f1eb5f21397291729f21 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,099 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ac30afe9571104f9c77d40f07e8fd90a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,099 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned ac30afe9571104f9c77d40f07e8fd90a to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:40:15,099 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a. on slave1,60020,1405031639695
2014-07-10 15:40:15,100 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e9b9c12fdbb0ab99d9463f3c554bf23b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,105 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3b0f1f9aafa5f1eb5f21397291729f21 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,106 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 3b0f1f9aafa5f1eb5f21397291729f21, NAME => 'usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-10 15:40:15,106 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3b0f1f9aafa5f1eb5f21397291729f21
2014-07-10 15:40:15,106 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:15,107 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e9b9c12fdbb0ab99d9463f3c554bf23b from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-10 15:40:15,107 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => e9b9c12fdbb0ab99d9463f3c554bf23b, NAME => 'usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-10 15:40:15,107 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 23bf7ec8a173b0eb7e717fa868992012; next sequenceid=1
2014-07-10 15:40:15,107 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 23bf7ec8a173b0eb7e717fa868992012
2014-07-10 15:40:15,107 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e9b9c12fdbb0ab99d9463f3c554bf23b
2014-07-10 15:40:15,108 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:40:15,111 INFO  [PostOpenDeployTasks:23bf7ec8a173b0eb7e717fa868992012] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:40:15,116 INFO  [StoreOpener-3b0f1f9aafa5f1eb5f21397291729f21-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:40:15,118 INFO  [PostOpenDeployTasks:23bf7ec8a173b0eb7e717fa868992012] catalog.MetaEditor: Updated row usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. with server=slave1,60020,1405031639695
2014-07-10 15:40:15,118 INFO  [PostOpenDeployTasks:23bf7ec8a173b0eb7e717fa868992012] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:40:15,120 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 23bf7ec8a173b0eb7e717fa868992012 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,122 INFO  [StoreOpener-e9b9c12fdbb0ab99d9463f3c554bf23b-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-10 15:40:15,124 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 23bf7ec8a173b0eb7e717fa868992012 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,124 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 23bf7ec8a173b0eb7e717fa868992012 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:40:15,124 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. on slave1,60020,1405031639695
2014-07-10 15:40:15,128 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21
2014-07-10 15:40:15,129 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e9b9c12fdbb0ab99d9463f3c554bf23b
2014-07-10 15:40:15,133 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 3b0f1f9aafa5f1eb5f21397291729f21; next sequenceid=1
2014-07-10 15:40:15,133 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3b0f1f9aafa5f1eb5f21397291729f21
2014-07-10 15:40:15,135 INFO  [PostOpenDeployTasks:3b0f1f9aafa5f1eb5f21397291729f21] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:15,141 INFO  [PostOpenDeployTasks:3b0f1f9aafa5f1eb5f21397291729f21] catalog.MetaEditor: Updated row usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. with server=slave1,60020,1405031639695
2014-07-10 15:40:15,142 INFO  [PostOpenDeployTasks:3b0f1f9aafa5f1eb5f21397291729f21] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:15,142 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3b0f1f9aafa5f1eb5f21397291729f21 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,147 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3b0f1f9aafa5f1eb5f21397291729f21 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,147 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 3b0f1f9aafa5f1eb5f21397291729f21 to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:40:15,147 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. on slave1,60020,1405031639695
2014-07-10 15:40:15,171 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined e9b9c12fdbb0ab99d9463f3c554bf23b; next sequenceid=1
2014-07-10 15:40:15,171 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e9b9c12fdbb0ab99d9463f3c554bf23b
2014-07-10 15:40:15,175 INFO  [PostOpenDeployTasks:e9b9c12fdbb0ab99d9463f3c554bf23b] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:40:15,186 INFO  [PostOpenDeployTasks:e9b9c12fdbb0ab99d9463f3c554bf23b] catalog.MetaEditor: Updated row usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b. with server=slave1,60020,1405031639695
2014-07-10 15:40:15,187 INFO  [PostOpenDeployTasks:e9b9c12fdbb0ab99d9463f3c554bf23b] regionserver.HRegionServer: Finished post open deploy task for usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:40:15,187 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e9b9c12fdbb0ab99d9463f3c554bf23b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,195 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e9b9c12fdbb0ab99d9463f3c554bf23b from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-10 15:40:15,195 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned e9b9c12fdbb0ab99d9463f3c554bf23b to OPENED in zk on slave1,60020,1405031639695
2014-07-10 15:40:15,195 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b. on slave1,60020,1405031639695
2014-07-10 15:40:33,852 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:33,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113 synced till here 84
2014-07-10 15:40:34,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405031673977 with entries=113, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032033853
2014-07-10 15:40:36,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:36,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 208 synced till here 207
2014-07-10 15:40:36,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032033853 with entries=95, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032036198
2014-07-10 15:40:39,100 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:39,128 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 327 synced till here 296
2014-07-10 15:40:39,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032036198 with entries=119, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032039100
2014-07-10 15:40:42,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:42,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 453 synced till here 445
2014-07-10 15:40:42,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032039100 with entries=126, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032042147
2014-07-10 15:40:44,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:44,452 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:44,457 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 258.8m
2014-07-10 15:40:44,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 562 synced till here 529
2014-07-10 15:40:44,980 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032042147 with entries=109, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032044443
2014-07-10 15:40:45,536 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:40:45,536 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 259.1m
2014-07-10 15:40:46,348 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:40:46,630 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:40:47,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:47,607 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 688 synced till here 648
2014-07-10 15:40:47,612 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:40:47,798 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-10 15:40:48,410 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032044443 with entries=126, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032047461
2014-07-10 15:40:50,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:50,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 798 synced till here 766
2014-07-10 15:40:50,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032047461 with entries=110, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032050001
2014-07-10 15:40:52,659 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:53,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 924 synced till here 904
2014-07-10 15:40:53,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032050001 with entries=126, filesize=98.3m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032052660
2014-07-10 15:40:55,174 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:55,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1032 synced till here 1000
2014-07-10 15:40:55,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032052660 with entries=108, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032055175
2014-07-10 15:40:57,528 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:57,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1120 synced till here 1112
2014-07-10 15:40:57,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032055175 with entries=88, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032057528
2014-07-10 15:40:58,269 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=171, memsize=95.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/4ec336f807c7487489a1415b3305a33b
2014-07-10 15:40:58,493 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/4ec336f807c7487489a1415b3305a33b as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/4ec336f807c7487489a1415b3305a33b
2014-07-10 15:40:58,537 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/4ec336f807c7487489a1415b3305a33b, entries=346840, sequenceid=171, filesize=24.7m
2014-07-10 15:40:58,547 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.4m/273098480, currentsize=271.8m/284984000 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 14090ms, sequenceid=171, compaction requested=false
2014-07-10 15:40:58,691 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 499.6m
2014-07-10 15:40:59,478 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=171, memsize=95.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/ee033d61d11347f0a65d4b9a8aecf308
2014-07-10 15:40:59,493 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/ee033d61d11347f0a65d4b9a8aecf308 as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/ee033d61d11347f0a65d4b9a8aecf308
2014-07-10 15:40:59,506 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/ee033d61d11347f0a65d4b9a8aecf308, entries=346610, sequenceid=171, filesize=24.7m
2014-07-10 15:40:59,507 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.7m/273329760, currentsize=272.1m/285339920 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 13971ms, sequenceid=171, compaction requested=false
2014-07-10 15:40:59,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:40:59,509 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:40:59,510 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 274.8m
2014-07-10 15:40:59,669 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1225 synced till here 1209
2014-07-10 15:40:59,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032057528 with entries=105, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032059509
2014-07-10 15:41:00,072 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:41:00,307 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:00,698 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:01,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:01,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1320 synced till here 1300
2014-07-10 15:41:01,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032059509 with entries=95, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032061005
2014-07-10 15:41:03,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:03,230 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032061005 with entries=96, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032063131
2014-07-10 15:41:05,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:06,940 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=379, memsize=73.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/6365483bb53c4206b4fc3b0fc401dd3d
2014-07-10 15:41:06,978 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/6365483bb53c4206b4fc3b0fc401dd3d as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/6365483bb53c4206b4fc3b0fc401dd3d
2014-07-10 15:41:07,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1567 synced till here 1562
2014-07-10 15:41:07,756 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/6365483bb53c4206b4fc3b0fc401dd3d, entries=267190, sequenceid=379, filesize=19.0m
2014-07-10 15:41:07,757 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~315.8m/331117200, currentsize=113.7m/119273920 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 8247ms, sequenceid=379, compaction requested=false
2014-07-10 15:41:07,758 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 421.7m
2014-07-10 15:41:07,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032063131 with entries=151, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032065549
2014-07-10 15:41:08,715 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=348, memsize=95.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/0d70c4e71f214b9b854bcebcff3e126b
2014-07-10 15:41:08,734 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/0d70c4e71f214b9b854bcebcff3e126b as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/0d70c4e71f214b9b854bcebcff3e126b
2014-07-10 15:41:08,741 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:08,865 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/0d70c4e71f214b9b854bcebcff3e126b, entries=346900, sequenceid=348, filesize=24.7m
2014-07-10 15:41:08,866 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~531.6m/557396160, currentsize=172.1m/180445200 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 10175ms, sequenceid=348, compaction requested=false
2014-07-10 15:41:09,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:09,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1678 synced till here 1676
2014-07-10 15:41:09,555 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032065549 with entries=111, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032069196
2014-07-10 15:41:12,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:12,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1788 synced till here 1779
2014-07-10 15:41:12,258 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032069196 with entries=110, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032072024
2014-07-10 15:41:13,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:13,867 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:41:13,867 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 257.2m
2014-07-10 15:41:13,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1896 synced till here 1895
2014-07-10 15:41:13,894 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032072024 with entries=108, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032073824
2014-07-10 15:41:14,070 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:17,195 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=452, memsize=118.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/e6ca5c63a5784674a6ec1bb66d777b62
2014-07-10 15:41:17,229 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/e6ca5c63a5784674a6ec1bb66d777b62 as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e6ca5c63a5784674a6ec1bb66d777b62
2014-07-10 15:41:17,245 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e6ca5c63a5784674a6ec1bb66d777b62, entries=430700, sequenceid=452, filesize=30.7m
2014-07-10 15:41:17,247 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~429.7m/450545040, currentsize=125.0m/131084000 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 9490ms, sequenceid=452, compaction requested=false
2014-07-10 15:41:18,794 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:18,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2013 synced till here 2012
2014-07-10 15:41:18,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032073824 with entries=117, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032078795
2014-07-10 15:41:19,686 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:41:19,686 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 256.9m
2014-07-10 15:41:19,972 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:22,404 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:22,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2134 synced till here 2132
2014-07-10 15:41:22,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032078795 with entries=121, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032082405
2014-07-10 15:41:23,142 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=515, memsize=197.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/d013707cbaeb4787b6d1df5263f266ac
2014-07-10 15:41:23,167 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/d013707cbaeb4787b6d1df5263f266ac as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/d013707cbaeb4787b6d1df5263f266ac
2014-07-10 15:41:23,181 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/d013707cbaeb4787b6d1df5263f266ac, entries=718740, sequenceid=515, filesize=51.3m
2014-07-10 15:41:23,182 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.8m/271385200, currentsize=76.9m/80597520 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 9314ms, sequenceid=515, compaction requested=false
2014-07-10 15:41:26,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:26,427 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2246 synced till here 2243
2014-07-10 15:41:26,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032082405 with entries=112, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032086399
2014-07-10 15:41:30,026 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=544, memsize=241.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/81a5240506274bd883096e3f7247d01e
2014-07-10 15:41:30,047 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/81a5240506274bd883096e3f7247d01e as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/81a5240506274bd883096e3f7247d01e
2014-07-10 15:41:30,060 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/81a5240506274bd883096e3f7247d01e, entries=878780, sequenceid=544, filesize=62.6m
2014-07-10 15:41:30,061 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.7m/271267440, currentsize=91.8m/96232080 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 10375ms, sequenceid=544, compaction requested=true
2014-07-10 15:41:30,062 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-10 15:41:30,063 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-10 15:41:30,064 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 111565188 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-10 15:41:30,064 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: 3b0f1f9aafa5f1eb5f21397291729f21 - family: Initiating major compaction
2014-07-10 15:41:30,064 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: Starting compaction on family in region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:41:30,065 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp, totalSize=106.4m
2014-07-10 15:41:30,065 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/4ec336f807c7487489a1415b3305a33b, keycount=34684, bloomtype=ROW, size=24.7m, encoding=NONE, seqNum=171, earliestPutTs=1405032033600
2014-07-10 15:41:30,065 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/6365483bb53c4206b4fc3b0fc401dd3d, keycount=26719, bloomtype=ROW, size=19.0m, encoding=NONE, seqNum=379, earliestPutTs=1405032055597
2014-07-10 15:41:30,066 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/81a5240506274bd883096e3f7247d01e, keycount=87878, bloomtype=ROW, size=62.6m, encoding=NONE, seqNum=544, earliestPutTs=1405032060334
2014-07-10 15:41:30,095 DEBUG [regionserver60020-smallCompactions-1405031679550] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:31,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:31,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2348 synced till here 2343
2014-07-10 15:41:32,163 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032086399 with entries=102, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032091594
2014-07-10 15:41:32,836 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:41:32,836 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 258.7m
2014-07-10 15:41:33,231 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:34,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:34,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032091594 with entries=117, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032094385
2014-07-10 15:41:38,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:38,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2576 synced till here 2575
2014-07-10 15:41:38,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032094385 with entries=111, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032098468
2014-07-10 15:41:41,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:42,625 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2699 synced till here 2695
2014-07-10 15:41:42,646 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=622, memsize=260.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/fdc8fa1bd4a341c6bd25796003e4b2d8
2014-07-10 15:41:42,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032098468 with entries=123, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032101967
2014-07-10 15:41:42,745 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/fdc8fa1bd4a341c6bd25796003e4b2d8 as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/fdc8fa1bd4a341c6bd25796003e4b2d8
2014-07-10 15:41:42,770 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/fdc8fa1bd4a341c6bd25796003e4b2d8, entries=947910, sequenceid=622, filesize=67.5m
2014-07-10 15:41:42,771 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~270.0m/283070320, currentsize=85.2m/89320400 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 9935ms, sequenceid=622, compaction requested=true
2014-07-10 15:41:42,771 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:41:43,195 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:41:43,196 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 257.2m
2014-07-10 15:41:43,479 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:45,379 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:45,443 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2807 synced till here 2805
2014-07-10 15:41:45,464 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032101967 with entries=108, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032105380
2014-07-10 15:41:45,902 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/dde98475426a4701acbfeb386ca0dcdf as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/dde98475426a4701acbfeb386ca0dcdf
2014-07-10 15:41:45,939 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Removing store files after compaction...
2014-07-10 15:41:45,969 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/4ec336f807c7487489a1415b3305a33b, to hdfs://master:54310/hbase/archive/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/4ec336f807c7487489a1415b3305a33b
2014-07-10 15:41:45,985 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/6365483bb53c4206b4fc3b0fc401dd3d, to hdfs://master:54310/hbase/archive/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/6365483bb53c4206b4fc3b0fc401dd3d
2014-07-10 15:41:46,001 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/81a5240506274bd883096e3f7247d01e, to hdfs://master:54310/hbase/archive/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/81a5240506274bd883096e3f7247d01e
2014-07-10 15:41:46,001 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. into dde98475426a4701acbfeb386ca0dcdf(size=68.3m), total size for store is 68.3m. This selection was in queue for 0sec, and took 15sec to execute.
2014-07-10 15:41:46,002 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., storeName=family, fileCount=3, fileSize=106.4m, priority=17, time=38891449461110; duration=15sec
2014-07-10 15:41:46,002 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:41:46,002 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-10 15:41:46,002 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 128889776 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-10 15:41:46,002 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: a7dcb4e2617e2f18516bffa9660ca2e3 - family: Initiating major compaction
2014-07-10 15:41:46,003 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: Starting compaction on family in region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:41:46,003 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp, totalSize=122.9m
2014-07-10 15:41:46,003 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/ee033d61d11347f0a65d4b9a8aecf308, keycount=34661, bloomtype=ROW, size=24.7m, encoding=NONE, seqNum=171, earliestPutTs=1405032034063
2014-07-10 15:41:46,003 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e6ca5c63a5784674a6ec1bb66d777b62, keycount=43070, bloomtype=ROW, size=30.7m, encoding=NONE, seqNum=452, earliestPutTs=1405032059739
2014-07-10 15:41:46,003 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/fdc8fa1bd4a341c6bd25796003e4b2d8, keycount=94791, bloomtype=ROW, size=67.5m, encoding=NONE, seqNum=622, earliestPutTs=1405032067925
2014-07-10 15:41:46,023 DEBUG [regionserver60020-smallCompactions-1405031679550] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:46,776 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:41:46,776 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 256.3m
2014-07-10 15:41:46,992 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:41:46,999 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-10 15:41:47,000 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-10 15:41:50,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:50,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2923 synced till here 2914
2014-07-10 15:41:50,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032105380 with entries=116, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032110468
2014-07-10 15:41:53,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:54,762 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=676, memsize=249.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/6a3fb4bac5674d6b980de560f77c81b1
2014-07-10 15:41:54,781 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/6a3fb4bac5674d6b980de560f77c81b1 as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/6a3fb4bac5674d6b980de560f77c81b1
2014-07-10 15:41:55,214 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/6a3fb4bac5674d6b980de560f77c81b1, entries=907030, sequenceid=676, filesize=64.6m
2014-07-10 15:41:55,215 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.2m/269700880, currentsize=103.7m/108771760 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 12020ms, sequenceid=676, compaction requested=true
2014-07-10 15:41:55,215 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:41:55,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032110468 with entries=114, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032113660
2014-07-10 15:41:57,194 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=706, memsize=249.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/d1466ecb099c406db5afee03e079b01a
2014-07-10 15:41:57,220 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/d1466ecb099c406db5afee03e079b01a as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1466ecb099c406db5afee03e079b01a
2014-07-10 15:41:57,237 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1466ecb099c406db5afee03e079b01a, entries=909740, sequenceid=706, filesize=64.8m
2014-07-10 15:41:57,237 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.9m/270400640, currentsize=62.6m/65670640 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 10461ms, sequenceid=706, compaction requested=false
2014-07-10 15:41:59,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:41:59,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3158 synced till here 3155
2014-07-10 15:41:59,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032113660 with entries=121, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032119339
2014-07-10 15:42:00,702 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:42:00,703 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a., current region memstore size 256.4m
2014-07-10 15:42:00,966 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:01,975 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:42:01,975 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 256.6m
2014-07-10 15:42:02,139 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:03,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:03,223 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3264 synced till here 3263
2014-07-10 15:42:03,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032119339 with entries=106, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032123122
2014-07-10 15:42:05,280 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:05,401 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3375 synced till here 3374
2014-07-10 15:42:05,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032123122 with entries=111, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032125281
2014-07-10 15:42:08,300 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/e0847f938f6d47fbbbd42d685ea7c0fb as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e0847f938f6d47fbbbd42d685ea7c0fb
2014-07-10 15:42:08,331 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Removing store files after compaction...
2014-07-10 15:42:08,346 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/ee033d61d11347f0a65d4b9a8aecf308, to hdfs://master:54310/hbase/archive/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/ee033d61d11347f0a65d4b9a8aecf308
2014-07-10 15:42:08,354 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e6ca5c63a5784674a6ec1bb66d777b62, to hdfs://master:54310/hbase/archive/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/e6ca5c63a5784674a6ec1bb66d777b62
2014-07-10 15:42:08,371 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/fdc8fa1bd4a341c6bd25796003e4b2d8, to hdfs://master:54310/hbase/archive/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/fdc8fa1bd4a341c6bd25796003e4b2d8
2014-07-10 15:42:08,372 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. into e0847f938f6d47fbbbd42d685ea7c0fb(size=98.6m), total size for store is 98.6m. This selection was in queue for 0sec, and took 22sec to execute.
2014-07-10 15:42:08,372 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., storeName=family, fileCount=3, fileSize=122.9m, priority=17, time=38907388040962; duration=22sec
2014-07-10 15:42:08,373 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:42:08,373 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-10 15:42:08,373 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 147395609 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-10 15:42:08,374 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: 23bf7ec8a173b0eb7e717fa868992012 - family: Initiating major compaction
2014-07-10 15:42:08,383 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: Starting compaction on family in region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:42:08,383 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp, totalSize=140.6m
2014-07-10 15:42:08,384 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/0d70c4e71f214b9b854bcebcff3e126b, keycount=34690, bloomtype=ROW, size=24.7m, encoding=NONE, seqNum=348, earliestPutTs=1405032034538
2014-07-10 15:42:08,384 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/d013707cbaeb4787b6d1df5263f266ac, keycount=71874, bloomtype=ROW, size=51.3m, encoding=NONE, seqNum=515, earliestPutTs=1405032060749
2014-07-10 15:42:08,384 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/6a3fb4bac5674d6b980de560f77c81b1, keycount=90703, bloomtype=ROW, size=64.6m, encoding=NONE, seqNum=676, earliestPutTs=1405032073880
2014-07-10 15:42:08,610 DEBUG [regionserver60020-smallCompactions-1405031679550] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:10,019 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:10,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3491 synced till here 3489
2014-07-10 15:42:10,357 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032125281 with entries=116, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032130019
2014-07-10 15:42:11,826 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:42:12,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:12,424 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3623 synced till here 3609
2014-07-10 15:42:12,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032130019 with entries=132, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032132204
2014-07-10 15:42:13,324 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:42:13,743 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=901, memsize=196.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ac30afe9571104f9c77d40f07e8fd90a/.tmp/16840e115fa148e5bc79083313b1b927
2014-07-10 15:42:13,778 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ac30afe9571104f9c77d40f07e8fd90a/.tmp/16840e115fa148e5bc79083313b1b927 as hdfs://master:54310/hbase/data/default/usertable/ac30afe9571104f9c77d40f07e8fd90a/family/16840e115fa148e5bc79083313b1b927
2014-07-10 15:42:13,796 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ac30afe9571104f9c77d40f07e8fd90a/family/16840e115fa148e5bc79083313b1b927, entries=716500, sequenceid=901, filesize=51.0m
2014-07-10 15:42:13,797 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268840960, currentsize=42.9m/44940480 for region usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a. in 13094ms, sequenceid=901, compaction requested=false
2014-07-10 15:42:13,797 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 304.0m
2014-07-10 15:42:14,284 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:14,600 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:14,983 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032132204 with entries=111, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032134601
2014-07-10 15:42:14,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405031673977
2014-07-10 15:42:14,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032033853
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032036198
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032039100
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032042147
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032044443
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032047461
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032050001
2014-07-10 15:42:14,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032052660
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032055175
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032057528
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032059509
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032061005
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032063131
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032065549
2014-07-10 15:42:14,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032069196
2014-07-10 15:42:14,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032072024
2014-07-10 15:42:14,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032073824
2014-07-10 15:42:14,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032078795
2014-07-10 15:42:14,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032082405
2014-07-10 15:42:14,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032086399
2014-07-10 15:42:17,511 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=782, memsize=256.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/2230e097b4834872aafe9b2dd8042e2a
2014-07-10 15:42:17,529 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/2230e097b4834872aafe9b2dd8042e2a as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/2230e097b4834872aafe9b2dd8042e2a
2014-07-10 15:42:18,244 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/2230e097b4834872aafe9b2dd8042e2a, entries=934400, sequenceid=782, filesize=66.6m
2014-07-10 15:42:18,245 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.6m/269100160, currentsize=175.5m/184040880 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 16269ms, sequenceid=782, compaction requested=false
2014-07-10 15:42:18,245 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 304.3m
2014-07-10 15:42:18,525 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:19,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:19,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3840 synced till here 3838
2014-07-10 15:42:20,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032134601 with entries=106, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032139118
2014-07-10 15:42:20,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032091594
2014-07-10 15:42:20,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032094385
2014-07-10 15:42:20,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032098468
2014-07-10 15:42:23,244 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:23,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3955 synced till here 3936
2014-07-10 15:42:23,857 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032139118 with entries=115, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032143245
2014-07-10 15:42:25,140 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:42:25,524 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:25,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4060 synced till here 4043
2014-07-10 15:42:26,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032143245 with entries=105, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032145525
2014-07-10 15:42:28,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:29,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4144 synced till here 4139
2014-07-10 15:42:29,290 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032145525 with entries=84, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032148926
2014-07-10 15:42:31,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:31,650 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4237 synced till here 4226
2014-07-10 15:42:31,809 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032148926 with entries=93, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032151627
2014-07-10 15:42:34,951 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=866, memsize=295.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/9ec1dc91f1674007acae8f1efe2ece77
2014-07-10 15:42:34,980 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/9ec1dc91f1674007acae8f1efe2ece77 as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/9ec1dc91f1674007acae8f1efe2ece77
2014-07-10 15:42:35,020 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/9ec1dc91f1674007acae8f1efe2ece77, entries=1077260, sequenceid=866, filesize=76.7m
2014-07-10 15:42:35,021 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~305.6m/320416080, currentsize=224.8m/235731600 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 21224ms, sequenceid=866, compaction requested=false
2014-07-10 15:42:35,022 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 359.6m
2014-07-10 15:42:35,382 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:37,022 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=896, memsize=294.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/d1f8573d4a7e45688ad543f3521bd827
2014-07-10 15:42:37,051 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/d1f8573d4a7e45688ad543f3521bd827 as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1f8573d4a7e45688ad543f3521bd827
2014-07-10 15:42:37,063 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1f8573d4a7e45688ad543f3521bd827, entries=1072610, sequenceid=896, filesize=76.4m
2014-07-10 15:42:37,063 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~304.3m/319047120, currentsize=181.1m/189913680 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 18818ms, sequenceid=896, compaction requested=true
2014-07-10 15:42:37,063 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:42:40,844 WARN  [RpcServer.reader=5,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1415)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:42:41,296 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/4c1732064b35494a95ccff0859e7b8a6 as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/4c1732064b35494a95ccff0859e7b8a6
2014-07-10 15:42:41,322 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Removing store files after compaction...
2014-07-10 15:42:41,332 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/0d70c4e71f214b9b854bcebcff3e126b, to hdfs://master:54310/hbase/archive/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/0d70c4e71f214b9b854bcebcff3e126b
2014-07-10 15:42:41,335 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/d013707cbaeb4787b6d1df5263f266ac, to hdfs://master:54310/hbase/archive/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/d013707cbaeb4787b6d1df5263f266ac
2014-07-10 15:42:41,345 DEBUG [regionserver60020-smallCompactions-1405031679550] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/6a3fb4bac5674d6b980de560f77c81b1, to hdfs://master:54310/hbase/archive/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/6a3fb4bac5674d6b980de560f77c81b1
2014-07-10 15:42:41,345 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. into 4c1732064b35494a95ccff0859e7b8a6(size=121.0m), total size for store is 197.8m. This selection was in queue for 0sec, and took 32sec to execute.
2014-07-10 15:42:41,345 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., storeName=family, fileCount=3, fileSize=140.6m, priority=17, time=38929759261220; duration=32sec
2014-07-10 15:42:41,345 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:42:41,346 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-10 15:42:41,346 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 219642233 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-10 15:42:41,346 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: 3b0f1f9aafa5f1eb5f21397291729f21 - family: Initiating major compaction
2014-07-10 15:42:41,347 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HRegion: Starting compaction on family in region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:42:41,347 INFO  [regionserver60020-smallCompactions-1405031679550] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp, totalSize=209.5m
2014-07-10 15:42:41,347 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/dde98475426a4701acbfeb386ca0dcdf, keycount=95843, bloomtype=ROW, size=68.3m, encoding=NONE, seqNum=544, earliestPutTs=1405032033600
2014-07-10 15:42:41,347 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1466ecb099c406db5afee03e079b01a, keycount=90974, bloomtype=ROW, size=64.8m, encoding=NONE, seqNum=706, earliestPutTs=1405032079944
2014-07-10 15:42:41,347 DEBUG [regionserver60020-smallCompactions-1405031679550] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/d1f8573d4a7e45688ad543f3521bd827, keycount=107261, bloomtype=ROW, size=76.4m, encoding=NONE, seqNum=896, earliestPutTs=1405032107078
2014-07-10 15:42:41,363 DEBUG [regionserver60020-smallCompactions-1405031679550] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:42:44,602 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1008, memsize=258.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/9a23ec91dc80493787e9c276187d9ee9
2014-07-10 15:42:44,622 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/9a23ec91dc80493787e9c276187d9ee9 as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/9a23ec91dc80493787e9c276187d9ee9
2014-07-10 15:42:44,636 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/9a23ec91dc80493787e9c276187d9ee9, entries=941260, sequenceid=1008, filesize=67.1m
2014-07-10 15:42:44,636 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~359.6m/377055920, currentsize=0.0/0 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 9614ms, sequenceid=1008, compaction requested=true
2014-07-10 15:42:44,637 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-10 15:42:56,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:42:56,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4373 synced till here 4334
2014-07-10 15:42:57,561 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:42:57,762 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 261.0m
2014-07-10 15:42:58,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032151627 with entries=136, filesize=106.2m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032176259
2014-07-10 15:42:58,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032101967
2014-07-10 15:42:58,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032105380
2014-07-10 15:42:58,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032110468
2014-07-10 15:42:58,348 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032113660
2014-07-10 15:42:59,724 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:42:59,724 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 258.1m
2014-07-10 15:43:01,063 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:01,358 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:01,376 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:01,755 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4493 synced till here 4461
2014-07-10 15:43:03,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032176259 with entries=120, filesize=99.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032181376
2014-07-10 15:43:07,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:07,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4604 synced till here 4576
2014-07-10 15:43:07,709 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1091ms
GC pool 'ParNew' had collection(s): count=1 time=1514ms
2014-07-10 15:43:08,467 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032176398,"queuetimems":181,"class":"HRegionServer","responsesize":12240,"method":"Multi"}
2014-07-10 15:43:08,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032181376 with entries=111, filesize=88.0m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032187285
2014-07-10 15:43:10,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:11,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4717 synced till here 4714
2014-07-10 15:43:11,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032187285 with entries=113, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032190841
2014-07-10 15:43:13,447 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:13,530 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4829 synced till here 4799
2014-07-10 15:43:13,959 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:43:13,982 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032190841 with entries=112, filesize=90.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032193447
2014-07-10 15:43:16,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:16,649 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4966 synced till here 4925
2014-07-10 15:43:17,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032193447 with entries=137, filesize=111.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032196197
2014-07-10 15:43:18,419 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1072, memsize=96.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/9600a08b198842a1a52282386472cc91
2014-07-10 15:43:18,487 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/9600a08b198842a1a52282386472cc91 as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/9600a08b198842a1a52282386472cc91
2014-07-10 15:43:18,763 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/9600a08b198842a1a52282386472cc91, entries=350770, sequenceid=1072, filesize=25.0m
2014-07-10 15:43:18,775 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~282.0m/295707840, currentsize=281.3m/294965440 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 21013ms, sequenceid=1072, compaction requested=false
2014-07-10 15:43:18,776 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 352.9m
2014-07-10 15:43:19,395 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:43:19,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:19,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5070 synced till here 5053
2014-07-10 15:43:20,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032196197 with entries=104, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032199687
2014-07-10 15:43:20,773 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:23,221 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1042, memsize=139.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/cc98c272a86c42a68e69f727793411ad
2014-07-10 15:43:23,310 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/cc98c272a86c42a68e69f727793411ad as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/cc98c272a86c42a68e69f727793411ad
2014-07-10 15:43:23,330 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/cc98c272a86c42a68e69f727793411ad, entries=507270, sequenceid=1042, filesize=36.1m
2014-07-10 15:43:23,330 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~278.9m/292399520, currentsize=361.5m/379050720 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 23606ms, sequenceid=1042, compaction requested=true
2014-07-10 15:43:23,331 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-10 15:43:23,331 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., current region memstore size 316.4m
2014-07-10 15:43:23,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:23,399 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032199687 with entries=91, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032203370
2014-07-10 15:43:23,819 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:43:24,383 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:25,400 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:25,681 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5262 synced till here 5254
2014-07-10 15:43:25,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032203370 with entries=101, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032205400
2014-07-10 15:43:28,482 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:28,630 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5378 synced till here 5366
2014-07-10 15:43:29,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032205400 with entries=116, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032208483
2014-07-10 15:43:30,544 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1244, memsize=65.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/b67147d2ab6f418b9956a6a7caf0a10e
2014-07-10 15:43:30,664 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/b67147d2ab6f418b9956a6a7caf0a10e as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/b67147d2ab6f418b9956a6a7caf0a10e
2014-07-10 15:43:30,676 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/b67147d2ab6f418b9956a6a7caf0a10e, entries=237120, sequenceid=1244, filesize=16.9m
2014-07-10 15:43:30,677 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~371.7m/389766800, currentsize=145.8m/152871360 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 11901ms, sequenceid=1244, compaction requested=true
2014-07-10 15:43:30,677 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-10 15:43:30,677 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., current region memstore size 436.7m
2014-07-10 15:43:31,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:31,615 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5499 synced till here 5491
2014-07-10 15:43:31,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032208483 with entries=121, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032211315
2014-07-10 15:43:32,077 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:33,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:33,602 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5598 synced till here 5594
2014-07-10 15:43:33,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032211315 with entries=99, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032213583
2014-07-10 15:43:34,456 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1275, memsize=80.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/25fade2b5c294a4fae61f8b0ec037238
2014-07-10 15:43:34,585 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/25fade2b5c294a4fae61f8b0ec037238 as hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/25fade2b5c294a4fae61f8b0ec037238
2014-07-10 15:43:34,599 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/family/25fade2b5c294a4fae61f8b0ec037238, entries=291650, sequenceid=1275, filesize=20.8m
2014-07-10 15:43:34,600 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~321.3m/336908400, currentsize=184.4m/193352160 for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21. in 11269ms, sequenceid=1275, compaction requested=false
2014-07-10 15:43:36,544 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:36,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5706 synced till here 5704
2014-07-10 15:43:37,035 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032213583 with entries=108, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032216546
2014-07-10 15:43:38,067 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:43:38,067 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., current region memstore size 263.6m
2014-07-10 15:43:39,531 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:43:40,755 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-10 15:43:40,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5816 synced till here 5798
2014-07-10 15:43:41,059 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:43:41,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032216546 with entries=110, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756
2014-07-10 15:46:05,753 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=24796, hits=3, hitRatio=0.01%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-07-10 15:46:05,756 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:122)
	at sun.nio.ch.IOUtil.write(IOUtil.java:93)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:352)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3051)

2014-07-10 15:46:05,778 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 170380ms for sessionid 0x14722696b230002, closing socket connection and attempting reconnect
2014-07-10 15:46:05,778 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 150918ms for sessionid 0x4722696eb90004, closing socket connection and attempting reconnect
2014-07-10 15:46:05,757 WARN  [ResponseProcessor for block blk_-1917069282423284879_93972] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_-1917069282423284879_93972java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-10 15:46:05,782 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 151173ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-10 15:46:05,782 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 170373ms for sessionid 0x4722696eb90000, closing socket connection and attempting reconnect
2014-07-10 15:46:05,783 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 151162ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-10 15:46:05,795 WARN  [regionserver60020] util.Sleeper: We slept 144409ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-10 15:46:05,795 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 141458ms
GC pool 'ParNew' had collection(s): count=2 time=205ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=141512ms
2014-07-10 15:46:05,821 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:05,822 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:05,839 WARN  [ResponseProcessor for block blk_2822109057522635650_93956] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_2822109057522635650_93956java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.59:34448 remote=/9.1.143.59:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-10 15:46:05,839 WARN  [ResponseProcessor for block blk_2060847278674351554_93969] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_2060847278674351554_93969java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.59:34480 remote=/9.1.143.59:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-10 15:46:05,850 WARN  [ResponseProcessor for block blk_-3867401115380514539_93963] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_-3867401115380514539_93963java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.59:34466 remote=/9.1.143.59:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-10 15:46:05,868 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:05,868 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:05,888 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:05,888 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:05,888 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:05,888 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,069 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:46:06,070 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-10 15:46:06,072 FATAL [regionserver60020] regionserver.HRegionServer: ABORTING region server slave1,60020,1405031639695: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1405031639695 as dead server
org.apache.hadoop.hbase.YouAreDeadException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1405031639695 as dead server
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:285)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1065)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:901)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1405031639695 as dead server
	at org.apache.hadoop.hbase.master.ServerManager.checkIsDead(ServerManager.java:369)
	at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:274)
	at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:1357)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:5087)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1453)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1657)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1715)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerReport(RegionServerStatusProtos.java:5414)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1063)
	... 2 more
2014-07-10 15:46:06,112 FATAL [regionserver60020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-10 15:46:06,116 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x14722696b230002 has expired, closing socket connection
2014-07-10 15:46:06,116 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-10 15:46:06,127 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x14722696b230002
2014-07-10 15:46:06,127 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-10 15:46:06,282 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,283 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,289 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,289 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,318 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,319 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,319 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,319 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,324 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,325 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,329 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,329 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,342 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,343 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,345 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,344 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,345 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,346 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,347 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,348 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,351 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,352 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,353 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,353 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,359 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,359 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,360 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,360 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,360 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,360 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,361 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,361 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,362 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,370 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,375 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,375 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,379 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,379 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,380 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,382 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,385 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1405031639695 as dead server
2014-07-10 15:46:06,385 INFO  [regionserver60020] ipc.RpcServer: Stopping server on 60020
2014-07-10 15:46:06,385 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: stopping
2014-07-10 15:46:06,386 DEBUG [RpcServer.handler=14,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,386 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2014-07-10 15:46:06,386 DEBUG [RpcServer.handler=27,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,387 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2014-07-10 15:46:06,387 DEBUG [RpcServer.handler=49,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,388 INFO  [regionserver60020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2014-07-10 15:46:06,388 INFO  [regionserver60020] regionserver.HRegionServer: Stopping infoServer
2014-07-10 15:46:06,388 INFO  [SplitLogWorker-slave1,60020,1405031639695] regionserver.SplitLogWorker: SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2014-07-10 15:46:06,411 INFO  [SplitLogWorker-slave1,60020,1405031639695] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1405031639695 exiting
2014-07-10 15:46:06,411 DEBUG [RpcServer.handler=13,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,431 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,431 DEBUG [RpcServer.handler=0,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,432 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,434 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,437 DEBUG [RpcServer.handler=3,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,434 INFO  [regionserver60020] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:60030
2014-07-10 15:46:06,437 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,437 DEBUG [RpcServer.handler=8,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,438 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,438 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,440 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,440 DEBUG [RpcServer.handler=2,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,440 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,440 DEBUG [RpcServer.handler=20,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,441 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,441 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,450 DEBUG [RpcServer.handler=15,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,452 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,452 DEBUG [RpcServer.handler=21,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,452 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,453 DEBUG [RpcServer.handler=30,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,453 DEBUG [RpcServer.handler=47,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,458 DEBUG [RpcServer.handler=44,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,458 DEBUG [RpcServer.handler=35,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,469 DEBUG [RpcServer.handler=6,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,470 DEBUG [RpcServer.handler=32,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,472 DEBUG [RpcServer.handler=7,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,478 DEBUG [RpcServer.handler=26,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,478 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,478 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,479 DEBUG [RpcServer.handler=9,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,479 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,479 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,481 DEBUG [RpcServer.handler=34,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,481 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,481 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,483 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,483 DEBUG [RpcServer.handler=4,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,484 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,486 DEBUG [RpcServer.handler=39,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,489 DEBUG [RpcServer.handler=1,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,489 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for blk_2822109057522635650_93956 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,489 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,490 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,490 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,509 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,512 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,509 DEBUG [RpcServer.handler=42,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,511 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,512 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,513 DEBUG [RpcServer.handler=12,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,513 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for blk_-1917069282423284879_93972 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,513 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,513 DEBUG [RpcServer.handler=48,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,514 DEBUG [RpcServer.handler=36,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,516 DEBUG [RpcServer.handler=22,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,516 DEBUG [RpcServer.handler=28,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,517 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,517 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,528 DEBUG [RpcServer.handler=19,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,528 DEBUG [RpcServer.handler=38,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,528 DEBUG [RpcServer.handler=37,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,529 DEBUG [RpcServer.handler=29,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,530 DEBUG [RpcServer.handler=10,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,552 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,554 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,555 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,553 DEBUG [RpcServer.handler=24,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,554 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-10 15:46:06,556 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:46:06,556 DEBUG [RpcServer.handler=16,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,558 DEBUG [RpcServer.handler=23,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,558 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-10 15:46:06,558 DEBUG [RpcServer.handler=40,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,558 DEBUG [RpcServer.handler=11,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,559 DEBUG [RpcServer.handler=45,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,559 DEBUG [RpcServer.handler=31,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,559 DEBUG [RpcServer.handler=33,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-10 15:46:06,559 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x4722696eb90004 has expired, closing socket connection
2014-07-10 15:46:06,559 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-10 15:46:06,561 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x4722696eb90004
2014-07-10 15:46:06,561 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-10 15:46:06,567 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for blk_-3867401115380514539_93963 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,568 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,585 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,585 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-10 15:46:06,608 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,609 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-10 15:46:06,615 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,616 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,616 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for blk_2060847278674351554_93969 bad datanode[0] 9.1.143.59:50010
2014-07-10 15:46:06,616 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-10 15:46:06,618 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,618 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,636 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,637 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,640 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,641 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,648 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,648 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-10 15:46:06,649 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,651 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,652 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-10 15:46:06,651 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,652 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-10 15:46:06,653 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-10 15:46:06,665 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,666 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,667 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,665 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,676 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: ABORTING region server slave1,60020,1405031639695: regionserver:60020-0x4722696eb90000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:60020-0x4722696eb90000 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-10 15:46:06,677 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x4722696eb90000 has expired, closing socket connection
2014-07-10 15:46:06,677 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-10 15:46:06,679 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,679 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,682 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-10 15:46:06,696 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,698 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,700 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,700 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,702 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,702 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,709 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-1917069282423284879_93972 has out of date GS 93972 found 94057, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,709 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756 block blk_-1917069282423284879_93972] hdfs.DFSClient: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-10 15:46:06,709 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,709 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Error while syncing
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,710 INFO  [regionserver60020.logRoller] regionserver.LogRoller: LogRoller exiting.
2014-07-10 15:46:06,710 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,710 FATAL [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:06,710 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,711 WARN  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-10 15:46:06,711 FATAL [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:06,721 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,722 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,722 WARN  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-10 15:46:06,732 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,732 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,782 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,782 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,804 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,804 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,826 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2822109057522635650_93956 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,827 WARN  [DataStreamer for file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72 block blk_2822109057522635650_93956] hdfs.DFSClient: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-10 15:46:06,844 ERROR [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., storeName=family, fileCount=3, fileSize=209.5m (68.3m, 64.8m, 76.4m), priority=17, time=38962731951468
java.io.IOException: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,849 DEBUG [regionserver60020-smallCompactions-1405031679550] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-10 15:46:06,938 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,938 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,941 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,942 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:06,971 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:06,972 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:07,027 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1640 keyvalues from start:0 to end:164
2014-07-10 15:46:07,030 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: rollbackMemstore rolled back 1640 keyvalues from start:0 to end:164
2014-07-10 15:46:07,053 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4440 keyvalues from start:0 to end:444
2014-07-10 15:46:07,060 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4329 service: ClientService methodName: Multi size: 204.7k connection: 9.1.143.58:39695: output error
2014-07-10 15:46:07,062 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,098 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4990 keyvalues from start:0 to end:499
2014-07-10 15:46:07,113 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4390 keyvalues from start:0 to end:439
2014-07-10 15:46:07,118 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4333 service: ClientService methodName: Multi size: 204.7k connection: 9.1.143.58:39731: output error
2014-07-10 15:46:07,129 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,129 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,135 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:07,154 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5390 keyvalues from start:0 to end:539
2014-07-10 15:46:07,164 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5700 keyvalues from start:0 to end:570
2014-07-10 15:46:07,173 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5520 keyvalues from start:0 to end:552
2014-07-10 15:46:07,179 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,179 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4980 keyvalues from start:0 to end:498
2014-07-10 15:46:07,179 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-10 15:46:07,183 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5090 keyvalues from start:0 to end:509
2014-07-10 15:46:07,185 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-3867401115380514539_93963 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,191 WARN  [DataStreamer for file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0 block blk_-3867401115380514539_93963] hdfs.DFSClient: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-10 15:46:07,206 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6000 keyvalues from start:0 to end:600
2014-07-10 15:46:07,210 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5750 keyvalues from start:0 to end:575
2014-07-10 15:46:07,211 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5810 keyvalues from start:0 to end:581
2014-07-10 15:46:07,212 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5120 keyvalues from start:0 to end:512
2014-07-10 15:46:07,224 WARN  [MemStoreFlusher.0] regionserver.HStore: Failed flushing store file, retrying num=0
java.io.IOException: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,228 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5910 keyvalues from start:0 to end:591
2014-07-10 15:46:07,245 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6180 keyvalues from start:0 to end:618
2014-07-10 15:46:07,251 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6070 keyvalues from start:0 to end:607
2014-07-10 15:46:07,257 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5730 keyvalues from start:0 to end:573
2014-07-10 15:46:07,273 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5650 keyvalues from start:0 to end:565
2014-07-10 15:46:07,282 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6020 keyvalues from start:0 to end:602
2014-07-10 15:46:07,282 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6020 keyvalues from start:0 to end:602
2014-07-10 15:46:07,283 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5610 keyvalues from start:0 to end:561
2014-07-10 15:46:07,284 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5460 keyvalues from start:0 to end:546
2014-07-10 15:46:07,287 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5930 keyvalues from start:0 to end:593
2014-07-10 15:46:07,295 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_2060847278674351554_93969 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,304 WARN  [DataStreamer for file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c block blk_2060847278674351554_93969] hdfs.DFSClient: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-10 15:46:07,305 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6180 keyvalues from start:0 to end:618
2014-07-10 15:46:07,303 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5870 keyvalues from start:0 to end:587
2014-07-10 15:46:07,312 WARN  [MemStoreFlusher.1] regionserver.HStore: Failed flushing store file, retrying num=0
java.io.IOException: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,321 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6010 keyvalues from start:0 to end:601
2014-07-10 15:46:07,335 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5920 keyvalues from start:0 to end:592
2014-07-10 15:46:07,335 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5800 keyvalues from start:0 to end:580
2014-07-10 15:46:07,340 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5990 keyvalues from start:0 to end:599
2014-07-10 15:46:07,348 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6010 keyvalues from start:0 to end:601
2014-07-10 15:46:07,354 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5430 keyvalues from start:0 to end:543
2014-07-10 15:46:07,355 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5640 keyvalues from start:0 to end:564
2014-07-10 15:46:07,357 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5900 keyvalues from start:0 to end:590
2014-07-10 15:46:07,360 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5900 keyvalues from start:0 to end:590
2014-07-10 15:46:07,362 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5450 keyvalues from start:0 to end:545
2014-07-10 15:46:07,435 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,435 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,435 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-10 15:46:07,439 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,439 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,439 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-10 15:46:07,450 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6030 keyvalues from start:0 to end:603
2014-07-10 15:46:07,457 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5860 keyvalues from start:0 to end:586
2014-07-10 15:46:07,465 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,465 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,466 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-10 15:46:07,467 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5990 keyvalues from start:0 to end:599
2014-07-10 15:46:07,469 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221076,"queuetimems":0,"class":"HRegionServer","responsesize":12280,"method":"Multi"}
2014-07-10 15:46:07,469 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3671 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,472 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,661 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,662 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,662 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Too many consecutive RollWriter requests, it's a sign of the total number of live datanodes is lower than the tolerable replicas.
2014-07-10 15:46:07,669 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,669 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,675 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,679 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5600 keyvalues from start:0 to end:560
2014-07-10 15:46:07,679 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,682 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,682 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,693 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,694 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,695 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032220959,"queuetimems":0,"class":"HRegionServer","responsesize":283145,"method":"Multi"}
2014-07-10 15:46:07,700 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146431,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221264,"queuetimems":0,"class":"HRegionServer","responsesize":284040,"method":"Multi"}
2014-07-10 15:46:07,703 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: rollbackMemstore rolled back 10 keyvalues from start:0 to end:1
2014-07-10 15:46:07,712 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3670 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,712 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,722 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,722 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,735 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,736 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,737 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,737 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,752 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3672 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,752 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,757 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221091,"queuetimems":1,"class":"HRegionServer","responsesize":280429,"method":"Multi"}
2014-07-10 15:46:07,758 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221378,"queuetimems":1,"class":"HRegionServer","responsesize":291907,"method":"Multi"}
2014-07-10 15:46:07,758 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032220746,"queuetimems":1,"class":"HRegionServer","responsesize":311596,"method":"Multi"}
2014-07-10 15:46:07,758 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221398,"queuetimems":1,"class":"HRegionServer","responsesize":257341,"method":"Multi"}
2014-07-10 15:46:07,758 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146778,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032220934,"queuetimems":1,"class":"HRegionServer","responsesize":279393,"method":"Multi"}
2014-07-10 15:46:07,761 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3674 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,761 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,762 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146989,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032220762,"queuetimems":1,"class":"HRegionServer","responsesize":293392,"method":"Multi"}
2014-07-10 15:46:07,762 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3675 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,762 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,770 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,770 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,770 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3665 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,790 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,791 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3669 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,791 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,791 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3664 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,791 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,791 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3676 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,792 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,793 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,800 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,803 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,803 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,805 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221244,"queuetimems":1,"class":"HRegionServer","responsesize":278016,"method":"Multi"}
2014-07-10 15:46:07,806 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,806 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,807 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3673 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,807 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,813 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,813 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,814 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5740 keyvalues from start:0 to end:574
2014-07-10 15:46:07,819 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,819 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5860 keyvalues from start:0 to end:586
2014-07-10 15:46:07,832 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,834 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,840 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,849 FATAL [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,851 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,852 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,854 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,854 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,865 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,866 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,868 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221588,"queuetimems":1,"class":"HRegionServer","responsesize":577232,"method":"Multi"}
2014-07-10 15:46:07,874 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,875 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,877 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,878 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,878 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3680 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,878 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,883 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,883 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,886 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,886 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,893 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,893 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,901 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,901 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,908 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4980 keyvalues from start:0 to end:498
2014-07-10 15:46:07,908 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5650 keyvalues from start:0 to end:565
2014-07-10 15:46:07,912 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,912 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,914 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5530 keyvalues from start:0 to end:553
2014-07-10 15:46:07,917 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5630 keyvalues from start:0 to end:563
2014-07-10 15:46:07,920 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4950 keyvalues from start:0 to end:495
2014-07-10 15:46:07,937 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5830 keyvalues from start:0 to end:583
2014-07-10 15:46:07,942 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4740 keyvalues from start:0 to end:474
2014-07-10 15:46:07,947 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5410 keyvalues from start:0 to end:541
2014-07-10 15:46:07,963 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4760 keyvalues from start:0 to end:476
2014-07-10 15:46:07,964 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,965 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,975 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5670 keyvalues from start:0 to end:567
2014-07-10 15:46:07,976 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:07,976 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:07,979 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5870 keyvalues from start:0 to end:587
2014-07-10 15:46:07,981 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6070 keyvalues from start:0 to end:607
2014-07-10 15:46:07,988 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145692,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222296,"queuetimems":0,"class":"HRegionServer","responsesize":262777,"method":"Multi"}
2014-07-10 15:46:07,990 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3691 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:07,990 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:07,997 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4810 keyvalues from start:0 to end:481
2014-07-10 15:46:08,002 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6070 keyvalues from start:0 to end:607
2014-07-10 15:46:08,023 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,024 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5750 keyvalues from start:0 to end:575
2014-07-10 15:46:08,021 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5810 keyvalues from start:0 to end:581
2014-07-10 15:46:08,012 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5980 keyvalues from start:0 to end:598
2014-07-10 15:46:08,009 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6180 keyvalues from start:0 to end:618
2014-07-10 15:46:08,024 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,027 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5630 keyvalues from start:0 to end:563
2014-07-10 15:46:08,028 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6030 keyvalues from start:0 to end:603
2014-07-10 15:46:08,068 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6170 keyvalues from start:0 to end:617
2014-07-10 15:46:08,083 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146668,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221414,"queuetimems":0,"class":"HRegionServer","responsesize":573060,"method":"Multi"}
2014-07-10 15:46:08,085 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5880 keyvalues from start:0 to end:588
2014-07-10 15:46:08,088 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,089 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,091 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5940 keyvalues from start:0 to end:594
2014-07-10 15:46:08,057 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5600 keyvalues from start:0 to end:560
2014-07-10 15:46:08,101 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3677 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,101 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,117 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5560 keyvalues from start:0 to end:556
2014-07-10 15:46:08,126 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221569,"queuetimems":1,"class":"HRegionServer","responsesize":530958,"method":"Multi"}
2014-07-10 15:46:08,760 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":147151,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221607,"queuetimems":1,"class":"HRegionServer","responsesize":548597,"method":"Multi"}
2014-07-10 15:46:08,761 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3679 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,761 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,762 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,763 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,766 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,766 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,769 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3681 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,769 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,771 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146925,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221845,"queuetimems":1,"class":"HRegionServer","responsesize":269085,"method":"Multi"}
2014-07-10 15:46:08,772 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3683 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,772 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,775 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146448,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222326,"queuetimems":0,"class":"HRegionServer","responsesize":564250,"method":"Multi"}
2014-07-10 15:46:08,777 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3692 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,777 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,777 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,778 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,782 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146770,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222008,"queuetimems":1,"class":"HRegionServer","responsesize":534266,"method":"Multi"}
2014-07-10 15:46:08,784 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3694 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,784 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,797 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146182,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222614,"queuetimems":1,"class":"HRegionServer","responsesize":565654,"method":"Multi"}
2014-07-10 15:46:08,798 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3686 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,798 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,798 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221989,"queuetimems":0,"class":"HRegionServer","responsesize":568122,"method":"Multi"}
2014-07-10 15:46:08,799 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3682 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,799 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,802 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5960 keyvalues from start:0 to end:596
2014-07-10 15:46:08,828 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5950 keyvalues from start:0 to end:595
2014-07-10 15:46:08,828 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5660 keyvalues from start:0 to end:566
2014-07-10 15:46:08,829 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":147018,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032221811,"queuetimems":1,"class":"HRegionServer","responsesize":264858,"method":"Multi"}
2014-07-10 15:46:08,830 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3678 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,830 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,836 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222831,"queuetimems":0,"class":"HRegionServer","responsesize":800364,"method":"Multi"}
2014-07-10 15:46:08,837 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3701 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,837 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,837 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222362,"queuetimems":1,"class":"HRegionServer","responsesize":510972,"method":"Multi"}
2014-07-10 15:46:08,841 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3690 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,841 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223782,"queuetimems":0,"class":"HRegionServer","responsesize":523719,"method":"Multi"}
2014-07-10 15:46:08,841 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,841 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,842 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,842 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3714 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,843 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,843 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146816,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222026,"queuetimems":0,"class":"HRegionServer","responsesize":538779,"method":"Multi"}
2014-07-10 15:46:08,844 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3693 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,844 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,848 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146270,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222577,"queuetimems":0,"class":"HRegionServer","responsesize":549463,"method":"Multi"}
2014-07-10 15:46:08,848 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3689 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,848 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,855 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,855 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,857 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,857 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,859 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,859 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,861 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,861 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,868 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,868 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,871 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,871 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,877 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,877 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,879 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5920 keyvalues from start:0 to end:592
2014-07-10 15:46:08,885 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,885 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,888 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,888 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,889 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223527,"queuetimems":2,"class":"HRegionServer","responsesize":807723,"method":"Multi"}
2014-07-10 15:46:08,890 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3715 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,890 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,893 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5740 keyvalues from start:0 to end:574
2014-07-10 15:46:08,897 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:08,897 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:08,898 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5250 keyvalues from start:0 to end:525
2014-07-10 15:46:08,904 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146268,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222635,"queuetimems":0,"class":"HRegionServer","responsesize":786106,"method":"Multi"}
2014-07-10 15:46:08,905 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3702 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,905 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,906 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5060 keyvalues from start:0 to end:506
2014-07-10 15:46:08,912 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223377,"queuetimems":0,"class":"HRegionServer","responsesize":472002,"method":"Multi"}
2014-07-10 15:46:08,913 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3712 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,913 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,925 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223110,"queuetimems":0,"class":"HRegionServer","responsesize":744243,"method":"Multi"}
2014-07-10 15:46:08,925 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3704 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,926 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,935 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4720 keyvalues from start:0 to end:472
2014-07-10 15:46:08,942 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3723 service: ClientService methodName: Multi size: 1.7m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,942 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,953 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4930 keyvalues from start:0 to end:493
2014-07-10 15:46:08,956 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3725 service: ClientService methodName: Multi size: 1.7m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,956 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,959 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5070 keyvalues from start:0 to end:507
2014-07-10 15:46:08,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":144807,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032224153,"queuetimems":0,"class":"HRegionServer","responsesize":696672,"method":"Multi"}
2014-07-10 15:46:08,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3722 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:08,963 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:08,965 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:46:08,973 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5340 keyvalues from start:0 to end:534
2014-07-10 15:46:08,978 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6050 keyvalues from start:0 to end:605
2014-07-10 15:46:08,986 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-10 15:46:08,997 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5860 keyvalues from start:0 to end:586
2014-07-10 15:46:08,999 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-10 15:46:09,004 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5910 keyvalues from start:0 to end:591
2014-07-10 15:46:09,008 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5740 keyvalues from start:0 to end:574
2014-07-10 15:46:09,023 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223335,"queuetimems":4,"class":"HRegionServer","responsesize":532168,"method":"Multi"}
2014-07-10 15:46:09,023 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145502,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223512,"queuetimems":1,"class":"HRegionServer","responsesize":564901,"method":"Multi"}
2014-07-10 15:46:09,024 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223496,"queuetimems":0,"class":"HRegionServer","responsesize":827960,"method":"Multi"}
2014-07-10 15:46:09,025 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3710 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,025 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,025 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3716 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,025 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,026 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3713 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,026 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,028 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222852,"queuetimems":1,"class":"HRegionServer","responsesize":836379,"method":"Multi"}
2014-07-10 15:46:09,029 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":146150,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032222878,"queuetimems":0,"class":"HRegionServer","responsesize":827600,"method":"Multi"}
2014-07-10 15:46:09,029 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":144943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032224084,"queuetimems":1,"class":"HRegionServer","responsesize":691802,"method":"Multi"}
2014-07-10 15:46:09,030 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223095,"queuetimems":201,"class":"HRegionServer","responsesize":857950,"method":"Multi"}
2014-07-10 15:46:09,030 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3700 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,030 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,030 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3699 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3719 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":145898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:39080","starttimems":1405032223130,"queuetimems":0,"class":"HRegionServer","responsesize":746036,"method":"Multi"}
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3695 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,031 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,032 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3706 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.58:39080: output error
2014-07-10 15:46:09,032 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-10 15:46:09,129 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-10 15:46:09,162 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-10 15:46:09,162 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-10 15:46:14,507 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1335, memsize=139.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/60f2b64735f34cee9345877e295a55d5
2014-07-10 15:46:14,539 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/60f2b64735f34cee9345877e295a55d5 as hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/60f2b64735f34cee9345877e295a55d5
2014-07-10 15:46:14,573 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/family/60f2b64735f34cee9345877e295a55d5, entries=507070, sequenceid=1335, filesize=36.1m
2014-07-10 15:46:14,573 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~464.2m/486732400, currentsize=172.6m/180973120 for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012. in 163896ms, sequenceid=1335, compaction requested=true
2014-07-10 15:46:14,574 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: MemStoreFlusher.0 exiting
2014-07-10 15:46:15,782 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-07-10 15:46:15,783 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-07-10 15:46:15,820 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-07-10 15:46:15,820 INFO  [regionserver60020.compactionChecker] regionserver.HRegionServer$CompactionChecker: regionserver60020.compactionChecker exiting
2014-07-10 15:46:15,887 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1411, memsize=214.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/22400a819a504829b16e3a0a3204d420
2014-07-10 15:46:15,924 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/22400a819a504829b16e3a0a3204d420 as hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/22400a819a504829b16e3a0a3204d420
2014-07-10 15:46:15,973 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/family/22400a819a504829b16e3a0a3204d420, entries=781410, sequenceid=1411, filesize=55.6m
2014-07-10 15:46:15,974 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~265.3m/278183680, currentsize=65.1m/68313920 for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3. in 157907ms, sequenceid=1411, compaction requested=true
2014-07-10 15:46:15,974 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: MemStoreFlusher.1 exiting
2014-07-10 15:46:15,975 INFO  [regionserver60020] snapshot.RegionServerSnapshotManager: Stopping RegionServerSnapshotManager abruptly.
2014-07-10 15:46:15,976 INFO  [regionserver60020] regionserver.HRegionServer: aborting server slave1,60020,1405031639695
2014-07-10 15:46:15,976 DEBUG [regionserver60020] catalog.CatalogTracker: Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@56adba61
2014-07-10 15:46:15,976 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:46:15,976 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 6 regions to close
2014-07-10 15:46:15,976 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.: disabling compactions & flushes
2014-07-10 15:46:15,976 DEBUG [regionserver60020] regionserver.HRegionServer: {a7dcb4e2617e2f18516bffa9660ca2e3=usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3., 3b0f1f9aafa5f1eb5f21397291729f21=usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21., e5ee55a21ff19d69490518939b0887e0=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0., e9b9c12fdbb0ab99d9463f3c554bf23b=usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b., 23bf7ec8a173b0eb7e717fa868992012=usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012., ac30afe9571104f9c77d40f07e8fd90a=usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.}
2014-07-10 15:46:15,977 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:46:15,990 INFO  [regionserver60020.nonceCleaner] regionserver.ServerNonceManager$1: regionserver60020.nonceCleaner exiting
2014-07-10 15:46:15,991 INFO  [StoreCloserThread-usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.-1] regionserver.HStore: Closed family
2014-07-10 15:46:15,991 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:46:16,005 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 68313920
2014-07-10 15:46:16,005 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:46:16,006 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user7,1405032015663.a7dcb4e2617e2f18516bffa9660ca2e3.
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.: disabling compactions & flushes
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.: disabling compactions & flushes
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.: disabling compactions & flushes
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:46:16,006 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:46:16,007 INFO  [StoreCloserThread-usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.-1] regionserver.HStore: Closed family
2014-07-10 15:46:16,008 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:46:16,008 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,,1405032015663.e9b9c12fdbb0ab99d9463f3c554bf23b.
2014-07-10 15:46:16,009 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:46:16,009 INFO  [StoreCloserThread-hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.-1] regionserver.HStore: Closed info
2014-07-10 15:46:16,009 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.: disabling compactions & flushes
2014-07-10 15:46:16,009 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:46:16,009 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:46:16,009 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-10 15:46:16,009 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:46:16,010 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.: disabling compactions & flushes
2014-07-10 15:46:16,010 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:46:16,012 INFO  [StoreCloserThread-usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.-1] regionserver.HStore: Closed family
2014-07-10 15:46:16,013 ERROR [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Memstore size is 317549760
2014-07-10 15:46:16,013 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:46:16,013 INFO  [StoreCloserThread-usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.-1] regionserver.HStore: Closed family
2014-07-10 15:46:16,014 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user2,1405032015663.3b0f1f9aafa5f1eb5f21397291729f21.
2014-07-10 15:46:16,013 INFO  [StoreCloserThread-usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.-1] regionserver.HStore: Closed family
2014-07-10 15:46:16,014 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 180973120
2014-07-10 15:46:16,015 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:46:16,015 ERROR [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Memstore size is 186914240
2014-07-10 15:46:16,015 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user8,1405032015663.23bf7ec8a173b0eb7e717fa868992012.
2014-07-10 15:46:16,015 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:46:16,016 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user9,1405032015664.ac30afe9571104f9c77d40f07e8fd90a.
2014-07-10 15:46:16,177 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1405031639695; all regions closed.
2014-07-10 15:46:16,178 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-07-10 15:46:16,178 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-07-10 15:46:16,179 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-07-10 15:46:16,179 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-07-10 15:46:16,179 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-07-10 15:46:16,179 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-07-10 15:46:16,180 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-07-10 15:46:16,180 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-07-10 15:46:16,180 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-07-10 15:46:16,180 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-07-10 15:46:16,187 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-07-10 15:46:16,187 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-07-10 15:46:16,188 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-07-10 15:46:16,188 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-07-10 15:46:16,188 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/slave1,60020,1405031639695
2014-07-10 15:46:16,188 ERROR [regionserver60020] regionserver.HRegionServer: Close and delete failed
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:16,289 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-07-10 15:46:16,289 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-07-10 15:46:16,290 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-07-10 15:46:16,290 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-07-10 15:46:16,290 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-07-10 15:46:16,290 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-07-10 15:46:16,291 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1405031639695
2014-07-10 15:46:16,292 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-10 15:46:17,292 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1405031639695
2014-07-10 15:46:17,293 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-10 15:46:19,293 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1405031639695
2014-07-10 15:46:19,293 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-10 15:46:23,294 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1405031639695
2014-07-10 15:46:23,294 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-10 15:46:31,295 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1405031639695
2014-07-10 15:46:31,295 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper getChildren failed after 4 attempts
2014-07-10 15:46:31,305 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
2014-07-10 15:46:31,305 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-10 15:46:32,305 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
2014-07-10 15:46:32,306 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-10 15:46:34,306 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
2014-07-10 15:46:34,306 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-10 15:46:38,307 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
2014-07-10 15:46:38,307 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-10 15:46:46,307 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
2014-07-10 15:46:46,308 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts
2014-07-10 15:46:46,308 WARN  [regionserver60020] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1405031639695
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:156)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1273)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1262)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1292)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1008)
	at java.lang.Thread.run(Thread.java:701)
2014-07-10 15:46:46,318 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1405031639695; zookeeper connection closed.
2014-07-10 15:46:46,318 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-07-10 15:46:46,376 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-10 15:46:46,380 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=Thread[Thread-9,5,main]
2014-07-10 15:46:46,380 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-07-10 15:46:46,381 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/slave1,60020,1405031639695/slave1%2C60020%2C1405031639695.1405032220756
java.io.IOException: Error Recovery for block blk_-1917069282423284879_93972 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:46,381 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/data/default/usertable/3b0f1f9aafa5f1eb5f21397291729f21/.tmp/24341e92d61a4693a6206424daf88d72
java.io.IOException: Error Recovery for block blk_2822109057522635650_93956 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:46,382 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/data/default/usertable/a7dcb4e2617e2f18516bffa9660ca2e3/.tmp/3aa7598e6e3944038958325c086c171c
java.io.IOException: Error Recovery for block blk_2060847278674351554_93969 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:46,382 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/data/default/usertable/23bf7ec8a173b0eb7e717fa868992012/.tmp/2db2d4771aa0400286ed8cb78d9632b0
java.io.IOException: Error Recovery for block blk_-3867401115380514539_93963 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-10 15:46:46,382 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook finished.
