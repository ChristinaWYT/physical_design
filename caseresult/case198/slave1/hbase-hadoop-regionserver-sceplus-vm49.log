Tue Jul 22 12:39:17 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-22 12:39:18,458 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-22 12:39:18,459 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-22 12:39:18,459 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-22 12:39:18,696 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-22 12:39:18,696 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-22 12:39:18,696 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 45435 22
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-22 12:39:18,697 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 45435 9.1.143.59 22
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-22 12:39:18,698 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-22 12:39:18,701 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=336
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-22 12:39:18,702 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-22 12:39:18,704 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-22 12:39:18,705 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-22 12:39:18,943 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-22 12:39:19,408 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-22 12:39:19,543 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-22 12:39:19,555 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-22 12:39:19,618 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-22 12:39:19,619 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-22 12:39:19,619 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-22 12:39:19,624 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-22 12:39:19,629 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-22 12:39:19,710 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-22 12:39:19,710 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-22 12:39:19,714 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-22 12:39:19,717 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-22 12:39:19,789 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-22 12:39:19,846 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-22 12:39:19,856 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-22 12:39:19,858 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-22 12:39:19,858 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-22 12:39:19,858 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-22 12:39:20,176 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-22 12:39:20,230 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-22 12:39:20,231 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-22 12:39:20,232 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-22 12:39:20,232 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-22 12:39:20,232 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-22 12:39:20,233 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 12:39:20,259 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 12:39:20,266 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 12:39:20,270 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-22 12:39:20,276 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-22 12:39:20,390 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2014-07-22 12:39:20,390 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-22 12:39:20,971 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 12:39:20,972 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-22 12:39:20,973 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-22 12:39:22,385 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 12:39:22,386 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-22 12:39:22,408 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x475f95e04c0000, negotiated timeout = 90000
2014-07-22 12:39:53,866 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x6d2acc34, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 12:39:53,868 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6d2acc34 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 12:39:53,868 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 12:39:53,869 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-22 12:39:53,882 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x1475f95d97c0001, negotiated timeout = 90000
2014-07-22 12:39:54,138 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@759b6377
2014-07-22 12:39:54,143 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-22 12:39:54,148 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-22 12:39:54,157 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-22 12:39:54,194 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-22 12:39:54,201 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-22 12:39:54,213 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-22 12:39:54,238 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1406057959275 with port=60020, startcode=1406057959640
2014-07-22 12:39:54,591 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-22 12:39:54,591 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-22 12:39:54,591 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-22 12:39:54,620 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-22 12:39:54,629 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640
2014-07-22 12:39:54,666 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-22 12:39:54,677 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-22 12:39:54,767 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406057994683
2014-07-22 12:39:54,781 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-22 12:39:54,786 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-22 12:39:54,789 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-22 12:39:54,793 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-22 12:39:54,796 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 12:39:54,796 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 12:39:54,796 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-22 12:39:54,796 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-22 12:39:54,797 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-22 12:39:54,808 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1406057961053, slave1,60020,1406057959640] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1406057961053, slave1,60020,1406057959640]
2014-07-22 12:39:54,831 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-22 12:39:54,833 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x5373562d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-22 12:39:54,834 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x5373562d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-22 12:39:54,834 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-22 12:39:54,835 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-22 12:39:54,839 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x475f95e04c0004, negotiated timeout = 90000
2014-07-22 12:39:54,846 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-22 12:39:54,846 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-22 12:39:54,913 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1406057959640, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x475f95e04c0000
2014-07-22 12:39:54,914 INFO  [SplitLogWorker-slave1,60020,1406057959640] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1406057959640 starting
2014-07-22 12:39:54,914 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-22 12:39:54,914 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1406057959640
2014-07-22 12:39:54,914 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1406057959640'
2014-07-22 12:39:54,914 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-22 12:39:54,916 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-22 12:39:54,918 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-22 12:39:59,530 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:39:59,691 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,691 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:39:59,693 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:39:59,693 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,695 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:39:59,695 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,696 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:39:59,696 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:39:59,696 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:39:59,697 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 12:39:59,707 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:39:59,707 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:39:59,708 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:39:59,723 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,723 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,724 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:39:59,749 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 4d714c493433d51e92af518b12e607b2, NAME => 'usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-22 12:39:59,749 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 92ba9ea7e04274b0bbdc5cbfefd7393e, NAME => 'usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-22 12:39:59,749 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 4264c18b25708212810563a172f7f7f7, NAME => 'usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-22 12:39:59,775 INFO  [RS_OPEN_REGION-slave1:60020-1] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-22 12:39:59,775 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 4d714c493433d51e92af518b12e607b2
2014-07-22 12:39:59,775 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 12:39:59,775 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 4264c18b25708212810563a172f7f7f7
2014-07-22 12:39:59,776 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:39:59,776 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:39:59,776 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:39:59,784 INFO  [RS_OPEN_REGION-slave1:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-22 12:39:59,786 INFO  [RS_OPEN_REGION-slave1:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-22 12:39:59,790 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-22 12:39:59,790 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-22 12:39:59,790 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-22 12:39:59,871 INFO  [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:39:59,871 INFO  [StoreOpener-4d714c493433d51e92af518b12e607b2-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:39:59,877 INFO  [StoreOpener-4264c18b25708212810563a172f7f7f7-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:39:59,941 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-22 12:40:00,010 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-22 12:40:00,016 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-22 12:40:00,025 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/0b062afec8cc4b13b1ae93457fca580a, isReference=false, isBulkLoadResult=false, seqid=7658, majorCompaction=false
2014-07-22 12:40:00,029 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/02413e11acd242e9a73ff16c82c57cbd, isReference=false, isBulkLoadResult=false, seqid=14456, majorCompaction=false
2014-07-22 12:40:00,035 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/053bd94bdbc649ae918d57d140ab81e4, isReference=false, isBulkLoadResult=false, seqid=21314, majorCompaction=false
2014-07-22 12:40:00,056 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/0e08c2af318143a5a3e3fb64a09011b1, isReference=false, isBulkLoadResult=false, seqid=15783, majorCompaction=false
2014-07-22 12:40:00,084 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/0fd0c24985fb4534a52df29510e9f5f6, isReference=false, isBulkLoadResult=false, seqid=9314, majorCompaction=false
2014-07-22 12:40:00,086 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/23ab1d3aef8045d4b236b50154ec4057, isReference=false, isBulkLoadResult=false, seqid=20209, majorCompaction=false
2014-07-22 12:40:00,088 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/06ddd1211a594ed78f692b68659c4823, isReference=false, isBulkLoadResult=false, seqid=15570, majorCompaction=false
2014-07-22 12:40:00,174 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/0fe892ee798248eeae1198cddf42de1a, isReference=false, isBulkLoadResult=false, seqid=10851, majorCompaction=false
2014-07-22 12:40:00,177 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/27fb9838001f440f9af94763d22c9258, isReference=false, isBulkLoadResult=false, seqid=13967, majorCompaction=false
2014-07-22 12:40:00,202 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/206be05c18024644b62422190d63cb9a, isReference=false, isBulkLoadResult=false, seqid=3586, majorCompaction=false
2014-07-22 12:40:00,211 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/26364ddd1d724ce18fd75a7e1b455fb0, isReference=false, isBulkLoadResult=false, seqid=21543, majorCompaction=false
2014-07-22 12:40:00,216 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/22366cbe85b0468481963a110ff909fa, isReference=false, isBulkLoadResult=false, seqid=23965, majorCompaction=false
2014-07-22 12:40:00,227 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/29260524ad3042adbce9a2b421bbd979, isReference=false, isBulkLoadResult=false, seqid=14331, majorCompaction=false
2014-07-22 12:40:00,237 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/2676e2031f334c0fa8ad5ed4a67ffff4, isReference=false, isBulkLoadResult=false, seqid=1463, majorCompaction=false
2014-07-22 12:40:00,263 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/34eaf70cf57d4d9cb3558876a144490e, isReference=false, isBulkLoadResult=false, seqid=15665, majorCompaction=false
2014-07-22 12:40:00,317 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/27a9d54023124d1ca7c07651397d558e, isReference=false, isBulkLoadResult=false, seqid=21935, majorCompaction=false
2014-07-22 12:40:00,317 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/299b985832124c1d8f7a527805f2a8ba, isReference=false, isBulkLoadResult=false, seqid=1616, majorCompaction=true
2014-07-22 12:40:00,328 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/4e30d8f8c0704d98b7b1b7a6eaf1618f, isReference=false, isBulkLoadResult=false, seqid=9712, majorCompaction=false
2014-07-22 12:40:00,344 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/2d38ba8b3e574abd93c4c234bbde4077, isReference=false, isBulkLoadResult=false, seqid=15983, majorCompaction=false
2014-07-22 12:40:00,366 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/2c14869440694685ad26e299c96ec74a, isReference=false, isBulkLoadResult=false, seqid=8031, majorCompaction=false
2014-07-22 12:40:00,373 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/2f27642e1bf241079249ad2f74b4ab50, isReference=false, isBulkLoadResult=false, seqid=19605, majorCompaction=false
2014-07-22 12:40:00,437 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/50b1fdf5c8ee4cdc836b723951482d31, isReference=false, isBulkLoadResult=false, seqid=20550, majorCompaction=false
2014-07-22 12:40:00,462 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/2d008121bbd34e30ae46c6bceaa7440f, isReference=false, isBulkLoadResult=false, seqid=9556, majorCompaction=false
2014-07-22 12:40:00,464 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/32a2535e17864ab184af938c3d0bc0fd, isReference=false, isBulkLoadResult=false, seqid=22292, majorCompaction=false
2014-07-22 12:40:00,467 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/56793172a95e4dba8b065addc918bc43, isReference=false, isBulkLoadResult=false, seqid=8963, majorCompaction=false
2014-07-22 12:40:00,481 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/3a692b2bdf5c4436906389b97b87862d, isReference=false, isBulkLoadResult=false, seqid=24070, majorCompaction=false
2014-07-22 12:40:00,487 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/30642a533b5443d4a4f029c93eed7f54, isReference=false, isBulkLoadResult=false, seqid=21735, majorCompaction=false
2014-07-22 12:40:00,495 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/7807db9af0e4464a87f9643fe12745aa, isReference=false, isBulkLoadResult=false, seqid=24077, majorCompaction=false
2014-07-22 12:40:00,513 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/4a0f3facefd5449fa96b5974fb7eda65, isReference=false, isBulkLoadResult=false, seqid=20615, majorCompaction=false
2014-07-22 12:40:00,518 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/41181500bad048e7bb581c06b4ba51a0, isReference=false, isBulkLoadResult=false, seqid=3911, majorCompaction=false
2014-07-22 12:40:00,524 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/7f10e37f1cfa41cab141263bb7a21c63, isReference=false, isBulkLoadResult=false, seqid=4855, majorCompaction=false
2014-07-22 12:40:00,568 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/4b023e2f641e4a41a4a2754df3fcdb31, isReference=false, isBulkLoadResult=false, seqid=7265, majorCompaction=false
2014-07-22 12:40:00,592 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4b2f034d741c4554b7c2ae0f4e7a37fd, isReference=false, isBulkLoadResult=false, seqid=20390, majorCompaction=false
2014-07-22 12:40:00,598 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/835ec9c28ac147b0afbb4aaefd94b621, isReference=false, isBulkLoadResult=false, seqid=20930, majorCompaction=false
2014-07-22 12:40:00,629 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/4d9f4882dfb541be99a73a84df198532, isReference=false, isBulkLoadResult=false, seqid=2171, majorCompaction=false
2014-07-22 12:40:00,635 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/4cde648d982849578064b9d316452369, isReference=false, isBulkLoadResult=false, seqid=8080, majorCompaction=false
2014-07-22 12:40:00,658 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/840405b7fcda485ba3acb7cfac6be692, isReference=false, isBulkLoadResult=false, seqid=10091, majorCompaction=false
2014-07-22 12:40:00,660 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/7f5084ef6447482c8588bb576d7a73b2, isReference=false, isBulkLoadResult=false, seqid=13332, majorCompaction=false
2014-07-22 12:40:00,689 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/842ae79273f44a55813c74a5c918f190, isReference=false, isBulkLoadResult=false, seqid=20016, majorCompaction=false
2014-07-22 12:40:00,691 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/8d462e9dd8254e808befc7de29c444b0, isReference=false, isBulkLoadResult=false, seqid=4465, majorCompaction=false
2014-07-22 12:40:00,692 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/5aa9075a90d14eeebb4aa2d7a3d7400f, isReference=false, isBulkLoadResult=false, seqid=3558, majorCompaction=false
2014-07-22 12:40:00,709 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/677257813ef049d39b5bc4b072ee6640, isReference=false, isBulkLoadResult=false, seqid=22866, majorCompaction=false
2014-07-22 12:40:00,716 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/8551416e90ae403ea576c9e4f1a99379, isReference=false, isBulkLoadResult=false, seqid=1127, majorCompaction=true
2014-07-22 12:40:00,720 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/97a5ac8488bf48b1a004d363d04e359a, isReference=false, isBulkLoadResult=false, seqid=15300, majorCompaction=false
2014-07-22 12:40:00,742 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/860f14add7bb4ff6b73673c2510ec9cd, isReference=false, isBulkLoadResult=false, seqid=8483, majorCompaction=false
2014-07-22 12:40:00,743 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/6bbd0f3cf0d34658be82275bcb71a52f, isReference=false, isBulkLoadResult=false, seqid=2590, majorCompaction=false
2014-07-22 12:40:00,745 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/9ddf9c513db94ccb82384a249f9f1534, isReference=false, isBulkLoadResult=false, seqid=22459, majorCompaction=false
2014-07-22 12:40:00,781 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/6bdfbbda741c4bc49cd7631b33e28171, isReference=false, isBulkLoadResult=false, seqid=9163, majorCompaction=false
2014-07-22 12:40:00,787 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/8c4f790176e24c06a9faacc13ccded63, isReference=false, isBulkLoadResult=false, seqid=14107, majorCompaction=false
2014-07-22 12:40:00,792 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/a304871008ed40c689b751043c2774b7, isReference=false, isBulkLoadResult=false, seqid=22093, majorCompaction=false
2014-07-22 12:40:00,810 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/7a1a7b07205149c786b588731fca19db, isReference=false, isBulkLoadResult=false, seqid=10296, majorCompaction=false
2014-07-22 12:40:00,824 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/91ca8429f27742ec9d96815600c0d80c, isReference=false, isBulkLoadResult=false, seqid=7711, majorCompaction=false
2014-07-22 12:40:00,827 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/b1e0808be58f46e5ba950fcab3aa1fe7, isReference=false, isBulkLoadResult=false, seqid=8580, majorCompaction=false
2014-07-22 12:40:00,846 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/92201a195a8541858a0116c02146cdaa, isReference=false, isBulkLoadResult=false, seqid=14852, majorCompaction=false
2014-07-22 12:40:00,858 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/b55cdc884ec3449fac6596a3b1814d5a, isReference=false, isBulkLoadResult=false, seqid=16061, majorCompaction=false
2014-07-22 12:40:00,860 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/84843eedb74e4a9c8b78dd2de4ebc557, isReference=false, isBulkLoadResult=false, seqid=9859, majorCompaction=false
2014-07-22 12:40:00,882 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/a53ae4f440014b148b06c6e5dbfc3a83, isReference=false, isBulkLoadResult=false, seqid=20775, majorCompaction=false
2014-07-22 12:40:00,884 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/92a0dc565ca749378ec76fe20881f4b3, isReference=false, isBulkLoadResult=false, seqid=22125, majorCompaction=false
2014-07-22 12:40:00,897 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/ba045d6982cf4336844e3fc9f4239f74, isReference=false, isBulkLoadResult=false, seqid=21691, majorCompaction=false
2014-07-22 12:40:00,905 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/a872a66be9bd439ebb67d8e2f6ab5bbe, isReference=false, isBulkLoadResult=false, seqid=22688, majorCompaction=false
2014-07-22 12:40:00,916 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/be0f3d17af9c4faeb2ac621ecc7d071a, isReference=false, isBulkLoadResult=false, seqid=23573, majorCompaction=false
2014-07-22 12:40:00,922 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/a5747079a59c4b9e8d06926c4e11689f, isReference=false, isBulkLoadResult=false, seqid=22503, majorCompaction=false
2014-07-22 12:40:00,947 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/aa68057066c14bb2959475e89d17f016, isReference=false, isBulkLoadResult=false, seqid=9194, majorCompaction=false
2014-07-22 12:40:00,951 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/b4f1d07fe3bd46b5a31211f30425b156, isReference=false, isBulkLoadResult=false, seqid=23334, majorCompaction=false
2014-07-22 12:40:00,955 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/c75a68dd68684d3794345e735a41fa0f, isReference=false, isBulkLoadResult=false, seqid=4010, majorCompaction=false
2014-07-22 12:40:00,960 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/c060e010928547359316cb2c157ae1a4, isReference=false, isBulkLoadResult=false, seqid=24061, majorCompaction=false
2014-07-22 12:40:00,975 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/c85de67943ed4513b01b236f360e8104, isReference=false, isBulkLoadResult=false, seqid=11575, majorCompaction=false
2014-07-22 12:40:00,985 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/c6dd4c9e577c48bbb7b8861367b09d03, isReference=false, isBulkLoadResult=false, seqid=23880, majorCompaction=false
2014-07-22 12:40:01,027 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/b4f38494899d4ac8aec34bd12c646089, isReference=false, isBulkLoadResult=false, seqid=21380, majorCompaction=false
2014-07-22 12:40:01,028 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/ce79f810b67444a781c6236de6d6bf16, isReference=false, isBulkLoadResult=false, seqid=20157, majorCompaction=false
2014-07-22 12:40:01,048 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/cb795ed70c394645bc67c69be3f553fd, isReference=false, isBulkLoadResult=false, seqid=9601, majorCompaction=false
2014-07-22 12:40:01,053 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/b6ee98697439425399155df3963e99f8, isReference=false, isBulkLoadResult=false, seqid=2869, majorCompaction=false
2014-07-22 12:40:01,065 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/d2648ec4012c47de914d049782e93c3d, isReference=false, isBulkLoadResult=false, seqid=3143, majorCompaction=false
2014-07-22 12:40:01,083 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/d0f445682c7f440ca09feb5b33304e94, isReference=false, isBulkLoadResult=false, seqid=15191, majorCompaction=false
2014-07-22 12:40:01,094 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/d77e394b7c2349aca04d4d7c420fb41b, isReference=false, isBulkLoadResult=false, seqid=10471, majorCompaction=false
2014-07-22 12:40:01,100 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/b855df7e42624bf181f6da4b8a80659b, isReference=false, isBulkLoadResult=false, seqid=8751, majorCompaction=false
2014-07-22 12:40:01,111 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/e2b06a7ac7db452aa427eb07e0c8796e, isReference=false, isBulkLoadResult=false, seqid=21166, majorCompaction=false
2014-07-22 12:40:01,117 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/c957d2bda18d4f82bdf3357bdfdb8aee, isReference=false, isBulkLoadResult=false, seqid=15085, majorCompaction=false
2014-07-22 12:40:01,160 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/d7d1922e708f42139571f84576a1bbbf, isReference=false, isBulkLoadResult=false, seqid=16439, majorCompaction=false
2014-07-22 12:40:01,166 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/e86ecb28d8124025ad4ccfa063495b7f, isReference=false, isBulkLoadResult=false, seqid=13729, majorCompaction=false
2014-07-22 12:40:01,179 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/cb0b10e9207c4fada1b9b95f59b61efc, isReference=false, isBulkLoadResult=false, seqid=19469, majorCompaction=false
2014-07-22 12:40:01,217 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/e828c4d271ac4dd597eb05fabb2f6526, isReference=false, isBulkLoadResult=false, seqid=22808, majorCompaction=false
2014-07-22 12:40:01,224 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/e97dd1d377da4e03a16b176ecc2b65ef, isReference=false, isBulkLoadResult=false, seqid=23045, majorCompaction=false
2014-07-22 12:40:01,269 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/cd08ba7d4493428ca942935de211bb49, isReference=false, isBulkLoadResult=false, seqid=20991, majorCompaction=false
2014-07-22 12:40:01,286 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/eda3ef0e2f40490b96b1913fe72851e3, isReference=false, isBulkLoadResult=false, seqid=23478, majorCompaction=false
2014-07-22 12:40:01,294 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/cfb8a0713f76415f93b49bc30e8c4613, isReference=false, isBulkLoadResult=false, seqid=23237, majorCompaction=false
2014-07-22 12:40:01,302 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/f2d9366d89c14bd499967184d86c226b, isReference=false, isBulkLoadResult=false, seqid=2739, majorCompaction=true
2014-07-22 12:40:01,329 DEBUG [StoreOpener-4264c18b25708212810563a172f7f7f7-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/eebae194dd2f4221a8fab3b8e69227ba, isReference=false, isBulkLoadResult=false, seqid=8852, majorCompaction=false
2014-07-22 12:40:01,329 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/d2833913ed9243ec95fef4d21aad3073, isReference=false, isBulkLoadResult=false, seqid=8409, majorCompaction=false
2014-07-22 12:40:01,342 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/facbc7fcb2604bd3b7a629f077a8e645, isReference=false, isBulkLoadResult=false, seqid=23227, majorCompaction=false
2014-07-22 12:40:01,358 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/d60adf9a5839475e95d4d3390aa6d4be, isReference=false, isBulkLoadResult=false, seqid=15422, majorCompaction=false
2014-07-22 12:40:01,364 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7
2014-07-22 12:40:01,369 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 4264c18b25708212810563a172f7f7f7; next sequenceid=24071
2014-07-22 12:40:01,369 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 4264c18b25708212810563a172f7f7f7
2014-07-22 12:40:01,373 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:01,377 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/e3e31d006b244d4c89442820ce0471ba, isReference=false, isBulkLoadResult=false, seqid=14717, majorCompaction=false
2014-07-22 12:40:01,378 DEBUG [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:40:01,381 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 12:40:01,384 DEBUG [StoreOpener-92ba9ea7e04274b0bbdc5cbfefd7393e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e/family/fe4c856b53ee4480ae2758017c6a56ec, isReference=false, isBulkLoadResult=false, seqid=11196, majorCompaction=false
2014-07-22 12:40:01,390 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 12:40:01,391 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 10 files of size 781702595 starting at candidate #21 after considering 204 permutations with 147 in ratio
2014-07-22 12:40:01,393 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.HStore: 4264c18b25708212810563a172f7f7f7 - family: Initiating minor compaction
2014-07-22 12:40:01,393 INFO  [regionserver60020-smallCompactions-1406058001377] regionserver.HRegion: Starting compaction on family in region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:01,394 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 92ba9ea7e04274b0bbdc5cbfefd7393e; next sequenceid=24078
2014-07-22 12:40:01,394 INFO  [regionserver60020-smallCompactions-1406058001377] regionserver.HStore: Starting compaction of 10 file(s) in family of usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/.tmp, totalSize=745.5m
2014-07-22 12:40:01,394 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 92ba9ea7e04274b0bbdc5cbfefd7393e
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/e2b06a7ac7db452aa427eb07e0c8796e, keycount=97920, bloomtype=ROW, size=69.8m, encoding=NONE, seqNum=21166
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/26364ddd1d724ce18fd75a7e1b455fb0, keycount=86597, bloomtype=ROW, size=61.7m, encoding=NONE, seqNum=21543
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/27a9d54023124d1ca7c07651397d558e, keycount=108735, bloomtype=ROW, size=77.5m, encoding=NONE, seqNum=21935
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/32a2535e17864ab184af938c3d0bc0fd, keycount=123809, bloomtype=ROW, size=88.2m, encoding=NONE, seqNum=22292
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/a872a66be9bd439ebb67d8e2f6ab5bbe, keycount=136431, bloomtype=ROW, size=97.2m, encoding=NONE, seqNum=22688
2014-07-22 12:40:01,395 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/e97dd1d377da4e03a16b176ecc2b65ef, keycount=85133, bloomtype=ROW, size=60.7m, encoding=NONE, seqNum=23045
2014-07-22 12:40:01,396 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/eda3ef0e2f40490b96b1913fe72851e3, keycount=162331, bloomtype=ROW, size=115.6m, encoding=NONE, seqNum=23478
2014-07-22 12:40:01,396 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/c6dd4c9e577c48bbb7b8861367b09d03, keycount=142229, bloomtype=ROW, size=101.3m, encoding=NONE, seqNum=23880
2014-07-22 12:40:01,396 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/c060e010928547359316cb2c157ae1a4, keycount=98652, bloomtype=ROW, size=70.3m, encoding=NONE, seqNum=24061
2014-07-22 12:40:01,396 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/4264c18b25708212810563a172f7f7f7/family/3a692b2bdf5c4436906389b97b87862d, keycount=4421, bloomtype=ROW, size=3.1m, encoding=NONE, seqNum=24070
2014-07-22 12:40:01,397 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:01,399 DEBUG [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-22 12:40:01,405 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/ef1fbff12ba44099938a0281d236b493, isReference=false, isBulkLoadResult=false, seqid=3173, majorCompaction=false
2014-07-22 12:40:01,447 DEBUG [StoreOpener-4d714c493433d51e92af518b12e607b2-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2/family/f4f39ede007c44dc9ae06283b37258ae, isReference=false, isBulkLoadResult=false, seqid=19851, majorCompaction=false
2014-07-22 12:40:01,457 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/4d714c493433d51e92af518b12e607b2
2014-07-22 12:40:01,460 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 4d714c493433d51e92af518b12e607b2; next sequenceid=23335
2014-07-22 12:40:01,461 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 4d714c493433d51e92af518b12e607b2
2014-07-22 12:40:01,463 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:01,463 DEBUG [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-22 12:40:01,560 DEBUG [regionserver60020-smallCompactions-1406058001377] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:40:01,721 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] catalog.MetaEditor: Updated row usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. with server=slave1,60020,1406057959640
2014-07-22 12:40:01,721 INFO  [PostOpenDeployTasks:92ba9ea7e04274b0bbdc5cbfefd7393e] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:01,721 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] catalog.MetaEditor: Updated row usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. with server=slave1,60020,1406057959640
2014-07-22 12:40:01,721 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] catalog.MetaEditor: Updated row usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. with server=slave1,60020,1406057959640
2014-07-22 12:40:01,723 INFO  [PostOpenDeployTasks:4d714c493433d51e92af518b12e607b2] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:01,723 INFO  [PostOpenDeployTasks:4264c18b25708212810563a172f7f7f7] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:01,724 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4d714c493433d51e92af518b12e607b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,724 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92ba9ea7e04274b0bbdc5cbfefd7393e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,725 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4264c18b25708212810563a172f7f7f7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,730 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4d714c493433d51e92af518b12e607b2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,730 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 4d714c493433d51e92af518b12e607b2 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:01,731 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. on slave1,60020,1406057959640
2014-07-22 12:40:01,731 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92ba9ea7e04274b0bbdc5cbfefd7393e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,731 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4264c18b25708212810563a172f7f7f7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:01,731 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 92ba9ea7e04274b0bbdc5cbfefd7393e to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:01,731 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 4264c18b25708212810563a172f7f7f7 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:01,732 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. on slave1,60020,1406057959640
2014-07-22 12:40:01,732 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. on slave1,60020,1406057959640
2014-07-22 12:40:01,732 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3dae59807fb4d98cc09913214b3da6ff from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,733 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 119de48677ef5d044e05e2fc2701307f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,734 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,738 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3dae59807fb4d98cc09913214b3da6ff from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,739 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 3dae59807fb4d98cc09913214b3da6ff, NAME => 'usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-22 12:40:01,740 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 119de48677ef5d044e05e2fc2701307f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,740 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 119de48677ef5d044e05e2fc2701307f, NAME => 'usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-22 12:40:01,740 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3dae59807fb4d98cc09913214b3da6ff
2014-07-22 12:40:01,741 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:01,741 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 119de48677ef5d044e05e2fc2701307f
2014-07-22 12:40:01,741 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:01,743 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:01,744 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 20f69fa9e49e2ee9c1b670c938523b8e, NAME => 'usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-22 12:40:01,745 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 12:40:01,745 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:01,748 INFO  [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:01,750 INFO  [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:01,761 INFO  [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:01,788 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/05b642561c8541a9a892f28fe64089e0, isReference=false, isBulkLoadResult=false, seqid=23422, majorCompaction=false
2014-07-22 12:40:01,790 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/016c4e036aa143a883d83f88d34f24d3, isReference=false, isBulkLoadResult=false, seqid=15553, majorCompaction=false
2014-07-22 12:40:01,792 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/080aec99853747108817f486bbc39d67, isReference=false, isBulkLoadResult=false, seqid=11951, majorCompaction=false
2014-07-22 12:40:01,802 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/0177c1858f6e454c886015b1eb7b73a9, isReference=false, isBulkLoadResult=false, seqid=21892, majorCompaction=false
2014-07-22 12:40:01,805 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/0b862fac1da842359b626382c2ed491a, isReference=false, isBulkLoadResult=false, seqid=22647, majorCompaction=false
2014-07-22 12:40:01,815 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/13305561f8094500866ae48a24e4162a, isReference=false, isBulkLoadResult=false, seqid=18680, majorCompaction=false
2014-07-22 12:40:01,829 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/065420cdf1a54ab28e16269c63cdc79f, isReference=false, isBulkLoadResult=false, seqid=6506, majorCompaction=false
2014-07-22 12:40:01,837 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/153ea2b4c2bc49a9b0b21a52fdbaf017, isReference=false, isBulkLoadResult=false, seqid=20115, majorCompaction=false
2014-07-22 12:40:01,844 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/101f5ce021a04a778213193068296e09, isReference=false, isBulkLoadResult=false, seqid=14402, majorCompaction=false
2014-07-22 12:40:01,848 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/067fd8f5bc544d1697e7e838c68f87d7, isReference=false, isBulkLoadResult=false, seqid=21511, majorCompaction=false
2014-07-22 12:40:01,867 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/1954101f1b254ef1b1265f93b5c087fe, isReference=false, isBulkLoadResult=false, seqid=17925, majorCompaction=false
2014-07-22 12:40:01,868 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/1428e97a71e9421fb69dfe6dc5fa0691, isReference=false, isBulkLoadResult=false, seqid=16244, majorCompaction=false
2014-07-22 12:40:01,875 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/13303e57add9487d84cb83b911257007, isReference=false, isBulkLoadResult=false, seqid=12517, majorCompaction=false
2014-07-22 12:40:01,892 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/1bad0aef855e42a69d27773618a188ac, isReference=false, isBulkLoadResult=false, seqid=16436, majorCompaction=false
2014-07-22 12:40:01,901 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/1524594e98d5493c8defbe6e96f0db7f, isReference=false, isBulkLoadResult=false, seqid=11783, majorCompaction=false
2014-07-22 12:40:01,915 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/1be5ad2252fe47cab737a41aa2991fb1, isReference=false, isBulkLoadResult=false, seqid=1976, majorCompaction=true
2014-07-22 12:40:01,919 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/17b09c5fdf1240568650c5f3f990a526, isReference=false, isBulkLoadResult=false, seqid=1421, majorCompaction=false
2014-07-22 12:40:01,924 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/1cd28187175c4e27af6e6c9f371699ac, isReference=false, isBulkLoadResult=false, seqid=12902, majorCompaction=false
2014-07-22 12:40:01,927 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/23d207a550b240f284360fe99bbf41eb, isReference=false, isBulkLoadResult=false, seqid=24883, majorCompaction=false
2014-07-22 12:40:01,928 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/190f556f0a864d2a8e47eb90943e0ac3, isReference=false, isBulkLoadResult=false, seqid=23338, majorCompaction=false
2014-07-22 12:40:01,973 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/2772f715633a4987a7c4244d64ee938d, isReference=false, isBulkLoadResult=false, seqid=24539, majorCompaction=false
2014-07-22 12:40:01,975 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/266913a458c8453fbd6ccb3b2aa949ca, isReference=false, isBulkLoadResult=false, seqid=10825, majorCompaction=false
2014-07-22 12:40:02,018 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/212ecd563e5745e9837e102999aa45b3, isReference=false, isBulkLoadResult=false, seqid=6911, majorCompaction=false
2014-07-22 12:40:02,034 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/2a56b16bc7f947728a32bcdd3d5bdb51, isReference=false, isBulkLoadResult=false, seqid=23176, majorCompaction=false
2014-07-22 12:40:02,038 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/2d1fd8fc6fe047d69f876ec24ce4503f, isReference=false, isBulkLoadResult=false, seqid=792, majorCompaction=false
2014-07-22 12:40:02,040 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/5069674c157f46e9864b0a2a6338c624, isReference=false, isBulkLoadResult=false, seqid=18905, majorCompaction=false
2014-07-22 12:40:02,044 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/2f1f59e5a3c64b23ac89d63ad4d00041, isReference=false, isBulkLoadResult=false, seqid=24856, majorCompaction=false
2014-07-22 12:40:02,057 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/5cd5d942cf2047869ac8bf166577487d, isReference=false, isBulkLoadResult=false, seqid=25322, majorCompaction=false
2014-07-22 12:40:02,060 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/3548e771ef064b63931b41e97a5a5ac5, isReference=false, isBulkLoadResult=false, seqid=20757, majorCompaction=false
2014-07-22 12:40:02,063 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/2fbce2835bbf47daad6babda44a694f3, isReference=false, isBulkLoadResult=false, seqid=24203, majorCompaction=false
2014-07-22 12:40:02,083 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/609d8d37453f41e88cd35f41674d9889, isReference=false, isBulkLoadResult=false, seqid=14054, majorCompaction=false
2014-07-22 12:40:02,093 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/31a02f60c8da44298267694fe77b1d8b, isReference=false, isBulkLoadResult=false, seqid=19764, majorCompaction=false
2014-07-22 12:40:02,094 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/4c5056d5b7e545df8521b6481c77ddee, isReference=false, isBulkLoadResult=false, seqid=5418, majorCompaction=false
2014-07-22 12:40:02,113 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/662162a413344be593dd978b32eb63ee, isReference=false, isBulkLoadResult=false, seqid=14764, majorCompaction=false
2014-07-22 12:40:02,116 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/32c5be2b943d40a18b1895866cc37722, isReference=false, isBulkLoadResult=false, seqid=17556, majorCompaction=false
2014-07-22 12:40:02,122 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/4e0387fd3bba4d329b18c122103bda2e, isReference=false, isBulkLoadResult=false, seqid=19990, majorCompaction=false
2014-07-22 12:40:02,140 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/67c358a8934a4dde9ed8b2fe912e30f5, isReference=false, isBulkLoadResult=false, seqid=11422, majorCompaction=false
2014-07-22 12:40:02,145 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/36b600d85379432aa97eccd2ccde6798, isReference=false, isBulkLoadResult=false, seqid=11596, majorCompaction=false
2014-07-22 12:40:02,150 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/4ed0fd5f90e34cefa3d424d45392e587, isReference=false, isBulkLoadResult=false, seqid=17038, majorCompaction=false
2014-07-22 12:40:02,153 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/6a8318aebbc44483b3a0c987a3dde4fe, isReference=false, isBulkLoadResult=false, seqid=25265, majorCompaction=false
2014-07-22 12:40:02,186 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/3ff34a95b8e341a18b72d378de6e20c8, isReference=false, isBulkLoadResult=false, seqid=22404, majorCompaction=false
2014-07-22 12:40:02,189 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/509fc9dd0deb44e9a0905335f23d108e, isReference=false, isBulkLoadResult=false, seqid=15924, majorCompaction=false
2014-07-22 12:40:02,189 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/718769c76cbe48ce83efc0a3adb89cb2, isReference=false, isBulkLoadResult=false, seqid=24914, majorCompaction=false
2014-07-22 12:40:02,210 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/56d1c23022b747ef88ca5487d9d6b98d, isReference=false, isBulkLoadResult=false, seqid=22802, majorCompaction=false
2014-07-22 12:40:02,213 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/524007a773634abcbc5a1d72e32f0b34, isReference=false, isBulkLoadResult=false, seqid=5804, majorCompaction=false
2014-07-22 12:40:02,232 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/5988d0e9719b47e9a4bc2aab483a6d53, isReference=false, isBulkLoadResult=false, seqid=16818, majorCompaction=false
2014-07-22 12:40:02,232 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/8804c331a2034dc79307c4eb4d3bac7e, isReference=false, isBulkLoadResult=false, seqid=24141, majorCompaction=false
2014-07-22 12:40:02,240 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/552f4529eee645e5b2c18d7444af260f, isReference=false, isBulkLoadResult=false, seqid=22637, majorCompaction=false
2014-07-22 12:40:02,263 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/5f4b7d85e0074d82933d4a61a779a2eb, isReference=false, isBulkLoadResult=false, seqid=12318, majorCompaction=false
2014-07-22 12:40:02,270 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/887d109a53bd439e8aa14494b411c356, isReference=false, isBulkLoadResult=false, seqid=10643, majorCompaction=false
2014-07-22 12:40:02,270 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/5956f14a6073402f8c4d9365dd032267, isReference=false, isBulkLoadResult=false, seqid=17727, majorCompaction=false
2014-07-22 12:40:02,315 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/929c8b5c4f0245da93a4f6900adb77ad, isReference=false, isBulkLoadResult=false, seqid=9916, majorCompaction=false
2014-07-22 12:40:02,315 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/6776027fafb8443b98f490b884296031, isReference=false, isBulkLoadResult=false, seqid=2438, majorCompaction=false
2014-07-22 12:40:02,331 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/69b07355a8414ab7baf72e181be14699, isReference=false, isBulkLoadResult=false, seqid=21155, majorCompaction=false
2014-07-22 12:40:02,338 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/941baf8ba194428eaa588181fb62bcd2, isReference=false, isBulkLoadResult=false, seqid=12178, majorCompaction=false
2014-07-22 12:40:02,339 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/68d5f3b31b354433b1e43baecc66ee6c, isReference=false, isBulkLoadResult=false, seqid=3096, majorCompaction=false
2014-07-22 12:40:02,381 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/6b42bd1423f24b43a0366bc77fe45b9e, isReference=false, isBulkLoadResult=false, seqid=932, majorCompaction=false
2014-07-22 12:40:02,398 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/a1c15517a56541e69217af532afedd0c, isReference=false, isBulkLoadResult=false, seqid=13251, majorCompaction=false
2014-07-22 12:40:02,451 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/70b98aa7b0004512b2c8f27ee71ae6ad, isReference=false, isBulkLoadResult=false, seqid=16007, majorCompaction=false
2014-07-22 12:40:02,465 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/75ba8b88518b4d6296c99ea678d4dab9, isReference=false, isBulkLoadResult=false, seqid=19235, majorCompaction=false
2014-07-22 12:40:02,469 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/72c359d3945f4475bc87a369dd8eb389, isReference=false, isBulkLoadResult=false, seqid=17167, majorCompaction=false
2014-07-22 12:40:02,477 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/a4d3d5cc4f8e49db8a7e5a698ccafcab, isReference=false, isBulkLoadResult=false, seqid=15160, majorCompaction=false
2014-07-22 12:40:02,483 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/81e0de01e20040a29dead239762ed4b0, isReference=false, isBulkLoadResult=false, seqid=16659, majorCompaction=false
2014-07-22 12:40:02,490 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/7406045dd72d485eb3c57c402945f750, isReference=false, isBulkLoadResult=false, seqid=23860, majorCompaction=false
2014-07-22 12:40:02,507 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/83c8227982f34cf48b969e49958b4205, isReference=false, isBulkLoadResult=false, seqid=19426, majorCompaction=false
2014-07-22 12:40:02,510 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/b21801c350d24d13a80176c2dc685364, isReference=false, isBulkLoadResult=false, seqid=1665, majorCompaction=true
2014-07-22 12:40:02,513 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/8f3a7690ec814785a52ab8a380a67214, isReference=false, isBulkLoadResult=false, seqid=10586, majorCompaction=false
2014-07-22 12:40:02,554 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/906efdc1eb724c7aa914f39cb4215d3c, isReference=false, isBulkLoadResult=false, seqid=11231, majorCompaction=false
2014-07-22 12:40:02,570 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/9b6e92a26cac4102af9195d923ad95da, isReference=false, isBulkLoadResult=false, seqid=19601, majorCompaction=false
2014-07-22 12:40:02,614 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/a507337ad39b467594e355ebf90c5c7d, isReference=false, isBulkLoadResult=false, seqid=15137, majorCompaction=false
2014-07-22 12:40:02,620 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/921f4ba119874d448b77dcfdbf2412c7, isReference=false, isBulkLoadResult=false, seqid=18306, majorCompaction=false
2014-07-22 12:40:02,623 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/adf0ceac5061482a9ef52ce0799cc274, isReference=false, isBulkLoadResult=false, seqid=23389, majorCompaction=false
2014-07-22 12:40:02,623 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/b766e0b67f5f41f796676abb306c7b52, isReference=false, isBulkLoadResult=false, seqid=13664, majorCompaction=false
2014-07-22 12:40:02,642 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/94a75527060e462ab5cd75c86e9ec3b8, isReference=false, isBulkLoadResult=false, seqid=7123, majorCompaction=false
2014-07-22 12:40:02,644 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ae141a677f3e471b81c105f4dce4dd30, isReference=false, isBulkLoadResult=false, seqid=18445, majorCompaction=false
2014-07-22 12:40:02,650 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/b7c89604c3b34339a226f563495e99dc, isReference=false, isBulkLoadResult=false, seqid=6190, majorCompaction=false
2014-07-22 12:40:02,659 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/97bf241fdbc44767a70b7147b4ab4f2d, isReference=false, isBulkLoadResult=false, seqid=22028, majorCompaction=false
2014-07-22 12:40:02,664 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/c61fbef075b444fdbd89839d2b05b34c, isReference=false, isBulkLoadResult=false, seqid=11046, majorCompaction=false
2014-07-22 12:40:02,679 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b0d23469b2c84d4a9123950c3add0b82, isReference=false, isBulkLoadResult=false, seqid=1220, majorCompaction=false
2014-07-22 12:40:02,688 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/d179effb71424494bd7ac83d3222dfa1, isReference=false, isBulkLoadResult=false, seqid=2239, majorCompaction=false
2014-07-22 12:40:02,689 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/b12aafbd0d374ccfb5b2a2955b18ae86, isReference=false, isBulkLoadResult=false, seqid=21260, majorCompaction=false
2014-07-22 12:40:02,702 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b376c147577549f6b9f266625a4b258a, isReference=false, isBulkLoadResult=false, seqid=11009, majorCompaction=false
2014-07-22 12:40:02,702 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/b3e5011efc1a4891928e7a0c2e299f25, isReference=false, isBulkLoadResult=false, seqid=23516, majorCompaction=false
2014-07-22 12:40:02,705 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/e3c135faf35a4f6ab6b66798bcb2f01e, isReference=false, isBulkLoadResult=false, seqid=23064, majorCompaction=false
2014-07-22 12:40:02,724 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/b46971242298497f91f41252d52ffc2c, isReference=false, isBulkLoadResult=false, seqid=19056, majorCompaction=false
2014-07-22 12:40:02,732 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/b776baa54a9c42248f82bab7fb343854, isReference=false, isBulkLoadResult=false, seqid=6157, majorCompaction=false
2014-07-22 12:40:02,768 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/e423ffedf56b401688695e0218e7aa16, isReference=false, isBulkLoadResult=false, seqid=10297, majorCompaction=false
2014-07-22 12:40:02,769 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/c025405a23e64029a8c9b220644dc67e, isReference=false, isBulkLoadResult=false, seqid=6730, majorCompaction=false
2014-07-22 12:40:02,827 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/bcbb09d382f741da8532606ac3302191, isReference=false, isBulkLoadResult=false, seqid=17403, majorCompaction=false
2014-07-22 12:40:02,830 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/f039a906cfb64a31ac314072f2deec89, isReference=false, isBulkLoadResult=false, seqid=23759, majorCompaction=false
2014-07-22 12:40:02,868 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/c8c30e222bbe496985802ba74cd88c58, isReference=false, isBulkLoadResult=false, seqid=2843, majorCompaction=false
2014-07-22 12:40:02,910 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/cb9619f96f3d401d80d2452f08a53cdd, isReference=false, isBulkLoadResult=false, seqid=5043, majorCompaction=false
2014-07-22 12:40:02,919 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/cca7b20db4df4810be0d44726ddc8751, isReference=false, isBulkLoadResult=false, seqid=18156, majorCompaction=false
2014-07-22 12:40:02,919 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/cc30576ffdd54f2a95d1b63648932796, isReference=false, isBulkLoadResult=false, seqid=24606, majorCompaction=false
2014-07-22 12:40:02,935 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/ce45890540774b46892b7ad510c93894, isReference=false, isBulkLoadResult=false, seqid=20875, majorCompaction=false
2014-07-22 12:40:02,977 DEBUG [StoreOpener-3dae59807fb4d98cc09913214b3da6ff-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff/family/fc5276d3c47e4cd4a974d827d281fda1, isReference=false, isBulkLoadResult=false, seqid=5802, majorCompaction=false
2014-07-22 12:40:02,982 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3dae59807fb4d98cc09913214b3da6ff
2014-07-22 12:40:02,982 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/cce6c9ee22d84adba53a8f898dc442f3, isReference=false, isBulkLoadResult=false, seqid=11406, majorCompaction=false
2014-07-22 12:40:02,986 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 3dae59807fb4d98cc09913214b3da6ff; next sequenceid=25323
2014-07-22 12:40:02,987 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3dae59807fb4d98cc09913214b3da6ff
2014-07-22 12:40:02,989 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/d675ebe18f0142a2956cb4d19f1ddf8a, isReference=false, isBulkLoadResult=false, seqid=20488, majorCompaction=false
2014-07-22 12:40:02,989 INFO  [PostOpenDeployTasks:3dae59807fb4d98cc09913214b3da6ff] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:02,989 DEBUG [PostOpenDeployTasks:3dae59807fb4d98cc09913214b3da6ff] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-22 12:40:02,999 INFO  [PostOpenDeployTasks:3dae59807fb4d98cc09913214b3da6ff] catalog.MetaEditor: Updated row usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff. with server=slave1,60020,1406057959640
2014-07-22 12:40:02,999 INFO  [PostOpenDeployTasks:3dae59807fb4d98cc09913214b3da6ff] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:03,000 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3dae59807fb4d98cc09913214b3da6ff from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,004 DEBUG [StoreOpener-119de48677ef5d044e05e2fc2701307f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f/family/f1a1577ed89d45d7b1c1699b98b8d4bd, isReference=false, isBulkLoadResult=false, seqid=21637, majorCompaction=false
2014-07-22 12:40:03,005 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3dae59807fb4d98cc09913214b3da6ff from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,006 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 3dae59807fb4d98cc09913214b3da6ff to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:03,006 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff. on slave1,60020,1406057959640
2014-07-22 12:40:03,006 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2ce54845f27a3204f61d9894dc014adc from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,009 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/119de48677ef5d044e05e2fc2701307f
2014-07-22 12:40:03,011 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2ce54845f27a3204f61d9894dc014adc from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,012 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 2ce54845f27a3204f61d9894dc014adc, NAME => 'usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.', STARTKEY => 'user1', ENDKEY => 'user2'}
2014-07-22 12:40:03,012 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 119de48677ef5d044e05e2fc2701307f; next sequenceid=24884
2014-07-22 12:40:03,012 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 119de48677ef5d044e05e2fc2701307f
2014-07-22 12:40:03,012 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/d36788c3b0224727ba1e9c3f6b188b52, isReference=false, isBulkLoadResult=false, seqid=648, majorCompaction=true
2014-07-22 12:40:03,012 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 2ce54845f27a3204f61d9894dc014adc
2014-07-22 12:40:03,012 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:03,014 INFO  [PostOpenDeployTasks:119de48677ef5d044e05e2fc2701307f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:03,014 DEBUG [PostOpenDeployTasks:119de48677ef5d044e05e2fc2701307f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-22 12:40:03,020 INFO  [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:03,023 INFO  [PostOpenDeployTasks:119de48677ef5d044e05e2fc2701307f] catalog.MetaEditor: Updated row usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f. with server=slave1,60020,1406057959640
2014-07-22 12:40:03,024 INFO  [PostOpenDeployTasks:119de48677ef5d044e05e2fc2701307f] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:03,025 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 119de48677ef5d044e05e2fc2701307f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,028 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/d72b3bd1aead467a80ccf4fab954f0d8, isReference=false, isBulkLoadResult=false, seqid=23017, majorCompaction=false
2014-07-22 12:40:03,029 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 119de48677ef5d044e05e2fc2701307f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,029 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 119de48677ef5d044e05e2fc2701307f to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:03,029 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f. on slave1,60020,1406057959640
2014-07-22 12:40:03,030 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,034 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,035 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-07-22 12:40:03,035 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-07-22 12:40:03,035 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 12:40:03,039 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/e749bdf8efc04597a76ce2a6cb8d35ea, isReference=false, isBulkLoadResult=false, seqid=22265, majorCompaction=false
2014-07-22 12:40:03,043 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/07dffa4b986d4398b8cddf1b3af53db9, isReference=false, isBulkLoadResult=false, seqid=9248, majorCompaction=false
2014-07-22 12:40:03,043 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-22 12:40:03,085 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/ee2ab61afe974e58a11b096f108cddc5, isReference=false, isBulkLoadResult=false, seqid=18841, majorCompaction=false
2014-07-22 12:40:03,098 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/08766ed13e4d40849a78fd9373dfbe93, isReference=false, isBulkLoadResult=false, seqid=8093, majorCompaction=false
2014-07-22 12:40:03,136 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-07-22 12:40:03,139 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-07-22 12:40:03,140 DEBUG [StoreOpener-20f69fa9e49e2ee9c1b670c938523b8e-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e/family/feecba0164d7409a9269c32edce9c877, isReference=false, isBulkLoadResult=false, seqid=20345, majorCompaction=false
2014-07-22 12:40:03,145 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-07-22 12:40:03,145 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-07-22 12:40:03,148 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 12:40:03,156 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=slave1,60020,1406057959640
2014-07-22 12:40:03,156 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-07-22 12:40:03,156 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,162 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,162 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:03,162 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on slave1,60020,1406057959640
2014-07-22 12:40:03,162 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f907c3140d776d2aa54b0d5e29da2ae9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,164 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 12:40:03,168 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/0fff03d71d2546d6bf2c6357e7a2629c, isReference=false, isBulkLoadResult=false, seqid=4490, majorCompaction=false
2014-07-22 12:40:03,168 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f907c3140d776d2aa54b0d5e29da2ae9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,169 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => f907c3140d776d2aa54b0d5e29da2ae9, NAME => 'usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-22 12:40:03,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable f907c3140d776d2aa54b0d5e29da2ae9
2014-07-22 12:40:03,170 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:03,170 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 20f69fa9e49e2ee9c1b670c938523b8e; next sequenceid=23390
2014-07-22 12:40:03,170 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 20f69fa9e49e2ee9c1b670c938523b8e
2014-07-22 12:40:03,173 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:03,173 DEBUG [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-22 12:40:03,180 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] catalog.MetaEditor: Updated row usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. with server=slave1,60020,1406057959640
2014-07-22 12:40:03,181 INFO  [PostOpenDeployTasks:20f69fa9e49e2ee9c1b670c938523b8e] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:03,181 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 20f69fa9e49e2ee9c1b670c938523b8e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,186 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 20f69fa9e49e2ee9c1b670c938523b8e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:03,186 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 20f69fa9e49e2ee9c1b670c938523b8e to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:03,186 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. on slave1,60020,1406057959640
2014-07-22 12:40:03,186 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b4227bd82d287303700b1960a94f313f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,191 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b4227bd82d287303700b1960a94f313f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:03,192 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => b4227bd82d287303700b1960a94f313f, NAME => 'usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-22 12:40:03,192 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b4227bd82d287303700b1960a94f313f
2014-07-22 12:40:03,192 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:03,213 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/1135320aa0c5463bbbf501cc39d391fd, isReference=false, isBulkLoadResult=false, seqid=18573, majorCompaction=false
2014-07-22 12:40:03,215 INFO  [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:03,217 INFO  [StoreOpener-b4227bd82d287303700b1960a94f313f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:03,236 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/1362e5b3d1e143848c9e84d32538b45e, isReference=false, isBulkLoadResult=false, seqid=9578, majorCompaction=false
2014-07-22 12:40:03,240 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/0084e66538b944fca7dc2f3f9a500344, isReference=false, isBulkLoadResult=false, seqid=22919, majorCompaction=false
2014-07-22 12:40:03,262 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/04c3fbd8e0274a68925fca716e2a1997, isReference=false, isBulkLoadResult=false, seqid=16524, majorCompaction=false
2014-07-22 12:40:03,266 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/1a36f94603104f6b8794f9c6fdf28c51, isReference=false, isBulkLoadResult=false, seqid=22733, majorCompaction=false
2014-07-22 12:40:03,287 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/0304dbb3cbda4fc3a7a7b8e5af8c65cb, isReference=false, isBulkLoadResult=false, seqid=4708, majorCompaction=false
2014-07-22 12:40:03,290 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/1a85263b371143daaf2dda68ee196a33, isReference=false, isBulkLoadResult=false, seqid=16899, majorCompaction=false
2014-07-22 12:40:03,320 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/1b61ee7982fd49819154e0016c21fe55, isReference=false, isBulkLoadResult=false, seqid=3025, majorCompaction=false
2014-07-22 12:40:03,334 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/0ee8c4fa1f4b448bb8cd98b3986dff18, isReference=false, isBulkLoadResult=false, seqid=2254, majorCompaction=false
2014-07-22 12:40:03,335 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/26f338c84a5240abb2251866c6c35975, isReference=false, isBulkLoadResult=false, seqid=10181, majorCompaction=false
2014-07-22 12:40:03,343 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/1f9e898a10ee48c4a7569541aceb30e4, isReference=false, isBulkLoadResult=false, seqid=21966, majorCompaction=false
2014-07-22 12:40:03,354 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/358da20bfb4841ad854ae335db0bbfb7, isReference=false, isBulkLoadResult=false, seqid=14288, majorCompaction=false
2014-07-22 12:40:03,362 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/2d51b15674ea4e05a9ed4d010452a483, isReference=false, isBulkLoadResult=false, seqid=21148, majorCompaction=false
2014-07-22 12:40:03,364 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/10d125934f5944ae9319feca22449b32, isReference=false, isBulkLoadResult=false, seqid=13182, majorCompaction=false
2014-07-22 12:40:03,386 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/39e8645ead584eb3b5e325b15d030dfa, isReference=false, isBulkLoadResult=false, seqid=19725, majorCompaction=false
2014-07-22 12:40:03,389 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/3712c9044f9345d48b7eb24c7ae50afa, isReference=false, isBulkLoadResult=false, seqid=8323, majorCompaction=false
2014-07-22 12:40:03,390 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/1759aca876f74fef95547840eb1bd8d5, isReference=false, isBulkLoadResult=false, seqid=21545, majorCompaction=false
2014-07-22 12:40:03,434 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/3b95480717cf4cc58cb95408d68e9291, isReference=false, isBulkLoadResult=false, seqid=20026, majorCompaction=false
2014-07-22 12:40:03,455 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/1a3bae1d59f547dcabe385fe825f895b, isReference=false, isBulkLoadResult=false, seqid=5328, majorCompaction=false
2014-07-22 12:40:03,466 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/41a7a83db4dc4b77b4c053e8b42a6865, isReference=false, isBulkLoadResult=false, seqid=3099, majorCompaction=true
2014-07-22 12:40:03,475 INFO  [regionserver60020-smallCompactions-1406058001377] compress.CodecPool: Got brand-new decompressor
2014-07-22 12:40:03,478 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/42e04023105d448391296fcf9509414c, isReference=false, isBulkLoadResult=false, seqid=22339, majorCompaction=false
2014-07-22 12:40:03,478 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/24f9c0f704184a61902782f85134310b, isReference=false, isBulkLoadResult=false, seqid=22226, majorCompaction=false
2014-07-22 12:40:03,497 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/4c2cfd1541214cabb60714d52465f79d, isReference=false, isBulkLoadResult=false, seqid=20809, majorCompaction=false
2014-07-22 12:40:03,507 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/7bec1b7939df47d488513a55992cc0d2, isReference=false, isBulkLoadResult=false, seqid=21573, majorCompaction=false
2014-07-22 12:40:03,507 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/45880d6411984ca5a8ae61be390079bb, isReference=false, isBulkLoadResult=false, seqid=3630, majorCompaction=false
2014-07-22 12:40:03,513 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/2ab025db36da4c6b81fd0b9c02c5db2a, isReference=false, isBulkLoadResult=false, seqid=12525, majorCompaction=false
2014-07-22 12:40:03,534 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/4b72b1b6d7844048950e488046990eb3, isReference=false, isBulkLoadResult=false, seqid=4616, majorCompaction=false
2014-07-22 12:40:03,556 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/36debc28234d41beb69bd747330b7a5a, isReference=false, isBulkLoadResult=false, seqid=7945, majorCompaction=false
2014-07-22 12:40:03,562 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/7de71324f0284f43b6648fd00bddc1b5, isReference=false, isBulkLoadResult=false, seqid=3849, majorCompaction=false
2014-07-22 12:40:03,571 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/4e71292208854569941b08b277f6f8bb, isReference=false, isBulkLoadResult=false, seqid=15395, majorCompaction=false
2014-07-22 12:40:03,575 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/3f77677e5ee5417395883e5a76299551, isReference=false, isBulkLoadResult=false, seqid=23621, majorCompaction=false
2014-07-22 12:40:03,578 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/83a31aff609f4202850badf4fa0fe7d0, isReference=false, isBulkLoadResult=false, seqid=24242, majorCompaction=false
2014-07-22 12:40:03,593 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/626b3a6aed904dbd851463d8a7f9f7cb, isReference=false, isBulkLoadResult=false, seqid=22441, majorCompaction=false
2014-07-22 12:40:03,599 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/4400922d84504666a355479cbce2ed8f, isReference=false, isBulkLoadResult=false, seqid=3270, majorCompaction=false
2014-07-22 12:40:03,612 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/88deec41726340d7a7a4cc34ca70e7b2, isReference=false, isBulkLoadResult=false, seqid=4188, majorCompaction=false
2014-07-22 12:40:03,616 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/6b5e815672c4415c88cceb8dfea842e0, isReference=false, isBulkLoadResult=false, seqid=16150, majorCompaction=false
2014-07-22 12:40:03,623 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/4ef2272624084d79998ee70cea28dc0a, isReference=false, isBulkLoadResult=false, seqid=11870, majorCompaction=false
2014-07-22 12:40:03,643 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/7195db9088984f69adc43c11e9412710, isReference=false, isBulkLoadResult=false, seqid=9806, majorCompaction=false
2014-07-22 12:40:03,645 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/8d72a76abd9542df82e2b01f8caf91d2, isReference=false, isBulkLoadResult=false, seqid=20418, majorCompaction=false
2014-07-22 12:40:03,650 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/6c6f469269f6459f8762ce2ba3d69058, isReference=false, isBulkLoadResult=false, seqid=8653, majorCompaction=false
2014-07-22 12:40:03,655 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/91c534dc07ee4a799e8aa1456a4ef7f8, isReference=false, isBulkLoadResult=false, seqid=24922, majorCompaction=false
2014-07-22 12:40:03,657 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/808a55ab0d6a4529a117ff4eb9a33835, isReference=false, isBulkLoadResult=false, seqid=10561, majorCompaction=false
2014-07-22 12:40:03,670 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/920233c691d34879bdc9b53544b3c6a4, isReference=false, isBulkLoadResult=false, seqid=10385, majorCompaction=false
2014-07-22 12:40:03,679 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/734afb982e724d4cacb858cbef8f340e, isReference=false, isBulkLoadResult=false, seqid=6661, majorCompaction=false
2014-07-22 12:40:03,682 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/84d9f37180c145d699548503773a2441, isReference=false, isBulkLoadResult=false, seqid=14670, majorCompaction=false
2014-07-22 12:40:03,684 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/9209a9d1823d490da792be23ec001bb6, isReference=false, isBulkLoadResult=false, seqid=3500, majorCompaction=false
2014-07-22 12:40:03,708 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/8ab2ba3c572e44bdb0c610edd0b26914, isReference=false, isBulkLoadResult=false, seqid=23194, majorCompaction=false
2014-07-22 12:40:03,710 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/7582789cbf5749769a8a44096f63379a, isReference=false, isBulkLoadResult=false, seqid=20880, majorCompaction=false
2014-07-22 12:40:03,710 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/921e92e2a77e425895f56827ff4d5a35, isReference=false, isBulkLoadResult=false, seqid=24605, majorCompaction=false
2014-07-22 12:40:03,742 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/a1f059e50d00429bb2f8c339a536c803, isReference=false, isBulkLoadResult=false, seqid=3936, majorCompaction=false
2014-07-22 12:40:03,745 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/77e62fca1e044f83bac6181aab942425, isReference=false, isBulkLoadResult=false, seqid=9354, majorCompaction=false
2014-07-22 12:40:03,746 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/9fd72940ba2d407eb4f8f15be6f3e5ad, isReference=false, isBulkLoadResult=false, seqid=24890, majorCompaction=false
2014-07-22 12:40:03,768 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/a5bbd173ff2f4b928eacce8f6ebad09d, isReference=false, isBulkLoadResult=false, seqid=20917, majorCompaction=false
2014-07-22 12:40:03,770 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/88cbda18a3de4b6d898a6312425584d1, isReference=false, isBulkLoadResult=false, seqid=1020, majorCompaction=false
2014-07-22 12:40:03,785 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/9722ec4e63fe4b448c93e1b495fc254e, isReference=false, isBulkLoadResult=false, seqid=6011, majorCompaction=false
2014-07-22 12:40:03,786 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/a9a64b7f4ad14fe1bd3175ba3ebcafd3, isReference=false, isBulkLoadResult=false, seqid=2611, majorCompaction=true
2014-07-22 12:40:03,809 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/af60d3d6a7924b8fbb5ed4aa50c3bce4, isReference=false, isBulkLoadResult=false, seqid=19364, majorCompaction=false
2014-07-22 12:40:03,828 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/b2d3882b36b2455ab03d2b6661e03010, isReference=false, isBulkLoadResult=false, seqid=23454, majorCompaction=false
2014-07-22 12:40:03,835 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/ab35e3aa17504d46b74a29fa4d964a73, isReference=false, isBulkLoadResult=false, seqid=21658, majorCompaction=false
2014-07-22 12:40:03,839 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/97ae6df4eaf8400d95103ff3701b1318, isReference=false, isBulkLoadResult=false, seqid=20168, majorCompaction=false
2014-07-22 12:40:03,848 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/b63ed6fad41a4d2fbbd3f3b33c967b47, isReference=false, isBulkLoadResult=false, seqid=23117, majorCompaction=false
2014-07-22 12:40:03,891 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/bb5cf362e466414ba9578eb2184b378c, isReference=false, isBulkLoadResult=false, seqid=14138, majorCompaction=false
2014-07-22 12:40:03,901 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/b1de016d2874499c98c4e02eacd5b5cd, isReference=false, isBulkLoadResult=false, seqid=21301, majorCompaction=false
2014-07-22 12:40:03,919 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/d32fb18bdc4d424f98ee6ead791f6d07, isReference=false, isBulkLoadResult=false, seqid=14509, majorCompaction=false
2014-07-22 12:40:03,920 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/beb122e1b7c0441b84d23068cfc57e96, isReference=false, isBulkLoadResult=false, seqid=13851, majorCompaction=false
2014-07-22 12:40:03,939 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/daab881749b2472f83afefbe0b223ae0, isReference=false, isBulkLoadResult=false, seqid=14925, majorCompaction=false
2014-07-22 12:40:03,945 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/b81b91865a8f4f1e8d51d3bcec942117, isReference=false, isBulkLoadResult=false, seqid=15066, majorCompaction=false
2014-07-22 12:40:03,949 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/dab713de633c47908fc4d655bf1b0b38, isReference=false, isBulkLoadResult=false, seqid=18990, majorCompaction=false
2014-07-22 12:40:03,969 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/b9acf7019d1e406d92a7a7054b32a5af, isReference=false, isBulkLoadResult=false, seqid=22060, majorCompaction=false
2014-07-22 12:40:03,974 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/dea30c8c75f645189f99004afefb9173, isReference=false, isBulkLoadResult=false, seqid=8872, majorCompaction=false
2014-07-22 12:40:03,976 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/c3b38efb708648498e221613433fc3be, isReference=false, isBulkLoadResult=false, seqid=4007, majorCompaction=false
2014-07-22 12:40:03,992 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/e1b10e52289b4757b24aeff2f2bce9a5, isReference=false, isBulkLoadResult=false, seqid=10004, majorCompaction=false
2014-07-22 12:40:04,006 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/c5ef8e0b2be34fa291952ed2fd074cf8, isReference=false, isBulkLoadResult=false, seqid=9984, majorCompaction=false
2014-07-22 12:40:04,006 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/bd3273b76fb84ff9acc7abfc78b2989d, isReference=false, isBulkLoadResult=false, seqid=22831, majorCompaction=false
2014-07-22 12:40:04,016 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/bdffc5994bd64ba2ac652ac5287f2070, isReference=false, isBulkLoadResult=false, seqid=24880, majorCompaction=false
2014-07-22 12:40:04,036 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/fdec39bdcbf247588f6cc604cfaf99dd, isReference=false, isBulkLoadResult=false, seqid=23847, majorCompaction=false
2014-07-22 12:40:04,046 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/c4e943f6bab441cd867c299906366a6c, isReference=false, isBulkLoadResult=false, seqid=9046, majorCompaction=false
2014-07-22 12:40:04,072 DEBUG [StoreOpener-2ce54845f27a3204f61d9894dc014adc-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc/family/ffd31a9e5f8c4cefa98539255175d700, isReference=false, isBulkLoadResult=false, seqid=8511, majorCompaction=false
2014-07-22 12:40:04,076 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/2ce54845f27a3204f61d9894dc014adc
2014-07-22 12:40:04,079 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 2ce54845f27a3204f61d9894dc014adc; next sequenceid=24923
2014-07-22 12:40:04,079 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 2ce54845f27a3204f61d9894dc014adc
2014-07-22 12:40:04,080 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/cbd88930e0fc4c3e80cd3a648ee8f6b2, isReference=false, isBulkLoadResult=false, seqid=15780, majorCompaction=false
2014-07-22 12:40:04,082 INFO  [PostOpenDeployTasks:2ce54845f27a3204f61d9894dc014adc] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:04,083 DEBUG [PostOpenDeployTasks:2ce54845f27a3204f61d9894dc014adc] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-22 12:40:04,088 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/ce55bb03e76e4c92b658d20c910a2e5e, isReference=false, isBulkLoadResult=false, seqid=10586, majorCompaction=false
2014-07-22 12:40:04,091 INFO  [PostOpenDeployTasks:2ce54845f27a3204f61d9894dc014adc] catalog.MetaEditor: Updated row usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc. with server=slave1,60020,1406057959640
2014-07-22 12:40:04,091 INFO  [PostOpenDeployTasks:2ce54845f27a3204f61d9894dc014adc] regionserver.HRegionServer: Finished post open deploy task for usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:04,092 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2ce54845f27a3204f61d9894dc014adc from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,097 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2ce54845f27a3204f61d9894dc014adc from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,097 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 2ce54845f27a3204f61d9894dc014adc to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:04,097 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc. on slave1,60020,1406057959640
2014-07-22 12:40:04,098 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a277bd980f361ff89a5e8ab9873c49e2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:04,103 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a277bd980f361ff89a5e8ab9873c49e2 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:40:04,103 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => a277bd980f361ff89a5e8ab9873c49e2, NAME => 'usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.', STARTKEY => '', ENDKEY => 'user1'}
2014-07-22 12:40:04,104 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a277bd980f361ff89a5e8ab9873c49e2
2014-07-22 12:40:04,104 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:04,108 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/dc034235104f4134b8f1fb6123eb51cc, isReference=false, isBulkLoadResult=false, seqid=11180, majorCompaction=false
2014-07-22 12:40:04,109 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/d42905da1e814befbc7798b8faab3096, isReference=false, isBulkLoadResult=false, seqid=24708, majorCompaction=false
2014-07-22 12:40:04,111 INFO  [StoreOpener-a277bd980f361ff89a5e8ab9873c49e2-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:40:04,117 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a277bd980f361ff89a5e8ab9873c49e2
2014-07-22 12:40:04,120 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined a277bd980f361ff89a5e8ab9873c49e2; next sequenceid=1
2014-07-22 12:40:04,121 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a277bd980f361ff89a5e8ab9873c49e2
2014-07-22 12:40:04,124 INFO  [PostOpenDeployTasks:a277bd980f361ff89a5e8ab9873c49e2] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:04,131 INFO  [PostOpenDeployTasks:a277bd980f361ff89a5e8ab9873c49e2] catalog.MetaEditor: Updated row usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2. with server=slave1,60020,1406057959640
2014-07-22 12:40:04,131 INFO  [PostOpenDeployTasks:a277bd980f361ff89a5e8ab9873c49e2] regionserver.HRegionServer: Finished post open deploy task for usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:04,132 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a277bd980f361ff89a5e8ab9873c49e2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,138 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a277bd980f361ff89a5e8ab9873c49e2 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,138 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned a277bd980f361ff89a5e8ab9873c49e2 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:04,138 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2. on slave1,60020,1406057959640
2014-07-22 12:40:04,146 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/d675c9acb5154ddf8812534d25d1b490, isReference=false, isBulkLoadResult=false, seqid=8687, majorCompaction=false
2014-07-22 12:40:04,161 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fa6c3e1fc388457fbc3c76491b98a538, isReference=false, isBulkLoadResult=false, seqid=7269, majorCompaction=false
2014-07-22 12:40:04,167 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/d826225c189b4cccafdf5e91d2449ec9, isReference=false, isBulkLoadResult=false, seqid=24311, majorCompaction=false
2014-07-22 12:40:04,180 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/d9e31937f1114396bbcb48c509b65e43, isReference=false, isBulkLoadResult=false, seqid=20536, majorCompaction=false
2014-07-22 12:40:04,193 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fb69618522a545d98be00c7b345feb32, isReference=false, isBulkLoadResult=false, seqid=1809, majorCompaction=false
2014-07-22 12:40:04,207 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/dc15e4a4ebca4a76938cbd223c4f5e7a, isReference=false, isBulkLoadResult=false, seqid=23949, majorCompaction=false
2014-07-22 12:40:04,237 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fc75b7b09e2c4750842dffb9c71c3211, isReference=false, isBulkLoadResult=false, seqid=2669, majorCompaction=false
2014-07-22 12:40:04,242 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/def39cc78f5b4eebbfa919073ae7fdcc, isReference=false, isBulkLoadResult=false, seqid=4262, majorCompaction=false
2014-07-22 12:40:04,247 DEBUG [StoreOpener-b4227bd82d287303700b1960a94f313f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f/family/fffd221287ef4cb1bd2675e63906da58, isReference=false, isBulkLoadResult=false, seqid=23852, majorCompaction=false
2014-07-22 12:40:04,253 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b4227bd82d287303700b1960a94f313f
2014-07-22 12:40:04,255 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined b4227bd82d287303700b1960a94f313f; next sequenceid=23853
2014-07-22 12:40:04,255 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b4227bd82d287303700b1960a94f313f
2014-07-22 12:40:04,256 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/e40b545d7bde4899982367f2e434c5fb, isReference=false, isBulkLoadResult=false, seqid=23582, majorCompaction=false
2014-07-22 12:40:04,258 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:04,258 DEBUG [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-22 12:40:04,265 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/e6629f5d14a6468ca5b20c326430f5df, isReference=false, isBulkLoadResult=false, seqid=24877, majorCompaction=false
2014-07-22 12:40:04,267 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] catalog.MetaEditor: Updated row usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. with server=slave1,60020,1406057959640
2014-07-22 12:40:04,267 INFO  [PostOpenDeployTasks:b4227bd82d287303700b1960a94f313f] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:04,268 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b4227bd82d287303700b1960a94f313f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,272 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b4227bd82d287303700b1960a94f313f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,272 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned b4227bd82d287303700b1960a94f313f to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:04,272 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. on slave1,60020,1406057959640
2014-07-22 12:40:04,294 DEBUG [StoreOpener-f907c3140d776d2aa54b0d5e29da2ae9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9/family/f9bc61959d684ed9bb17312cfeb95fe6, isReference=false, isBulkLoadResult=false, seqid=9411, majorCompaction=false
2014-07-22 12:40:04,299 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/f907c3140d776d2aa54b0d5e29da2ae9
2014-07-22 12:40:04,302 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined f907c3140d776d2aa54b0d5e29da2ae9; next sequenceid=24881
2014-07-22 12:40:04,302 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node f907c3140d776d2aa54b0d5e29da2ae9
2014-07-22 12:40:04,306 INFO  [PostOpenDeployTasks:f907c3140d776d2aa54b0d5e29da2ae9] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:04,306 DEBUG [PostOpenDeployTasks:f907c3140d776d2aa54b0d5e29da2ae9] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-22 12:40:04,313 INFO  [PostOpenDeployTasks:f907c3140d776d2aa54b0d5e29da2ae9] catalog.MetaEditor: Updated row usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9. with server=slave1,60020,1406057959640
2014-07-22 12:40:04,313 INFO  [PostOpenDeployTasks:f907c3140d776d2aa54b0d5e29da2ae9] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:04,314 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f907c3140d776d2aa54b0d5e29da2ae9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,325 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f907c3140d776d2aa54b0d5e29da2ae9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:40:04,325 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned f907c3140d776d2aa54b0d5e29da2ae9 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:40:04,325 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9. on slave1,60020,1406057959640
2014-07-22 12:40:04,800 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-22 12:40:04,800 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-22 12:40:04,800 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-22 12:40:04,800 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:12), split_queue=0, merge_queue=0
2014-07-22 12:40:04,800 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:13), split_queue=0, merge_queue=0
2014-07-22 12:40:04,801 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:14), split_queue=0, merge_queue=0
2014-07-22 12:40:04,801 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-22 12:40:04,801 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-22 12:40:04,801 DEBUG [regionserver60020.compactionChecker] regionserver.CompactSplitThread: Small Compaction requested: system; Because: regionserver60020.compactionChecker requests compaction; compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-22 12:40:26,786 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close a277bd980f361ff89a5e8ab9873c49e2, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,787 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 4d714c493433d51e92af518b12e607b2, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,787 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close 20f69fa9e49e2ee9c1b670c938523b8e, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,788 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close b4227bd82d287303700b1960a94f313f, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,788 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close f907c3140d776d2aa54b0d5e29da2ae9, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,790 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Close 4264c18b25708212810563a172f7f7f7, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,790 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Close 92ba9ea7e04274b0bbdc5cbfefd7393e, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,791 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 3dae59807fb4d98cc09913214b3da6ff, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,794 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:26,795 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:26,795 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:26,799 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.: disabling compactions & flushes
2014-07-22 12:40:26,800 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:26,800 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Close 2ce54845f27a3204f61d9894dc014adc, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,801 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Close 119de48677ef5d044e05e2fc2701307f, via zk=yes, znode version=0, on null
2014-07-22 12:40:26,801 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.: disabling compactions & flushes
2014-07-22 12:40:26,801 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.: disabling compactions & flushes
2014-07-22 12:40:26,802 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:26,802 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:26,804 INFO  [StoreCloserThread-usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,810 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:26,811 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a277bd980f361ff89a5e8ab9873c49e2 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,817 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a277bd980f361ff89a5e8ab9873c49e2 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,817 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2. on slave1,60020,1406057959640
2014-07-22 12:40:26,817 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,,1406052786781.a277bd980f361ff89a5e8ab9873c49e2.
2014-07-22 12:40:26,817 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:26,820 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.: disabling compactions & flushes
2014-07-22 12:40:26,820 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: waiting for 1 compactions to complete for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:26,860 INFO  [StoreCloserThread-usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,861 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:26,861 INFO  [StoreCloserThread-usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,862 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,862 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:26,862 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning f907c3140d776d2aa54b0d5e29da2ae9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,868 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4d714c493433d51e92af518b12e607b2 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,868 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. on slave1,60020,1406057959640
2014-07-22 12:40:26,868 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2.
2014-07-22 12:40:26,869 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:26,869 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node f907c3140d776d2aa54b0d5e29da2ae9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,869 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9. on slave1,60020,1406057959640
2014-07-22 12:40:26,869 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9.
2014-07-22 12:40:26,869 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:26,872 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.: disabling compactions & flushes
2014-07-22 12:40:26,872 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:26,872 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.: disabling compactions & flushes
2014-07-22 12:40:26,872 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:26,900 INFO  [StoreCloserThread-usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,901 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:26,901 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,901 INFO  [StoreCloserThread-usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,901 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:26,902 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,907 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 92ba9ea7e04274b0bbdc5cbfefd7393e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,907 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. on slave1,60020,1406057959640
2014-07-22 12:40:26,907 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e.
2014-07-22 12:40:26,907 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:26,914 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 20f69fa9e49e2ee9c1b670c938523b8e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,914 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. on slave1,60020,1406057959640
2014-07-22 12:40:26,914 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e.
2014-07-22 12:40:26,914 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:26,915 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.: disabling compactions & flushes
2014-07-22 12:40:26,915 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:26,917 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.: disabling compactions & flushes
2014-07-22 12:40:26,917 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:26,939 INFO  [StoreCloserThread-usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,939 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:26,939 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b4227bd82d287303700b1960a94f313f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,940 INFO  [StoreCloserThread-usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,940 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:26,940 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3dae59807fb4d98cc09913214b3da6ff from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,945 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b4227bd82d287303700b1960a94f313f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,945 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. on slave1,60020,1406057959640
2014-07-22 12:40:26,945 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f.
2014-07-22 12:40:26,945 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:26,945 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3dae59807fb4d98cc09913214b3da6ff from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,946 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff. on slave1,60020,1406057959640
2014-07-22 12:40:26,946 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff.
2014-07-22 12:40:26,946 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:26,948 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.: disabling compactions & flushes
2014-07-22 12:40:26,949 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.: disabling compactions & flushes
2014-07-22 12:40:26,949 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:26,949 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:26,972 INFO  [StoreCloserThread-usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,972 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:26,972 INFO  [StoreCloserThread-usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.-1] regionserver.HStore: Closed family
2014-07-22 12:40:26,972 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2ce54845f27a3204f61d9894dc014adc from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,972 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:26,972 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 119de48677ef5d044e05e2fc2701307f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,980 DEBUG [RS_CLOSE_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2ce54845f27a3204f61d9894dc014adc from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,980 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc. on slave1,60020,1406057959640
2014-07-22 12:40:26,980 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc.
2014-07-22 12:40:26,980 DEBUG [RS_CLOSE_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 119de48677ef5d044e05e2fc2701307f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:26,980 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f. on slave1,60020,1406057959640
2014-07-22 12:40:26,981 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f.
2014-07-22 12:40:27,319 INFO  [regionserver60020-smallCompactions-1406058001377] regionserver.HRegion: compaction interrupted
java.io.InterruptedIOException: Aborting compaction of store family in region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. because it was interrupted.
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:81)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-22 12:40:27,319 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:27,323 INFO  [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Aborted compaction: Request = regionName=usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7., storeName=family, fileCount=10, fileSize=745.5m (69.8m, 61.7m, 77.5m, 88.2m, 97.2m, 60.7m, 115.6m, 101.3m, 70.3m, 3.1m), priority=1969, time=154981783562465; duration=25sec
2014-07-22 12:40:27,323 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-22 12:40:27,324 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. because compaction request was cancelled
2014-07-22 12:40:27,324 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406052786782.b4227bd82d287303700b1960a94f313f. because compaction request was cancelled
2014-07-22 12:40:27,324 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. because compaction request was cancelled
2014-07-22 12:40:27,324 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f. because compaction request was cancelled
2014-07-22 12:40:27,324 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406052786781.20f69fa9e49e2ee9c1b670c938523b8e. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user1,1406052786781.2ce54845f27a3204f61d9894dc014adc. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406052786781.f907c3140d776d2aa54b0d5e29da2ae9. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406052786781.119de48677ef5d044e05e2fc2701307f. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user5,1406052786781.4d714c493433d51e92af518b12e607b2. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user8,1406052786782.92ba9ea7e04274b0bbdc5cbfefd7393e. because compaction request was cancelled
2014-07-22 12:40:27,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff. because compaction request was cancelled
2014-07-22 12:40:27,326 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406052786782.3dae59807fb4d98cc09913214b3da6ff. because compaction request was cancelled
2014-07-22 12:40:27,336 INFO  [StoreCloserThread-usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.-1] regionserver.HStore: Closed family
2014-07-22 12:40:27,336 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:40:27,336 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:27,341 DEBUG [RS_CLOSE_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 4264c18b25708212810563a172f7f7f7 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-22 12:40:27,341 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7. on slave1,60020,1406057959640
2014-07-22 12:40:27,341 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user7,1406052786782.4264c18b25708212810563a172f7f7f7.
2014-07-22 12:44:19,727 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 12:45:38,304 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:45:38,320 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:45:38,321 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:45:38,321 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ba046f8ec13a73ed30320b2e3edb3349 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,322 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 25e1297b777ae6ae4c4565d60879d0b0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,322 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:45:38,324 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cdf12d16f55023d641440ce7cf8436fb from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,325 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:45:38,332 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ba046f8ec13a73ed30320b2e3edb3349 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,332 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => ba046f8ec13a73ed30320b2e3edb3349, NAME => 'usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-22 12:45:38,333 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 25e1297b777ae6ae4c4565d60879d0b0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,333 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ba046f8ec13a73ed30320b2e3edb3349
2014-07-22 12:45:38,334 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cdf12d16f55023d641440ce7cf8436fb from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,334 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:45:38,334 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 25e1297b777ae6ae4c4565d60879d0b0, NAME => 'usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-22 12:45:38,334 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => cdf12d16f55023d641440ce7cf8436fb, NAME => 'usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-22 12:45:38,336 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:45:38,336 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable cdf12d16f55023d641440ce7cf8436fb
2014-07-22 12:45:38,337 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:45:38,337 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:45:38,348 INFO  [StoreOpener-ba046f8ec13a73ed30320b2e3edb3349-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:45:38,349 INFO  [StoreOpener-25e1297b777ae6ae4c4565d60879d0b0-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:45:38,351 INFO  [StoreOpener-cdf12d16f55023d641440ce7cf8436fb-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:45:38,353 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349
2014-07-22 12:45:38,354 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:45:38,355 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb
2014-07-22 12:45:38,356 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined ba046f8ec13a73ed30320b2e3edb3349; next sequenceid=1
2014-07-22 12:45:38,357 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ba046f8ec13a73ed30320b2e3edb3349
2014-07-22 12:45:38,357 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 25e1297b777ae6ae4c4565d60879d0b0; next sequenceid=1
2014-07-22 12:45:38,358 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:45:38,359 INFO  [PostOpenDeployTasks:ba046f8ec13a73ed30320b2e3edb3349] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:45:38,360 INFO  [PostOpenDeployTasks:25e1297b777ae6ae4c4565d60879d0b0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:45:38,372 INFO  [PostOpenDeployTasks:ba046f8ec13a73ed30320b2e3edb3349] catalog.MetaEditor: Updated row usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. with server=slave1,60020,1406057959640
2014-07-22 12:45:38,372 INFO  [PostOpenDeployTasks:ba046f8ec13a73ed30320b2e3edb3349] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:45:38,373 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ba046f8ec13a73ed30320b2e3edb3349 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,374 INFO  [PostOpenDeployTasks:25e1297b777ae6ae4c4565d60879d0b0] catalog.MetaEditor: Updated row usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. with server=slave1,60020,1406057959640
2014-07-22 12:45:38,374 INFO  [PostOpenDeployTasks:25e1297b777ae6ae4c4565d60879d0b0] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:45:38,374 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 25e1297b777ae6ae4c4565d60879d0b0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,379 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ba046f8ec13a73ed30320b2e3edb3349 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,379 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned ba046f8ec13a73ed30320b2e3edb3349 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:45:38,379 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. on slave1,60020,1406057959640
2014-07-22 12:45:38,379 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 25e1297b777ae6ae4c4565d60879d0b0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,380 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 25e1297b777ae6ae4c4565d60879d0b0 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:45:38,380 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. on slave1,60020,1406057959640
2014-07-22 12:45:38,380 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 847263c284c07b6767aa3cc0c6497be7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,381 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1723c12a1a2e0533afeaf8fb8dd82e27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,390 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 847263c284c07b6767aa3cc0c6497be7 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1723c12a1a2e0533afeaf8fb8dd82e27 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 847263c284c07b6767aa3cc0c6497be7, NAME => 'usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 1723c12a1a2e0533afeaf8fb8dd82e27, NAME => 'usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 847263c284c07b6767aa3cc0c6497be7
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:45:38,391 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 1723c12a1a2e0533afeaf8fb8dd82e27
2014-07-22 12:45:38,392 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:45:38,392 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined cdf12d16f55023d641440ce7cf8436fb; next sequenceid=1
2014-07-22 12:45:38,392 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node cdf12d16f55023d641440ce7cf8436fb
2014-07-22 12:45:38,394 INFO  [PostOpenDeployTasks:cdf12d16f55023d641440ce7cf8436fb] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:45:38,399 INFO  [StoreOpener-847263c284c07b6767aa3cc0c6497be7-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:45:38,403 INFO  [PostOpenDeployTasks:cdf12d16f55023d641440ce7cf8436fb] catalog.MetaEditor: Updated row usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. with server=slave1,60020,1406057959640
2014-07-22 12:45:38,403 INFO  [PostOpenDeployTasks:cdf12d16f55023d641440ce7cf8436fb] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:45:38,403 INFO  [StoreOpener-1723c12a1a2e0533afeaf8fb8dd82e27-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-22 12:45:38,404 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning cdf12d16f55023d641440ce7cf8436fb from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,407 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7
2014-07-22 12:45:38,407 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27
2014-07-22 12:45:38,409 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 847263c284c07b6767aa3cc0c6497be7; next sequenceid=1
2014-07-22 12:45:38,409 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 847263c284c07b6767aa3cc0c6497be7
2014-07-22 12:45:38,409 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node cdf12d16f55023d641440ce7cf8436fb from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,409 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned cdf12d16f55023d641440ce7cf8436fb to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:45:38,409 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. on slave1,60020,1406057959640
2014-07-22 12:45:38,411 INFO  [PostOpenDeployTasks:847263c284c07b6767aa3cc0c6497be7] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:45:38,418 INFO  [PostOpenDeployTasks:847263c284c07b6767aa3cc0c6497be7] catalog.MetaEditor: Updated row usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. with server=slave1,60020,1406057959640
2014-07-22 12:45:38,418 INFO  [PostOpenDeployTasks:847263c284c07b6767aa3cc0c6497be7] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:45:38,419 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 847263c284c07b6767aa3cc0c6497be7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,423 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 847263c284c07b6767aa3cc0c6497be7 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,423 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 847263c284c07b6767aa3cc0c6497be7 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:45:38,423 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. on slave1,60020,1406057959640
2014-07-22 12:45:38,446 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 1723c12a1a2e0533afeaf8fb8dd82e27; next sequenceid=1
2014-07-22 12:45:38,446 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1723c12a1a2e0533afeaf8fb8dd82e27
2014-07-22 12:45:38,449 INFO  [PostOpenDeployTasks:1723c12a1a2e0533afeaf8fb8dd82e27] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:45:38,456 INFO  [PostOpenDeployTasks:1723c12a1a2e0533afeaf8fb8dd82e27] catalog.MetaEditor: Updated row usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. with server=slave1,60020,1406057959640
2014-07-22 12:45:38,456 INFO  [PostOpenDeployTasks:1723c12a1a2e0533afeaf8fb8dd82e27] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:45:38,457 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1723c12a1a2e0533afeaf8fb8dd82e27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,461 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x475f95e04c0000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1723c12a1a2e0533afeaf8fb8dd82e27 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-22 12:45:38,461 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 1723c12a1a2e0533afeaf8fb8dd82e27 to OPENED in zk on slave1,60020,1406057959640
2014-07-22 12:45:38,461 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. on slave1,60020,1406057959640
2014-07-22 12:45:58,294 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:45:58,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100 synced till here 76
2014-07-22 12:45:58,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406057994683 with entries=100, filesize=84.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058358294
2014-07-22 12:46:00,025 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:00,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 208 synced till here 206
2014-07-22 12:46:00,197 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058358294 with entries=108, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058360025
2014-07-22 12:46:01,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:01,742 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 300 synced till here 296
2014-07-22 12:46:01,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058360025 with entries=92, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058361726
2014-07-22 12:46:03,649 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:03,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 384 synced till here 382
2014-07-22 12:46:03,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058361726 with entries=84, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058363650
2014-07-22 12:46:05,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:05,401 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 473 synced till here 465
2014-07-22 12:46:05,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058363650 with entries=89, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058365374
2014-07-22 12:46:07,106 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:07,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 566 synced till here 558
2014-07-22 12:46:07,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058365374 with entries=93, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058367106
2014-07-22 12:46:08,912 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:08,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 655 synced till here 648
2014-07-22 12:46:09,142 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058367106 with entries=89, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058368913
2014-07-22 12:46:11,043 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:11,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 763 synced till here 741
2014-07-22 12:46:11,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058368913 with entries=108, filesize=81.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058371043
2014-07-22 12:46:11,982 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:46:11,984 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 259.9m
2014-07-22 12:46:12,790 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:46:12,790 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 262.3m
2014-07-22 12:46:13,099 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:13,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:13,491 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:46:13,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 879 synced till here 856
2014-07-22 12:46:14,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058371043 with entries=116, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058373487
2014-07-22 12:46:14,291 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:14,296 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new compressor
2014-07-22 12:46:14,373 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:46:15,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:15,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 987 synced till here 971
2014-07-22 12:46:15,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058373487 with entries=108, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058375612
2014-07-22 12:46:18,340 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:18,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1097 synced till here 1080
2014-07-22 12:46:18,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058375612 with entries=110, filesize=83.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058378340
2014-07-22 12:46:20,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:21,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1218 synced till here 1200
2014-07-22 12:46:21,274 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058378340 with entries=121, filesize=91.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058380902
2014-07-22 12:46:23,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:23,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1328 synced till here 1299
2014-07-22 12:46:23,762 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=179, memsize=77.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/6c19efb72ba14d8ab12956a94f746faa
2014-07-22 12:46:23,779 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/6c19efb72ba14d8ab12956a94f746faa as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/6c19efb72ba14d8ab12956a94f746faa
2014-07-22 12:46:23,791 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/6c19efb72ba14d8ab12956a94f746faa, entries=283390, sequenceid=179, filesize=20.2m
2014-07-22 12:46:23,792 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~275.7m/289050400, currentsize=153.1m/160569440 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 11808ms, sequenceid=179, compaction requested=false
2014-07-22 12:46:23,799 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 404.3m
2014-07-22 12:46:23,936 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058380902 with entries=110, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058383432
2014-07-22 12:46:25,121 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=182, memsize=77.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/43329e745acb4553b35b651a648dc346
2014-07-22 12:46:25,138 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/43329e745acb4553b35b651a648dc346 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/43329e745acb4553b35b651a648dc346
2014-07-22 12:46:25,152 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/43329e745acb4553b35b651a648dc346, entries=282160, sequenceid=182, filesize=20.1m
2014-07-22 12:46:25,152 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.6m/293206560, currentsize=164.1m/172040320 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 12362ms, sequenceid=182, compaction requested=false
2014-07-22 12:46:25,153 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 416.2m
2014-07-22 12:46:25,158 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:25,537 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:25,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1431 synced till here 1416
2014-07-22 12:46:25,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058383432 with entries=103, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058385537
2014-07-22 12:46:26,390 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:28,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:28,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1529 synced till here 1527
2014-07-22 12:46:28,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058385537 with entries=98, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058388117
2014-07-22 12:46:30,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:30,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1617 synced till here 1614
2014-07-22 12:46:30,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058388117 with entries=88, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058390126
2014-07-22 12:46:31,908 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:46:32,512 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:32,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1710 synced till here 1700
2014-07-22 12:46:32,704 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:46:32,714 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058390126 with entries=93, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058392513
2014-07-22 12:46:34,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:34,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1803 synced till here 1794
2014-07-22 12:46:34,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058392513 with entries=93, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058394357
2014-07-22 12:46:36,386 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:36,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1900 synced till here 1888
2014-07-22 12:46:36,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058394357 with entries=97, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058396387
2014-07-22 12:46:37,255 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=271, memsize=77.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/8c614482fd9f46e1915cad48c858fa53
2014-07-22 12:46:37,274 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/8c614482fd9f46e1915cad48c858fa53 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/8c614482fd9f46e1915cad48c858fa53
2014-07-22 12:46:37,525 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/8c614482fd9f46e1915cad48c858fa53, entries=283150, sequenceid=271, filesize=20.2m
2014-07-22 12:46:37,526 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~418.2m/438554000, currentsize=170.9m/179167200 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 13727ms, sequenceid=271, compaction requested=false
2014-07-22 12:46:37,527 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 324.8m
2014-07-22 12:46:38,628 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=273, memsize=77.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a884adc3ce184b4299a929d6cf6fe81d
2014-07-22 12:46:38,638 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:38,647 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a884adc3ce184b4299a929d6cf6fe81d as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a884adc3ce184b4299a929d6cf6fe81d
2014-07-22 12:46:38,659 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a884adc3ce184b4299a929d6cf6fe81d, entries=282540, sequenceid=273, filesize=20.2m
2014-07-22 12:46:38,659 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~419.2m/439567680, currentsize=176.1m/184648640 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 13507ms, sequenceid=273, compaction requested=false
2014-07-22 12:46:38,659 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 339.3m
2014-07-22 12:46:38,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:38,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1997 synced till here 1984
2014-07-22 12:46:39,099 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058396387 with entries=97, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058398823
2014-07-22 12:46:39,464 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:46:40,869 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:40,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2092 synced till here 2089
2014-07-22 12:46:40,912 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058398823 with entries=95, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058400870
2014-07-22 12:46:42,651 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:46:42,874 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:42,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2182 synced till here 2175
2014-07-22 12:46:43,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058400870 with entries=90, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058402875
2014-07-22 12:46:43,734 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:46:44,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:44,887 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2269 synced till here 2268
2014-07-22 12:46:44,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058402875 with entries=87, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058404869
2014-07-22 12:46:47,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:47,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2359 synced till here 2350
2014-07-22 12:46:47,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058404869 with entries=90, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058407336
2014-07-22 12:46:49,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:46:49,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2459 synced till here 2438
2014-07-22 12:46:50,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058407336 with entries=100, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058409877
2014-07-22 12:47:08,819 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 23879ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:47:08,819 WARN  [regionserver60020] util.Sleeper: We slept 18709ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:47:08,819 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 23880ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:47:08,867 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 17968ms
GC pool 'ParNew' had collection(s): count=1 time=1ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=18415ms
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407544,"queuetimems":0,"class":"HRegionServer","responsesize":17578,"method":"Multi"}
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407076,"queuetimems":0,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21685,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407295,"queuetimems":1,"class":"HRegionServer","responsesize":17534,"method":"Multi"}
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21667,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407319,"queuetimems":2,"class":"HRegionServer","responsesize":17214,"method":"Multi"}
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21402,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407584,"queuetimems":0,"class":"HRegionServer","responsesize":17038,"method":"Multi"}
2014-07-22 12:47:09,063 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 831 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,062 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407356,"queuetimems":0,"class":"HRegionServer","responsesize":16874,"method":"Multi"}
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 816 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 817 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 821 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,064 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,065 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 815 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,065 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,065 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 832 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,065 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,184 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407768,"queuetimems":0,"class":"HRegionServer","responsesize":17137,"method":"Multi"}
2014-07-22 12:47:09,184 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21382,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407801,"queuetimems":0,"class":"HRegionServer","responsesize":16810,"method":"Multi"}
2014-07-22 12:47:09,184 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407920,"queuetimems":1,"class":"HRegionServer","responsesize":17081,"method":"Multi"}
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 835 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 833 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 834 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,185 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,251 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058407954,"queuetimems":1,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-07-22 12:47:09,252 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 839 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,252 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,251 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21156,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408095,"queuetimems":0,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 12:47:09,252 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20930,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408321,"queuetimems":0,"class":"HRegionServer","responsesize":17208,"method":"Multi"}
2014-07-22 12:47:09,252 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 838 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,253 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,255 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 836 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,255 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,252 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21118,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408134,"queuetimems":2,"class":"HRegionServer","responsesize":17192,"method":"Multi"}
2014-07-22 12:47:09,256 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 837 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,256 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,321 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408495,"queuetimems":3,"class":"HRegionServer","responsesize":17534,"method":"Multi"}
2014-07-22 12:47:09,321 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20608,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408713,"queuetimems":1,"class":"HRegionServer","responsesize":17214,"method":"Multi"}
2014-07-22 12:47:09,322 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 845 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,322 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,322 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 843 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,322 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:09,461 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408930,"queuetimems":159,"class":"HRegionServer","responsesize":16709,"method":"Multi"}
2014-07-22 12:47:09,461 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408736,"queuetimems":0,"class":"HRegionServer","responsesize":16895,"method":"Multi"}
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408517,"queuetimems":0,"class":"HRegionServer","responsesize":16874,"method":"Multi"}
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 851 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,461 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058408953,"queuetimems":1,"class":"HRegionServer","responsesize":17014,"method":"Multi"}
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409135,"queuetimems":1,"class":"HRegionServer","responsesize":16792,"method":"Multi"}
2014-07-22 12:47:09,461 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20357,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409103,"queuetimems":0,"class":"HRegionServer","responsesize":16503,"method":"Multi"}
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 842 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,462 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 848 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 850 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 844 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 849 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,463 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2550 synced till here 2544
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20222,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409288,"queuetimems":1,"class":"HRegionServer","responsesize":17310,"method":"Multi"}
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409498,"queuetimems":1,"class":"HRegionServer","responsesize":17974,"method":"Multi"}
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409725,"queuetimems":0,"class":"HRegionServer","responsesize":16850,"method":"Multi"}
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 847 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 856 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 859 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,511 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058409877 with entries=91, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058429458
2014-07-22 12:47:09,561 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19705,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409856,"queuetimems":0,"class":"HRegionServer","responsesize":17495,"method":"Multi"}
2014-07-22 12:47:09,561 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409694,"queuetimems":0,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-07-22 12:47:09,562 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 855 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,562 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,562 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 857 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,562 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20242,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409320,"queuetimems":2,"class":"HRegionServer","responsesize":17022,"method":"Multi"}
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409522,"queuetimems":1,"class":"HRegionServer","responsesize":17008,"method":"Multi"}
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 846 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 858 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,563 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058410098,"queuetimems":0,"class":"HRegionServer","responsesize":17578,"method":"Multi"}
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058410076,"queuetimems":0,"class":"HRegionServer","responsesize":17038,"method":"Multi"}
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058409888,"queuetimems":1,"class":"HRegionServer","responsesize":17470,"method":"Multi"}
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 862 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 863 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,566 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,567 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 854 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,567 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,583 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 870 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,583 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19244,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54734","starttimems":1406058410344,"queuetimems":0,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 871 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 872 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 866 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54734: output error
2014-07-22 12:47:09,588 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:47:10,403 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=401, memsize=135.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/157b749e85d14eb9849f961e82730de1
2014-07-22 12:47:10,471 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/157b749e85d14eb9849f961e82730de1 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/157b749e85d14eb9849f961e82730de1
2014-07-22 12:47:10,505 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/157b749e85d14eb9849f961e82730de1, entries=493140, sequenceid=401, filesize=35.1m
2014-07-22 12:47:10,506 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~345.0m/361754240, currentsize=180.5m/189282080 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 32978ms, sequenceid=401, compaction requested=false
2014-07-22 12:47:10,508 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 381.2m
2014-07-22 12:47:10,622 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=407, memsize=136.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/03fff708a3cf49d9a6deb7ff6555b831
2014-07-22 12:47:10,657 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/03fff708a3cf49d9a6deb7ff6555b831 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/03fff708a3cf49d9a6deb7ff6555b831
2014-07-22 12:47:10,701 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/03fff708a3cf49d9a6deb7ff6555b831, entries=497480, sequenceid=407, filesize=35.4m
2014-07-22 12:47:10,702 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~347.1m/363937120, currentsize=168.6m/176801040 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 32043ms, sequenceid=407, compaction requested=false
2014-07-22 12:47:10,703 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 375.8m
2014-07-22 12:47:10,786 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:10,980 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:12,706 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:12,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2644 synced till here 2639
2014-07-22 12:47:12,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058429458 with entries=94, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058432706
2014-07-22 12:47:14,041 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:14,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2732 synced till here 2731
2014-07-22 12:47:14,130 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058432706 with entries=88, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058434041
2014-07-22 12:47:14,242 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:47:15,532 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:47:15,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:15,728 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2821 synced till here 2818
2014-07-22 12:47:15,753 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058434041 with entries=89, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058435679
2014-07-22 12:47:15,968 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:47:17,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:17,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2911 synced till here 2904
2014-07-22 12:47:17,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058435679 with entries=90, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058437127
2014-07-22 12:47:19,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:20,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3072 synced till here 3068
2014-07-22 12:47:20,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058437127 with entries=161, filesize=120.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058439409
2014-07-22 12:47:21,042 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=516, memsize=215.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d49fe7bfb8b54807b2d22e65b1a04867
2014-07-22 12:47:21,059 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d49fe7bfb8b54807b2d22e65b1a04867 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d49fe7bfb8b54807b2d22e65b1a04867
2014-07-22 12:47:21,071 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d49fe7bfb8b54807b2d22e65b1a04867, entries=783070, sequenceid=516, filesize=55.8m
2014-07-22 12:47:21,072 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~381.2m/399757840, currentsize=167.2m/175366000 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 10564ms, sequenceid=516, compaction requested=false
2014-07-22 12:47:21,073 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 288.2m
2014-07-22 12:47:21,346 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=516, memsize=213.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/b0bad6db96ed45a1ac11f7532677c438
2014-07-22 12:47:21,365 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/b0bad6db96ed45a1ac11f7532677c438 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/b0bad6db96ed45a1ac11f7532677c438
2014-07-22 12:47:21,498 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:21,565 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/b0bad6db96ed45a1ac11f7532677c438, entries=778780, sequenceid=516, filesize=55.5m
2014-07-22 12:47:21,565 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~375.8m/394046800, currentsize=180.2m/188937520 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 10863ms, sequenceid=516, compaction requested=false
2014-07-22 12:47:21,565 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 361.5m
2014-07-22 12:47:21,824 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:21,994 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:22,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3161 synced till here 3160
2014-07-22 12:47:22,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058439409 with entries=89, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058441995
2014-07-22 12:47:23,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:23,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3252 synced till here 3240
2014-07-22 12:47:23,377 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058441995 with entries=91, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058443212
2014-07-22 12:47:24,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:26,642 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:47:26,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3435 synced till here 3416
2014-07-22 12:47:26,974 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:47:26,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058443212 with entries=183, filesize=137.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058444927
2014-07-22 12:47:28,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:28,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3543 synced till here 3521
2014-07-22 12:47:28,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058444927 with entries=108, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058448548
2014-07-22 12:47:30,240 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1046ms
GC pool 'ParNew' had collection(s): count=1 time=1116ms
2014-07-22 12:47:30,281 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=618, memsize=105.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9c1fbc3a535e47e6afff75df9c403ff0
2014-07-22 12:47:30,294 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9c1fbc3a535e47e6afff75df9c403ff0 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9c1fbc3a535e47e6afff75df9c403ff0
2014-07-22 12:47:30,305 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9c1fbc3a535e47e6afff75df9c403ff0, entries=383890, sequenceid=618, filesize=27.4m
2014-07-22 12:47:30,305 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~288.2m/302179760, currentsize=36.9m/38680880 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 9232ms, sequenceid=618, compaction requested=false
2014-07-22 12:47:30,306 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 483.9m
2014-07-22 12:47:30,901 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:30,968 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:31,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3669 synced till here 3639
2014-07-22 12:47:32,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058448548 with entries=126, filesize=96.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058450969
2014-07-22 12:47:32,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406057994683
2014-07-22 12:47:32,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058358294
2014-07-22 12:47:32,260 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058360025
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058361726
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058363650
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058365374
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058367106
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058368913
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058371043
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058373487
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058375612
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058378340
2014-07-22 12:47:32,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058380902
2014-07-22 12:47:32,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058383432
2014-07-22 12:47:32,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058385537
2014-07-22 12:47:32,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058388117
2014-07-22 12:47:32,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058390126
2014-07-22 12:47:32,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058392513
2014-07-22 12:47:32,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058394357
2014-07-22 12:47:34,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:34,250 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3795 synced till here 3783
2014-07-22 12:47:34,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058450969 with entries=126, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058454198
2014-07-22 12:47:36,139 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:36,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3970 synced till here 3931
2014-07-22 12:47:36,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058454198 with entries=175, filesize=123.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058456140
2014-07-22 12:47:38,997 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:40,205 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4136 synced till here 4114
2014-07-22 12:47:40,242 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=633, memsize=241.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b05062453274493a80e09e0092c696ec
2014-07-22 12:47:40,259 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b05062453274493a80e09e0092c696ec as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b05062453274493a80e09e0092c696ec
2014-07-22 12:47:40,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058456140 with entries=166, filesize=120.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058458998
2014-07-22 12:47:40,590 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b05062453274493a80e09e0092c696ec, entries=879310, sequenceid=633, filesize=62.7m
2014-07-22 12:47:40,591 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~361.5m/379008000, currentsize=323.1m/338795120 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 19026ms, sequenceid=633, compaction requested=true
2014-07-22 12:47:40,592 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:47:40,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 12:47:40,592 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 495.3m
2014-07-22 12:47:40,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 12:47:40,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:47:40,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:47:40,592 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:47:40,852 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:47:42,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:42,173 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058458998 with entries=101, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462109
2014-07-22 12:47:42,226 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:42,805 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:42,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4335 synced till here 4315
2014-07-22 12:47:42,878 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462109 with entries=98, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462805
2014-07-22 12:47:44,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:44,237 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462805 with entries=86, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058464214
2014-07-22 12:47:45,517 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:45,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4507 synced till here 4506
2014-07-22 12:47:45,574 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058464214 with entries=86, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058465517
2014-07-22 12:47:47,235 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:47,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4595 synced till here 4586
2014-07-22 12:47:47,355 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058465517 with entries=88, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058467235
2014-07-22 12:47:48,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:48,652 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4683 synced till here 4681
2014-07-22 12:47:48,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058467235 with entries=88, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058468630
2014-07-22 12:47:49,205 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=726, memsize=280.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/cd1ae123cc344d749de373be73a08ff9
2014-07-22 12:47:49,226 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/cd1ae123cc344d749de373be73a08ff9 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/cd1ae123cc344d749de373be73a08ff9
2014-07-22 12:47:49,255 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/cd1ae123cc344d749de373be73a08ff9, entries=1019400, sequenceid=726, filesize=72.6m
2014-07-22 12:47:49,255 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~493.0m/516964960, currentsize=348.8m/365747680 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 18949ms, sequenceid=726, compaction requested=true
2014-07-22 12:47:49,256 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 12:47:49,257 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 12:47:49,257 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:47:49,257 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:47:49,257 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:47:49,257 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:47:49,271 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 663.2m
2014-07-22 12:47:49,289 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:47:49,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:50,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4776 synced till here 4764
2014-07-22 12:47:50,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058468630 with entries=93, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058469605
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058396387
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058398823
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058400870
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058402875
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058404869
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058407336
2014-07-22 12:47:50,240 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058409877
2014-07-22 12:47:50,451 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:51,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:52,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4883 synced till here 4882
2014-07-22 12:47:52,819 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=838, memsize=218.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/683985a7be2743d0b7366ac7af9d3e3e
2014-07-22 12:47:52,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058469605 with entries=107, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058471928
2014-07-22 12:47:52,879 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/683985a7be2743d0b7366ac7af9d3e3e as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/683985a7be2743d0b7366ac7af9d3e3e
2014-07-22 12:47:53,035 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/683985a7be2743d0b7366ac7af9d3e3e, entries=796690, sequenceid=838, filesize=56.8m
2014-07-22 12:47:53,036 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~501.5m/525869760, currentsize=225.6m/236533840 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 12444ms, sequenceid=838, compaction requested=true
2014-07-22 12:47:53,036 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:47:53,036 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 12:47:53,036 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 12:47:53,036 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:47:53,037 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 545.8m
2014-07-22 12:47:53,037 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:47:53,037 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:47:53,470 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:47:54,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:54,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058471928 with entries=88, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058474151
2014-07-22 12:47:54,957 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:47:55,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:55,425 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5059 synced till here 5058
2014-07-22 12:47:55,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058474151 with entries=88, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058475398
2014-07-22 12:47:56,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:56,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058475398 with entries=85, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058476510
2014-07-22 12:47:58,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:58,361 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5233 synced till here 5227
2014-07-22 12:47:58,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058476510 with entries=89, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058478340
2014-07-22 12:47:59,544 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:47:59,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5320 synced till here 5315
2014-07-22 12:47:59,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058478340 with entries=87, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058479544
2014-07-22 12:48:01,710 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:01,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058479544 with entries=85, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058481710
2014-07-22 12:48:03,101 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=984, memsize=194.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/325713921c9540508605210c88aeb62c
2014-07-22 12:48:03,114 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/325713921c9540508605210c88aeb62c as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/325713921c9540508605210c88aeb62c
2014-07-22 12:48:03,126 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/325713921c9540508605210c88aeb62c, entries=707500, sequenceid=984, filesize=50.4m
2014-07-22 12:48:03,127 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~545.8m/572346960, currentsize=170.5m/178799520 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 10090ms, sequenceid=984, compaction requested=true
2014-07-22 12:48:03,127 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:03,127 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 12:48:03,127 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 12:48:03,127 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:03,127 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:03,127 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 566.7m
2014-07-22 12:48:03,127 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:48:03,203 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=949, memsize=289.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/cf141d5334344fcaa7f048c4edbda479
2014-07-22 12:48:03,216 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/cf141d5334344fcaa7f048c4edbda479 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/cf141d5334344fcaa7f048c4edbda479
2014-07-22 12:48:03,227 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/cf141d5334344fcaa7f048c4edbda479, entries=1052510, sequenceid=949, filesize=74.9m
2014-07-22 12:48:03,227 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~669.4m/701929120, currentsize=221.4m/232118080 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 13956ms, sequenceid=949, compaction requested=true
2014-07-22 12:48:03,228 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:03,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 12:48:03,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 12:48:03,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:03,228 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 394.5m
2014-07-22 12:48:03,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:03,228 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:48:04,524 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:48:04,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:04,563 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:48:05,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5541 synced till here 5536
2014-07-22 12:48:05,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058481710 with entries=136, filesize=101.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058484546
2014-07-22 12:48:05,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058429458
2014-07-22 12:48:05,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058432706
2014-07-22 12:48:05,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058434041
2014-07-22 12:48:05,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058435679
2014-07-22 12:48:05,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058437127
2014-07-22 12:48:05,895 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:48:07,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:07,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5627 synced till here 5624
2014-07-22 12:48:07,119 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058484546 with entries=86, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058487088
2014-07-22 12:48:08,400 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:48:08,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:08,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5714 synced till here 5711
2014-07-22 12:48:08,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058487088 with entries=87, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058488412
2014-07-22 12:48:10,477 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:10,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5800 synced till here 5796
2014-07-22 12:48:10,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058488412 with entries=86, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058490477
2014-07-22 12:48:10,982 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:48:13,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:13,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5904 synced till here 5895
2014-07-22 12:48:13,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058490477 with entries=104, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058493057
2014-07-22 12:48:14,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:14,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5995 synced till here 5987
2014-07-22 12:48:14,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058493057 with entries=91, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058494726
2014-07-22 12:48:16,748 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1178ms
GC pool 'ParNew' had collection(s): count=1 time=1181ms
2014-07-22 12:48:16,799 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:17,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6122 synced till here 6108
2014-07-22 12:48:17,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058494726 with entries=127, filesize=96.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058496800
2014-07-22 12:48:19,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:19,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6236 synced till here 6208
2014-07-22 12:48:19,989 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1096, memsize=324.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b3cba6e93760462abcbb3f28e0c3c5ff
2014-07-22 12:48:20,001 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b3cba6e93760462abcbb3f28e0c3c5ff as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b3cba6e93760462abcbb3f28e0c3c5ff
2014-07-22 12:48:20,010 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b3cba6e93760462abcbb3f28e0c3c5ff, entries=1180420, sequenceid=1096, filesize=84.1m
2014-07-22 12:48:20,010 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~399.1m/418524560, currentsize=228.2m/239331760 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 16782ms, sequenceid=1096, compaction requested=true
2014-07-22 12:48:20,011 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:20,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 12:48:20,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 12:48:20,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:20,011 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 447.3m
2014-07-22 12:48:20,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:20,011 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:48:39,152 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 18846ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=19093ms
2014-07-22 12:48:39,152 WARN  [regionserver60020] util.Sleeper: We slept 21498ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:48:39,190 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058496800 with entries=114, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058499787
2014-07-22 12:48:39,368 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22259,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497108,"queuetimems":0,"class":"HRegionServer","responsesize":16823,"method":"Multi"}
2014-07-22 12:48:39,368 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2217 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,370 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,370 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058496999,"queuetimems":0,"class":"HRegionServer","responsesize":17350,"method":"Multi"}
2014-07-22 12:48:39,370 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497076,"queuetimems":0,"class":"HRegionServer","responsesize":17335,"method":"Multi"}
2014-07-22 12:48:39,371 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2200 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,371 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,371 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2219 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,371 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,377 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497044,"queuetimems":0,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 12:48:39,378 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2199 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,378 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,578 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:48:39,674 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22526,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497148,"queuetimems":2,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 12:48:39,674 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497372,"queuetimems":1,"class":"HRegionServer","responsesize":17085,"method":"Multi"}
2014-07-22 12:48:39,674 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22345,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497329,"queuetimems":0,"class":"HRegionServer","responsesize":16832,"method":"Multi"}
2014-07-22 12:48:39,674 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497265,"queuetimems":0,"class":"HRegionServer","responsesize":17161,"method":"Multi"}
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2214 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2236 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2212 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2235 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,675 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,676 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:48:39,674 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497297,"queuetimems":0,"class":"HRegionServer","responsesize":16450,"method":"Multi"}
2014-07-22 12:48:39,686 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2211 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,686 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,881 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:39,882 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1094, memsize=322.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/717b58fb1a914c7ba0f4dd9ff0fd0413
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22474,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497408,"queuetimems":0,"class":"HRegionServer","responsesize":16906,"method":"Multi"}
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497473,"queuetimems":0,"class":"HRegionServer","responsesize":16794,"method":"Multi"}
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497442,"queuetimems":1,"class":"HRegionServer","responsesize":17062,"method":"Multi"}
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2234 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2233 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,883 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,884 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2232 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,884 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497538,"queuetimems":1,"class":"HRegionServer","responsesize":16548,"method":"Multi"}
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22254,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497631,"queuetimems":0,"class":"HRegionServer","responsesize":16462,"method":"Multi"}
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2230 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2227 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,886 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,887 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22318,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497569,"queuetimems":0,"class":"HRegionServer","responsesize":16529,"method":"Multi"}
2014-07-22 12:48:39,887 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2229 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,887 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,887 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497506,"queuetimems":0,"class":"HRegionServer","responsesize":17301,"method":"Multi"}
2014-07-22 12:48:39,888 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497601,"queuetimems":1,"class":"HRegionServer","responsesize":17199,"method":"Multi"}
2014-07-22 12:48:39,888 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2231 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,888 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,888 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2228 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,888 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,898 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6346 synced till here 6331
2014-07-22 12:48:39,925 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22200,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497725,"queuetimems":0,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 12:48:39,926 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497667,"queuetimems":0,"class":"HRegionServer","responsesize":17258,"method":"Multi"}
2014-07-22 12:48:39,926 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2225 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,926 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,926 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2226 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:39,926 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:39,941 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/717b58fb1a914c7ba0f4dd9ff0fd0413 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/717b58fb1a914c7ba0f4dd9ff0fd0413
2014-07-22 12:48:39,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058499787 with entries=110, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058519882
2014-07-22 12:48:39,964 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/717b58fb1a914c7ba0f4dd9ff0fd0413, entries=1173560, sequenceid=1094, filesize=83.6m
2014-07-22 12:48:39,964 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~569.8m/597512400, currentsize=293.5m/307779120 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 36837ms, sequenceid=1094, compaction requested=true
2014-07-22 12:48:39,965 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 12:48:39,965 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:39,965 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 12:48:39,965 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:39,965 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:39,965 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 467.3m
2014-07-22 12:48:39,965 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:48:40,013 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498848,"queuetimems":0,"class":"HRegionServer","responsesize":16858,"method":"Multi"}
2014-07-22 12:48:40,013 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498834,"queuetimems":0,"class":"HRegionServer","responsesize":16886,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2245 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,013 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499480,"queuetimems":0,"class":"HRegionServer","responsesize":17335,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2222 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,013 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21196,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498816,"queuetimems":0,"class":"HRegionServer","responsesize":17170,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2251 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498965,"queuetimems":0,"class":"HRegionServer","responsesize":17242,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498998,"queuetimems":1,"class":"HRegionServer","responsesize":17120,"method":"Multi"}
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498902,"queuetimems":0,"class":"HRegionServer","responsesize":17350,"method":"Multi"}
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058497769,"queuetimems":0,"class":"HRegionServer","responsesize":16862,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20952,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499063,"queuetimems":0,"class":"HRegionServer","responsesize":18477,"method":"Multi"}
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499414,"queuetimems":0,"class":"HRegionServer","responsesize":16450,"method":"Multi"}
2014-07-22 12:48:40,014 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2223 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20986,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499029,"queuetimems":0,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2240 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21081,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058498934,"queuetimems":0,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 12:48:40,015 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20569,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499446,"queuetimems":0,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2248 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,016 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499511,"queuetimems":0,"class":"HRegionServer","responsesize":16823,"method":"Multi"}
2014-07-22 12:48:40,021 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2252 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,021 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,021 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2224 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2244 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2241 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2242 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2250 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,022 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2243 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,023 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,023 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2239 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,023 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,124 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20549,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499574,"queuetimems":0,"class":"HRegionServer","responsesize":16886,"method":"Multi"}
2014-07-22 12:48:40,124 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2262 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,124 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,124 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499648,"queuetimems":0,"class":"HRegionServer","responsesize":16906,"method":"Multi"}
2014-07-22 12:48:40,125 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2260 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,126 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,138 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499746,"queuetimems":1,"class":"HRegionServer","responsesize":17258,"method":"Multi"}
2014-07-22 12:48:40,139 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2257 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,139 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,140 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499607,"queuetimems":1,"class":"HRegionServer","responsesize":16462,"method":"Multi"}
2014-07-22 12:48:40,140 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20426,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499714,"queuetimems":1,"class":"HRegionServer","responsesize":17062,"method":"Multi"}
2014-07-22 12:48:40,140 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20597,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499543,"queuetimems":1,"class":"HRegionServer","responsesize":17161,"method":"Multi"}
2014-07-22 12:48:40,140 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20288,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499852,"queuetimems":0,"class":"HRegionServer","responsesize":17085,"method":"Multi"}
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2261 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2254 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2249 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2258 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,141 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,148 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499681,"queuetimems":0,"class":"HRegionServer","responsesize":16862,"method":"Multi"}
2014-07-22 12:48:40,148 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2259 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,148 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,151 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20170,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499981,"queuetimems":0,"class":"HRegionServer","responsesize":17199,"method":"Multi"}
2014-07-22 12:48:40,151 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2264 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,151 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,158 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499777,"queuetimems":0,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 12:48:40,159 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2256 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,159 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,164 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20215,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499949,"queuetimems":1,"class":"HRegionServer","responsesize":17301,"method":"Multi"}
2014-07-22 12:48:40,164 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2269 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20281,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499883,"queuetimems":0,"class":"HRegionServer","responsesize":16794,"method":"Multi"}
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2270 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,164 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058500012,"queuetimems":0,"class":"HRegionServer","responsesize":16529,"method":"Multi"}
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2265 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,164 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499813,"queuetimems":0,"class":"HRegionServer","responsesize":17170,"method":"Multi"}
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54738","starttimems":1406058499915,"queuetimems":0,"class":"HRegionServer","responsesize":16548,"method":"Multi"}
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,165 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2253 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2266 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2263 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,166 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2255 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54738: output error
2014-07-22 12:48:40,167 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:48:40,291 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:48:41,041 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:48:41,741 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:42,136 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6468 synced till here 6466
2014-07-22 12:48:42,153 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058519882 with entries=122, filesize=78.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058521741
2014-07-22 12:48:43,741 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:44,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058521741 with entries=100, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058523742
2014-07-22 12:48:44,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:45,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:45,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6652 synced till here 6651
2014-07-22 12:48:45,826 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058523742 with entries=84, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058525783
2014-07-22 12:48:45,828 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:47,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:47,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6739 synced till here 6734
2014-07-22 12:48:47,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058525783 with entries=87, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058527375
2014-07-22 12:48:47,466 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:48,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:48,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6827 synced till here 6824
2014-07-22 12:48:48,714 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058527375 with entries=88, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058528651
2014-07-22 12:48:48,716 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:50,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:50,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6925 synced till here 6911
2014-07-22 12:48:50,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058528651 with entries=98, filesize=73.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058530599
2014-07-22 12:48:50,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:52,390 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1171ms
GC pool 'ParNew' had collection(s): count=1 time=1239ms
2014-07-22 12:48:52,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:52,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7024 synced till here 7012
2014-07-22 12:48:52,795 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058530599 with entries=99, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058532657
2014-07-22 12:48:52,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:54,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:54,587 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7133 synced till here 7114
2014-07-22 12:48:54,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058532657 with entries=109, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058534548
2014-07-22 12:48:54,745 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:48:57,094 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1285, memsize=340.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/629307a11b7f4d6ca51a195d2fc74cbb
2014-07-22 12:48:57,109 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/629307a11b7f4d6ca51a195d2fc74cbb as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/629307a11b7f4d6ca51a195d2fc74cbb
2014-07-22 12:48:58,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:48:58,226 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/629307a11b7f4d6ca51a195d2fc74cbb, entries=1239800, sequenceid=1285, filesize=88.3m
2014-07-22 12:48:58,227 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~467.3m/489989040, currentsize=279.5m/293035200 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 18262ms, sequenceid=1285, compaction requested=true
2014-07-22 12:48:58,227 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:58,227 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 12:48:58,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 12:48:58,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:58,228 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 375.9m
2014-07-22 12:48:58,228 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:58,228 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:48:58,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7264 synced till here 7218
2014-07-22 12:48:58,237 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:48:58,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058534548 with entries=131, filesize=101.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058538215
2014-07-22 12:48:58,859 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:48:59,066 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1244, memsize=382.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/04a2d05244184567a682db93aeffbcb3
2014-07-22 12:48:59,098 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/04a2d05244184567a682db93aeffbcb3 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/04a2d05244184567a682db93aeffbcb3
2014-07-22 12:48:59,110 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/04a2d05244184567a682db93aeffbcb3, entries=1393740, sequenceid=1244, filesize=99.3m
2014-07-22 12:48:59,114 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~458.1m/480330800, currentsize=335.2m/351482000 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 39103ms, sequenceid=1244, compaction requested=true
2014-07-22 12:48:59,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 12:48:59,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 12:48:59,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:48:59,115 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:48:59,115 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:48:59,115 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:48:59,115 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 571.6m
2014-07-22 12:49:00,232 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:49:00,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:00,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7408 synced till here 7376
2014-07-22 12:49:00,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058538215 with entries=144, filesize=106.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058540445
2014-07-22 12:49:00,977 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:02,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:02,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7525 synced till here 7521
2014-07-22 12:49:02,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058540445 with entries=117, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058542274
2014-07-22 12:49:04,283 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:04,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7658 synced till here 7634
2014-07-22 12:49:04,568 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058542274 with entries=133, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058544283
2014-07-22 12:49:06,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:06,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7782 synced till here 7763
2014-07-22 12:49:07,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058544283 with entries=124, filesize=91.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058546259
2014-07-22 12:49:08,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:09,170 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7952 synced till here 7925
2014-07-22 12:49:09,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058546259 with entries=170, filesize=126.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058548193
2014-07-22 12:49:11,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:11,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8055 synced till here 8038
2014-07-22 12:49:11,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058548193 with entries=103, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058551036
2014-07-22 12:49:12,367 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:12,389 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8157 synced till here 8153
2014-07-22 12:49:12,434 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058551036 with entries=102, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058552368
2014-07-22 12:49:13,173 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1435, memsize=209.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/f4fa06e4c4cd4b24930de4627daf0223
2014-07-22 12:49:13,189 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/f4fa06e4c4cd4b24930de4627daf0223 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/f4fa06e4c4cd4b24930de4627daf0223
2014-07-22 12:49:13,204 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/f4fa06e4c4cd4b24930de4627daf0223, entries=761630, sequenceid=1435, filesize=54.3m
2014-07-22 12:49:13,204 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~379.1m/397552080, currentsize=94.2m/98759760 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 14976ms, sequenceid=1435, compaction requested=false
2014-07-22 12:49:13,205 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 855.0m
2014-07-22 12:49:13,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:13,896 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8244 synced till here 8243
2014-07-22 12:49:13,948 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058552368 with entries=87, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058553876
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058439409
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058441995
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058443212
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058444927
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058448548
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058450969
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058454198
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058456140
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058458998
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462109
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058462805
2014-07-22 12:49:13,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058464214
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058465517
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058467235
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058468630
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058469605
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058471928
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058474151
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058475398
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058476510
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058478340
2014-07-22 12:49:13,950 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058479544
2014-07-22 12:49:13,969 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:15,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:15,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8329 synced till here 8327
2014-07-22 12:49:15,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058553876 with entries=85, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058555857
2014-07-22 12:49:17,125 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:17,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8418 synced till here 8410
2014-07-22 12:49:17,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058555857 with entries=89, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058557126
2014-07-22 12:49:18,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:18,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8512 synced till here 8502
2014-07-22 12:49:18,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058557126 with entries=94, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058558564
2014-07-22 12:49:19,870 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 12:49:20,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:20,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8603 synced till here 8600
2014-07-22 12:49:20,070 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058558564 with entries=91, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058560005
2014-07-22 12:49:21,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:22,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8742 synced till here 8741
2014-07-22 12:49:22,303 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1481, memsize=348.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e4becf4a06f04c5489b061d4c533952a
2014-07-22 12:49:22,318 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e4becf4a06f04c5489b061d4c533952a as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e4becf4a06f04c5489b061d4c533952a
2014-07-22 12:49:22,440 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058560005 with entries=139, filesize=100.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058561035
2014-07-22 12:49:22,651 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e4becf4a06f04c5489b061d4c533952a, entries=1267760, sequenceid=1481, filesize=90.4m
2014-07-22 12:49:22,652 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~596.4m/625376000, currentsize=426.3m/446955840 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 23537ms, sequenceid=1481, compaction requested=true
2014-07-22 12:49:22,653 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:49:22,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 12:49:22,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 12:49:22,653 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 744.4m
2014-07-22 12:49:22,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:49:22,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:49:22,653 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:49:22,740 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:49:23,460 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:23,857 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:23,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8828 synced till here 8826
2014-07-22 12:49:23,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058561035 with entries=86, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058563857
2014-07-22 12:49:24,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:24,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8914 synced till here 8912
2014-07-22 12:49:25,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058563857 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058564967
2014-07-22 12:49:25,997 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:26,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9000 synced till here 8999
2014-07-22 12:49:26,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058564967 with entries=86, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058565998
2014-07-22 12:49:27,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:27,255 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9086 synced till here 9085
2014-07-22 12:49:27,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058565998 with entries=86, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058567234
2014-07-22 12:49:29,287 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:29,330 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9174 synced till here 9170
2014-07-22 12:49:29,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058567234 with entries=88, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058569288
2014-07-22 12:49:31,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:31,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9282 synced till here 9281
2014-07-22 12:49:31,414 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058569288 with entries=108, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058571165
2014-07-22 12:49:32,660 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1645, memsize=355.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7a6138a50d5247f294cc1aca9665cd4a
2014-07-22 12:49:32,682 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7a6138a50d5247f294cc1aca9665cd4a as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7a6138a50d5247f294cc1aca9665cd4a
2014-07-22 12:49:32,695 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7a6138a50d5247f294cc1aca9665cd4a, entries=1293300, sequenceid=1645, filesize=92.2m
2014-07-22 12:49:32,696 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~855.0m/896525280, currentsize=362.8m/380465280 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 19491ms, sequenceid=1645, compaction requested=true
2014-07-22 12:49:32,696 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:49:32,696 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 12:49:32,696 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 12:49:32,696 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 981.2m
2014-07-22 12:49:32,696 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:49:32,697 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:49:32,697 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:49:32,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:32,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058571165 with entries=85, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058572774
2014-07-22 12:49:32,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058481710
2014-07-22 12:49:32,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058484546
2014-07-22 12:49:32,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058487088
2014-07-22 12:49:32,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058488412
2014-07-22 12:49:32,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058490477
2014-07-22 12:49:32,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058493057
2014-07-22 12:49:32,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058494726
2014-07-22 12:49:32,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058496800
2014-07-22 12:49:32,839 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:49:33,576 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:34,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:34,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9452 synced till here 9449
2014-07-22 12:49:34,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058572774 with entries=85, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058574724
2014-07-22 12:49:36,428 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:36,445 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9540 synced till here 9536
2014-07-22 12:49:36,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058574724 with entries=88, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058576428
2014-07-22 12:49:37,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:37,760 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9627 synced till here 9625
2014-07-22 12:49:37,795 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058576428 with entries=87, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058577744
2014-07-22 12:49:38,309 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1766, memsize=329.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5145f3f9bb0542b5bcf3486daea36f0d
2014-07-22 12:49:38,323 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5145f3f9bb0542b5bcf3486daea36f0d as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5145f3f9bb0542b5bcf3486daea36f0d
2014-07-22 12:49:38,336 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5145f3f9bb0542b5bcf3486daea36f0d, entries=1199190, sequenceid=1766, filesize=85.4m
2014-07-22 12:49:38,336 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~744.4m/780597280, currentsize=284.0m/297795360 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 15683ms, sequenceid=1766, compaction requested=true
2014-07-22 12:49:38,337 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:49:38,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 12:49:38,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 12:49:38,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:49:38,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:49:38,337 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:49:38,337 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 716.3m
2014-07-22 12:49:38,374 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:49:38,460 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:39,035 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9727 synced till here 9726
2014-07-22 12:49:39,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058577744 with entries=100, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058578461
2014-07-22 12:49:39,340 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:39,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:40,901 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058578461 with entries=124, filesize=90.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058579827
2014-07-22 12:49:42,047 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:49:42,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:42,573 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058579827 with entries=82, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058582197
2014-07-22 12:49:43,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:43,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10017 synced till here 10015
2014-07-22 12:49:43,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058582197 with entries=84, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058583620
2014-07-22 12:49:46,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:46,281 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058583620 with entries=84, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058586251
2014-07-22 12:49:48,474 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:48,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10191 synced till here 10187
2014-07-22 12:49:48,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058586251 with entries=90, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058588475
2014-07-22 12:49:51,529 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:51,598 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10280 synced till here 10277
2014-07-22 12:49:51,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058588475 with entries=89, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058591530
2014-07-22 12:49:54,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:54,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058591530 with entries=105, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058594010
2014-07-22 12:49:55,745 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:49:55,782 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058594010 with entries=84, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058595746
2014-07-22 12:49:57,316 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1879, memsize=481.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/4121661588a34cb1aec870fc0e2e9c56
2014-07-22 12:49:57,340 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/4121661588a34cb1aec870fc0e2e9c56 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/4121661588a34cb1aec870fc0e2e9c56
2014-07-22 12:49:57,362 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/4121661588a34cb1aec870fc0e2e9c56, entries=1753510, sequenceid=1879, filesize=124.9m
2014-07-22 12:49:57,363 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~984.3m/1032065200, currentsize=357.2m/374594560 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 24667ms, sequenceid=1879, compaction requested=true
2014-07-22 12:49:57,363 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:49:57,363 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 12:49:57,363 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 12:49:57,364 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:49:57,364 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 719.2m
2014-07-22 12:49:57,364 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:49:57,364 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:49:57,792 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:49:58,103 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1947, memsize=434.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3c77098c5cf94472b8c16ca8c82d55c2
2014-07-22 12:49:58,123 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:49:58,138 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3c77098c5cf94472b8c16ca8c82d55c2 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3c77098c5cf94472b8c16ca8c82d55c2
2014-07-22 12:49:58,167 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3c77098c5cf94472b8c16ca8c82d55c2, entries=1580420, sequenceid=1947, filesize=112.5m
2014-07-22 12:49:58,168 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~720.9m/755893520, currentsize=258.1m/270645040 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 19831ms, sequenceid=1947, compaction requested=true
2014-07-22 12:49:58,168 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:49:58,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 12:49:58,168 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 540.2m
2014-07-22 12:49:58,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 12:49:58,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:49:58,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:49:58,169 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:49:58,221 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:49:58,478 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:50:01,785 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:01,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058595746 with entries=83, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058601785
2014-07-22 12:50:01,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058499787
2014-07-22 12:50:01,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058519882
2014-07-22 12:50:01,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058521741
2014-07-22 12:50:01,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058523742
2014-07-22 12:50:01,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058525783
2014-07-22 12:50:01,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058527375
2014-07-22 12:50:01,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058528651
2014-07-22 12:50:01,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058530599
2014-07-22 12:50:01,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058532657
2014-07-22 12:50:01,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058534548
2014-07-22 12:50:01,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:04,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:04,109 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10635 synced till here 10633
2014-07-22 12:50:04,125 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058601785 with entries=83, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058604089
2014-07-22 12:50:04,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:07,853 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:07,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058604089 with entries=83, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058607854
2014-07-22 12:50:07,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:10,545 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:10,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10803 synced till here 10802
2014-07-22 12:50:10,577 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058607854 with entries=85, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058610546
2014-07-22 12:50:10,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:12,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:12,035 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10888 synced till here 10887
2014-07-22 12:50:12,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058610546 with entries=85, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058612019
2014-07-22 12:50:12,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:13,644 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:13,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10975 synced till here 10974
2014-07-22 12:50:13,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058612019 with entries=87, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058613645
2014-07-22 12:50:13,690 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:15,445 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:15,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058613645 with entries=87, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058615446
2014-07-22 12:50:15,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:50:16,408 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2115, memsize=521.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/8fe5d2f4946f493f8604a3ff6f3566b0
2014-07-22 12:50:16,427 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/8fe5d2f4946f493f8604a3ff6f3566b0 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/8fe5d2f4946f493f8604a3ff6f3566b0
2014-07-22 12:50:16,959 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/8fe5d2f4946f493f8604a3ff6f3566b0, entries=1899340, sequenceid=2115, filesize=135.1m
2014-07-22 12:50:16,960 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~540.2m/566408320, currentsize=177.1m/185751520 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 18791ms, sequenceid=2115, compaction requested=true
2014-07-22 12:50:16,960 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:50:16,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 12:50:16,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 12:50:16,960 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 368.2m
2014-07-22 12:50:16,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:50:16,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:50:16,960 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:50:17,246 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:50:19,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:19,193 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11148 synced till here 11147
2014-07-22 12:50:19,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058615446 with entries=86, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058619177
2014-07-22 12:50:21,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:21,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11234 synced till here 11233
2014-07-22 12:50:21,819 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058619177 with entries=86, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058621788
2014-07-22 12:50:22,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:23,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11326 synced till here 11320
2014-07-22 12:50:23,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058621788 with entries=92, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058622640
2014-07-22 12:50:23,704 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:50:23,717 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2110, memsize=668.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e4c3b5c6f5fc471abe4894eaebf46819
2014-07-22 12:50:23,731 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e4c3b5c6f5fc471abe4894eaebf46819 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e4c3b5c6f5fc471abe4894eaebf46819
2014-07-22 12:50:23,741 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e4c3b5c6f5fc471abe4894eaebf46819, entries=2432500, sequenceid=2110, filesize=173.1m
2014-07-22 12:50:23,741 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~720.7m/755743200, currentsize=256.5m/268975280 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 26377ms, sequenceid=2110, compaction requested=true
2014-07-22 12:50:23,742 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:50:23,742 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 12:50:23,742 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 610.3m
2014-07-22 12:50:23,742 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 12:50:23,742 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:50:23,742 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:50:23,742 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:50:23,774 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:50:24,194 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:50:24,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:24,581 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11415 synced till here 11412
2014-07-22 12:50:24,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058622640 with entries=89, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058624547
2014-07-22 12:50:26,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:26,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058624547 with entries=92, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058626157
2014-07-22 12:50:28,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:28,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11595 synced till here 11593
2014-07-22 12:50:28,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058626157 with entries=88, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058628306
2014-07-22 12:50:28,933 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2225, memsize=262.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/ecc05c5934df4fb99b760d8447f06a6c
2014-07-22 12:50:28,948 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/ecc05c5934df4fb99b760d8447f06a6c as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/ecc05c5934df4fb99b760d8447f06a6c
2014-07-22 12:50:28,961 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/ecc05c5934df4fb99b760d8447f06a6c, entries=954260, sequenceid=2225, filesize=67.9m
2014-07-22 12:50:28,962 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~368.2m/386083200, currentsize=47.2m/49479040 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 12002ms, sequenceid=2225, compaction requested=true
2014-07-22 12:50:28,962 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:50:28,962 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 2000 blocking
2014-07-22 12:50:28,962 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-22 12:50:28,962 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:50:28,962 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 594.1m
2014-07-22 12:50:28,962 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:50:28,962 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:50:29,943 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:50:30,954 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:30,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058628306 with entries=84, filesize=61.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058630954
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058538215
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058540445
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058542274
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058544283
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058546259
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058548193
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058551036
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058552368
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058553876
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058555857
2014-07-22 12:50:30,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058557126
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058558564
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058560005
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058561035
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058563857
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058564967
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058565998
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058567234
2014-07-22 12:50:30,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058569288
2014-07-22 12:50:34,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:34,359 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11779 synced till here 11778
2014-07-22 12:50:34,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058630954 with entries=100, filesize=74.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058634195
2014-07-22 12:50:36,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:36,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058634195 with entries=86, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058636479
2014-07-22 12:50:38,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:50:38,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058636479 with entries=85, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058638673
2014-07-22 12:50:46,594 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2275, memsize=605.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/159031c0e5994778b23d06c6a025a7ba
2014-07-22 12:50:46,617 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/159031c0e5994778b23d06c6a025a7ba as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/159031c0e5994778b23d06c6a025a7ba
2014-07-22 12:50:46,634 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/159031c0e5994778b23d06c6a025a7ba, entries=2205430, sequenceid=2275, filesize=156.9m
2014-07-22 12:50:46,635 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~613.6m/643393200, currentsize=211.4m/221701120 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 22893ms, sequenceid=2275, compaction requested=true
2014-07-22 12:50:46,635 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:50:46,635 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 12:50:46,635 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 12:50:46,635 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:50:46,635 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 459.9m
2014-07-22 12:50:46,635 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:50:46,636 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:50:46,962 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:50:50,797 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2330, memsize=594.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/26a8c8158d364aa78ec696320ae03548
2014-07-22 12:50:50,814 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/26a8c8158d364aa78ec696320ae03548 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/26a8c8158d364aa78ec696320ae03548
2014-07-22 12:50:50,829 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/26a8c8158d364aa78ec696320ae03548, entries=2163150, sequenceid=2330, filesize=154.0m
2014-07-22 12:50:50,829 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~594.1m/622967600, currentsize=125.7m/131765840 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 21867ms, sequenceid=2330, compaction requested=true
2014-07-22 12:50:50,830 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:50:50,830 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 12:50:50,830 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 12:50:50,830 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:50:50,830 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 465.5m
2014-07-22 12:50:50,830 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:50:50,830 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:50:51,085 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:51:01,106 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2412, memsize=459.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/53ccff77916c4b68abd374e7adefbc8b
2014-07-22 12:51:01,171 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/53ccff77916c4b68abd374e7adefbc8b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/53ccff77916c4b68abd374e7adefbc8b
2014-07-22 12:51:01,216 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/53ccff77916c4b68abd374e7adefbc8b, entries=1674430, sequenceid=2412, filesize=119.3m
2014-07-22 12:51:01,217 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~459.9m/482221920, currentsize=0.0/0 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 14582ms, sequenceid=2412, compaction requested=true
2014-07-22 12:51:01,217 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:51:01,217 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 12:51:01,217 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 12:51:01,217 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:51:01,218 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:51:01,218 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:51:04,972 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2411, memsize=465.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2904a245c1484a538c344b146f4b2e17
2014-07-22 12:51:05,000 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2904a245c1484a538c344b146f4b2e17 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2904a245c1484a538c344b146f4b2e17
2014-07-22 12:51:05,011 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2904a245c1484a538c344b146f4b2e17, entries=1694850, sequenceid=2411, filesize=120.8m
2014-07-22 12:51:05,011 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~465.5m/488102960, currentsize=0.0/0 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 14181ms, sequenceid=2411, compaction requested=true
2014-07-22 12:51:05,012 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:51:05,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 12:51:05,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 12:51:05,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:51:05,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:51:05,013 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:51:47,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:47,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12037 synced till here 12035
2014-07-22 12:51:47,616 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058638673 with entries=87, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058707547
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058571165
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058572774
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058574724
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058576428
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058577744
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058578461
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058579827
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058582197
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058583620
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058586251
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058588475
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058591530
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058594010
2014-07-22 12:51:47,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058595746
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058601785
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058604089
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058607854
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058610546
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058612019
2014-07-22 12:51:47,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058613645
2014-07-22 12:51:49,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:49,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12124 synced till here 12121
2014-07-22 12:51:49,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058707547 with entries=87, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058709311
2014-07-22 12:51:49,929 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:51:49,931 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 256.8m
2014-07-22 12:51:50,170 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:51:52,688 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:53,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058709311 with entries=85, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058712688
2014-07-22 12:51:54,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:54,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12294 synced till here 12292
2014-07-22 12:51:54,580 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058712688 with entries=85, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058714547
2014-07-22 12:51:56,175 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:56,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12383 synced till here 12374
2014-07-22 12:51:56,287 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058714547 with entries=89, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058716176
2014-07-22 12:51:57,156 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:51:57,157 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 257.3m
2014-07-22 12:51:57,326 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:51:57,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:51:58,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058716176 with entries=128, filesize=89.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058717387
2014-07-22 12:52:00,271 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2443, memsize=259.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5bbb7655031f4cdd904034ae25db150d
2014-07-22 12:52:00,292 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5bbb7655031f4cdd904034ae25db150d as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5bbb7655031f4cdd904034ae25db150d
2014-07-22 12:52:00,305 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5bbb7655031f4cdd904034ae25db150d, entries=946440, sequenceid=2443, filesize=67.4m
2014-07-22 12:52:00,306 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.9m/272566400, currentsize=110.3m/115608160 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 10375ms, sequenceid=2443, compaction requested=true
2014-07-22 12:52:00,306 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:00,307 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 12:52:00,307 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 12:52:00,307 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:00,307 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:00,307 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:52:04,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:04,433 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058717387 with entries=81, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058724383
2014-07-22 12:52:05,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:05,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12679 synced till here 12671
2014-07-22 12:52:05,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058724383 with entries=87, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058725761
2014-07-22 12:52:06,110 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2498, memsize=238.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/850bb265c9ea48719260ec50a6d020a0
2014-07-22 12:52:06,930 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/850bb265c9ea48719260ec50a6d020a0 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/850bb265c9ea48719260ec50a6d020a0
2014-07-22 12:52:06,953 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/850bb265c9ea48719260ec50a6d020a0, entries=868550, sequenceid=2498, filesize=61.9m
2014-07-22 12:52:06,954 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~258.9m/271455040, currentsize=83.0m/87063520 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 9797ms, sequenceid=2498, compaction requested=true
2014-07-22 12:52:06,954 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:06,955 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 12:52:06,955 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 12:52:06,955 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:06,955 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:06,955 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:52:07,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:07,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12812 synced till here 12809
2014-07-22 12:52:07,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058725761 with entries=133, filesize=97.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058727558
2014-07-22 12:52:09,823 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:52:09,824 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 256.8m
2014-07-22 12:52:09,897 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:52:09,898 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 257.5m
2014-07-22 12:52:10,022 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:10,106 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:10,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:10,491 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12910 synced till here 12899
2014-07-22 12:52:10,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058727558 with entries=98, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058730471
2014-07-22 12:52:12,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:12,254 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:52:12,388 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13020 synced till here 13018
2014-07-22 12:52:12,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058730471 with entries=110, filesize=80.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058732143
2014-07-22 12:52:14,034 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:14,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13110 synced till here 13099
2014-07-22 12:52:14,144 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058732143 with entries=90, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058734035
2014-07-22 12:52:16,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:16,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13218 synced till here 13212
2014-07-22 12:52:16,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058734035 with entries=108, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058736035
2014-07-22 12:52:17,454 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:52:17,748 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:19,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13335 synced till here 13322
2014-07-22 12:52:19,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058736035 with entries=117, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058737749
2014-07-22 12:52:20,989 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:21,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13447 synced till here 13423
2014-07-22 12:52:21,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058737749 with entries=112, filesize=83.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058740990
2014-07-22 12:52:23,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:23,211 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13554 synced till here 13535
2014-07-22 12:52:23,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058740990 with entries=107, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743148
2014-07-22 12:52:23,964 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2580, memsize=224.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ba86dc647ca843e6857b0004895c4746
2014-07-22 12:52:23,985 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ba86dc647ca843e6857b0004895c4746 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ba86dc647ca843e6857b0004895c4746
2014-07-22 12:52:23,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:23,998 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ba86dc647ca843e6857b0004895c4746, entries=816900, sequenceid=2580, filesize=58.2m
2014-07-22 12:52:23,999 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.0m/272639760, currentsize=257.6m/270133840 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 14174ms, sequenceid=2580, compaction requested=true
2014-07-22 12:52:23,999 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:23,999 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 12:52:23,999 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 12:52:23,999 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:23,999 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:23,999 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:52:24,000 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 447.5m
2014-07-22 12:52:24,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13652 synced till here 13643
2014-07-22 12:52:24,073 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2580, memsize=226.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/1fc79f96150f4a249324b6f513801d3d
2014-07-22 12:52:24,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743148 with entries=98, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743996
2014-07-22 12:52:24,141 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/1fc79f96150f4a249324b6f513801d3d as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/1fc79f96150f4a249324b6f513801d3d
2014-07-22 12:52:25,011 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/1fc79f96150f4a249324b6f513801d3d, entries=823920, sequenceid=2580, filesize=58.7m
2014-07-22 12:52:25,011 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.3m/274994720, currentsize=254.7m/267079520 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 15113ms, sequenceid=2580, compaction requested=true
2014-07-22 12:52:25,012 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:25,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 12:52:25,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 12:52:25,012 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 384.7m
2014-07-22 12:52:25,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:25,012 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:25,012 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:52:25,042 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:52:25,177 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:52:25,293 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:25,365 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:25,608 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:25,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13749 synced till here 13741
2014-07-22 12:52:25,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743996 with entries=97, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058745608
2014-07-22 12:52:26,865 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:27,127 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:52:28,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13881 synced till here 13877
2014-07-22 12:52:28,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058745608 with entries=132, filesize=97.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058746866
2014-07-22 12:52:29,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:30,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13984 synced till here 13979
2014-07-22 12:52:30,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058746866 with entries=103, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058749227
2014-07-22 12:52:30,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:30,734 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14074 synced till here 14066
2014-07-22 12:52:30,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058749227 with entries=90, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058750714
2014-07-22 12:52:32,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:32,426 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14171 synced till here 14155
2014-07-22 12:52:32,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058750714 with entries=97, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058752273
2014-07-22 12:52:34,016 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:34,037 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14264 synced till here 14255
2014-07-22 12:52:34,080 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058752273 with entries=93, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058754017
2014-07-22 12:52:34,081 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:36,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:36,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14351 synced till here 14349
2014-07-22 12:52:36,245 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058754017 with entries=87, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058756183
2014-07-22 12:52:36,246 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:36,928 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2746, memsize=213.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/83144c5cd53a47f7910f0a7a1259f744
2014-07-22 12:52:36,984 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/83144c5cd53a47f7910f0a7a1259f744 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/83144c5cd53a47f7910f0a7a1259f744
2014-07-22 12:52:37,094 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/83144c5cd53a47f7910f0a7a1259f744, entries=777130, sequenceid=2746, filesize=55.4m
2014-07-22 12:52:37,095 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~384.7m/403376160, currentsize=218.6m/229178640 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 12083ms, sequenceid=2746, compaction requested=true
2014-07-22 12:52:37,095 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:37,096 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 12:52:37,096 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 12:52:37,096 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 478.5m
2014-07-22 12:52:37,096 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:37,096 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:37,096 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:52:37,439 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:39,405 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2737, memsize=277.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/339a23c279bd4e48b6ca4b9610711bbd
2014-07-22 12:52:39,432 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/339a23c279bd4e48b6ca4b9610711bbd as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/339a23c279bd4e48b6ca4b9610711bbd
2014-07-22 12:52:39,449 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/339a23c279bd4e48b6ca4b9610711bbd, entries=1011250, sequenceid=2737, filesize=72.0m
2014-07-22 12:52:39,450 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~455.2m/477302880, currentsize=246.3m/258248560 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15450ms, sequenceid=2737, compaction requested=true
2014-07-22 12:52:39,450 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:39,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 12:52:39,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 12:52:39,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:39,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:39,450 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 485.7m
2014-07-22 12:52:39,450 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:52:39,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:39,869 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:39,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14445 synced till here 14442
2014-07-22 12:52:39,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058756183 with entries=94, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058759708
2014-07-22 12:52:39,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:40,176 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:52:41,045 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:52:41,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:41,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14537 synced till here 14530
2014-07-22 12:52:41,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058759708 with entries=92, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058761341
2014-07-22 12:52:41,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:42,543 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:42,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14628 synced till here 14622
2014-07-22 12:52:42,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058761341 with entries=91, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058762544
2014-07-22 12:52:42,612 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:44,144 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:44,168 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14715 synced till here 14713
2014-07-22 12:52:44,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058762544 with entries=87, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058764144
2014-07-22 12:52:44,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:45,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:45,473 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14803 synced till here 14798
2014-07-22 12:52:45,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058764144 with entries=88, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058765002
2014-07-22 12:52:45,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:46,290 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:46,415 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14899 synced till here 14898
2014-07-22 12:52:46,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058765002 with entries=96, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058766290
2014-07-22 12:52:46,961 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:52:47,702 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2889, memsize=220.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/e51b4f4b56b74e34b4149927032684ab
2014-07-22 12:52:47,720 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/e51b4f4b56b74e34b4149927032684ab as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/e51b4f4b56b74e34b4149927032684ab
2014-07-22 12:52:47,747 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/e51b4f4b56b74e34b4149927032684ab, entries=803960, sequenceid=2889, filesize=57.3m
2014-07-22 12:52:47,748 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~478.5m/501722960, currentsize=175.3m/183805600 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 10652ms, sequenceid=2889, compaction requested=true
2014-07-22 12:52:47,748 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:47,748 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 12:52:47,749 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 12:52:47,749 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:47,749 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 360.8m
2014-07-22 12:52:47,749 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:47,749 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:52:48,007 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:48,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:49,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14996 synced till here 14990
2014-07-22 12:52:49,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058766290 with entries=97, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058768810
2014-07-22 12:52:50,852 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2895, memsize=229.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/198835575823462a90e617189c9b35c3
2014-07-22 12:52:50,871 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/198835575823462a90e617189c9b35c3 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/198835575823462a90e617189c9b35c3
2014-07-22 12:52:50,894 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/198835575823462a90e617189c9b35c3, entries=836610, sequenceid=2895, filesize=59.6m
2014-07-22 12:52:50,895 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~485.7m/509342480, currentsize=199.5m/209236640 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 11445ms, sequenceid=2895, compaction requested=true
2014-07-22 12:52:50,895 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:50,895 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 12:52:50,895 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 12:52:50,895 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:50,895 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 443.4m
2014-07-22 12:52:50,896 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:50,896 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:52:51,204 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:51,225 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:52:51,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058768810 with entries=85, filesize=60.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058771204
2014-07-22 12:52:54,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:55,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15180 synced till here 15176
2014-07-22 12:52:55,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058771204 with entries=99, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058774550
2014-07-22 12:52:55,241 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:52:55,886 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:52:56,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:56,142 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15273 synced till here 15268
2014-07-22 12:52:56,178 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058774550 with entries=93, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058776029
2014-07-22 12:52:57,779 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:58,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15371 synced till here 15369
2014-07-22 12:52:58,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058776029 with entries=98, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058777779
2014-07-22 12:52:59,698 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2996, memsize=271.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/5e7cf09bd1f74f8ca64a34ab6d4ddc11
2014-07-22 12:52:59,714 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/5e7cf09bd1f74f8ca64a34ab6d4ddc11 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/5e7cf09bd1f74f8ca64a34ab6d4ddc11
2014-07-22 12:52:59,725 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/5e7cf09bd1f74f8ca64a34ab6d4ddc11, entries=987220, sequenceid=2996, filesize=70.3m
2014-07-22 12:52:59,726 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~360.8m/378378160, currentsize=45.5m/47693600 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 11977ms, sequenceid=2996, compaction requested=true
2014-07-22 12:52:59,727 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:52:59,727 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 2000 blocking
2014-07-22 12:52:59,727 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-22 12:52:59,727 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:52:59,727 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 550.4m
2014-07-22 12:52:59,727 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:52:59,727 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:52:59,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:52:59,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15458 synced till here 15456
2014-07-22 12:52:59,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058777779 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058779740
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058615446
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058619177
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058621788
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058622640
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058624547
2014-07-22 12:52:59,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058626157
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058628306
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058630954
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058634195
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058636479
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058638673
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058707547
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058709311
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058712688
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058714547
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058716176
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058717387
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058724383
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058725761
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058727558
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058730471
2014-07-22 12:52:59,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058732143
2014-07-22 12:52:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058734035
2014-07-22 12:52:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058736035
2014-07-22 12:52:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058737749
2014-07-22 12:52:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058740990
2014-07-22 12:52:59,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743148
2014-07-22 12:53:00,113 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:01,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:01,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058779740 with entries=83, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058781734
2014-07-22 12:53:03,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:03,831 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15627 synced till here 15625
2014-07-22 12:53:03,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058781734 with entries=86, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058783806
2014-07-22 12:53:05,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:05,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15742 synced till here 15741
2014-07-22 12:53:05,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058783806 with entries=115, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058785221
2014-07-22 12:53:06,148 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3024, memsize=371.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/9201cdf3dcdf4249a03f2e437a3ea293
2014-07-22 12:53:06,162 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/9201cdf3dcdf4249a03f2e437a3ea293 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/9201cdf3dcdf4249a03f2e437a3ea293
2014-07-22 12:53:06,187 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/9201cdf3dcdf4249a03f2e437a3ea293, entries=1352640, sequenceid=3024, filesize=96.3m
2014-07-22 12:53:06,187 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~444.8m/466457360, currentsize=209.5m/219727200 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15292ms, sequenceid=3024, compaction requested=true
2014-07-22 12:53:06,188 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:06,189 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 12:53:06,189 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 12:53:06,189 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:06,189 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:06,189 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:53:06,189 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 431.6m
2014-07-22 12:53:06,518 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:07,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:07,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15826 synced till here 15824
2014-07-22 12:53:07,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058785221 with entries=84, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058787307
2014-07-22 12:53:17,437 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3102, memsize=487.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/259dde05dafe415aa48fb3183589d54d
2014-07-22 12:53:17,462 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/259dde05dafe415aa48fb3183589d54d as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/259dde05dafe415aa48fb3183589d54d
2014-07-22 12:53:17,479 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/259dde05dafe415aa48fb3183589d54d, entries=1775920, sequenceid=3102, filesize=126.5m
2014-07-22 12:53:17,479 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~551.9m/578683280, currentsize=126.1m/132210720 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 17752ms, sequenceid=3102, compaction requested=true
2014-07-22 12:53:17,480 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:17,480 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 12:53:17,480 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 12:53:17,480 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 445.4m
2014-07-22 12:53:17,480 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:17,480 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:17,480 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:53:17,824 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:19,107 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:53:19,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:19,938 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15912 synced till here 15911
2014-07-22 12:53:19,959 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058787307 with entries=86, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058799914
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058743996
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058745608
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058746866
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058749227
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058750714
2014-07-22 12:53:19,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058752273
2014-07-22 12:53:19,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058754017
2014-07-22 12:53:20,318 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3168, memsize=427.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/14af88652c89427db5e106414639a97b
2014-07-22 12:53:20,331 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/14af88652c89427db5e106414639a97b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/14af88652c89427db5e106414639a97b
2014-07-22 12:53:20,385 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/14af88652c89427db5e106414639a97b, entries=1554960, sequenceid=3168, filesize=110.7m
2014-07-22 12:53:20,386 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~431.6m/452558080, currentsize=52.6m/55188000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 14197ms, sequenceid=3168, compaction requested=true
2014-07-22 12:53:20,386 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:20,386 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 12:53:20,387 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 268.6m
2014-07-22 12:53:20,387 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 12:53:20,387 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:20,387 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:20,387 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:53:20,574 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:21,752 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:21,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058799914 with entries=84, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058801753
2014-07-22 12:53:22,688 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:23,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16090 synced till here 16080
2014-07-22 12:53:23,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058801753 with entries=94, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058802688
2014-07-22 12:53:24,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:25,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16187 synced till here 16172
2014-07-22 12:53:25,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058802688 with entries=97, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058804325
2014-07-22 12:53:26,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:27,317 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1030ms
GC pool 'ParNew' had collection(s): count=1 time=1192ms
2014-07-22 12:53:27,352 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16288 synced till here 16268
2014-07-22 12:53:27,465 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:53:27,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058804325 with entries=101, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058806090
2014-07-22 12:53:29,458 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:29,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16387 synced till here 16379
2014-07-22 12:53:29,536 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058806090 with entries=99, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058809458
2014-07-22 12:53:31,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:31,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16518 synced till here 16478
2014-07-22 12:53:31,749 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058809458 with entries=131, filesize=95.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058811481
2014-07-22 12:53:32,026 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:53:33,295 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:33,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16612 synced till here 16605
2014-07-22 12:53:33,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058811481 with entries=94, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058813296
2014-07-22 12:53:35,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:35,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16722 synced till here 16702
2014-07-22 12:53:35,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058813296 with entries=110, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058815147
2014-07-22 12:53:36,066 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3199, memsize=265.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a92d19d61d834e078d137f458d03ae91
2014-07-22 12:53:36,081 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a92d19d61d834e078d137f458d03ae91 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a92d19d61d834e078d137f458d03ae91
2014-07-22 12:53:36,099 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a92d19d61d834e078d137f458d03ae91, entries=966580, sequenceid=3199, filesize=68.8m
2014-07-22 12:53:36,099 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~270.1m/283221600, currentsize=227.4m/238404720 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15712ms, sequenceid=3199, compaction requested=true
2014-07-22 12:53:36,101 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:36,101 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 12:53:36,101 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 392.6m
2014-07-22 12:53:36,102 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 12:53:36,102 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:36,102 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:36,103 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:53:37,833 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:37,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:38,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16842 synced till here 16830
2014-07-22 12:53:38,044 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:53:38,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058815147 with entries=120, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058817987
2014-07-22 12:53:39,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:39,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16963 synced till here 16957
2014-07-22 12:53:39,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058817987 with entries=121, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058819541
2014-07-22 12:53:41,026 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:41,075 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17057 synced till here 17053
2014-07-22 12:53:41,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058819541 with entries=94, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058821027
2014-07-22 12:53:41,778 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3183, memsize=440.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2529bd14cec34d13b0bbdc366b38f8aa
2014-07-22 12:53:41,808 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2529bd14cec34d13b0bbdc366b38f8aa as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2529bd14cec34d13b0bbdc366b38f8aa
2014-07-22 12:53:41,819 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2529bd14cec34d13b0bbdc366b38f8aa, entries=1604500, sequenceid=3183, filesize=114.2m
2014-07-22 12:53:41,820 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~445.4m/467085840, currentsize=381.0m/399554160 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 24340ms, sequenceid=3183, compaction requested=true
2014-07-22 12:53:41,820 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:41,820 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 12:53:41,820 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 401.8m
2014-07-22 12:53:41,820 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 12:53:41,821 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:41,821 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:41,821 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:53:41,832 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:53:42,130 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:42,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:42,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17146 synced till here 17144
2014-07-22 12:53:42,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058821027 with entries=89, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058822431
2014-07-22 12:53:42,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058756183
2014-07-22 12:53:42,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058759708
2014-07-22 12:53:42,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058761341
2014-07-22 12:53:42,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058762544
2014-07-22 12:53:42,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058764144
2014-07-22 12:53:42,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058765002
2014-07-22 12:53:43,820 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:44,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17249 synced till here 17243
2014-07-22 12:53:44,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058822431 with entries=103, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058823821
2014-07-22 12:53:45,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:46,703 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1079ms
GC pool 'ParNew' had collection(s): count=1 time=1128ms
2014-07-22 12:53:46,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17348 synced till here 17337
2014-07-22 12:53:46,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058823821 with entries=99, filesize=75.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058825473
2014-07-22 12:53:48,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:48,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17454 synced till here 17436
2014-07-22 12:53:48,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058825473 with entries=106, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058828479
2014-07-22 12:53:50,437 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:50,484 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17571 synced till here 17549
2014-07-22 12:53:50,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058828479 with entries=117, filesize=85.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058830438
2014-07-22 12:53:51,237 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:51,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17671 synced till here 17667
2014-07-22 12:53:51,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058830438 with entries=100, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058831237
2014-07-22 12:53:52,631 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3364, memsize=240.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/5a54ef86161c4d828ff54322aa6defc9
2014-07-22 12:53:52,646 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/5a54ef86161c4d828ff54322aa6defc9 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/5a54ef86161c4d828ff54322aa6defc9
2014-07-22 12:53:52,657 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/5a54ef86161c4d828ff54322aa6defc9, entries=876590, sequenceid=3364, filesize=62.5m
2014-07-22 12:53:52,658 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~404.7m/424321920, currentsize=304.6m/319447920 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 16557ms, sequenceid=3364, compaction requested=true
2014-07-22 12:53:52,658 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:52,658 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 12:53:52,658 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 549.7m
2014-07-22 12:53:52,659 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 12:53:52,659 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:52,659 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:52,659 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:53:52,678 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:53:52,681 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:53:52,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:52,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17761 synced till here 17752
2014-07-22 12:53:52,845 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058831237 with entries=90, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058832751
2014-07-22 12:53:53,846 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3431, memsize=149.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/1b265781bfc84b03aa937548fae9968f
2014-07-22 12:53:53,860 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/1b265781bfc84b03aa937548fae9968f as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/1b265781bfc84b03aa937548fae9968f
2014-07-22 12:53:53,900 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:54,065 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/1b265781bfc84b03aa937548fae9968f, entries=545100, sequenceid=3431, filesize=38.8m
2014-07-22 12:53:54,066 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~403.2m/422795840, currentsize=241.2m/252925680 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 12246ms, sequenceid=3431, compaction requested=true
2014-07-22 12:53:54,067 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:53:54,067 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 12:53:54,067 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 616.3m
2014-07-22 12:53:54,067 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 12:53:54,067 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:53:54,067 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:53:54,067 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:53:54,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:54,116 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17852 synced till here 17847
2014-07-22 12:53:54,162 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058832751 with entries=91, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058834099
2014-07-22 12:53:55,105 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:53:55,158 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:53:55,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:55,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17938 synced till here 17937
2014-07-22 12:53:55,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058834099 with entries=86, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058835410
2014-07-22 12:53:56,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:56,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18038 synced till here 18029
2014-07-22 12:53:57,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058835410 with entries=100, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058836776
2014-07-22 12:53:58,484 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:58,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18136 synced till here 18126
2014-07-22 12:53:58,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058836776 with entries=98, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058838485
2014-07-22 12:53:58,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:53:59,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:53:59,786 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18228 synced till here 18225
2014-07-22 12:53:59,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058838485 with entries=92, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058839749
2014-07-22 12:53:59,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:00,649 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:01,301 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18320 synced till here 18309
2014-07-22 12:54:01,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058839749 with entries=92, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058840649
2014-07-22 12:54:01,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:02,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:02,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18419 synced till here 18415
2014-07-22 12:54:02,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058840649 with entries=99, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058842447
2014-07-22 12:54:02,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:03,903 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:03,929 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18506 synced till here 18500
2014-07-22 12:54:04,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058842447 with entries=87, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058843904
2014-07-22 12:54:04,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:05,356 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3560, memsize=183.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c4450761938a4c9894f3d09b92a8117c
2014-07-22 12:54:05,371 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c4450761938a4c9894f3d09b92a8117c as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c4450761938a4c9894f3d09b92a8117c
2014-07-22 12:54:05,393 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c4450761938a4c9894f3d09b92a8117c, entries=667200, sequenceid=3560, filesize=47.6m
2014-07-22 12:54:05,393 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~559.1m/586300160, currentsize=247.3m/259353920 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 12735ms, sequenceid=3560, compaction requested=true
2014-07-22 12:54:05,394 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:05,394 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 12:54:05,394 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 12:54:05,394 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 557.9m
2014-07-22 12:54:05,394 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:05,394 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:05,394 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:54:05,671 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:54:05,774 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:07,084 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1004ms
GC pool 'ParNew' had collection(s): count=1 time=1083ms
2014-07-22 12:54:07,087 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:07,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18626 synced till here 18603
2014-07-22 12:54:07,243 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058843904 with entries=120, filesize=91.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058845774
2014-07-22 12:54:07,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:09,070 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:09,120 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18732 synced till here 18706
2014-07-22 12:54:09,478 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058845774 with entries=106, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058849071
2014-07-22 12:54:09,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:54:11,084 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3583, memsize=238.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/0ccc3c786f744d949fadc8ca630a1c4a
2014-07-22 12:54:11,099 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/0ccc3c786f744d949fadc8ca630a1c4a as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/0ccc3c786f744d949fadc8ca630a1c4a
2014-07-22 12:54:11,113 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/0ccc3c786f744d949fadc8ca630a1c4a, entries=867520, sequenceid=3583, filesize=61.8m
2014-07-22 12:54:11,114 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~619.7m/649759360, currentsize=302.4m/317065120 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 17047ms, sequenceid=3583, compaction requested=true
2014-07-22 12:54:11,114 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:11,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 12:54:11,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 12:54:11,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:11,114 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 351.4m
2014-07-22 12:54:11,114 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:11,114 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:54:11,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:11,401 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:54:11,433 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18856 synced till here 18828
2014-07-22 12:54:11,632 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058849071 with entries=124, filesize=93.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058851399
2014-07-22 12:54:11,955 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:13,378 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:13,419 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18978 synced till here 18945
2014-07-22 12:54:13,663 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058851399 with entries=122, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058853379
2014-07-22 12:54:14,982 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:15,030 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19084 synced till here 19071
2014-07-22 12:54:15,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058853379 with entries=106, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058854983
2014-07-22 12:54:17,050 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:17,109 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19188 synced till here 19171
2014-07-22 12:54:17,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058854983 with entries=104, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058857051
2014-07-22 12:54:18,909 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:19,090 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19295 synced till here 19294
2014-07-22 12:54:19,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058857051 with entries=107, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058858912
2014-07-22 12:54:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 12:54:20,489 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:20,507 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19381 synced till here 19376
2014-07-22 12:54:20,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058858912 with entries=86, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058860489
2014-07-22 12:54:21,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:21,755 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19489 synced till here 19487
2014-07-22 12:54:21,776 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058860489 with entries=108, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058861230
2014-07-22 12:54:22,488 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:22,508 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19575 synced till here 19574
2014-07-22 12:54:22,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058861230 with entries=86, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058862488
2014-07-22 12:54:23,724 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:23,769 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19664 synced till here 19657
2014-07-22 12:54:23,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058862488 with entries=89, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058863724
2014-07-22 12:54:24,892 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:25,055 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3754, memsize=210.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/1f65852ad0224bf3ac5e955ec906d621
2014-07-22 12:54:25,130 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/1f65852ad0224bf3ac5e955ec906d621 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/1f65852ad0224bf3ac5e955ec906d621
2014-07-22 12:54:25,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19767 synced till here 19764
2014-07-22 12:54:25,172 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/1f65852ad0224bf3ac5e955ec906d621, entries=768080, sequenceid=3754, filesize=54.7m
2014-07-22 12:54:25,173 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~352.7m/369791120, currentsize=95.1m/99674240 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 14059ms, sequenceid=3754, compaction requested=true
2014-07-22 12:54:25,173 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:25,173 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 2000 blocking
2014-07-22 12:54:25,174 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-22 12:54:25,174 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 839.1m
2014-07-22 12:54:25,174 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:25,174 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:25,174 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:54:25,206 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058863724 with entries=103, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058864893
2014-07-22 12:54:25,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058766290
2014-07-22 12:54:25,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058768810
2014-07-22 12:54:25,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058771204
2014-07-22 12:54:25,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058774550
2014-07-22 12:54:25,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058776029
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058777779
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058779740
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058781734
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058783806
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058785221
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058787307
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058799914
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058801753
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058802688
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058804325
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058806090
2014-07-22 12:54:25,207 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058809458
2014-07-22 12:54:25,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058811481
2014-07-22 12:54:25,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058813296
2014-07-22 12:54:26,157 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:26,964 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3725, memsize=341.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b58b79d347f6490099ec4e68df3816b8
2014-07-22 12:54:26,976 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b58b79d347f6490099ec4e68df3816b8 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b58b79d347f6490099ec4e68df3816b8
2014-07-22 12:54:26,988 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b58b79d347f6490099ec4e68df3816b8, entries=1243630, sequenceid=3725, filesize=88.5m
2014-07-22 12:54:26,989 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~560.9m/588142800, currentsize=393.0m/412048160 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 21595ms, sequenceid=3725, compaction requested=true
2014-07-22 12:54:26,989 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:26,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 12:54:26,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 12:54:26,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:26,989 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 648.8m
2014-07-22 12:54:26,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:26,990 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:54:27,110 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:54:27,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:27,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19851 synced till here 19850
2014-07-22 12:54:27,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058864893 with entries=84, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058867263
2014-07-22 12:54:27,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058815147
2014-07-22 12:54:27,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058817987
2014-07-22 12:54:27,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058819541
2014-07-22 12:54:27,488 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:28,928 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:28,952 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058867263 with entries=83, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058868928
2014-07-22 12:54:30,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:30,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20019 synced till here 20018
2014-07-22 12:54:30,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058868928 with entries=85, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058870109
2014-07-22 12:54:32,484 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:32,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20103 synced till here 20102
2014-07-22 12:54:32,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058870109 with entries=84, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058872485
2014-07-22 12:54:33,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:33,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058872485 with entries=84, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058873679
2014-07-22 12:54:35,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:35,629 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20272 synced till here 20271
2014-07-22 12:54:35,646 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058873679 with entries=85, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058875613
2014-07-22 12:54:37,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:38,102 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20372 synced till here 20360
2014-07-22 12:54:38,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058875613 with entries=100, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058877598
2014-07-22 12:54:39,643 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:39,670 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20459 synced till here 20458
2014-07-22 12:54:39,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058877598 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058879644
2014-07-22 12:54:41,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:41,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20595 synced till here 20586
2014-07-22 12:54:41,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058879644 with entries=136, filesize=104.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058881264
2014-07-22 12:54:43,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:44,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20700 synced till here 20691
2014-07-22 12:54:45,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058881264 with entries=105, filesize=76.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058883966
2014-07-22 12:54:48,171 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1198ms
GC pool 'ParNew' had collection(s): count=1 time=1517ms
2014-07-22 12:54:48,179 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:48,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20806 synced till here 20787
2014-07-22 12:54:48,394 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058883966 with entries=106, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058888179
2014-07-22 12:54:50,363 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:50,377 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,377 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,385 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,389 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20924 synced till here 20899
2014-07-22 12:54:50,391 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,392 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,395 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,405 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,409 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,409 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,439 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,470 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,501 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,503 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,503 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,503 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,503 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,504 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,504 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,506 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,506 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,506 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,507 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,507 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,507 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,507 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,509 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,509 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,509 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,510 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,510 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,517 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,517 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,532 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,552 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058888179 with entries=118, filesize=90.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058890363
2014-07-22 12:54:50,565 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,587 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,588 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,588 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,589 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,597 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,611 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,612 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,612 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,613 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,629 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,660 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,693 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,723 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,755 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,788 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:54:50,799 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3979, memsize=373.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/65241a299fe047058fc598398840f8ce
2014-07-22 12:54:50,986 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/65241a299fe047058fc598398840f8ce as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/65241a299fe047058fc598398840f8ce
2014-07-22 12:54:50,997 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/65241a299fe047058fc598398840f8ce, entries=1358330, sequenceid=3979, filesize=96.6m
2014-07-22 12:54:50,997 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~648.8m/680351280, currentsize=329.6m/345632000 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 24008ms, sequenceid=3979, compaction requested=true
2014-07-22 12:54:50,998 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:50,998 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 12:54:50,998 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 12:54:50,998 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-07-22 12:54:50,998 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:50,998 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 964.0m
2014-07-22 12:54:50,998 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:50,998 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:50,998 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 243ms
2014-07-22 12:54:50,998 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:50,998 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:54:50,999 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 276ms
2014-07-22 12:54:50,999 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:50,999 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 307ms
2014-07-22 12:54:50,999 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,005 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 345ms
2014-07-22 12:54:51,005 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,006 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 376ms
2014-07-22 12:54:51,006 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,007 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 395ms
2014-07-22 12:54:51,007 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,008 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 397ms
2014-07-22 12:54:51,008 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,008 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 397ms
2014-07-22 12:54:51,008 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,013 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 402ms
2014-07-22 12:54:51,013 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,013 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 416ms
2014-07-22 12:54:51,014 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,014 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 425ms
2014-07-22 12:54:51,014 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,016 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 428ms
2014-07-22 12:54:51,016 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,017 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 430ms
2014-07-22 12:54:51,017 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,031 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 444ms
2014-07-22 12:54:51,032 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,032 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 467ms
2014-07-22 12:54:51,032 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,037 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 505ms
2014-07-22 12:54:51,037 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,037 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 520ms
2014-07-22 12:54:51,037 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,037 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 520ms
2014-07-22 12:54:51,037 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,037 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 527ms
2014-07-22 12:54:51,037 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,037 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 527ms
2014-07-22 12:54:51,038 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,038 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 529ms
2014-07-22 12:54:51,038 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,069 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 560ms
2014-07-22 12:54:51,069 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,070 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 561ms
2014-07-22 12:54:51,070 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,070 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 564ms
2014-07-22 12:54:51,070 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,070 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 564ms
2014-07-22 12:54:51,070 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,070 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 564ms
2014-07-22 12:54:51,070 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,071 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 565ms
2014-07-22 12:54:51,071 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,076 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-07-22 12:54:51,076 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,076 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-07-22 12:54:51,076 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,077 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 572ms
2014-07-22 12:54:51,077 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,086 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 12:54:51,086 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,086 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 12:54:51,086 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,086 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 12:54:51,086 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,089 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 586ms
2014-07-22 12:54:51,089 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,089 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 586ms
2014-07-22 12:54:51,089 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,093 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 592ms
2014-07-22 12:54:51,093 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,094 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 594ms
2014-07-22 12:54:51,094 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,094 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 624ms
2014-07-22 12:54:51,094 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,097 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 658ms
2014-07-22 12:54:51,097 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,097 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 688ms
2014-07-22 12:54:51,098 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,104 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 695ms
2014-07-22 12:54:51,104 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,109 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 704ms
2014-07-22 12:54:51,109 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,110 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 715ms
2014-07-22 12:54:51,110 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,110 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 719ms
2014-07-22 12:54:51,110 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,117 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 726ms
2014-07-22 12:54:51,117 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,117 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 728ms
2014-07-22 12:54:51,118 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,125 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 745ms
2014-07-22 12:54:51,125 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,125 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 748ms
2014-07-22 12:54:51,126 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:51,136 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:54:51,137 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 761ms
2014-07-22 12:54:51,137 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:54:53,215 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:53,495 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:53,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21049 synced till here 21012
2014-07-22 12:54:53,666 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3971, memsize=443.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/667ae24c1b3146e583ae7f8ac40d9ca2
2014-07-22 12:54:53,678 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/667ae24c1b3146e583ae7f8ac40d9ca2 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/667ae24c1b3146e583ae7f8ac40d9ca2
2014-07-22 12:54:53,692 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/667ae24c1b3146e583ae7f8ac40d9ca2, entries=1614290, sequenceid=3971, filesize=114.9m
2014-07-22 12:54:53,693 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~842.3m/883231360, currentsize=420.0m/440428560 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 28519ms, sequenceid=3971, compaction requested=true
2014-07-22 12:54:53,694 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:54:53,694 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 12:54:53,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 12:54:53,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:54:53,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:54:53,695 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:54:53,695 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 765.2m
2014-07-22 12:54:53,707 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:54:55,233 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1108ms
GC pool 'ParNew' had collection(s): count=1 time=1419ms
2014-07-22 12:54:55,550 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058890363 with entries=125, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058893496
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058821027
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058822431
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058823821
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058825473
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058828479
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058830438
2014-07-22 12:54:55,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058831237
2014-07-22 12:54:56,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:54:57,538 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1303ms
GC pool 'ParNew' had collection(s): count=1 time=1323ms
2014-07-22 12:54:57,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21190 synced till here 21148
2014-07-22 12:54:57,661 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:54:57,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058893496 with entries=141, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058896124
2014-07-22 12:55:00,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:00,344 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21326 synced till here 21295
2014-07-22 12:55:00,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058896124 with entries=136, filesize=101.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058900262
2014-07-22 12:55:02,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:04,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21460 synced till here 21441
2014-07-22 12:55:04,194 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058900262 with entries=134, filesize=98.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058902846
2014-07-22 12:55:06,074 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:06,132 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21587 synced till here 21561
2014-07-22 12:55:06,260 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:55:06,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058902846 with entries=127, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058906074
2014-07-22 12:55:07,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:08,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21698 synced till here 21681
2014-07-22 12:55:08,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058906074 with entries=111, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058907215
2014-07-22 12:55:09,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:09,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21819 synced till here 21795
2014-07-22 12:55:10,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058907215 with entries=121, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058909031
2014-07-22 12:55:10,894 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:10,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21919 synced till here 21900
2014-07-22 12:55:11,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058909031 with entries=100, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058910894
2014-07-22 12:55:12,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:12,778 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,778 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,780 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,780 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,781 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,784 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,784 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,784 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,787 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,788 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,790 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,790 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,790 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,791 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,793 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,795 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,800 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,801 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,836 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,838 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,879 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,879 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,880 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,881 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,883 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,889 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,889 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,892 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,892 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,893 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,894 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,895 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,934 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,936 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,939 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,997 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:12,999 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,000 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,000 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,002 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,003 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22053 synced till here 22049
2014-07-22 12:55:13,026 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,028 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,029 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,030 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,030 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,086 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,086 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,087 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,087 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:13,096 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058910894 with entries=134, filesize=105.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058912726
2014-07-22 12:55:17,778 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,779 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,780 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,780 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,781 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,784 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,784 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,784 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,787 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,788 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,790 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,790 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 12:55:17,790 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,791 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,793 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,795 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,800 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,801 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,836 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,839 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,879 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,880 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,880 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,881 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,883 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,889 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,890 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,893 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,893 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,894 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,894 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,895 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,935 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:17,936 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,939 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:17,997 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:18,000 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,001 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,002 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,003 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,004 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,027 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:55:18,029 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,030 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,030 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:55:18,087 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5056ms
2014-07-22 12:55:18,087 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5055ms
2014-07-22 12:55:18,087 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5057ms
2014-07-22 12:55:18,087 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5055ms
2014-07-22 12:55:18,088 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5057ms
2014-07-22 12:55:22,779 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:55:22,780 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:55:22,781 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,781 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,782 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,785 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,785 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,785 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,788 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,789 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,791 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,791 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,792 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-22 12:55:22,792 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:55:22,793 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,796 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,801 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,802 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,837 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,839 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,879 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,880 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,881 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,882 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,884 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,890 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,890 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,893 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,893 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,894 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,895 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,895 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:22,935 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,936 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,939 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:22,998 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:55:23,001 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:55:23,001 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,002 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:55:23,003 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,004 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,027 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,029 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,030 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,031 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:55:23,087 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10057ms
2014-07-22 12:55:23,087 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10055ms
2014-07-22 12:55:23,088 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10058ms
2014-07-22 12:55:23,088 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10056ms
2014-07-22 12:55:23,089 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10058ms
2014-07-22 12:55:25,302 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4227, memsize=491.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/1d03a44797524cb9a39c0a5f407ac002
2014-07-22 12:55:25,314 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/1d03a44797524cb9a39c0a5f407ac002 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/1d03a44797524cb9a39c0a5f407ac002
2014-07-22 12:55:25,326 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/1d03a44797524cb9a39c0a5f407ac002, entries=1789920, sequenceid=4227, filesize=127.5m
2014-07-22 12:55:25,327 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~780.7m/818650960, currentsize=318.8m/334256640 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 31631ms, sequenceid=4227, compaction requested=true
2014-07-22 12:55:25,327 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:55:25,328 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 12:55:25,328 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 12:55:25,328 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:55:25,328 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12298ms
2014-07-22 12:55:25,328 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,328 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:55:25,328 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12296ms
2014-07-22 12:55:25,329 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,328 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 676.7m
2014-07-22 12:55:25,329 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12299ms
2014-07-22 12:55:25,329 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,329 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12297ms
2014-07-22 12:55:25,330 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,334 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12303ms
2014-07-22 12:55:25,334 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,334 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12305ms
2014-07-22 12:55:25,334 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,335 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12306ms
2014-07-22 12:55:25,335 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,335 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12307ms
2014-07-22 12:55:25,335 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,329 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:55:25,342 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12316ms
2014-07-22 12:55:25,342 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,342 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12339ms
2014-07-22 12:55:25,342 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,342 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12340ms
2014-07-22 12:55:25,342 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,343 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12343ms
2014-07-22 12:55:25,343 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,362 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12362ms
2014-07-22 12:55:25,362 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,363 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12364ms
2014-07-22 12:55:25,363 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,365 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12368ms
2014-07-22 12:55:25,365 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,366 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12427ms
2014-07-22 12:55:25,366 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,371 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12435ms
2014-07-22 12:55:25,371 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,371 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12437ms
2014-07-22 12:55:25,371 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,373 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12478ms
2014-07-22 12:55:25,373 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,377 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12484ms
2014-07-22 12:55:25,377 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,378 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12485ms
2014-07-22 12:55:25,379 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,385 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12493ms
2014-07-22 12:55:25,385 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,386 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12493ms
2014-07-22 12:55:25,386 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,386 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12497ms
2014-07-22 12:55:25,386 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,386 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12497ms
2014-07-22 12:55:25,387 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,389 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12506ms
2014-07-22 12:55:25,389 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,389 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12509ms
2014-07-22 12:55:25,389 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,390 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12509ms
2014-07-22 12:55:25,390 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,393 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12514ms
2014-07-22 12:55:25,393 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,397 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12519ms
2014-07-22 12:55:25,398 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,401 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12563ms
2014-07-22 12:55:25,401 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,405 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12569ms
2014-07-22 12:55:25,405 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,406 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12604ms
2014-07-22 12:55:25,406 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,406 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12606ms
2014-07-22 12:55:25,406 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,409 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12614ms
2014-07-22 12:55:25,409 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,409 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12616ms
2014-07-22 12:55:25,410 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,413 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12623ms
2014-07-22 12:55:25,413 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,414 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12627ms
2014-07-22 12:55:25,414 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,421 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12631ms
2014-07-22 12:55:25,421 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,421 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12631ms
2014-07-22 12:55:25,422 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,422 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12634ms
2014-07-22 12:55:25,422 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,422 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12635ms
2014-07-22 12:55:25,422 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,424 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12640ms
2014-07-22 12:55:25,424 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,428 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12644ms
2014-07-22 12:55:25,428 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,431 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12647ms
2014-07-22 12:55:25,432 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,432 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12651ms
2014-07-22 12:55:25,432 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,432 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12652ms
2014-07-22 12:55:25,432 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,432 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12652ms
2014-07-22 12:55:25,432 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,437 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12659ms
2014-07-22 12:55:25,437 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,437 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12660ms
2014-07-22 12:55:25,437 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:25,446 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15481,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909965,"queuetimems":4843,"class":"HRegionServer","responsesize":16827,"method":"Multi"}
2014-07-22 12:55:25,446 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909942,"queuetimems":5220,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 12:55:25,447 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909940,"queuetimems":5347,"class":"HRegionServer","responsesize":17028,"method":"Multi"}
2014-07-22 12:55:25,458 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15483,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909974,"queuetimems":3016,"class":"HRegionServer","responsesize":17035,"method":"Multi"}
2014-07-22 12:55:25,462 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909931,"queuetimems":5500,"class":"HRegionServer","responsesize":16899,"method":"Multi"}
2014-07-22 12:55:25,466 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15524,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909941,"queuetimems":5275,"class":"HRegionServer","responsesize":16950,"method":"Multi"}
2014-07-22 12:55:25,474 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910442,"queuetimems":2291,"class":"HRegionServer","responsesize":16820,"method":"Multi"}
2014-07-22 12:55:25,474 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15493,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909981,"queuetimems":2958,"class":"HRegionServer","responsesize":17094,"method":"Multi"}
2014-07-22 12:55:25,474 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909951,"queuetimems":5084,"class":"HRegionServer","responsesize":17329,"method":"Multi"}
2014-07-22 12:55:25,474 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909939,"queuetimems":5414,"class":"HRegionServer","responsesize":17189,"method":"Multi"}
2014-07-22 12:55:25,475 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910228,"queuetimems":3102,"class":"HRegionServer","responsesize":16763,"method":"Multi"}
2014-07-22 12:55:25,475 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15485,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909990,"queuetimems":2901,"class":"HRegionServer","responsesize":17129,"method":"Multi"}
2014-07-22 12:55:25,475 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909962,"queuetimems":4886,"class":"HRegionServer","responsesize":16713,"method":"Multi"}
2014-07-22 12:55:25,474 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909944,"queuetimems":5156,"class":"HRegionServer","responsesize":16813,"method":"Multi"}
2014-07-22 12:55:25,518 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058911124,"queuetimems":2602,"class":"HRegionServer","responsesize":16950,"method":"Multi"}
2014-07-22 12:55:25,518 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909982,"queuetimems":2926,"class":"HRegionServer","responsesize":17287,"method":"Multi"}
2014-07-22 12:55:25,523 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058911923,"queuetimems":3365,"class":"HRegionServer","responsesize":17002,"method":"Multi"}
2014-07-22 12:55:25,606 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:55:26,139 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:55:26,190 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:26,202 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910656,"queuetimems":2409,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 12:55:26,202 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16232,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909970,"queuetimems":3781,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-07-22 12:55:26,204 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16233,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909970,"queuetimems":3944,"class":"HRegionServer","responsesize":16813,"method":"Multi"}
2014-07-22 12:55:26,206 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15760,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910446,"queuetimems":2247,"class":"HRegionServer","responsesize":16895,"method":"Multi"}
2014-07-22 12:55:26,206 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910895,"queuetimems":2449,"class":"HRegionServer","responsesize":17189,"method":"Multi"}
2014-07-22 12:55:26,208 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058911124,"queuetimems":2638,"class":"HRegionServer","responsesize":17028,"method":"Multi"}
2014-07-22 12:55:26,208 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13737,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912470,"queuetimems":3780,"class":"HRegionServer","responsesize":16899,"method":"Multi"}
2014-07-22 12:55:26,222 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15782,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910439,"queuetimems":3247,"class":"HRegionServer","responsesize":17121,"method":"Multi"}
2014-07-22 12:55:26,230 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912462,"queuetimems":3805,"class":"HRegionServer","responsesize":16922,"method":"Multi"}
2014-07-22 12:55:26,234 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909966,"queuetimems":3977,"class":"HRegionServer","responsesize":16831,"method":"Multi"}
2014-07-22 12:55:26,238 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910246,"queuetimems":3087,"class":"HRegionServer","responsesize":16628,"method":"Multi"}
2014-07-22 12:55:26,238 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909962,"queuetimems":4958,"class":"HRegionServer","responsesize":17486,"method":"Multi"}
2014-07-22 12:55:26,238 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910856,"queuetimems":2506,"class":"HRegionServer","responsesize":16756,"method":"Multi"}
2014-07-22 12:55:26,246 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909970,"queuetimems":3969,"class":"HRegionServer","responsesize":16306,"method":"Multi"}
2014-07-22 12:55:26,250 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15594,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910656,"queuetimems":2443,"class":"HRegionServer","responsesize":16802,"method":"Multi"}
2014-07-22 12:55:26,258 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15412,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910846,"queuetimems":2561,"class":"HRegionServer","responsesize":16783,"method":"Multi"}
2014-07-22 12:55:26,262 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16310,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909951,"queuetimems":5050,"class":"HRegionServer","responsesize":17399,"method":"Multi"}
2014-07-22 12:55:26,266 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058910856,"queuetimems":2463,"class":"HRegionServer","responsesize":16948,"method":"Multi"}
2014-07-22 12:55:26,274 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058909958,"queuetimems":5024,"class":"HRegionServer","responsesize":17240,"method":"Multi"}
2014-07-22 12:55:27,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22173 synced till here 22150
2014-07-22 12:55:27,613 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912454,"queuetimems":3833,"class":"HRegionServer","responsesize":16769,"method":"Multi"}
2014-07-22 12:55:27,746 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912746,"queuetimems":3825,"class":"HRegionServer","responsesize":17471,"method":"Multi"}
2014-07-22 12:55:27,746 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15301,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912445,"queuetimems":3855,"class":"HRegionServer","responsesize":17205,"method":"Multi"}
2014-07-22 12:55:27,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058912726 with entries=120, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058926190
2014-07-22 12:55:28,063 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912751,"queuetimems":3757,"class":"HRegionServer","responsesize":17308,"method":"Multi"}
2014-07-22 12:55:28,588 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15860,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912728,"queuetimems":3930,"class":"HRegionServer","responsesize":17316,"method":"Multi"}
2014-07-22 12:55:28,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:28,837 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912889,"queuetimems":2113,"class":"HRegionServer","responsesize":16713,"method":"Multi"}
2014-07-22 12:55:28,893 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22291 synced till here 22265
2014-07-22 12:55:29,896 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17018,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912878,"queuetimems":2991,"class":"HRegionServer","responsesize":17137,"method":"Multi"}
2014-07-22 12:55:29,897 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912885,"queuetimems":2626,"class":"HRegionServer","responsesize":16306,"method":"Multi"}
2014-07-22 12:55:29,900 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912889,"queuetimems":2074,"class":"HRegionServer","responsesize":17329,"method":"Multi"}
2014-07-22 12:55:29,910 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912881,"queuetimems":2979,"class":"HRegionServer","responsesize":17152,"method":"Multi"}
2014-07-22 12:55:29,927 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912886,"queuetimems":2588,"class":"HRegionServer","responsesize":16813,"method":"Multi"}
2014-07-22 12:55:29,946 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17069,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912877,"queuetimems":3850,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-22 12:55:29,954 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17072,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912878,"queuetimems":3003,"class":"HRegionServer","responsesize":16360,"method":"Multi"}
2014-07-22 12:55:29,963 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058912735,"queuetimems":3863,"class":"HRegionServer","responsesize":16809,"method":"Multi"}
2014-07-22 12:55:29,982 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058926190 with entries=118, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058928835
2014-07-22 12:55:30,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:30,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22400 synced till here 22384
2014-07-22 12:55:30,842 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058928835 with entries=109, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058930708
2014-07-22 12:55:32,549 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4207, memsize=636.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c3eb2af585a345de9cad782735a497ca
2014-07-22 12:55:32,581 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c3eb2af585a345de9cad782735a497ca as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c3eb2af585a345de9cad782735a497ca
2014-07-22 12:55:32,604 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c3eb2af585a345de9cad782735a497ca, entries=2318050, sequenceid=4207, filesize=164.9m
2014-07-22 12:55:32,604 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~964.0m/1010846720, currentsize=466.0m/488605840 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 41606ms, sequenceid=4207, compaction requested=true
2014-07-22 12:55:32,605 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:55:32,605 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 12:55:32,605 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 837.8m
2014-07-22 12:55:32,605 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 12:55:32,606 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:55:32,606 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:55:32,606 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:55:33,123 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:55:33,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:33,376 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:55:33,531 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22522 synced till here 22516
2014-07-22 12:55:33,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058930708 with entries=122, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058933266
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058832751
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058834099
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058835410
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058836776
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058838485
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058839749
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058840649
2014-07-22 12:55:33,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058842447
2014-07-22 12:55:33,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058843904
2014-07-22 12:55:33,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058845774
2014-07-22 12:55:33,686 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:34,999 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:35,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22632 synced till here 22630
2014-07-22 12:55:35,260 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058933266 with entries=110, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058935000
2014-07-22 12:55:35,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:36,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:36,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22717 synced till here 22714
2014-07-22 12:55:37,023 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058935000 with entries=85, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058936794
2014-07-22 12:55:37,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:38,854 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:38,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22805 synced till here 22798
2014-07-22 12:55:38,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058936794 with entries=88, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058938855
2014-07-22 12:55:38,943 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:40,933 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:41,012 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22906 synced till here 22888
2014-07-22 12:55:41,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058938855 with entries=101, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058940934
2014-07-22 12:55:41,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:43,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:43,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23019 synced till here 22994
2014-07-22 12:55:43,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058940934 with entries=113, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058943444
2014-07-22 12:55:43,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:45,428 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:45,752 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23169 synced till here 23161
2014-07-22 12:55:46,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058943444 with entries=150, filesize=106.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058945430
2014-07-22 12:55:46,359 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:47,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:47,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23261 synced till here 23257
2014-07-22 12:55:47,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058945430 with entries=92, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058947829
2014-07-22 12:55:47,941 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:55:49,360 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,366 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,367 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,384 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,397 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,408 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,420 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,428 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,439 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,477 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,511 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,545 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,579 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,613 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,648 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,682 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,716 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,734 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4415, memsize=300.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/d29bba9794d6411990d2cc65175e3a9d
2014-07-22 12:55:49,748 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:49,750 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/d29bba9794d6411990d2cc65175e3a9d as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/d29bba9794d6411990d2cc65175e3a9d
2014-07-22 12:55:49,766 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/d29bba9794d6411990d2cc65175e3a9d, entries=1092410, sequenceid=4415, filesize=77.8m
2014-07-22 12:55:49,767 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~676.7m/709604320, currentsize=409.9m/429795760 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 24439ms, sequenceid=4415, compaction requested=true
2014-07-22 12:55:49,767 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:55:49,767 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 12:55:49,767 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19ms
2014-07-22 12:55:49,767 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,767 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 12:55:49,768 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 52ms
2014-07-22 12:55:49,768 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,767 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 426.5m
2014-07-22 12:55:49,768 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 86ms
2014-07-22 12:55:49,768 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,768 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:55:49,768 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 120ms
2014-07-22 12:55:49,768 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,768 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:55:49,769 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:55:49,769 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 156ms
2014-07-22 12:55:49,769 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,769 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 191ms
2014-07-22 12:55:49,769 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,769 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 224ms
2014-07-22 12:55:49,769 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,770 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 260ms
2014-07-22 12:55:49,770 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,773 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 297ms
2014-07-22 12:55:49,773 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,773 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 335ms
2014-07-22 12:55:49,774 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,774 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 346ms
2014-07-22 12:55:49,774 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,784 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 364ms
2014-07-22 12:55:49,785 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,785 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 377ms
2014-07-22 12:55:49,785 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,785 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 388ms
2014-07-22 12:55:49,785 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,785 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 401ms
2014-07-22 12:55:49,785 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,785 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 418ms
2014-07-22 12:55:49,785 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,785 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-07-22 12:55:49,786 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,786 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 426ms
2014-07-22 12:55:49,786 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:55:49,870 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:55:49,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:49,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23358 synced till here 23345
2014-07-22 12:55:50,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058947829 with entries=97, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058949899
2014-07-22 12:55:50,286 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:55:51,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:51,868 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23469 synced till here 23462
2014-07-22 12:55:51,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058949899 with entries=111, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058951662
2014-07-22 12:55:53,526 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:53,824 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23586 synced till here 23584
2014-07-22 12:55:53,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058951662 with entries=117, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058953527
2014-07-22 12:55:55,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:55,483 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23675 synced till here 23668
2014-07-22 12:55:55,555 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058953527 with entries=89, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058955451
2014-07-22 12:55:57,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:57,869 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23768 synced till here 23763
2014-07-22 12:55:57,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058955451 with entries=93, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058957836
2014-07-22 12:55:59,311 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,311 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,316 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,341 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,351 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,352 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,352 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,354 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,375 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,378 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,405 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:55:59,406 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,408 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,409 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,410 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,410 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,410 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,411 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,411 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,411 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,416 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058957836 with entries=82, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058959406
2014-07-22 12:55:59,444 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,475 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,506 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,536 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,569 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,606 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,637 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,669 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:55:59,872 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:00,694 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4511, memsize=311.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/4ddc3c56865a481da007c3f8f79647b8
2014-07-22 12:56:00,713 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/4ddc3c56865a481da007c3f8f79647b8 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/4ddc3c56865a481da007c3f8f79647b8
2014-07-22 12:56:00,732 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/4ddc3c56865a481da007c3f8f79647b8, entries=1133750, sequenceid=4511, filesize=80.8m
2014-07-22 12:56:00,732 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~837.8m/878517200, currentsize=437.4m/458647760 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 28127ms, sequenceid=4511, compaction requested=true
2014-07-22 12:56:00,732 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:56:00,733 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 12:56:00,733 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 862ms
2014-07-22 12:56:00,733 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 12:56:00,733 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,733 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 861.0m
2014-07-22 12:56:00,733 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:56:00,733 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1064ms
2014-07-22 12:56:00,733 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:56:00,733 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,733 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:56:00,734 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1097ms
2014-07-22 12:56:00,734 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,734 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1128ms
2014-07-22 12:56:00,734 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,735 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1166ms
2014-07-22 12:56:00,735 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,735 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1199ms
2014-07-22 12:56:00,735 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,741 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1235ms
2014-07-22 12:56:00,741 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,742 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1267ms
2014-07-22 12:56:00,742 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,742 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1298ms
2014-07-22 12:56:00,742 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,742 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1326ms
2014-07-22 12:56:00,742 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,746 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1334ms
2014-07-22 12:56:00,746 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,746 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1335ms
2014-07-22 12:56:00,746 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,746 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1335ms
2014-07-22 12:56:00,747 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,749 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1338ms
2014-07-22 12:56:00,749 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,749 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1339ms
2014-07-22 12:56:00,749 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,749 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1341ms
2014-07-22 12:56:00,749 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,749 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1343ms
2014-07-22 12:56:00,749 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,769 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1361ms
2014-07-22 12:56:00,769 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,770 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1364ms
2014-07-22 12:56:00,770 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,789 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1411ms
2014-07-22 12:56:00,789 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,790 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1415ms
2014-07-22 12:56:00,790 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,794 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1440ms
2014-07-22 12:56:00,794 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,800 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1448ms
2014-07-22 12:56:00,800 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,801 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1450ms
2014-07-22 12:56:00,801 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,801 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1450ms
2014-07-22 12:56:00,801 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,801 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1460ms
2014-07-22 12:56:00,801 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,809 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1493ms
2014-07-22 12:56:00,809 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,810 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1499ms
2014-07-22 12:56:00,810 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,810 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1499ms
2014-07-22 12:56:00,811 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:00,978 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:56:02,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:02,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23947 synced till here 23935
2014-07-22 12:56:02,565 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:56:02,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058959406 with entries=97, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058962336
2014-07-22 12:56:04,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:04,525 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4671, memsize=179.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/fe321f87168c4b4abf1b243c7d7521ab
2014-07-22 12:56:04,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24047 synced till here 24041
2014-07-22 12:56:04,561 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/fe321f87168c4b4abf1b243c7d7521ab as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/fe321f87168c4b4abf1b243c7d7521ab
2014-07-22 12:56:04,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058962336 with entries=100, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058964509
2014-07-22 12:56:04,622 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/fe321f87168c4b4abf1b243c7d7521ab, entries=652110, sequenceid=4671, filesize=46.5m
2014-07-22 12:56:04,623 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~426.5m/447183120, currentsize=63.9m/66985200 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 14856ms, sequenceid=4671, compaction requested=true
2014-07-22 12:56:04,625 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 2000 blocking
2014-07-22 12:56:04,625 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-22 12:56:04,625 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:56:04,625 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:56:04,625 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:56:04,625 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:56:04,626 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 959.4m
2014-07-22 12:56:05,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:05,449 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24159 synced till here 24151
2014-07-22 12:56:06,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058964509 with entries=112, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058965373
2014-07-22 12:56:06,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058849071
2014-07-22 12:56:06,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058851399
2014-07-22 12:56:06,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058853379
2014-07-22 12:56:06,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058854983
2014-07-22 12:56:06,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058857051
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058858912
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058860489
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058861230
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058862488
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058863724
2014-07-22 12:56:06,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058864893
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058867263
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058868928
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058870109
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058872485
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058873679
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058875613
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058877598
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058879644
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058881264
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058883966
2014-07-22 12:56:06,338 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058888179
2014-07-22 12:56:06,572 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:56:07,042 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:07,170 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24260 synced till here 24249
2014-07-22 12:56:07,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058965373 with entries=101, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058967043
2014-07-22 12:56:08,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:08,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24353 synced till here 24347
2014-07-22 12:56:08,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058967043 with entries=93, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058968720
2014-07-22 12:56:10,601 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:10,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24449 synced till here 24434
2014-07-22 12:56:10,850 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058968720 with entries=96, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058970602
2014-07-22 12:56:12,969 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:13,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24559 synced till here 24534
2014-07-22 12:56:14,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058970602 with entries=110, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058972970
2014-07-22 12:56:15,726 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:15,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24659 synced till here 24641
2014-07-22 12:56:17,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058972970 with entries=100, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058975726
2014-07-22 12:56:19,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:19,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24784 synced till here 24763
2014-07-22 12:56:19,962 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:19,964 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:19,964 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,010 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,011 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,011 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,012 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,014 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,014 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,015 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,018 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,020 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058975726 with entries=125, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058979789
2014-07-22 12:56:20,042 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,073 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,077 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,108 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,139 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,166 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,252 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,254 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,254 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,254 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,257 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,257 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,257 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,258 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,258 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,258 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,259 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,260 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,261 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,261 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,263 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,263 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,263 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,265 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,265 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,266 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,266 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,266 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,266 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,267 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,267 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,267 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,267 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,268 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,268 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,268 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,270 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:20,270 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:23,950 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1166ms
GC pool 'ParNew' had collection(s): count=1 time=1376ms
2014-07-22 12:56:24,963 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:24,964 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:24,965 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,010 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,011 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,013 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 12:56:25,013 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 12:56:25,014 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,014 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,016 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,018 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,020 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,042 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,073 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,077 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,108 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,140 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,166 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,252 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,255 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,255 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,255 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,257 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,257 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,258 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,258 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,259 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,260 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,260 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,260 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,262 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,262 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,263 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,264 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 12:56:25,264 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,265 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,266 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,266 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 12:56:25,267 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 12:56:25,267 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 12:56:25,267 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 12:56:25,267 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,267 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,268 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 12:56:25,268 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,268 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,268 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:56:25,268 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,270 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:25,271 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:56:26,352 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4783, memsize=279.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/ded0e9877708489696c5d9972e39d299
2014-07-22 12:56:26,365 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/ded0e9877708489696c5d9972e39d299 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/ded0e9877708489696c5d9972e39d299
2014-07-22 12:56:26,377 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/ded0e9877708489696c5d9972e39d299, entries=1017570, sequenceid=4783, filesize=72.5m
2014-07-22 12:56:26,378 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~861.0m/902825440, currentsize=302.4m/317051840 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 25645ms, sequenceid=4783, compaction requested=true
2014-07-22 12:56:26,378 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:56:26,378 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 12:56:26,378 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 12:56:26,378 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:56:26,379 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:56:26,379 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:56:26,378 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6108ms
2014-07-22 12:56:26,379 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,381 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6111ms
2014-07-22 12:56:26,381 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,381 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6113ms
2014-07-22 12:56:26,381 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,382 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6115ms
2014-07-22 12:56:26,382 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,382 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6115ms
2014-07-22 12:56:26,382 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,385 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6118ms
2014-07-22 12:56:26,385 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,385 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6122ms
2014-07-22 12:56:26,386 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,386 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6120ms
2014-07-22 12:56:26,386 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,386 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6120ms
2014-07-22 12:56:26,386 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,386 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6124ms
2014-07-22 12:56:26,387 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,379 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 856.8m
2014-07-22 12:56:26,401 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6139ms
2014-07-22 12:56:26,401 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,401 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6139ms
2014-07-22 12:56:26,401 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,402 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6140ms
2014-07-22 12:56:26,402 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,402 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6137ms
2014-07-22 12:56:26,402 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,410 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6145ms
2014-07-22 12:56:26,410 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,411 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6147ms
2014-07-22 12:56:26,411 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,415 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6154ms
2014-07-22 12:56:26,415 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,415 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6153ms
2014-07-22 12:56:26,415 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,416 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6155ms
2014-07-22 12:56:26,416 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,416 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6155ms
2014-07-22 12:56:26,416 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,416 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6157ms
2014-07-22 12:56:26,416 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,416 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6157ms
2014-07-22 12:56:26,416 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,421 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6163ms
2014-07-22 12:56:26,421 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,421 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6163ms
2014-07-22 12:56:26,421 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,422 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6163ms
2014-07-22 12:56:26,422 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,422 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6165ms
2014-07-22 12:56:26,422 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,422 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6165ms
2014-07-22 12:56:26,422 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,422 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6165ms
2014-07-22 12:56:26,422 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,422 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6168ms
2014-07-22 12:56:26,422 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,423 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6169ms
2014-07-22 12:56:26,423 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,433 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6179ms
2014-07-22 12:56:26,433 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,438 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6186ms
2014-07-22 12:56:26,438 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,438 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6272ms
2014-07-22 12:56:26,439 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,439 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6300ms
2014-07-22 12:56:26,439 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,439 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6331ms
2014-07-22 12:56:26,439 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,439 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6362ms
2014-07-22 12:56:26,439 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,439 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6366ms
2014-07-22 12:56:26,439 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,440 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6398ms
2014-07-22 12:56:26,440 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,440 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6420ms
2014-07-22 12:56:26,440 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,440 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6422ms
2014-07-22 12:56:26,440 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,441 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6426ms
2014-07-22 12:56:26,441 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,441 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6427ms
2014-07-22 12:56:26,441 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,441 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6427ms
2014-07-22 12:56:26,441 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,441 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6430ms
2014-07-22 12:56:26,441 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,442 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6431ms
2014-07-22 12:56:26,442 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,442 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6432ms
2014-07-22 12:56:26,442 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,442 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6432ms
2014-07-22 12:56:26,442 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,442 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6478ms
2014-07-22 12:56:26,442 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,442 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6478ms
2014-07-22 12:56:26,442 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,443 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6481ms
2014-07-22 12:56:26,443 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:26,446 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10882,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975563,"queuetimems":0,"class":"HRegionServer","responsesize":16932,"method":"Multi"}
2014-07-22 12:56:26,450 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10765,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975684,"queuetimems":0,"class":"HRegionServer","responsesize":16949,"method":"Multi"}
2014-07-22 12:56:26,450 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975454,"queuetimems":0,"class":"HRegionServer","responsesize":17104,"method":"Multi"}
2014-07-22 12:56:26,462 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975616,"queuetimems":1,"class":"HRegionServer","responsesize":16151,"method":"Multi"}
2014-07-22 12:56:26,487 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11001,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975486,"queuetimems":0,"class":"HRegionServer","responsesize":17302,"method":"Multi"}
2014-07-22 12:56:26,487 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975518,"queuetimems":0,"class":"HRegionServer","responsesize":16969,"method":"Multi"}
2014-07-22 12:56:26,494 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975377,"queuetimems":0,"class":"HRegionServer","responsesize":16523,"method":"Multi"}
2014-07-22 12:56:26,494 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975652,"queuetimems":0,"class":"HRegionServer","responsesize":16844,"method":"Multi"}
2014-07-22 12:56:26,706 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:56:26,722 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975804,"queuetimems":0,"class":"HRegionServer","responsesize":17109,"method":"Multi"}
2014-07-22 12:56:26,783 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11022,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975760,"queuetimems":0,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 12:56:27,005 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:27,008 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975901,"queuetimems":0,"class":"HRegionServer","responsesize":17209,"method":"Multi"}
2014-07-22 12:56:27,243 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24899 synced till here 24881
2014-07-22 12:56:28,462 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1010ms
GC pool 'ParNew' had collection(s): count=1 time=1182ms
2014-07-22 12:56:28,539 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977704,"queuetimems":0,"class":"HRegionServer","responsesize":17314,"method":"Multi"}
2014-07-22 12:56:28,543 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11172,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977371,"queuetimems":0,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-22 12:56:28,544 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977531,"queuetimems":0,"class":"HRegionServer","responsesize":16804,"method":"Multi"}
2014-07-22 12:56:28,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058979789 with entries=115, filesize=87.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058987005
2014-07-22 12:56:28,840 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10938,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977901,"queuetimems":0,"class":"HRegionServer","responsesize":16523,"method":"Multi"}
2014-07-22 12:56:28,842 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12989,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975853,"queuetimems":0,"class":"HRegionServer","responsesize":17155,"method":"Multi"}
2014-07-22 12:56:28,848 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11028,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977819,"queuetimems":0,"class":"HRegionServer","responsesize":17325,"method":"Multi"}
2014-07-22 12:56:28,858 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10701,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978156,"queuetimems":0,"class":"HRegionServer","responsesize":16623,"method":"Multi"}
2014-07-22 12:56:28,858 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10760,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978097,"queuetimems":4,"class":"HRegionServer","responsesize":17302,"method":"Multi"}
2014-07-22 12:56:28,866 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11002,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977863,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 12:56:28,878 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978058,"queuetimems":0,"class":"HRegionServer","responsesize":16949,"method":"Multi"}
2014-07-22 12:56:28,878 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10860,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978017,"queuetimems":0,"class":"HRegionServer","responsesize":16151,"method":"Multi"}
2014-07-22 12:56:28,878 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10895,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977982,"queuetimems":0,"class":"HRegionServer","responsesize":17050,"method":"Multi"}
2014-07-22 12:56:28,890 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11119,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977770,"queuetimems":27,"class":"HRegionServer","responsesize":16952,"method":"Multi"}
2014-07-22 12:56:28,890 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977381,"queuetimems":0,"class":"HRegionServer","responsesize":16599,"method":"Multi"}
2014-07-22 12:56:28,891 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13173,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058975717,"queuetimems":0,"class":"HRegionServer","responsesize":16641,"method":"Multi"}
2014-07-22 12:56:28,891 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977569,"queuetimems":0,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 12:56:28,891 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11228,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977663,"queuetimems":0,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 12:56:28,901 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977625,"queuetimems":0,"class":"HRegionServer","responsesize":17170,"method":"Multi"}
2014-07-22 12:56:29,050 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058977945,"queuetimems":0,"class":"HRegionServer","responsesize":16641,"method":"Multi"}
2014-07-22 12:56:29,118 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:56:29,155 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11029,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978125,"queuetimems":0,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 12:56:29,162 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978194,"queuetimems":0,"class":"HRegionServer","responsesize":17104,"method":"Multi"}
2014-07-22 12:56:29,170 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058978230,"queuetimems":0,"class":"HRegionServer","responsesize":16844,"method":"Multi"}
2014-07-22 12:56:29,302 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:29,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25008 synced till here 24983
2014-07-22 12:56:29,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058987005 with entries=109, filesize=82.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058989302
2014-07-22 12:56:30,681 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10429,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980251,"queuetimems":43,"class":"HRegionServer","responsesize":17109,"method":"Multi"}
2014-07-22 12:56:30,681 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10673,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980007,"queuetimems":1737,"class":"HRegionServer","responsesize":16932,"method":"Multi"}
2014-07-22 12:56:30,832 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10792,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980039,"queuetimems":0,"class":"HRegionServer","responsesize":16804,"method":"Multi"}
2014-07-22 12:56:30,834 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10825,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980008,"queuetimems":1656,"class":"HRegionServer","responsesize":17325,"method":"Multi"}
2014-07-22 12:56:30,832 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980251,"queuetimems":77,"class":"HRegionServer","responsesize":17155,"method":"Multi"}
2014-07-22 12:56:30,842 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980008,"queuetimems":219,"class":"HRegionServer","responsesize":16952,"method":"Multi"}
2014-07-22 12:56:30,843 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980015,"queuetimems":9,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 12:56:30,843 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10834,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980009,"queuetimems":174,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 12:56:30,844 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980137,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 12:56:30,844 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980010,"queuetimems":142,"class":"HRegionServer","responsesize":17021,"method":"Multi"}
2014-07-22 12:56:30,854 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10747,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980106,"queuetimems":0,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 12:56:31,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:31,031 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11017,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980014,"queuetimems":49,"class":"HRegionServer","responsesize":17170,"method":"Multi"}
2014-07-22 12:56:31,032 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11025,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980007,"queuetimems":1692,"class":"HRegionServer","responsesize":16969,"method":"Multi"}
2014-07-22 12:56:31,033 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980074,"queuetimems":0,"class":"HRegionServer","responsesize":17314,"method":"Multi"}
2014-07-22 12:56:31,053 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11042,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406058980011,"queuetimems":107,"class":"HRegionServer","responsesize":16599,"method":"Multi"}
2014-07-22 12:56:31,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25142 synced till here 25106
2014-07-22 12:56:31,200 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4831, memsize=279.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/075a613aa46b4b8c9058f641dea17947
2014-07-22 12:56:31,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/075a613aa46b4b8c9058f641dea17947 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/075a613aa46b4b8c9058f641dea17947
2014-07-22 12:56:31,220 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/075a613aa46b4b8c9058f641dea17947, entries=1016790, sequenceid=4831, filesize=72.4m
2014-07-22 12:56:31,221 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~965.7m/1012605280, currentsize=332.4m/348527040 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 26595ms, sequenceid=4831, compaction requested=true
2014-07-22 12:56:31,221 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:56:31,222 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 12:56:31,222 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 826.4m
2014-07-22 12:56:31,222 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 12:56:31,222 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:56:31,222 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:56:31,222 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:56:31,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058989302 with entries=134, filesize=95.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058991030
2014-07-22 12:56:31,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058890363
2014-07-22 12:56:31,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058893496
2014-07-22 12:56:31,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058896124
2014-07-22 12:56:31,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058900262
2014-07-22 12:56:31,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058902846
2014-07-22 12:56:31,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058906074
2014-07-22 12:56:31,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058907215
2014-07-22 12:56:31,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058909031
2014-07-22 12:56:31,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058910894
2014-07-22 12:56:32,723 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:56:33,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:33,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25248 synced till here 25231
2014-07-22 12:56:33,403 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058991030 with entries=106, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058993221
2014-07-22 12:56:33,860 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:56:34,818 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:35,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25372 synced till here 25368
2014-07-22 12:56:35,096 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058993221 with entries=124, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058994819
2014-07-22 12:56:37,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:37,182 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058994819 with entries=84, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058997073
2014-07-22 12:56:38,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:38,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25549 synced till here 25540
2014-07-22 12:56:38,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058997073 with entries=93, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058998703
2014-07-22 12:56:39,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:39,754 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25639 synced till here 25637
2014-07-22 12:56:39,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058998703 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058999733
2014-07-22 12:56:41,722 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:41,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25724 synced till here 25722
2014-07-22 12:56:41,759 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058999733 with entries=85, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059001722
2014-07-22 12:56:43,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:43,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25809 synced till here 25807
2014-07-22 12:56:43,135 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059001722 with entries=85, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059003084
2014-07-22 12:56:45,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:45,296 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25919 synced till here 25917
2014-07-22 12:56:45,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059003084 with entries=110, filesize=82.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059005031
2014-07-22 12:56:47,008 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:47,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26024 synced till here 26022
2014-07-22 12:56:47,631 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059005031 with entries=105, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059007008
2014-07-22 12:56:49,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:49,413 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:56:49,558 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26153 synced till here 26142
2014-07-22 12:56:49,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059007008 with entries=129, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059009131
2014-07-22 12:56:49,781 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:49,781 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:49,786 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:49,789 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:49,790 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:49,792 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,865 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,869 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,869 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,871 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,876 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,892 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,909 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,909 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,910 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,910 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,910 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,912 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,912 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,937 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:50,975 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,008 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,040 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,072 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,104 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,135 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,169 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,200 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:51,231 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,044 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,077 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,109 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,140 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,172 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,204 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,236 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,267 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,299 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,330 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,361 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,395 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,426 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:53,459 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:56:54,338 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4976, memsize=342.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/688cb4e4d24a4620b19b874a47d08234
2014-07-22 12:56:54,354 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/688cb4e4d24a4620b19b874a47d08234 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/688cb4e4d24a4620b19b874a47d08234
2014-07-22 12:56:54,371 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/688cb4e4d24a4620b19b874a47d08234, entries=1245170, sequenceid=4976, filesize=88.7m
2014-07-22 12:56:54,372 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~869.3m/911510000, currentsize=417.7m/438000160 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 27993ms, sequenceid=4976, compaction requested=true
2014-07-22 12:56:54,372 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:56:54,372 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 12:56:54,372 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 12:56:54,372 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 913ms
2014-07-22 12:56:54,372 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:56:54,373 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:56:54,372 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,373 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:56:54,373 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 718.8m
2014-07-22 12:56:54,373 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 947ms
2014-07-22 12:56:54,373 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,373 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 978ms
2014-07-22 12:56:54,373 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,373 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1012ms
2014-07-22 12:56:54,374 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,374 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1044ms
2014-07-22 12:56:54,374 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,374 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1075ms
2014-07-22 12:56:54,374 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,374 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1107ms
2014-07-22 12:56:54,374 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,385 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1149ms
2014-07-22 12:56:54,385 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,385 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1181ms
2014-07-22 12:56:54,385 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,386 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1214ms
2014-07-22 12:56:54,386 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,386 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1247ms
2014-07-22 12:56:54,386 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,386 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1277ms
2014-07-22 12:56:54,386 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,386 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1309ms
2014-07-22 12:56:54,386 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,389 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1345ms
2014-07-22 12:56:54,389 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,390 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3158ms
2014-07-22 12:56:54,390 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,391 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3190ms
2014-07-22 12:56:54,391 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,391 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3222ms
2014-07-22 12:56:54,391 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,392 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3256ms
2014-07-22 12:56:54,392 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,392 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3289ms
2014-07-22 12:56:54,392 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,393 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3321ms
2014-07-22 12:56:54,393 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,394 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3354ms
2014-07-22 12:56:54,394 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,394 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3386ms
2014-07-22 12:56:54,394 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,394 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3419ms
2014-07-22 12:56:54,395 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,397 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3460ms
2014-07-22 12:56:54,397 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,398 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3486ms
2014-07-22 12:56:54,398 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,398 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3486ms
2014-07-22 12:56:54,398 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,398 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3488ms
2014-07-22 12:56:54,398 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,398 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3489ms
2014-07-22 12:56:54,398 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,399 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3490ms
2014-07-22 12:56:54,399 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,401 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3492ms
2014-07-22 12:56:54,405 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,406 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3496ms
2014-07-22 12:56:54,406 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,406 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3514ms
2014-07-22 12:56:54,406 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,406 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3530ms
2014-07-22 12:56:54,406 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,409 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3538ms
2014-07-22 12:56:54,409 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,417 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3548ms
2014-07-22 12:56:54,417 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,418 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3548ms
2014-07-22 12:56:54,418 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,418 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3554ms
2014-07-22 12:56:54,418 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,428 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4636ms
2014-07-22 12:56:54,428 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,429 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4639ms
2014-07-22 12:56:54,429 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,430 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4640ms
2014-07-22 12:56:54,430 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,430 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4644ms
2014-07-22 12:56:54,430 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,430 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4650ms
2014-07-22 12:56:54,430 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,437 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4657ms
2014-07-22 12:56:54,437 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:56:54,654 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:56:56,370 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1513ms
GC pool 'ParNew' had collection(s): count=1 time=1589ms
2014-07-22 12:56:56,699 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:56:56,746 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:56,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26270 synced till here 26251
2014-07-22 12:56:56,896 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059009131 with entries=117, filesize=89.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059016746
2014-07-22 12:56:56,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058912726
2014-07-22 12:56:56,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058926190
2014-07-22 12:56:56,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058928835
2014-07-22 12:56:58,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:58,487 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26368 synced till here 26353
2014-07-22 12:56:58,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059016746 with entries=98, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059018470
2014-07-22 12:56:59,401 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:56:59,552 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26469 synced till here 26461
2014-07-22 12:56:59,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059018470 with entries=101, filesize=70.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059019401
2014-07-22 12:57:00,148 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5073, memsize=339.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/fd81a48024e142e19af3ec9fd717c17b
2014-07-22 12:57:00,165 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/fd81a48024e142e19af3ec9fd717c17b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/fd81a48024e142e19af3ec9fd717c17b
2014-07-22 12:57:00,178 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/fd81a48024e142e19af3ec9fd717c17b, entries=1236980, sequenceid=5073, filesize=88.1m
2014-07-22 12:57:00,179 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~866.4m/908489040, currentsize=375.9m/394160160 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 28956ms, sequenceid=5073, compaction requested=true
2014-07-22 12:57:00,179 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 12:57:00,179 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 12:57:00,179 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:57:00,179 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:57:00,179 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:57:00,179 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:57:00,180 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 746.7m
2014-07-22 12:57:00,193 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:57:00,878 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:00,893 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:57:00,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26559 synced till here 26556
2014-07-22 12:57:00,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059019401 with entries=90, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059020878
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058930708
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058933266
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058935000
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058936794
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058938855
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058940934
2014-07-22 12:57:00,976 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058943444
2014-07-22 12:57:00,977 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058945430
2014-07-22 12:57:02,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:02,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26648 synced till here 26644
2014-07-22 12:57:02,289 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059020878 with entries=89, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059022199
2014-07-22 12:57:02,289 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:03,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:03,834 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059022199 with entries=98, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059023623
2014-07-22 12:57:03,835 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:05,259 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:05,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059023623 with entries=86, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059025259
2014-07-22 12:57:05,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:06,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:06,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059025259 with entries=84, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059026894
2014-07-22 12:57:06,923 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:08,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:08,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27018 synced till here 27005
2014-07-22 12:57:08,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059026894 with entries=102, filesize=78.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059028412
2014-07-22 12:57:08,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:10,907 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:11,056 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27138 synced till here 27122
2014-07-22 12:57:11,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059028412 with entries=120, filesize=90.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059030908
2014-07-22 12:57:11,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:12,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:12,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27233 synced till here 27213
2014-07-22 12:57:13,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059030908 with entries=95, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059032882
2014-07-22 12:57:13,093 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:14,869 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:14,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27332 synced till here 27316
2014-07-22 12:57:15,048 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059032882 with entries=99, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059034869
2014-07-22 12:57:15,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:17,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:17,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27428 synced till here 27416
2014-07-22 12:57:17,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059034869 with entries=96, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059037056
2014-07-22 12:57:17,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:57:17,476 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,477 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,477 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,478 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,478 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,480 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,485 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,485 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,485 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,486 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,557 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,559 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,560 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,560 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,561 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,561 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,562 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,564 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,565 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,585 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,593 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,594 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,594 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,594 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,617 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,642 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,642 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,642 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,643 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,643 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,644 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,644 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,644 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,789 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,821 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,852 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,883 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,914 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,946 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:17,977 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,009 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,041 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,272 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,305 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,338 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,372 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,403 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,437 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,470 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:18,502 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:19,010 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5249, memsize=294.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/abd0d203d32745ea8eb2a423e6f5953d
2014-07-22 12:57:19,029 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/abd0d203d32745ea8eb2a423e6f5953d as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/abd0d203d32745ea8eb2a423e6f5953d
2014-07-22 12:57:19,046 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/abd0d203d32745ea8eb2a423e6f5953d, entries=1071980, sequenceid=5249, filesize=76.3m
2014-07-22 12:57:19,047 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~718.8m/753681040, currentsize=405.7m/425417600 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 24674ms, sequenceid=5249, compaction requested=true
2014-07-22 12:57:19,048 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:57:19,048 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 546ms
2014-07-22 12:57:19,048 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 12:57:19,048 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,049 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 12:57:19,049 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 378.4m
2014-07-22 12:57:19,049 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:57:19,049 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 580ms
2014-07-22 12:57:19,049 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,049 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:57:19,050 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:57:19,053 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 616ms
2014-07-22 12:57:19,053 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,054 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 651ms
2014-07-22 12:57:19,054 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,054 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 682ms
2014-07-22 12:57:19,054 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,056 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 718ms
2014-07-22 12:57:19,056 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,061 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 757ms
2014-07-22 12:57:19,061 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,061 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 789ms
2014-07-22 12:57:19,061 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,062 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1020ms
2014-07-22 12:57:19,062 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,062 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1053ms
2014-07-22 12:57:19,062 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,062 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1085ms
2014-07-22 12:57:19,062 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,062 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1116ms
2014-07-22 12:57:19,062 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,067 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1153ms
2014-07-22 12:57:19,067 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,069 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1186ms
2014-07-22 12:57:19,069 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,069 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1217ms
2014-07-22 12:57:19,069 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,069 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1248ms
2014-07-22 12:57:19,069 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,073 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1284ms
2014-07-22 12:57:19,073 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,073 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1429ms
2014-07-22 12:57:19,074 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,074 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1430ms
2014-07-22 12:57:19,074 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,075 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1432ms
2014-07-22 12:57:19,075 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,075 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1432ms
2014-07-22 12:57:19,075 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,100 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1457ms
2014-07-22 12:57:19,100 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,100 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1458ms
2014-07-22 12:57:19,100 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,100 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1458ms
2014-07-22 12:57:19,101 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,101 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1459ms
2014-07-22 12:57:19,101 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,101 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1484ms
2014-07-22 12:57:19,101 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,109 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1515ms
2014-07-22 12:57:19,109 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,109 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1516ms
2014-07-22 12:57:19,109 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,109 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1516ms
2014-07-22 12:57:19,109 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,109 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1516ms
2014-07-22 12:57:19,109 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,113 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1528ms
2014-07-22 12:57:19,113 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,121 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1557ms
2014-07-22 12:57:19,121 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,121 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1557ms
2014-07-22 12:57:19,121 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,122 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1560ms
2014-07-22 12:57:19,122 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:19,122 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1561ms
2014-07-22 12:57:19,122 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,245 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2685ms
2014-07-22 12:57:20,246 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,255 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2695ms
2014-07-22 12:57:20,255 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,263 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2703ms
2014-07-22 12:57:20,263 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,264 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2705ms
2014-07-22 12:57:20,264 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,269 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 12:57:20,269 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,270 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2783ms
2014-07-22 12:57:20,270 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,270 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2785ms
2014-07-22 12:57:20,270 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,270 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2785ms
2014-07-22 12:57:20,270 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,277 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2792ms
2014-07-22 12:57:20,277 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,285 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2805ms
2014-07-22 12:57:20,285 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,286 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2807ms
2014-07-22 12:57:20,286 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,293 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2815ms
2014-07-22 12:57:20,293 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,294 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2816ms
2014-07-22 12:57:20,295 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,301 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2824ms
2014-07-22 12:57:20,301 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,309 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2832ms
2014-07-22 12:57:20,309 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:20,537 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:57:20,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:20,786 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:57:20,806 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27551 synced till here 27536
2014-07-22 12:57:20,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059037056 with entries=123, filesize=90.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059040747
2014-07-22 12:57:22,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:22,936 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27666 synced till here 27644
2014-07-22 12:57:23,127 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059040747 with entries=115, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059042905
2014-07-22 12:57:25,510 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:25,668 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27791 synced till here 27782
2014-07-22 12:57:25,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059042905 with entries=125, filesize=88.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059045623
2014-07-22 12:57:27,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:27,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27886 synced till here 27885
2014-07-22 12:57:27,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059045623 with entries=95, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059047211
2014-07-22 12:57:28,097 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5314, memsize=315.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8f06bcbe2dca4ad29df2cda428ca38b5
2014-07-22 12:57:28,111 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8f06bcbe2dca4ad29df2cda428ca38b5 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8f06bcbe2dca4ad29df2cda428ca38b5
2014-07-22 12:57:28,245 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8f06bcbe2dca4ad29df2cda428ca38b5, entries=1149710, sequenceid=5314, filesize=81.8m
2014-07-22 12:57:28,246 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~752.7m/789214400, currentsize=460.3m/482706720 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 28066ms, sequenceid=5314, compaction requested=true
2014-07-22 12:57:28,246 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:57:28,246 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 12:57:28,247 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 977.6m
2014-07-22 12:57:28,247 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 12:57:28,247 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:57:28,247 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:57:28,247 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:57:28,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:28,305 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:57:28,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059047211 with entries=86, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059048302
2014-07-22 12:57:29,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:29,827 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:57:30,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28083 synced till here 28074
2014-07-22 12:57:30,786 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059048302 with entries=111, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059049726
2014-07-22 12:57:31,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:31,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28174 synced till here 28173
2014-07-22 12:57:31,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059049726 with entries=91, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059051662
2014-07-22 12:57:33,012 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5489, memsize=165.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/45e0bd51ba464be692d3e02a6a075548
2014-07-22 12:57:33,029 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/45e0bd51ba464be692d3e02a6a075548 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/45e0bd51ba464be692d3e02a6a075548
2014-07-22 12:57:33,041 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/45e0bd51ba464be692d3e02a6a075548, entries=602450, sequenceid=5489, filesize=43.0m
2014-07-22 12:57:33,041 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~378.4m/396801440, currentsize=77.5m/81287440 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 13992ms, sequenceid=5489, compaction requested=true
2014-07-22 12:57:33,042 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:57:33,042 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 2000 blocking
2014-07-22 12:57:33,042 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-22 12:57:33,042 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 922.4m
2014-07-22 12:57:33,042 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:57:33,042 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:57:33,042 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:57:33,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:33,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28263 synced till here 28258
2014-07-22 12:57:33,183 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059051662 with entries=89, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059053115
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058947829
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058949899
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058951662
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058953527
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058955451
2014-07-22 12:57:33,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058957836
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058959406
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058962336
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058964509
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058965373
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058967043
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058968720
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058970602
2014-07-22 12:57:33,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058972970
2014-07-22 12:57:33,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058975726
2014-07-22 12:57:34,682 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:34,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28357 synced till here 28345
2014-07-22 12:57:34,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059053115 with entries=94, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059054683
2014-07-22 12:57:34,804 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:57:36,212 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1026ms
GC pool 'ParNew' had collection(s): count=1 time=1059ms
2014-07-22 12:57:36,538 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:36,574 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28450 synced till here 28439
2014-07-22 12:57:36,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059054683 with entries=93, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059056538
2014-07-22 12:57:38,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:38,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28546 synced till here 28535
2014-07-22 12:57:38,554 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059056538 with entries=96, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059058312
2014-07-22 12:57:40,619 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:41,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28702 synced till here 28685
2014-07-22 12:57:42,287 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1108ms
GC pool 'ParNew' had collection(s): count=1 time=1199ms
2014-07-22 12:57:42,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059058312 with entries=156, filesize=117.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059060619
2014-07-22 12:57:43,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:57:43,165 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28810 synced till here 28781
2014-07-22 12:57:44,296 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
GC pool 'ParNew' had collection(s): count=1 time=1114ms
2014-07-22 12:57:44,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059060619 with entries=108, filesize=82.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059063125
2014-07-22 12:57:44,978 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,978 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,979 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,980 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,981 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,982 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,983 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,984 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,985 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,985 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,987 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,987 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,989 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,991 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,991 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,992 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,993 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,995 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,995 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,996 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:44,998 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,001 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,001 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,002 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,007 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,124 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,125 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,125 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,126 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,126 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,127 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,127 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,131 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,132 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,132 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,132 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,133 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,133 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,133 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,134 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,135 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,139 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,140 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,140 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,141 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,143 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,150 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,150 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,166 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:45,198 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:57:49,978 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,979 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,979 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,981 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,982 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,982 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,983 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,984 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,985 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,985 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,987 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,987 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,989 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,991 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,992 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,992 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,994 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:49,996 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,996 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:49,997 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 12:57:49,999 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,001 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,001 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,002 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,007 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,125 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,126 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,126 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,127 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,127 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,127 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,127 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,131 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,132 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,132 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,133 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-07-22 12:57:50,133 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,133 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,134 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,134 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,135 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,139 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,140 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,141 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,141 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,144 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 12:57:50,150 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,150 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,167 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:50,198 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 12:57:51,909 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1020ms
GC pool 'ParNew' had collection(s): count=1 time=1264ms
2014-07-22 12:57:54,980 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,981 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,981 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,982 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 12:57:54,983 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,983 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:54,984 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,985 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,985 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:54,986 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,987 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:54,988 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,989 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:54,992 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,993 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,994 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:54,994 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,996 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,997 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:54,997 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:54,999 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,002 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,002 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,003 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,008 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:55,125 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,126 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,127 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:55,127 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:55,127 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,127 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,128 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,131 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:55,133 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,133 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,134 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10007ms
2014-07-22 12:57:55,134 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 12:57:55,135 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,135 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,135 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 12:57:55,136 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,140 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:55,141 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,141 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,142 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,144 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,151 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 12:57:55,151 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,167 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:55,199 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 12:57:59,897 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5609, memsize=416.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/59ebcd6ec0314fa8809fb0ff6d281082
2014-07-22 12:57:59,909 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/59ebcd6ec0314fa8809fb0ff6d281082 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/59ebcd6ec0314fa8809fb0ff6d281082
2014-07-22 12:57:59,919 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/59ebcd6ec0314fa8809fb0ff6d281082, entries=1514940, sequenceid=5609, filesize=107.9m
2014-07-22 12:57:59,919 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~979.1m/1026682560, currentsize=255.6m/267996720 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 31672ms, sequenceid=5609, compaction requested=true
2014-07-22 12:57:59,920 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:57:59,920 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 12:57:59,920 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14722ms
2014-07-22 12:57:59,920 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 12:57:59,920 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 844.8m
2014-07-22 12:57:59,920 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,920 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:57:59,920 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:57:59,921 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:57:59,921 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14755ms
2014-07-22 12:57:59,921 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,929 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14779ms
2014-07-22 12:57:59,929 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,929 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14779ms
2014-07-22 12:57:59,930 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,942 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14798ms
2014-07-22 12:57:59,942 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,943 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14802ms
2014-07-22 12:57:59,943 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,943 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14803ms
2014-07-22 12:57:59,943 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,943 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14804ms
2014-07-22 12:57:59,943 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,944 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14805ms
2014-07-22 12:57:59,944 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,945 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14809ms
2014-07-22 12:57:59,945 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,946 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14814ms
2014-07-22 12:57:59,946 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,947 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14812ms
2014-07-22 12:57:59,947 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,965 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14832ms
2014-07-22 12:57:59,965 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,966 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14833ms
2014-07-22 12:57:59,966 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,973 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14847ms
2014-07-22 12:57:59,973 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,980 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:57:59,981 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,981 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:57:59,981 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,982 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14849ms
2014-07-22 12:57:59,982 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,982 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14851ms
2014-07-22 12:57:59,982 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,983 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14852ms
2014-07-22 12:57:59,984 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,986 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 12:57:59,986 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,986 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 12:57:59,986 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,989 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 12:57:59,989 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,989 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15008ms
2014-07-22 12:57:59,989 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,990 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14862ms
2014-07-22 12:57:59,990 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,991 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14864ms
2014-07-22 12:57:59,991 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,992 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14865ms
2014-07-22 12:57:59,992 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,993 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14867ms
2014-07-22 12:57:59,993 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,993 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:57:59,993 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,994 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:57:59,994 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,994 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15003ms
2014-07-22 12:57:59,994 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,995 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15008ms
2014-07-22 12:57:59,995 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,997 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15014ms
2014-07-22 12:57:59,997 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:57:59,999 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 12:57:59,999 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,001 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14876ms
2014-07-22 12:58:00,001 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,002 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:58:00,002 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,003 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 12:58:00,003 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,004 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14879ms
2014-07-22 12:58:00,004 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,004 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14880ms
2014-07-22 12:58:00,004 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,009 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 12:58:00,009 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,009 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15008ms
2014-07-22 12:58:00,010 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,010 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15015ms
2014-07-22 12:58:00,010 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,017 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15021ms
2014-07-22 12:58:00,017 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,018 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15022ms
2014-07-22 12:58:00,018 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,019 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15026ms
2014-07-22 12:58:00,019 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,021 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15032ms
2014-07-22 12:58:00,021 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,021 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15036ms
2014-07-22 12:58:00,021 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,021 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15040ms
2014-07-22 12:58:00,022 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,025 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15047ms
2014-07-22 12:58:00,025 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,025 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15045ms
2014-07-22 12:58:00,026 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:00,097 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:00,210 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28911 synced till here 28888
2014-07-22 12:58:00,387 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:58:00,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059063125 with entries=101, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059080098
2014-07-22 12:58:00,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058979789
2014-07-22 12:58:00,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058987005
2014-07-22 12:58:00,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058989302
2014-07-22 12:58:00,494 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17635,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062859,"queuetimems":4,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 12:58:00,720 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18406,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062401,"queuetimems":1,"class":"HRegionServer","responsesize":17426,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064331,"queuetimems":0,"class":"HRegionServer","responsesize":16636,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18444,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062363,"queuetimems":0,"class":"HRegionServer","responsesize":16918,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062332,"queuetimems":0,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062922,"queuetimems":5,"class":"HRegionServer","responsesize":17323,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064364,"queuetimems":1,"class":"HRegionServer","responsesize":17127,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062296,"queuetimems":1,"class":"HRegionServer","responsesize":16946,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059061049,"queuetimems":1,"class":"HRegionServer","responsesize":16988,"method":"Multi"}
2014-07-22 12:58:00,808 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062741,"queuetimems":0,"class":"HRegionServer","responsesize":17715,"method":"Multi"}
2014-07-22 12:58:00,951 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062607,"queuetimems":0,"class":"HRegionServer","responsesize":17014,"method":"Multi"}
2014-07-22 12:58:00,955 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18645,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062309,"queuetimems":0,"class":"HRegionServer","responsesize":17286,"method":"Multi"}
2014-07-22 12:58:00,955 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062436,"queuetimems":0,"class":"HRegionServer","responsesize":17158,"method":"Multi"}
2014-07-22 12:58:00,955 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059061009,"queuetimems":0,"class":"HRegionServer","responsesize":17150,"method":"Multi"}
2014-07-22 12:58:00,956 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059063176,"queuetimems":1,"class":"HRegionServer","responsesize":16870,"method":"Multi"}
2014-07-22 12:58:00,956 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18380,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062575,"queuetimems":0,"class":"HRegionServer","responsesize":16681,"method":"Multi"}
2014-07-22 12:58:00,957 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062512,"queuetimems":0,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 12:58:00,958 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062639,"queuetimems":1,"class":"HRegionServer","responsesize":16817,"method":"Multi"}
2014-07-22 12:58:00,959 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18477,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062482,"queuetimems":1,"class":"HRegionServer","responsesize":17120,"method":"Multi"}
2014-07-22 12:58:00,959 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18149,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062805,"queuetimems":0,"class":"HRegionServer","responsesize":16869,"method":"Multi"}
2014-07-22 12:58:00,955 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18180,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062774,"queuetimems":1,"class":"HRegionServer","responsesize":17235,"method":"Multi"}
2014-07-22 12:58:00,955 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18277,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062677,"queuetimems":0,"class":"HRegionServer","responsesize":17156,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059063055,"queuetimems":1,"class":"HRegionServer","responsesize":16949,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064939,"queuetimems":0,"class":"HRegionServer","responsesize":17296,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064433,"queuetimems":3,"class":"HRegionServer","responsesize":17426,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15999,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064971,"queuetimems":0,"class":"HRegionServer","responsesize":16905,"method":"Multi"}
2014-07-22 12:58:00,971 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18428,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062543,"queuetimems":0,"class":"HRegionServer","responsesize":16844,"method":"Multi"}
2014-07-22 12:58:00,978 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17966,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059063011,"queuetimems":8,"class":"HRegionServer","responsesize":16662,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18261,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062708,"queuetimems":0,"class":"HRegionServer","responsesize":17255,"method":"Multi"}
2014-07-22 12:58:00,978 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16462,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064515,"queuetimems":1,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 12:58:00,978 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064312,"queuetimems":0,"class":"HRegionServer","responsesize":17286,"method":"Multi"}
2014-07-22 12:58:00,978 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059063093,"queuetimems":1,"class":"HRegionServer","responsesize":17158,"method":"Multi"}
2014-07-22 12:58:00,970 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18003,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059062967,"queuetimems":0,"class":"HRegionServer","responsesize":17122,"method":"Multi"}
2014-07-22 12:58:01,836 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:01,927 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29043 synced till here 29013
2014-07-22 12:58:01,946 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17549,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064396,"queuetimems":0,"class":"HRegionServer","responsesize":17150,"method":"Multi"}
2014-07-22 12:58:01,946 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059063127,"queuetimems":1,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-07-22 12:58:01,950 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17081,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064868,"queuetimems":0,"class":"HRegionServer","responsesize":17313,"method":"Multi"}
2014-07-22 12:58:01,957 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064906,"queuetimems":0,"class":"HRegionServer","responsesize":17040,"method":"Multi"}
2014-07-22 12:58:02,062 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17327,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064734,"queuetimems":0,"class":"HRegionServer","responsesize":16825,"method":"Multi"}
2014-07-22 12:58:02,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059080098 with entries=132, filesize=88.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059081837
2014-07-22 12:58:02,287 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064796,"queuetimems":0,"class":"HRegionServer","responsesize":16701,"method":"Multi"}
2014-07-22 12:58:02,287 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17719,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064567,"queuetimems":0,"class":"HRegionServer","responsesize":17120,"method":"Multi"}
2014-07-22 12:58:02,290 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17591,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064698,"queuetimems":0,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 12:58:02,294 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065196,"queuetimems":0,"class":"HRegionServer","responsesize":17122,"method":"Multi"}
2014-07-22 12:58:02,287 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064661,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 12:58:02,310 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17688,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064621,"queuetimems":0,"class":"HRegionServer","responsesize":16988,"method":"Multi"}
2014-07-22 12:58:02,318 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059064830,"queuetimems":1,"class":"HRegionServer","responsesize":16753,"method":"Multi"}
2014-07-22 12:58:02,323 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065122,"queuetimems":12,"class":"HRegionServer","responsesize":16681,"method":"Multi"}
2014-07-22 12:58:02,324 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17167,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065157,"queuetimems":0,"class":"HRegionServer","responsesize":17156,"method":"Multi"}
2014-07-22 12:58:02,324 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17203,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065121,"queuetimems":107,"class":"HRegionServer","responsesize":17239,"method":"Multi"}
2014-07-22 12:58:02,330 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065121,"queuetimems":42,"class":"HRegionServer","responsesize":17014,"method":"Multi"}
2014-07-22 12:58:02,330 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059065121,"queuetimems":75,"class":"HRegionServer","responsesize":16946,"method":"Multi"}
2014-07-22 12:58:03,682 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:03,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29156 synced till here 29139
2014-07-22 12:58:04,076 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059081837 with entries=113, filesize=86.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059083682
2014-07-22 12:58:05,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:05,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29292 synced till here 29253
2014-07-22 12:58:06,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059083682 with entries=136, filesize=100.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059085869
2014-07-22 12:58:06,706 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5676, memsize=489.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/8fe7a28835c04c9cb3656dbddb535fe8
2014-07-22 12:58:06,722 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/8fe7a28835c04c9cb3656dbddb535fe8 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/8fe7a28835c04c9cb3656dbddb535fe8
2014-07-22 12:58:06,734 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/8fe7a28835c04c9cb3656dbddb535fe8, entries=1781880, sequenceid=5676, filesize=127.0m
2014-07-22 12:58:06,734 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~928.9m/973989040, currentsize=348.5m/365465520 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 33692ms, sequenceid=5676, compaction requested=true
2014-07-22 12:58:06,735 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:58:06,735 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 12:58:06,735 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 12:58:06,735 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 883.1m
2014-07-22 12:58:06,735 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:58:06,735 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:58:06,735 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:58:06,765 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:58:07,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:07,177 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29398 synced till here 29383
2014-07-22 12:58:08,161 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059085869 with entries=106, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059087122
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058991030
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058993221
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058994819
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058997073
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058998703
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406058999733
2014-07-22 12:58:08,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059001722
2014-07-22 12:58:08,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059003084
2014-07-22 12:58:08,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059005031
2014-07-22 12:58:08,163 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059007008
2014-07-22 12:58:09,052 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:58:09,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:09,223 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29515 synced till here 29504
2014-07-22 12:58:09,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059087122 with entries=117, filesize=94.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059089151
2014-07-22 12:58:10,888 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:11,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29658 synced till here 29647
2014-07-22 12:58:11,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059089151 with entries=143, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059090889
2014-07-22 12:58:12,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:12,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29791 synced till here 29779
2014-07-22 12:58:12,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059090889 with entries=133, filesize=98.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059092426
2014-07-22 12:58:14,180 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:14,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29885 synced till here 29882
2014-07-22 12:58:14,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059092426 with entries=94, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059094181
2014-07-22 12:58:15,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:16,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30018 synced till here 30017
2014-07-22 12:58:16,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059094181 with entries=133, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059095844
2014-07-22 12:58:17,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:17,740 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30112 synced till here 30100
2014-07-22 12:58:17,835 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059095844 with entries=94, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059097699
2014-07-22 12:58:19,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:19,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30211 synced till here 30200
2014-07-22 12:58:19,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059097699 with entries=99, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059099847
2014-07-22 12:58:20,137 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 12:58:21,421 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,421 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,421 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,424 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,428 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,451 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,452 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,455 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,459 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,460 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,474 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,484 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,488 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,489 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,490 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,500 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,539 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,557 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,575 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,728 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,800 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:21,967 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:22,042 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:22,201 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:22,717 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,584 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,670 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,703 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,736 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,768 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,802 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,834 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,865 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,898 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,929 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:23,961 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:58:24,434 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5792, memsize=330.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/7ea167db415b4735ba530fa3b1ec193f
2014-07-22 12:58:24,457 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/7ea167db415b4735ba530fa3b1ec193f as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/7ea167db415b4735ba530fa3b1ec193f
2014-07-22 12:58:24,484 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/7ea167db415b4735ba530fa3b1ec193f, entries=1201920, sequenceid=5792, filesize=85.6m
2014-07-22 12:58:24,484 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~844.8m/885840560, currentsize=436.5m/457749600 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 24564ms, sequenceid=5792, compaction requested=true
2014-07-22 12:58:24,485 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:58:24,485 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 12:58:24,485 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 524ms
2014-07-22 12:58:24,486 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,486 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 12:58:24,486 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 557ms
2014-07-22 12:58:24,486 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,486 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 716.3m
2014-07-22 12:58:24,486 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 588ms
2014-07-22 12:58:24,486 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,486 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:58:24,486 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 621ms
2014-07-22 12:58:24,487 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,486 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:58:24,487 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 653ms
2014-07-22 12:58:24,487 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,487 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:58:24,497 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 695ms
2014-07-22 12:58:24,497 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,497 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 729ms
2014-07-22 12:58:24,497 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,498 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 762ms
2014-07-22 12:58:24,498 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,498 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 795ms
2014-07-22 12:58:24,498 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,498 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 828ms
2014-07-22 12:58:24,498 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,513 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 929ms
2014-07-22 12:58:24,513 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,515 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1799ms
2014-07-22 12:58:24,515 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,515 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2314ms
2014-07-22 12:58:24,515 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,518 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2475ms
2014-07-22 12:58:24,518 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,522 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2554ms
2014-07-22 12:58:24,522 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,522 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2722ms
2014-07-22 12:58:24,522 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,523 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2794ms
2014-07-22 12:58:24,523 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,524 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2949ms
2014-07-22 12:58:24,524 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,524 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2967ms
2014-07-22 12:58:24,524 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,525 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2985ms
2014-07-22 12:58:24,525 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,538 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3037ms
2014-07-22 12:58:24,538 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,538 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3048ms
2014-07-22 12:58:24,538 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,539 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3051ms
2014-07-22 12:58:24,539 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,539 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3051ms
2014-07-22 12:58:24,539 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,550 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3065ms
2014-07-22 12:58:24,550 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,550 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3076ms
2014-07-22 12:58:24,550 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,550 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3090ms
2014-07-22 12:58:24,550 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,551 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3092ms
2014-07-22 12:58:24,551 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,552 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3097ms
2014-07-22 12:58:24,552 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,552 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3100ms
2014-07-22 12:58:24,552 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,561 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3111ms
2014-07-22 12:58:24,561 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,562 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3133ms
2014-07-22 12:58:24,562 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:24,563 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3139ms
2014-07-22 12:58:24,563 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:25,101 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3143ms
2014-07-22 12:58:25,102 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:25,102 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3681ms
2014-07-22 12:58:25,102 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:25,103 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3682ms
2014-07-22 12:58:25,103 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:58:25,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:25,340 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30322 synced till here 30301
2014-07-22 12:58:26,478 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:58:26,538 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059099847 with entries=111, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059105305
2014-07-22 12:58:26,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059009131
2014-07-22 12:58:26,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059016746
2014-07-22 12:58:26,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059018470
2014-07-22 12:58:26,848 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:58:27,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:27,458 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30428 synced till here 30418
2014-07-22 12:58:28,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059105305 with entries=106, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059107399
2014-07-22 12:58:29,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:29,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30534 synced till here 30523
2014-07-22 12:58:29,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059107399 with entries=106, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059109113
2014-07-22 12:58:30,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:30,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30641 synced till here 30639
2014-07-22 12:58:30,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059109113 with entries=107, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059110665
2014-07-22 12:58:32,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:32,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30730 synced till here 30726
2014-07-22 12:58:32,194 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059110665 with entries=89, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059112118
2014-07-22 12:58:32,569 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5894, memsize=306.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/11dac323e7d04fb784cfba33bed783f2
2014-07-22 12:58:32,587 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/11dac323e7d04fb784cfba33bed783f2 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/11dac323e7d04fb784cfba33bed783f2
2014-07-22 12:58:32,607 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/11dac323e7d04fb784cfba33bed783f2, entries=1115560, sequenceid=5894, filesize=79.5m
2014-07-22 12:58:32,608 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~899.9m/943562400, currentsize=427.0m/447722960 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 25873ms, sequenceid=5894, compaction requested=true
2014-07-22 12:58:32,608 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 12:58:32,609 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 12:58:32,609 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:58:32,609 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:58:32,609 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:58:32,609 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:58:32,609 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 781.7m
2014-07-22 12:58:32,658 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:58:33,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:33,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30817 synced till here 30814
2014-07-22 12:58:33,547 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059112118 with entries=87, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059113486
2014-07-22 12:58:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059019401
2014-07-22 12:58:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059020878
2014-07-22 12:58:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059022199
2014-07-22 12:58:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059023623
2014-07-22 12:58:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059025259
2014-07-22 12:58:33,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059026894
2014-07-22 12:58:33,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059028412
2014-07-22 12:58:33,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059030908
2014-07-22 12:58:33,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059032882
2014-07-22 12:58:33,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059034869
2014-07-22 12:58:33,934 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:58:35,706 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:35,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30904 synced till here 30903
2014-07-22 12:58:35,756 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059113486 with entries=87, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059115706
2014-07-22 12:58:36,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:36,752 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30988 synced till here 30987
2014-07-22 12:58:36,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059115706 with entries=84, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059116738
2014-07-22 12:58:36,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:37,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:38,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31090 synced till here 31088
2014-07-22 12:58:38,281 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059116738 with entries=102, filesize=75.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059117976
2014-07-22 12:58:38,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:39,589 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:39,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31175 synced till here 31171
2014-07-22 12:58:39,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059117976 with entries=85, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059119590
2014-07-22 12:58:39,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:41,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:41,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31277 synced till here 31262
2014-07-22 12:58:41,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059119590 with entries=102, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059121158
2014-07-22 12:58:41,311 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:42,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:43,016 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31376 synced till here 31362
2014-07-22 12:58:43,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059121158 with entries=99, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059122988
2014-07-22 12:58:43,199 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:45,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:45,056 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6070, memsize=231.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/aec5f6aec5724132895c4ec1a19e0bb5
2014-07-22 12:58:45,084 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/aec5f6aec5724132895c4ec1a19e0bb5 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/aec5f6aec5724132895c4ec1a19e0bb5
2014-07-22 12:58:46,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31500 synced till here 31496
2014-07-22 12:58:46,561 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/aec5f6aec5724132895c4ec1a19e0bb5, entries=843600, sequenceid=6070, filesize=60.1m
2014-07-22 12:58:46,562 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~716.3m/751052320, currentsize=364.8m/382487360 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 22076ms, sequenceid=6070, compaction requested=true
2014-07-22 12:58:46,586 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:58:46,587 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 371.5m
2014-07-22 12:58:46,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059122988 with entries=124, filesize=96.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059125030
2014-07-22 12:58:46,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 12:58:46,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 12:58:46,653 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 12:58:46,654 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:58:46,654 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:58:46,654 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 12:58:46,795 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 12:58:47,215 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:58:48,553 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:48,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31628 synced till here 31603
2014-07-22 12:58:48,842 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059125030 with entries=128, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059128554
2014-07-22 12:58:51,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:51,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31759 synced till here 31730
2014-07-22 12:58:51,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059128554 with entries=131, filesize=97.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059131004
2014-07-22 12:58:53,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:53,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31886 synced till here 31857
2014-07-22 12:58:53,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059131004 with entries=127, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059133628
2014-07-22 12:58:55,018 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1030ms
GC pool 'ParNew' had collection(s): count=1 time=1061ms
2014-07-22 12:58:55,442 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6182, memsize=215.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bf43dfc8c1dd4305a64b19e54056a779
2014-07-22 12:58:55,513 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bf43dfc8c1dd4305a64b19e54056a779 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bf43dfc8c1dd4305a64b19e54056a779
2014-07-22 12:58:55,533 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bf43dfc8c1dd4305a64b19e54056a779, entries=782860, sequenceid=6182, filesize=55.7m
2014-07-22 12:58:55,550 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~784.8m/822936640, currentsize=389.2m/408095360 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 22941ms, sequenceid=6182, compaction requested=true
2014-07-22 12:58:55,550 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:58:55,551 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 12:58:55,551 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 950.4m
2014-07-22 12:58:55,551 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 12:58:55,551 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:58:55,551 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:58:55,551 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:58:55,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:55,719 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32001 synced till here 31971
2014-07-22 12:58:55,820 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:58:55,942 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059133628 with entries=115, filesize=87.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059135695
2014-07-22 12:58:57,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:58:57,841 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32114 synced till here 32093
2014-07-22 12:58:58,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059135695 with entries=113, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059137814
2014-07-22 12:58:58,157 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:59:17,694 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 20365ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:59:17,694 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 20368ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:59:17,698 WARN  [regionserver60020] util.Sleeper: We slept 20563ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 12:59:17,702 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 19069ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=19460ms
2014-07-22 12:59:17,894 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059133631,"queuetimems":1867,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-22 12:59:17,894 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24070,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059133824,"queuetimems":543,"class":"HRegionServer","responsesize":17022,"method":"Multi"}
2014-07-22 12:59:17,895 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135376,"queuetimems":1908,"class":"HRegionServer","responsesize":16855,"method":"Multi"}
2014-07-22 12:59:17,895 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10959 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10972 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10977 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:17,897 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24077,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059133818,"queuetimems":603,"class":"HRegionServer","responsesize":16811,"method":"Multi"}
2014-07-22 12:59:17,898 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10979 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,898 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:17,918 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135548,"queuetimems":2048,"class":"HRegionServer","responsesize":16911,"method":"Multi"}
2014-07-22 12:59:17,918 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10971 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,918 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:17,919 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22371,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135548,"queuetimems":1984,"class":"HRegionServer","responsesize":16826,"method":"Multi"}
2014-07-22 12:59:17,919 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10969 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:17,919 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,098 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135566,"queuetimems":1820,"class":"HRegionServer","responsesize":16700,"method":"Multi"}
2014-07-22 12:59:18,098 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10965 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,098 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,424 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:18,460 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135565,"queuetimems":1855,"class":"HRegionServer","responsesize":17183,"method":"Multi"}
2014-07-22 12:59:18,460 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10966 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,460 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32218 synced till here 32213
2014-07-22 12:59:18,481 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22904,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135577,"queuetimems":1760,"class":"HRegionServer","responsesize":16889,"method":"Multi"}
2014-07-22 12:59:18,482 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10987 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,482 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,493 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059137814 with entries=104, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059158424
2014-07-22 12:59:18,535 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22977,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135558,"queuetimems":1930,"class":"HRegionServer","responsesize":17049,"method":"Multi"}
2014-07-22 12:59:18,535 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135589,"queuetimems":550,"class":"HRegionServer","responsesize":16678,"method":"Multi"}
2014-07-22 12:59:18,535 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10967 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10984 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22406,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059136130,"queuetimems":634,"class":"HRegionServer","responsesize":16839,"method":"Multi"}
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137143,"queuetimems":0,"class":"HRegionServer","responsesize":16711,"method":"Multi"}
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11001 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11007 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137174,"queuetimems":1,"class":"HRegionServer","responsesize":17258,"method":"Multi"}
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135696,"queuetimems":612,"class":"HRegionServer","responsesize":17129,"method":"Multi"}
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135548,"queuetimems":2016,"class":"HRegionServer","responsesize":17313,"method":"Multi"}
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11011 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10995 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10970 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22954,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135582,"queuetimems":1720,"class":"HRegionServer","responsesize":17158,"method":"Multi"}
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22718,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135818,"queuetimems":612,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22962,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135574,"queuetimems":1793,"class":"HRegionServer","responsesize":17106,"method":"Multi"}
2014-07-22 12:59:18,538 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10986 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,538 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,538 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10992 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,538 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,537 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135608,"queuetimems":559,"class":"HRegionServer","responsesize":17029,"method":"Multi"}
2014-07-22 12:59:18,536 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22950,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135586,"queuetimems":1687,"class":"HRegionServer","responsesize":16944,"method":"Multi"}
2014-07-22 12:59:18,539 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10991 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,539 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,539 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10985 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,539 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,540 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10988 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,540 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,548 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059136127,"queuetimems":855,"class":"HRegionServer","responsesize":17715,"method":"Multi"}
2014-07-22 12:59:18,548 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10997 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,548 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137208,"queuetimems":0,"class":"HRegionServer","responsesize":16901,"method":"Multi"}
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22423,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059136126,"queuetimems":888,"class":"HRegionServer","responsesize":16949,"method":"Multi"}
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11010 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20733,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137816,"queuetimems":576,"class":"HRegionServer","responsesize":17108,"method":"Multi"}
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135698,"queuetimems":572,"class":"HRegionServer","responsesize":17050,"method":"Multi"}
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059136127,"queuetimems":822,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11009 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135553,"queuetimems":1957,"class":"HRegionServer","responsesize":17379,"method":"Multi"}
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10994 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,549 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059136141,"queuetimems":257,"class":"HRegionServer","responsesize":16833,"method":"Multi"}
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10998 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,550 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11004 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10968 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059135720,"queuetimems":557,"class":"HRegionServer","responsesize":17404,"method":"Multi"}
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10996 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10993 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,551 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,785 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137816,"queuetimems":545,"class":"HRegionServer","responsesize":16700,"method":"Multi"}
2014-07-22 12:59:18,785 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11008 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,785 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,786 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20807,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137979,"queuetimems":86,"class":"HRegionServer","responsesize":16826,"method":"Multi"}
2014-07-22 12:59:18,786 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11020 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,786 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,787 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137972,"queuetimems":190,"class":"HRegionServer","responsesize":17049,"method":"Multi"}
2014-07-22 12:59:18,787 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11023 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,787 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,902 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6297, memsize=131.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9d533b5267c141d88af4a9ec91ea06aa
2014-07-22 12:59:18,915 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9d533b5267c141d88af4a9ec91ea06aa as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9d533b5267c141d88af4a9ec91ea06aa
2014-07-22 12:59:18,926 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9d533b5267c141d88af4a9ec91ea06aa, entries=480140, sequenceid=6297, filesize=34.2m
2014-07-22 12:59:18,926 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137825,"queuetimems":279,"class":"HRegionServer","responsesize":16855,"method":"Multi"}
2014-07-22 12:59:18,927 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11030 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,927 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,927 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~374.3m/392453200, currentsize=72.9m/76450960 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 32340ms, sequenceid=6297, compaction requested=true
2014-07-22 12:59:18,929 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:59:18,929 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 2000 blocking
2014-07-22 12:59:18,930 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-22 12:59:18,930 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:59:18,930 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 896.4m
2014-07-22 12:59:18,930 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:59:18,933 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:59:18,941 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059138009,"queuetimems":1,"class":"HRegionServer","responsesize":16814,"method":"Multi"}
2014-07-22 12:59:18,941 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11017 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,941 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21137,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137821,"queuetimems":382,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137983,"queuetimems":58,"class":"HRegionServer","responsesize":17379,"method":"Multi"}
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20987,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137971,"queuetimems":313,"class":"HRegionServer","responsesize":16811,"method":"Multi"}
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20986,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137972,"queuetimems":221,"class":"HRegionServer","responsesize":16555,"method":"Multi"}
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11014 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,959 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11019 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,960 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,960 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11024 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,960 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,960 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11027 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,960 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,963 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20990,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137972,"queuetimems":124,"class":"HRegionServer","responsesize":17022,"method":"Multi"}
2014-07-22 12:59:18,963 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11021 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,963 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137990,"queuetimems":34,"class":"HRegionServer","responsesize":16911,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137822,"queuetimems":343,"class":"HRegionServer","responsesize":17029,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21156,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137825,"queuetimems":310,"class":"HRegionServer","responsesize":17404,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137826,"queuetimems":238,"class":"HRegionServer","responsesize":17106,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21163,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137818,"queuetimems":446,"class":"HRegionServer","responsesize":17129,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11018 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11029 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137972,"queuetimems":158,"class":"HRegionServer","responsesize":17048,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137826,"queuetimems":200,"class":"HRegionServer","responsesize":17313,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11015 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21010,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137972,"queuetimems":252,"class":"HRegionServer","responsesize":16849,"method":"Multi"}
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11028 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137817,"queuetimems":514,"class":"HRegionServer","responsesize":17050,"method":"Multi"}
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11012 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54744","starttimems":1406059137971,"queuetimems":282,"class":"HRegionServer","responsesize":17183,"method":"Multi"}
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11013 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11026 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11016 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,982 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11025 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,984 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11022 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54744: output error
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,983 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,984 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:18,984 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 12:59:19,570 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:59:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 12:59:20,724 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:20,759 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32321 synced till here 32319
2014-07-22 12:59:20,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059158424 with entries=103, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059160724
2014-07-22 12:59:20,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059037056
2014-07-22 12:59:20,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059040747
2014-07-22 12:59:20,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059042905
2014-07-22 12:59:20,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059045623
2014-07-22 12:59:20,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059047211
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059048302
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059049726
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059051662
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059053115
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059054683
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059056538
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059058312
2014-07-22 12:59:20,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059060619
2014-07-22 12:59:22,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:22,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32408 synced till here 32406
2014-07-22 12:59:22,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059160724 with entries=87, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162119
2014-07-22 12:59:22,968 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:23,299 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32495 synced till here 32494
2014-07-22 12:59:23,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162119 with entries=87, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162969
2014-07-22 12:59:24,089 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:24,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32580 synced till here 32579
2014-07-22 12:59:24,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162969 with entries=85, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059164090
2014-07-22 12:59:25,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:25,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32665 synced till here 32663
2014-07-22 12:59:25,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059164090 with entries=85, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059165311
2014-07-22 12:59:26,528 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:26,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32751 synced till here 32750
2014-07-22 12:59:26,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059165311 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059166529
2014-07-22 12:59:27,704 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:28,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32889 synced till here 32887
2014-07-22 12:59:28,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059166529 with entries=138, filesize=102.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059167707
2014-07-22 12:59:29,484 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:29,599 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,609 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,625 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,636 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,644 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,670 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,723 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,775 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,825 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,858 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,887 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,917 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,955 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:29,962 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059167707 with entries=98, filesize=71.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059169485
2014-07-22 12:59:29,988 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,019 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,064 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,098 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,136 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,180 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,220 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,265 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,988 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:30,998 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,014 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,043 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,074 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,107 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,139 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,751 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,785 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,817 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,849 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,882 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,913 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,944 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:31,975 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,008 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,042 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,074 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,105 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,139 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,171 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,203 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,236 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,270 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,302 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,336 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,368 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,402 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,433 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:32,554 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6414, memsize=315.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/aa823814a00a40d4b037f35ffd3d1af5
2014-07-22 12:59:32,577 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/aa823814a00a40d4b037f35ffd3d1af5 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/aa823814a00a40d4b037f35ffd3d1af5
2014-07-22 12:59:32,591 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/aa823814a00a40d4b037f35ffd3d1af5, entries=1149170, sequenceid=6414, filesize=81.8m
2014-07-22 12:59:32,591 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~962.9m/1009654560, currentsize=314.1m/329309040 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 37040ms, sequenceid=6414, compaction requested=true
2014-07-22 12:59:32,592 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:59:32,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 12:59:32,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 12:59:32,592 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 159ms
2014-07-22 12:59:32,592 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,592 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 144.3m
2014-07-22 12:59:32,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:59:32,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:59:32,592 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 190ms
2014-07-22 12:59:32,593 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,593 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 12:59:32,593 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 225ms
2014-07-22 12:59:32,593 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,593 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 257ms
2014-07-22 12:59:32,593 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,593 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 291ms
2014-07-22 12:59:32,593 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,594 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 323ms
2014-07-22 12:59:32,594 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,594 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 358ms
2014-07-22 12:59:32,594 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,594 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-07-22 12:59:32,594 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,595 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 423ms
2014-07-22 12:59:32,595 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,595 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 456ms
2014-07-22 12:59:32,595 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,595 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 490ms
2014-07-22 12:59:32,595 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,597 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 523ms
2014-07-22 12:59:32,597 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,598 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 555ms
2014-07-22 12:59:32,598 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,599 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 591ms
2014-07-22 12:59:32,599 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,600 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 625ms
2014-07-22 12:59:32,600 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,600 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 656ms
2014-07-22 12:59:32,600 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,600 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 687ms
2014-07-22 12:59:32,600 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,603 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 721ms
2014-07-22 12:59:32,603 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,604 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 754ms
2014-07-22 12:59:32,604 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,605 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 788ms
2014-07-22 12:59:32,605 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,610 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 824ms
2014-07-22 12:59:32,610 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,610 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 859ms
2014-07-22 12:59:32,610 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,613 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1473ms
2014-07-22 12:59:32,613 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,614 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1506ms
2014-07-22 12:59:32,614 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,614 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1541ms
2014-07-22 12:59:32,614 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,618 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1574ms
2014-07-22 12:59:32,618 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,618 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1604ms
2014-07-22 12:59:32,619 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,619 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1621ms
2014-07-22 12:59:32,620 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,620 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1632ms
2014-07-22 12:59:32,620 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,621 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2355ms
2014-07-22 12:59:32,621 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,621 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2401ms
2014-07-22 12:59:32,621 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,622 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2443ms
2014-07-22 12:59:32,622 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,623 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2487ms
2014-07-22 12:59:32,623 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,623 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2525ms
2014-07-22 12:59:32,623 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,623 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2559ms
2014-07-22 12:59:32,623 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,624 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2604ms
2014-07-22 12:59:32,625 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,626 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2637ms
2014-07-22 12:59:32,626 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,626 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2671ms
2014-07-22 12:59:32,626 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,626 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2709ms
2014-07-22 12:59:32,626 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,626 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2739ms
2014-07-22 12:59:32,626 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,628 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2769ms
2014-07-22 12:59:32,628 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,628 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2803ms
2014-07-22 12:59:32,628 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,628 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2853ms
2014-07-22 12:59:32,629 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,630 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2906ms
2014-07-22 12:59:32,630 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,630 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2960ms
2014-07-22 12:59:32,630 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,630 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2986ms
2014-07-22 12:59:32,630 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,632 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2995ms
2014-07-22 12:59:32,632 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,633 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3007ms
2014-07-22 12:59:32,633 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,634 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3024ms
2014-07-22 12:59:32,634 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,634 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3036ms
2014-07-22 12:59:32,634 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:32,734 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:59:32,873 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 12:59:34,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:34,819 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33111 synced till here 33085
2014-07-22 12:59:35,178 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059169485 with entries=124, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059174789
2014-07-22 12:59:35,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059063125
2014-07-22 12:59:35,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059080098
2014-07-22 12:59:35,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059081837
2014-07-22 12:59:35,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059083682
2014-07-22 12:59:36,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:36,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33233 synced till here 33201
2014-07-22 12:59:36,961 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6473, memsize=314.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/3f238e7261b84f18be5524eacdc42a85
2014-07-22 12:59:36,972 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/3f238e7261b84f18be5524eacdc42a85 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/3f238e7261b84f18be5524eacdc42a85
2014-07-22 12:59:36,982 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/3f238e7261b84f18be5524eacdc42a85, entries=1145850, sequenceid=6473, filesize=81.6m
2014-07-22 12:59:36,982 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~896.4m/939936240, currentsize=295.4m/309729600 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 18052ms, sequenceid=6473, compaction requested=true
2014-07-22 12:59:36,983 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:59:36,983 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 12:59:36,983 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 876.1m
2014-07-22 12:59:36,983 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 12:59:36,983 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:59:36,983 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:59:36,983 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 12:59:37,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059174789 with entries=122, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059176842
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059085869
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059087122
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059089151
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059090889
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059092426
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059094181
2014-07-22 12:59:37,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059095844
2014-07-22 12:59:37,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059097699
2014-07-22 12:59:37,212 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 12:59:37,376 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6605, memsize=65.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/3f3f19cbabcb4dc6a71ceac7588c5c8a
2014-07-22 12:59:37,396 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/3f3f19cbabcb4dc6a71ceac7588c5c8a as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/3f3f19cbabcb4dc6a71ceac7588c5c8a
2014-07-22 12:59:37,414 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/3f3f19cbabcb4dc6a71ceac7588c5c8a, entries=238880, sequenceid=6605, filesize=17.0m
2014-07-22 12:59:37,415 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~144.3m/151304400, currentsize=25.4m/26632240 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 4823ms, sequenceid=6605, compaction requested=true
2014-07-22 12:59:37,415 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:59:37,415 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 2000 blocking
2014-07-22 12:59:37,415 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-22 12:59:37,416 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 786.4m
2014-07-22 12:59:37,416 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:59:37,416 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:59:37,416 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 12:59:38,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:38,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33351 synced till here 33326
2014-07-22 12:59:38,590 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:59:38,597 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059176842 with entries=118, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059178365
2014-07-22 12:59:38,821 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 12:59:40,199 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1123ms
GC pool 'ParNew' had collection(s): count=1 time=1157ms
2014-07-22 12:59:40,683 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:40,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33455 synced till here 33441
2014-07-22 12:59:40,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059178365 with entries=104, filesize=75.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059180684
2014-07-22 12:59:42,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:42,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33561 synced till here 33540
2014-07-22 12:59:42,652 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059180684 with entries=106, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059182487
2014-07-22 12:59:43,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:43,511 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059182487 with entries=86, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059183473
2014-07-22 12:59:45,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:45,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33739 synced till here 33733
2014-07-22 12:59:45,123 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059183473 with entries=92, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059185046
2014-07-22 12:59:46,314 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:47,356 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059185046 with entries=98, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059186315
2014-07-22 12:59:48,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:48,569 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33926 synced till here 33924
2014-07-22 12:59:48,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059186315 with entries=89, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059188549
2014-07-22 12:59:49,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:50,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34035 synced till here 34028
2014-07-22 12:59:50,074 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059188549 with entries=109, filesize=80.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059189794
2014-07-22 12:59:51,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:52,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34170 synced till here 34163
2014-07-22 12:59:52,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059189794 with entries=135, filesize=99.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059191422
2014-07-22 12:59:54,126 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:54,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34258 synced till here 34254
2014-07-22 12:59:54,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059191422 with entries=88, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059194127
2014-07-22 12:59:55,560 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:55,632 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34349 synced till here 34339
2014-07-22 12:59:55,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059194127 with entries=91, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059195560
2014-07-22 12:59:57,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 12:59:57,195 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34437 synced till here 34436
2014-07-22 12:59:57,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059195560 with entries=88, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059197177
2014-07-22 12:59:59,567 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,580 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,581 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,594 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,594 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,609 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,609 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,616 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,631 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 12:59:59,702 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6702, memsize=320.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/73fc96e885dd4ae798883f1a2870d331
2014-07-22 12:59:59,728 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/73fc96e885dd4ae798883f1a2870d331 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/73fc96e885dd4ae798883f1a2870d331
2014-07-22 12:59:59,755 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/73fc96e885dd4ae798883f1a2870d331, entries=1167090, sequenceid=6702, filesize=83.1m
2014-07-22 12:59:59,756 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~798.7m/837525440, currentsize=354.3m/371562240 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 22340ms, sequenceid=6702, compaction requested=true
2014-07-22 12:59:59,756 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 12:59:59,756 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 12:59:59,756 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 125ms
2014-07-22 12:59:59,756 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 12:59:59,756 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,757 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 12:59:59,757 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 141ms
2014-07-22 12:59:59,757 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 777.3m
2014-07-22 12:59:59,757 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,757 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 12:59:59,757 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 148ms
2014-07-22 12:59:59,757 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,757 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 12:59:59,757 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 148ms
2014-07-22 12:59:59,757 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,757 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 163ms
2014-07-22 12:59:59,758 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,758 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 164ms
2014-07-22 12:59:59,758 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,758 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 178ms
2014-07-22 12:59:59,758 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,758 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 178ms
2014-07-22 12:59:59,758 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,758 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 191ms
2014-07-22 12:59:59,758 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 12:59:59,998 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 12:59:59,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:00,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34531 synced till here 34530
2014-07-22 13:00:00,304 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059197177 with entries=94, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059199998
2014-07-22 13:00:00,380 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:01,930 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:01,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34619 synced till here 34618
2014-07-22 13:00:02,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059199998 with entries=88, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059201930
2014-07-22 13:00:02,318 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6667, memsize=397.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a1d943d9ceba444c8a5677745b86e125
2014-07-22 13:00:02,337 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a1d943d9ceba444c8a5677745b86e125 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a1d943d9ceba444c8a5677745b86e125
2014-07-22 13:00:02,359 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a1d943d9ceba444c8a5677745b86e125, entries=1448120, sequenceid=6667, filesize=103.1m
2014-07-22 13:00:02,359 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~926.1m/971102640, currentsize=430.8m/451710240 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 25376ms, sequenceid=6667, compaction requested=true
2014-07-22 13:00:02,360 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:02,360 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 13:00:02,360 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 728.3m
2014-07-22 13:00:02,360 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 13:00:02,360 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:02,360 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:02,360 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:00:02,396 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:00:03,327 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:03,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:03,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059201930 with entries=84, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059203554
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059099847
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059105305
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059107399
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059109113
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059110665
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059112118
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059113486
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059115706
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059116738
2014-07-22 13:00:03,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059117976
2014-07-22 13:00:03,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059119590
2014-07-22 13:00:03,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059121158
2014-07-22 13:00:03,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059122988
2014-07-22 13:00:03,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059125030
2014-07-22 13:00:03,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059128554
2014-07-22 13:00:03,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059131004
2014-07-22 13:00:05,550 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:05,633 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34791 synced till here 34789
2014-07-22 13:00:05,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059203554 with entries=88, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059205551
2014-07-22 13:00:10,201 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:10,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34882 synced till here 34877
2014-07-22 13:00:10,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059205551 with entries=91, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059210201
2014-07-22 13:00:11,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:11,718 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34968 synced till here 34967
2014-07-22 13:00:11,741 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059210201 with entries=86, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059211697
2014-07-22 13:00:12,581 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:13,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35065 synced till here 35064
2014-07-22 13:00:13,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059211697 with entries=97, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059212581
2014-07-22 13:00:15,891 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:16,388 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35152 synced till here 35151
2014-07-22 13:00:16,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059212581 with entries=87, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059215891
2014-07-22 13:00:17,146 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:17,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35244 synced till here 35237
2014-07-22 13:00:17,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059215891 with entries=92, filesize=69.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059217146
2014-07-22 13:00:19,497 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6916, memsize=444.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/ff4d693f18ca4237baa0273257089e56
2014-07-22 13:00:19,511 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/ff4d693f18ca4237baa0273257089e56 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/ff4d693f18ca4237baa0273257089e56
2014-07-22 13:00:19,523 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/ff4d693f18ca4237baa0273257089e56, entries=1618020, sequenceid=6916, filesize=115.1m
2014-07-22 13:00:19,524 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~777.3m/815080000, currentsize=247.7m/259684560 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 19768ms, sequenceid=6916, compaction requested=true
2014-07-22 13:00:19,524 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:19,525 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 13:00:19,525 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 13:00:19,525 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:19,525 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:19,525 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 593.4m
2014-07-22 13:00:19,525 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:00:20,002 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:20,109 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:00:20,168 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:20,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35331 synced till here 35330
2014-07-22 13:00:20,201 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059217146 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059220169
2014-07-22 13:00:20,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059133628
2014-07-22 13:00:20,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059135695
2014-07-22 13:00:20,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059137814
2014-07-22 13:00:23,165 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=6944, memsize=488.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/ec48345d1a4144d08b798f60c8e5ead2
2014-07-22 13:00:23,178 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/ec48345d1a4144d08b798f60c8e5ead2 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/ec48345d1a4144d08b798f60c8e5ead2
2014-07-22 13:00:23,190 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/ec48345d1a4144d08b798f60c8e5ead2, entries=1777320, sequenceid=6944, filesize=126.5m
2014-07-22 13:00:23,191 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~728.3m/763656240, currentsize=233.5m/244824480 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 20831ms, sequenceid=6944, compaction requested=true
2014-07-22 13:00:23,191 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:23,192 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 13:00:23,192 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 662.5m
2014-07-22 13:00:23,192 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 13:00:23,192 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:23,192 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:23,192 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:00:23,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:23,742 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:23,964 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059220169 with entries=106, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059223576
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059158424
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059160724
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162119
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059162969
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059164090
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059165311
2014-07-22 13:00:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059166529
2014-07-22 13:00:23,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059167707
2014-07-22 13:00:25,284 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:00:27,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:27,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35521 synced till here 35519
2014-07-22 13:00:27,289 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059223576 with entries=84, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059227243
2014-07-22 13:00:28,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:29,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35628 synced till here 35625
2014-07-22 13:00:29,322 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059227243 with entries=107, filesize=79.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059228454
2014-07-22 13:00:30,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:30,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35715 synced till here 35714
2014-07-22 13:00:31,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059228454 with entries=87, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059230950
2014-07-22 13:00:31,935 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:00:32,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:32,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059230950 with entries=84, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059232737
2014-07-22 13:00:34,295 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:34,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059232737 with entries=91, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059234295
2014-07-22 13:00:38,555 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:38,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35976 synced till here 35974
2014-07-22 13:00:38,624 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7085, memsize=484.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/e34b3dd0d0af4513b505df3fa6dfa74f
2014-07-22 13:00:38,642 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/e34b3dd0d0af4513b505df3fa6dfa74f as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/e34b3dd0d0af4513b505df3fa6dfa74f
2014-07-22 13:00:38,677 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059234295 with entries=86, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059238556
2014-07-22 13:00:38,679 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/e34b3dd0d0af4513b505df3fa6dfa74f, entries=1763120, sequenceid=7085, filesize=125.6m
2014-07-22 13:00:38,679 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~593.4m/622265360, currentsize=214.5m/224904000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 19154ms, sequenceid=7085, compaction requested=true
2014-07-22 13:00:38,680 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:38,680 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 13:00:38,680 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 13:00:38,680 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:38,680 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 461.3m
2014-07-22 13:00:38,680 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:38,681 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:00:39,018 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:43,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:43,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059238556 with entries=85, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059243046
2014-07-22 13:00:43,382 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7096, memsize=516.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/739507b149c6418d9ac7ab2dc991f3b5
2014-07-22 13:00:43,402 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/739507b149c6418d9ac7ab2dc991f3b5 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/739507b149c6418d9ac7ab2dc991f3b5
2014-07-22 13:00:43,459 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/739507b149c6418d9ac7ab2dc991f3b5, entries=1879110, sequenceid=7096, filesize=133.9m
2014-07-22 13:00:43,460 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~665.6m/697926160, currentsize=209.5m/219686560 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 20268ms, sequenceid=7096, compaction requested=true
2014-07-22 13:00:43,461 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:43,461 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 13:00:43,461 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 443.0m
2014-07-22 13:00:43,461 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 13:00:43,461 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:43,461 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:43,461 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:00:43,722 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:46,043 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:00:46,560 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:46,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36148 synced till here 36145
2014-07-22 13:00:46,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059243046 with entries=87, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059246560
2014-07-22 13:00:46,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:00:48,179 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:00:48,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:48,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059246560 with entries=102, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059248267
2014-07-22 13:00:48,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:00:50,615 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:00:50,637 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36334 synced till here 36333
2014-07-22 13:00:50,660 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059248267 with entries=84, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059250616
2014-07-22 13:00:50,661 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:00:54,195 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7215, memsize=461.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/11253c98aa0f42c1955d8c85a65e4e66
2014-07-22 13:00:54,205 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/11253c98aa0f42c1955d8c85a65e4e66 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/11253c98aa0f42c1955d8c85a65e4e66
2014-07-22 13:00:54,214 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/11253c98aa0f42c1955d8c85a65e4e66, entries=1680740, sequenceid=7215, filesize=119.7m
2014-07-22 13:00:54,214 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~463.0m/485455200, currentsize=132.3m/138734800 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 15534ms, sequenceid=7215, compaction requested=true
2014-07-22 13:00:54,215 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:54,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 13:00:54,215 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 320.6m
2014-07-22 13:00:54,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 13:00:54,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:54,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:54,215 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:00:54,461 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:00:57,948 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7230, memsize=441.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/810cce389fa14d4d848d376cb74beb33
2014-07-22 13:00:57,966 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/810cce389fa14d4d848d376cb74beb33 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/810cce389fa14d4d848d376cb74beb33
2014-07-22 13:00:57,985 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/810cce389fa14d4d848d376cb74beb33, entries=1607320, sequenceid=7230, filesize=114.4m
2014-07-22 13:00:57,986 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~443.0m/464564800, currentsize=106.1m/111293520 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 14525ms, sequenceid=7230, compaction requested=true
2014-07-22 13:00:57,987 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:00:57,987 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 13:00:57,987 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 346.6m
2014-07-22 13:00:57,987 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 13:00:57,988 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:00:57,988 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:00:57,988 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:00:58,224 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:01:02,204 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7291, memsize=249.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/824c54c67fe44935a79d6921475dfb7e
2014-07-22 13:01:02,219 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/824c54c67fe44935a79d6921475dfb7e as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/824c54c67fe44935a79d6921475dfb7e
2014-07-22 13:01:02,231 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/824c54c67fe44935a79d6921475dfb7e, entries=910010, sequenceid=7291, filesize=64.8m
2014-07-22 13:01:02,231 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~320.6m/336140720, currentsize=0.0/0 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 8016ms, sequenceid=7291, compaction requested=true
2014-07-22 13:01:02,232 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:01:02,232 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 2000 blocking
2014-07-22 13:01:02,232 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-22 13:01:02,232 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 314.8m
2014-07-22 13:01:02,232 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:01:02,232 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:01:02,232 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:01:02,412 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:01:08,273 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7309, memsize=346.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/05c1bf99995c4f0a8e35659c82715e7b
2014-07-22 13:01:08,287 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/05c1bf99995c4f0a8e35659c82715e7b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/05c1bf99995c4f0a8e35659c82715e7b
2014-07-22 13:01:08,296 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/05c1bf99995c4f0a8e35659c82715e7b, entries=1261910, sequenceid=7309, filesize=89.8m
2014-07-22 13:01:08,296 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~346.6m/363419440, currentsize=0.0/0 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 10309ms, sequenceid=7309, compaction requested=true
2014-07-22 13:01:08,296 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:01:08,297 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 13:01:08,297 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 13:01:08,297 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:01:08,297 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:01:08,297 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:01:11,510 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7300, memsize=314.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/e552bff56e684fca9c886a29458cb716
2014-07-22 13:01:11,522 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/e552bff56e684fca9c886a29458cb716 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/e552bff56e684fca9c886a29458cb716
2014-07-22 13:01:11,531 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/e552bff56e684fca9c886a29458cb716, entries=1146180, sequenceid=7300, filesize=81.5m
2014-07-22 13:01:11,531 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~314.8m/330091520, currentsize=0.0/0 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 9299ms, sequenceid=7300, compaction requested=true
2014-07-22 13:01:11,532 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:01:11,532 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 13:01:11,532 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 13:01:11,532 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:01:11,532 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:01:11,532 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:02:01,372 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:01,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059250616 with entries=86, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059321372
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059169485
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059174789
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059176842
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059178365
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059180684
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059182487
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059183473
2014-07-22 13:02:01,406 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059185046
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059186315
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059188549
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059189794
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059191422
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059194127
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059195560
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059197177
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059199998
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059201930
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059203554
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059205551
2014-07-22 13:02:01,407 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059210201
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059211697
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059212581
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059215891
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059217146
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059220169
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059223576
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059227243
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059228454
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059230950
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059232737
2014-07-22 13:02:01,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059234295
2014-07-22 13:02:04,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:04,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36516 synced till here 36513
2014-07-22 13:02:04,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059321372 with entries=96, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059324130
2014-07-22 13:02:06,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:07,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059324130 with entries=91, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059326317
2014-07-22 13:02:08,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:08,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059326317 with entries=87, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059328368
2014-07-22 13:02:10,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:10,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36793 synced till here 36791
2014-07-22 13:02:10,320 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059328368 with entries=99, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059330037
2014-07-22 13:02:11,900 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:02:11,901 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 256.8m
2014-07-22 13:02:12,056 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:13,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:14,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36885 synced till here 36873
2014-07-22 13:02:14,225 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:02:14,225 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 257.1m
2014-07-22 13:02:14,234 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059330037 with entries=92, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059333994
2014-07-22 13:02:15,440 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:15,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:15,799 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36977 synced till here 36968
2014-07-22 13:02:15,916 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059333994 with entries=92, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059335769
2014-07-22 13:02:17,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:17,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37125 synced till here 37114
2014-07-22 13:02:17,787 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059335769 with entries=148, filesize=109.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059337264
2014-07-22 13:02:19,392 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:02:19,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:19,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37220 synced till here 37208
2014-07-22 13:02:19,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059337264 with entries=95, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059339461
2014-07-22 13:02:20,701 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1018ms
GC pool 'ParNew' had collection(s): count=1 time=1072ms
2014-07-22 13:02:21,234 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:02:21,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:21,521 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37320 synced till here 37303
2014-07-22 13:02:21,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059339461 with entries=100, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059341474
2014-07-22 13:02:22,710 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
GC pool 'ParNew' had collection(s): count=1 time=1045ms
2014-07-22 13:02:23,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:23,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37424 synced till here 37401
2014-07-22 13:02:24,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059341474 with entries=104, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059343499
2014-07-22 13:02:25,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:27,463 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1046ms
GC pool 'ParNew' had collection(s): count=1 time=1510ms
2014-07-22 13:02:27,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37551 synced till here 37539
2014-07-22 13:02:27,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059343499 with entries=127, filesize=92.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059345847
2014-07-22 13:02:28,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:28,313 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7381, memsize=256.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/55dbc96448d54cd9ba0f1c72c3d22d47
2014-07-22 13:02:29,507 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1043ms
GC pool 'ParNew' had collection(s): count=1 time=1185ms
2014-07-22 13:02:29,521 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37656 synced till here 37629
2014-07-22 13:02:29,533 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/55dbc96448d54cd9ba0f1c72c3d22d47 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/55dbc96448d54cd9ba0f1c72c3d22d47
2014-07-22 13:02:29,555 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/55dbc96448d54cd9ba0f1c72c3d22d47, entries=934860, sequenceid=7381, filesize=66.6m
2014-07-22 13:02:29,562 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.8m/269232240, currentsize=256.0m/268428080 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 17661ms, sequenceid=7381, compaction requested=true
2014-07-22 13:02:29,562 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 13:02:29,562 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 13:02:29,563 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:02:29,563 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:02:29,563 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:02:29,563 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:02:29,563 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 403.5m
2014-07-22 13:02:29,677 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:02:29,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059345847 with entries=105, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059348305
2014-07-22 13:02:29,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059238556
2014-07-22 13:02:31,609 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1033ms
GC pool 'ParNew' had collection(s): count=1 time=1095ms
2014-07-22 13:02:31,769 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:31,994 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:32,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37784 synced till here 37759
2014-07-22 13:02:32,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059348305 with entries=128, filesize=96.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059351995
2014-07-22 13:02:33,100 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7397, memsize=254.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/3ca2eb76bdce4df390412ab8a025e1c1
2014-07-22 13:02:33,113 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/3ca2eb76bdce4df390412ab8a025e1c1 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/3ca2eb76bdce4df390412ab8a025e1c1
2014-07-22 13:02:33,123 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/3ca2eb76bdce4df390412ab8a025e1c1, entries=924680, sequenceid=7397, filesize=65.9m
2014-07-22 13:02:33,123 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.7m/271247440, currentsize=312.3m/327448560 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 18898ms, sequenceid=7397, compaction requested=true
2014-07-22 13:02:33,124 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:02:33,124 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 13:02:33,124 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 13:02:33,124 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 428.0m
2014-07-22 13:02:33,124 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:02:33,124 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:02:33,124 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:02:33,152 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:34,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37893 synced till here 37868
2014-07-22 13:02:34,045 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:02:34,155 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059351995 with entries=109, filesize=74.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059353152
2014-07-22 13:02:34,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059243046
2014-07-22 13:02:34,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059246560
2014-07-22 13:02:34,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059248267
2014-07-22 13:02:34,519 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:34,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:35,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38008 synced till here 37997
2014-07-22 13:02:35,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059353152 with entries=115, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059354780
2014-07-22 13:02:37,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:37,133 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38102 synced till here 38100
2014-07-22 13:02:37,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059354780 with entries=94, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059357055
2014-07-22 13:02:38,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:38,413 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38187 synced till here 38186
2014-07-22 13:02:38,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059357055 with entries=85, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059358389
2014-07-22 13:02:39,965 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:39,985 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38277 synced till here 38272
2014-07-22 13:02:40,042 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059358389 with entries=90, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059359966
2014-07-22 13:02:41,656 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:41,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38371 synced till here 38363
2014-07-22 13:02:41,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059359966 with entries=94, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059361657
2014-07-22 13:02:42,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:42,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38462 synced till here 38456
2014-07-22 13:02:43,072 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059361657 with entries=91, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059362974
2014-07-22 13:02:44,938 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:44,960 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38549 synced till here 38548
2014-07-22 13:02:44,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059362974 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059364938
2014-07-22 13:02:45,353 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7579, memsize=224.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/c316fded8f1b420cb7b140a2e93ff4fa
2014-07-22 13:02:45,366 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/c316fded8f1b420cb7b140a2e93ff4fa as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/c316fded8f1b420cb7b140a2e93ff4fa
2014-07-22 13:02:45,376 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/c316fded8f1b420cb7b140a2e93ff4fa, entries=818340, sequenceid=7579, filesize=58.3m
2014-07-22 13:02:45,377 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~415.7m/435920560, currentsize=264.7m/277520000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 15814ms, sequenceid=7579, compaction requested=true
2014-07-22 13:02:45,377 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:02:45,377 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 13:02:45,377 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 13:02:45,377 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 544.2m
2014-07-22 13:02:45,377 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:02:45,378 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:02:45,378 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:02:45,389 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:02:46,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:46,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38646 synced till here 38633
2014-07-22 13:02:46,874 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:46,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059364938 with entries=97, filesize=74.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059366782
2014-07-22 13:02:48,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:48,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38757 synced till here 38740
2014-07-22 13:02:49,006 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7590, memsize=225.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/41688c955ae34e6f90d251f330f47346
2014-07-22 13:02:49,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059366782 with entries=111, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059368823
2014-07-22 13:02:49,028 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/41688c955ae34e6f90d251f330f47346 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/41688c955ae34e6f90d251f330f47346
2014-07-22 13:02:49,042 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/41688c955ae34e6f90d251f330f47346, entries=820120, sequenceid=7590, filesize=58.5m
2014-07-22 13:02:49,042 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~451.7m/473603680, currentsize=266.5m/279420800 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15918ms, sequenceid=7590, compaction requested=true
2014-07-22 13:02:49,043 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:02:49,043 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 13:02:49,043 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 13:02:49,043 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 586.6m
2014-07-22 13:02:49,043 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:02:49,043 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:02:49,044 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:02:49,181 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:02:50,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:50,648 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:02:50,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38898 synced till here 38892
2014-07-22 13:02:50,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059368823 with entries=141, filesize=101.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059370605
2014-07-22 13:02:52,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:52,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38989 synced till here 38988
2014-07-22 13:02:52,379 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059370605 with entries=91, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059372342
2014-07-22 13:02:53,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:53,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39109 synced till here 39100
2014-07-22 13:02:54,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059372342 with entries=120, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059373607
2014-07-22 13:02:55,519 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:02:55,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:55,809 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059373607 with entries=100, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059375609
2014-07-22 13:02:57,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:57,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39308 synced till here 39293
2014-07-22 13:02:57,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059375609 with entries=99, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059377186
2014-07-22 13:02:58,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:02:58,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39402 synced till here 39393
2014-07-22 13:02:59,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059377186 with entries=94, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059378885
2014-07-22 13:03:00,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:01,066 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39515 synced till here 39484
2014-07-22 13:03:01,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059378885 with entries=113, filesize=85.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059380985
2014-07-22 13:03:03,513 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7740, memsize=243.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a9bf01b635254d8c8afe7373f6568e2a
2014-07-22 13:03:03,538 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a9bf01b635254d8c8afe7373f6568e2a as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a9bf01b635254d8c8afe7373f6568e2a
2014-07-22 13:03:03,556 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a9bf01b635254d8c8afe7373f6568e2a, entries=887610, sequenceid=7740, filesize=63.2m
2014-07-22 13:03:03,557 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~550.3m/577005680, currentsize=306.3m/321144960 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 18179ms, sequenceid=7740, compaction requested=true
2014-07-22 13:03:03,557 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:03:03,557 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 13:03:03,557 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 13:03:03,557 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 592.7m
2014-07-22 13:03:03,557 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:03:03,557 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:03:03,557 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:03:03,619 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:03,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39638 synced till here 39600
2014-07-22 13:03:03,864 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:03:05,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059380985 with entries=123, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059383619
2014-07-22 13:03:05,780 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:03:06,476 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:07,829 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1323ms
GC pool 'ParNew' had collection(s): count=1 time=1323ms
2014-07-22 13:03:08,018 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39766 synced till here 39755
2014-07-22 13:03:08,299 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059383619 with entries=128, filesize=99.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059386477
2014-07-22 13:03:08,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:10,577 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1247ms
GC pool 'ParNew' had collection(s): count=1 time=1575ms
2014-07-22 13:03:10,850 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7779, memsize=260.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/6079a65b33ef49e8b82b0ab58473a473
2014-07-22 13:03:10,873 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/6079a65b33ef49e8b82b0ab58473a473 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/6079a65b33ef49e8b82b0ab58473a473
2014-07-22 13:03:10,883 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/6079a65b33ef49e8b82b0ab58473a473, entries=947520, sequenceid=7779, filesize=67.4m
2014-07-22 13:03:10,884 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~589.7m/618375760, currentsize=338.7m/355114640 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 21841ms, sequenceid=7779, compaction requested=true
2014-07-22 13:03:10,884 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:03:10,884 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 13:03:10,885 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 13:03:10,885 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:03:10,885 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 598.6m
2014-07-22 13:03:10,885 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:03:10,885 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:03:10,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:11,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39903 synced till here 39881
2014-07-22 13:03:11,124 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:03:11,225 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059386477 with entries=137, filesize=96.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059390968
2014-07-22 13:03:11,226 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:12,826 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1248ms
GC pool 'ParNew' had collection(s): count=1 time=1247ms
2014-07-22 13:03:13,130 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:03:13,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:13,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40025 synced till here 39988
2014-07-22 13:03:13,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059390968 with entries=122, filesize=93.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059393241
2014-07-22 13:03:13,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:15,558 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1730ms
GC pool 'ParNew' had collection(s): count=1 time=1769ms
2014-07-22 13:03:16,481 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:16,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40146 synced till here 40135
2014-07-22 13:03:18,088 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1028ms
GC pool 'ParNew' had collection(s): count=1 time=1499ms
2014-07-22 13:03:18,178 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059393241 with entries=121, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059396481
2014-07-22 13:03:18,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:20,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:20,917 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40282 synced till here 40241
2014-07-22 13:03:21,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059396481 with entries=136, filesize=103.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059400782
2014-07-22 13:03:21,435 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:23,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:23,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40409 synced till here 40386
2014-07-22 13:03:23,536 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059400782 with entries=127, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059403198
2014-07-22 13:03:23,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:25,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:25,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40542 synced till here 40510
2014-07-22 13:03:26,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059403198 with entries=133, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059405419
2014-07-22 13:03:26,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:28,105 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:28,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40685 synced till here 40678
2014-07-22 13:03:28,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059405419 with entries=143, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059408106
2014-07-22 13:03:28,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:29,994 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:30,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40781 synced till here 40769
2014-07-22 13:03:30,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059408106 with entries=96, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059409995
2014-07-22 13:03:30,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:32,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:32,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40878 synced till here 40877
2014-07-22 13:03:32,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059409995 with entries=97, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059412658
2014-07-22 13:03:32,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:03:34,160 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7972, memsize=355.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/31b7f7edeb1a47519986853ef5d3afff
2014-07-22 13:03:34,179 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/31b7f7edeb1a47519986853ef5d3afff as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/31b7f7edeb1a47519986853ef5d3afff
2014-07-22 13:03:34,228 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/31b7f7edeb1a47519986853ef5d3afff, entries=1294010, sequenceid=7972, filesize=92.1m
2014-07-22 13:03:34,228 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~608.1m/637599520, currentsize=374.7m/392890400 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 30671ms, sequenceid=7972, compaction requested=true
2014-07-22 13:03:34,229 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:03:34,230 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 421.2m
2014-07-22 13:03:34,230 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 13:03:34,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 13:03:34,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:03:34,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:03:34,232 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:03:34,282 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:03:34,680 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:03:35,235 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:35,268 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059412658 with entries=83, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059415235
2014-07-22 13:03:36,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:37,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41063 synced till here 41062
2014-07-22 13:03:37,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059415235 with entries=102, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059416688
2014-07-22 13:03:38,177 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=7993, memsize=356.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/e2d4eb20e4324697b137909cff61e433
2014-07-22 13:03:38,199 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/e2d4eb20e4324697b137909cff61e433 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/e2d4eb20e4324697b137909cff61e433
2014-07-22 13:03:38,227 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/e2d4eb20e4324697b137909cff61e433, entries=1298090, sequenceid=7993, filesize=92.3m
2014-07-22 13:03:38,297 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~623.5m/653763520, currentsize=380.0m/398479040 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 27412ms, sequenceid=7993, compaction requested=true
2014-07-22 13:03:38,297 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:03:38,297 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 13:03:38,298 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 13:03:38,298 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 780.2m
2014-07-22 13:03:38,298 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:03:38,298 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:03:38,298 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:03:38,457 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:03:39,619 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:39,664 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41169 synced till here 41148
2014-07-22 13:03:39,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059416688 with entries=106, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059419619
2014-07-22 13:03:40,469 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:03:41,836 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:41,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41276 synced till here 41254
2014-07-22 13:03:42,285 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059419619 with entries=107, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059421837
2014-07-22 13:03:44,119 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:44,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41384 synced till here 41361
2014-07-22 13:03:44,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059421837 with entries=108, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059424119
2014-07-22 13:03:45,008 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:45,053 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41479 synced till here 41471
2014-07-22 13:03:45,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059424119 with entries=95, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059425008
2014-07-22 13:03:46,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:46,816 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059425008 with entries=88, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059426772
2014-07-22 13:03:46,898 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8195, memsize=165.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9afebdd31d3d47e1add2aae37328ccde
2014-07-22 13:03:46,919 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9afebdd31d3d47e1add2aae37328ccde as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9afebdd31d3d47e1add2aae37328ccde
2014-07-22 13:03:46,931 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9afebdd31d3d47e1add2aae37328ccde, entries=600930, sequenceid=8195, filesize=42.8m
2014-07-22 13:03:46,931 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~421.2m/441619440, currentsize=60.1m/63015680 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 12701ms, sequenceid=8195, compaction requested=true
2014-07-22 13:03:46,932 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:03:46,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 2000 blocking
2014-07-22 13:03:46,932 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 869.7m
2014-07-22 13:03:46,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-22 13:03:46,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:03:46,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:03:46,932 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:03:48,033 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:48,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41651 synced till here 41650
2014-07-22 13:03:48,071 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059426772 with entries=84, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059428034
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059250616
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059321372
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059324130
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059326317
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059328368
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059330037
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059333994
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059335769
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059337264
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059339461
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059341474
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059343499
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059345847
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059348305
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059351995
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059353152
2014-07-22 13:03:48,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059354780
2014-07-22 13:03:48,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059357055
2014-07-22 13:03:48,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059358389
2014-07-22 13:03:48,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059359966
2014-07-22 13:03:48,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059361657
2014-07-22 13:03:48,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059362974
2014-07-22 13:03:48,124 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:03:50,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:50,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41739 synced till here 41737
2014-07-22 13:03:50,707 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059428034 with entries=88, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059430272
2014-07-22 13:03:52,744 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:55,152 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1545ms
GC pool 'ParNew' had collection(s): count=1 time=1794ms
2014-07-22 13:03:55,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41892 synced till here 41858
2014-07-22 13:03:55,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059430272 with entries=153, filesize=120.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059432745
2014-07-22 13:03:56,184 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:56,239 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41999 synced till here 41979
2014-07-22 13:03:57,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059432745 with entries=107, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059436184
2014-07-22 13:03:58,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:58,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42103 synced till here 42095
2014-07-22 13:03:58,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059436184 with entries=104, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059438019
2014-07-22 13:03:59,438 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:03:59,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42192 synced till here 42191
2014-07-22 13:03:59,472 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059438019 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059439438
2014-07-22 13:04:00,708 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8251, memsize=233.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a6027e63eaa141a38ea5bbc2e0ee0070
2014-07-22 13:04:00,735 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a6027e63eaa141a38ea5bbc2e0ee0070 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a6027e63eaa141a38ea5bbc2e0ee0070
2014-07-22 13:04:00,753 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a6027e63eaa141a38ea5bbc2e0ee0070, entries=849130, sequenceid=8251, filesize=60.5m
2014-07-22 13:04:00,754 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~788.1m/826398960, currentsize=340.3m/356858560 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 22456ms, sequenceid=8251, compaction requested=true
2014-07-22 13:04:00,761 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:04:00,761 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 13:04:00,761 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 788.5m
2014-07-22 13:04:00,761 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 13:04:00,761 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:04:00,762 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:04:00,762 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:04:00,762 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:04:01,015 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:01,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42278 synced till here 42276
2014-07-22 13:04:01,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059439438 with entries=86, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059441015
2014-07-22 13:04:01,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059364938
2014-07-22 13:04:01,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059366782
2014-07-22 13:04:01,589 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:04:02,405 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:02,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42371 synced till here 42367
2014-07-22 13:04:02,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059441015 with entries=93, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059442405
2014-07-22 13:04:04,323 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:04,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42462 synced till here 42452
2014-07-22 13:04:04,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059442405 with entries=91, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059444324
2014-07-22 13:04:05,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:06,631 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42556 synced till here 42546
2014-07-22 13:04:06,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059444324 with entries=94, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059445227
2014-07-22 13:04:07,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:07,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42650 synced till here 42641
2014-07-22 13:04:07,509 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059445227 with entries=94, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059447409
2014-07-22 13:04:09,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:09,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42755 synced till here 42753
2014-07-22 13:04:09,297 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059447409 with entries=105, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059449013
2014-07-22 13:04:10,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:10,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059449013 with entries=85, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059450806
2014-07-22 13:04:12,396 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:12,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 42928 synced till here 42926
2014-07-22 13:04:12,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059450806 with entries=88, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059452397
2014-07-22 13:04:12,659 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8337, memsize=276.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/56692a416cbf4e8bb765dfcceca8a6ba
2014-07-22 13:04:12,674 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/56692a416cbf4e8bb765dfcceca8a6ba as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/56692a416cbf4e8bb765dfcceca8a6ba
2014-07-22 13:04:12,685 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/56692a416cbf4e8bb765dfcceca8a6ba, entries=1005440, sequenceid=8337, filesize=71.6m
2014-07-22 13:04:12,685 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~872.8m/915241600, currentsize=423.6m/444141280 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 25753ms, sequenceid=8337, compaction requested=true
2014-07-22 13:04:12,685 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:04:12,686 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 13:04:12,686 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 13:04:12,686 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 949.2m
2014-07-22 13:04:12,686 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:04:12,686 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:04:12,686 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:04:12,753 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:04:13,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:13,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43016 synced till here 43012
2014-07-22 13:04:14,049 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059452397 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059453940
2014-07-22 13:04:14,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059368823
2014-07-22 13:04:14,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059370605
2014-07-22 13:04:14,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059372342
2014-07-22 13:04:14,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059373607
2014-07-22 13:04:14,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059375609
2014-07-22 13:04:14,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059377186
2014-07-22 13:04:14,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059378885
2014-07-22 13:04:14,261 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:04:15,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:15,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059453940 with entries=84, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059455062
2014-07-22 13:04:16,343 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:16,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43192 synced till here 43184
2014-07-22 13:04:16,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059455062 with entries=92, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059456343
2014-07-22 13:04:17,847 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:18,110 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43308 synced till here 43305
2014-07-22 13:04:19,377 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059456343 with entries=116, filesize=84.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059457848
2014-07-22 13:04:19,726 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:04:20,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:21,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43411 synced till here 43393
2014-07-22 13:04:21,734 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059457848 with entries=103, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059460341
2014-07-22 13:04:22,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:22,490 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43519 synced till here 43497
2014-07-22 13:04:24,015 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1297ms
GC pool 'ParNew' had collection(s): count=1 time=1373ms
2014-07-22 13:04:24,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059460341 with entries=108, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059462450
2014-07-22 13:04:24,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:26,240 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1223ms
GC pool 'ParNew' had collection(s): count=1 time=1233ms
2014-07-22 13:04:26,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43621 synced till here 43602
2014-07-22 13:04:26,455 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059462450 with entries=102, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059464996
2014-07-22 13:04:26,732 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,737 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,738 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,738 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,738 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,739 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,740 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,740 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,741 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,741 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,741 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,741 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,742 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,742 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,742 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,751 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,763 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,802 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,836 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,836 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,838 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,838 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,839 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,839 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,840 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,840 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,840 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,841 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,842 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,844 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,844 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,844 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,845 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,845 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,859 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,859 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,859 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,871 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,899 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,934 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:26,967 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,000 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,033 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,069 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,100 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,133 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,166 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,202 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,230 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8486, memsize=271.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/95938a98f2cf4a3e8280147a992fd2aa
2014-07-22 13:04:27,231 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:27,254 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/95938a98f2cf4a3e8280147a992fd2aa as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/95938a98f2cf4a3e8280147a992fd2aa
2014-07-22 13:04:27,265 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/95938a98f2cf4a3e8280147a992fd2aa, entries=989810, sequenceid=8486, filesize=70.5m
2014-07-22 13:04:27,265 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~791.6m/830090960, currentsize=461.0m/483344400 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 26504ms, sequenceid=8486, compaction requested=true
2014-07-22 13:04:27,266 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:04:27,266 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 13:04:27,266 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 35ms
2014-07-22 13:04:27,267 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,266 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 13:04:27,267 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 782.0m
2014-07-22 13:04:27,267 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:04:27,267 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:04:27,267 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:04:27,269 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 68ms
2014-07-22 13:04:27,269 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,270 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 104ms
2014-07-22 13:04:27,270 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,270 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 137ms
2014-07-22 13:04:27,270 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,273 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 173ms
2014-07-22 13:04:27,273 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,282 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 214ms
2014-07-22 13:04:27,282 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,282 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 249ms
2014-07-22 13:04:27,282 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,282 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 282ms
2014-07-22 13:04:27,282 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,282 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 315ms
2014-07-22 13:04:27,283 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,283 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 349ms
2014-07-22 13:04:27,283 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,285 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 386ms
2014-07-22 13:04:27,285 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,289 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 418ms
2014-07-22 13:04:27,289 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,292 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 433ms
2014-07-22 13:04:27,292 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,292 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 433ms
2014-07-22 13:04:27,292 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,292 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 433ms
2014-07-22 13:04:27,292 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,301 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 456ms
2014-07-22 13:04:27,301 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,301 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 456ms
2014-07-22 13:04:27,302 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,302 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 458ms
2014-07-22 13:04:27,302 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,302 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 458ms
2014-07-22 13:04:27,302 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,302 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 459ms
2014-07-22 13:04:27,302 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,309 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 467ms
2014-07-22 13:04:27,309 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,309 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 468ms
2014-07-22 13:04:27,309 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,310 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 474ms
2014-07-22 13:04:27,310 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,317 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 477ms
2014-07-22 13:04:27,317 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,317 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 477ms
2014-07-22 13:04:27,318 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,329 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 490ms
2014-07-22 13:04:27,329 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,337 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 498ms
2014-07-22 13:04:27,337 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,337 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 499ms
2014-07-22 13:04:27,337 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,338 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 500ms
2014-07-22 13:04:27,338 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,341 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 505ms
2014-07-22 13:04:27,341 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,341 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 505ms
2014-07-22 13:04:27,342 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,342 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 540ms
2014-07-22 13:04:27,342 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,347 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 13:04:27,347 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,348 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 598ms
2014-07-22 13:04:27,348 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,353 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 611ms
2014-07-22 13:04:27,353 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,361 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 623ms
2014-07-22 13:04:27,361 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,363 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 625ms
2014-07-22 13:04:27,363 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,364 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 623ms
2014-07-22 13:04:27,365 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,365 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 624ms
2014-07-22 13:04:27,366 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,366 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 628ms
2014-07-22 13:04:27,366 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,369 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 628ms
2014-07-22 13:04:27,370 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,370 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 630ms
2014-07-22 13:04:27,370 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,370 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 631ms
2014-07-22 13:04:27,370 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,372 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 633ms
2014-07-22 13:04:27,372 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,372 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 634ms
2014-07-22 13:04:27,373 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,373 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 635ms
2014-07-22 13:04:27,373 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,381 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 643ms
2014-07-22 13:04:27,381 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,384 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 648ms
2014-07-22 13:04:27,384 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:27,393 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 661ms
2014-07-22 13:04:27,393 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:28,901 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1159ms
GC pool 'ParNew' had collection(s): count=1 time=1392ms
2014-07-22 13:04:29,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:29,269 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:04:29,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43735 synced till here 43717
2014-07-22 13:04:29,391 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:04:29,418 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059464996 with entries=114, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059469250
2014-07-22 13:04:29,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059380985
2014-07-22 13:04:29,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059383619
2014-07-22 13:04:29,557 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:04:31,385 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:31,425 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43851 synced till here 43824
2014-07-22 13:04:31,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059469250 with entries=116, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059471386
2014-07-22 13:04:32,370 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:33,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 43951 synced till here 43949
2014-07-22 13:04:33,197 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059471386 with entries=100, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059472370
2014-07-22 13:04:33,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:35,181 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1065ms
GC pool 'ParNew' had collection(s): count=1 time=1080ms
2014-07-22 13:04:35,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44066 synced till here 44057
2014-07-22 13:04:35,278 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059472370 with entries=115, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059473993
2014-07-22 13:04:36,103 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:36,127 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44158 synced till here 44157
2014-07-22 13:04:36,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059473993 with entries=92, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059476104
2014-07-22 13:04:37,739 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,743 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,766 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,774 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,790 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,793 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,793 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,800 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,831 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,872 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,914 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:37,955 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,005 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,040 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,074 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,108 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,146 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,181 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,213 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,244 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,276 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,309 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,342 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,373 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,405 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,439 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,470 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,502 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,539 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:38,568 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:40,101 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:40,118 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:40,150 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:40,180 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,587 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,619 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,652 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,685 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,715 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:41,746 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:42,743 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 13:04:42,744 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:42,766 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:42,775 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:42,790 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:42,794 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:42,794 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:42,801 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:42,832 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:42,873 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:42,914 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:42,956 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,006 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,040 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,074 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,108 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,147 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,181 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,213 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,245 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,277 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,310 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,342 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,374 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,405 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,440 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,471 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,503 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,539 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:43,555 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,569 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:04:43,586 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,618 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,652 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,683 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,716 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,748 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,782 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,813 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:43,845 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:45,064 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8610, memsize=410.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/425f00ef435049199f10ba2cbe19d573
2014-07-22 13:04:45,083 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/425f00ef435049199f10ba2cbe19d573 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/425f00ef435049199f10ba2cbe19d573
2014-07-22 13:04:45,101 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:45,104 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/425f00ef435049199f10ba2cbe19d573, entries=1494200, sequenceid=8610, filesize=106.3m
2014-07-22 13:04:45,105 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~955.5m/1001963520, currentsize=398.8m/418182160 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 32419ms, sequenceid=8610, compaction requested=true
2014-07-22 13:04:45,105 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:04:45,105 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 13:04:45,106 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 13:04:45,106 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 13:04:45,106 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:04:45,106 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 823.9m
2014-07-22 13:04:45,106 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,106 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:04:45,106 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1261ms
2014-07-22 13:04:45,106 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,106 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:04:45,106 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1293ms
2014-07-22 13:04:45,106 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,107 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1326ms
2014-07-22 13:04:45,107 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,107 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1359ms
2014-07-22 13:04:45,107 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,118 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1401ms
2014-07-22 13:04:45,118 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,118 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:45,118 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,122 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1438ms
2014-07-22 13:04:45,122 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,122 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1470ms
2014-07-22 13:04:45,123 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,123 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1505ms
2014-07-22 13:04:45,123 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,123 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1537ms
2014-07-22 13:04:45,124 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,126 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6557ms
2014-07-22 13:04:45,126 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,137 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1582ms
2014-07-22 13:04:45,137 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,138 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6599ms
2014-07-22 13:04:45,138 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,139 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6637ms
2014-07-22 13:04:45,139 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,140 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6670ms
2014-07-22 13:04:45,140 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,140 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6702ms
2014-07-22 13:04:45,140 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,140 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6735ms
2014-07-22 13:04:45,140 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,141 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6767ms
2014-07-22 13:04:45,141 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,141 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6799ms
2014-07-22 13:04:45,141 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,144 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6835ms
2014-07-22 13:04:45,144 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,146 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6870ms
2014-07-22 13:04:45,146 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,149 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6905ms
2014-07-22 13:04:45,149 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,150 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6936ms
2014-07-22 13:04:45,150 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,151 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6971ms
2014-07-22 13:04:45,151 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,151 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7005ms
2014-07-22 13:04:45,151 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,157 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7049ms
2014-07-22 13:04:45,157 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,158 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7083ms
2014-07-22 13:04:45,158 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,159 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7119ms
2014-07-22 13:04:45,159 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,159 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7154ms
2014-07-22 13:04:45,159 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,159 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7204ms
2014-07-22 13:04:45,159 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,160 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7246ms
2014-07-22 13:04:45,160 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,163 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7291ms
2014-07-22 13:04:45,163 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,164 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7333ms
2014-07-22 13:04:45,164 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,164 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7364ms
2014-07-22 13:04:45,164 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,168 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7375ms
2014-07-22 13:04:45,168 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,168 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7375ms
2014-07-22 13:04:45,168 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,173 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7383ms
2014-07-22 13:04:45,173 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,174 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7400ms
2014-07-22 13:04:45,174 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,175 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7409ms
2014-07-22 13:04:45,175 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,176 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7433ms
2014-07-22 13:04:45,176 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,177 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7437ms
2014-07-22 13:04:45,177 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,181 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:04:45,182 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,182 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3437ms
2014-07-22 13:04:45,182 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,182 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3467ms
2014-07-22 13:04:45,182 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,182 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3497ms
2014-07-22 13:04:45,182 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,183 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3531ms
2014-07-22 13:04:45,183 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,185 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3564ms
2014-07-22 13:04:45,185 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,185 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3598ms
2014-07-22 13:04:45,185 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,186 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5036ms
2014-07-22 13:04:45,186 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:45,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:45,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44248 synced till here 44241
2014-07-22 13:04:45,442 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:04:45,458 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059476104 with entries=90, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059485292
2014-07-22 13:04:45,458 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059386477
2014-07-22 13:04:45,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059390968
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059393241
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059396481
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059400782
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059403198
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059405419
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059408106
2014-07-22 13:04:45,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059409995
2014-07-22 13:04:46,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:46,961 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:04:47,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:47,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44358 synced till here 44347
2014-07-22 13:04:47,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059485292 with entries=110, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059487232
2014-07-22 13:04:47,345 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:48,526 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477911,"queuetimems":0,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 13:04:48,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:48,809 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10272,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478536,"queuetimems":0,"class":"HRegionServer","responsesize":16831,"method":"Multi"}
2014-07-22 13:04:48,900 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44486 synced till here 44453
2014-07-22 13:04:48,967 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11096,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477870,"queuetimems":0,"class":"HRegionServer","responsesize":16984,"method":"Multi"}
2014-07-22 13:04:49,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059487232 with entries=128, filesize=95.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059488807
2014-07-22 13:04:49,035 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:49,164 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10761,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478403,"queuetimems":1,"class":"HRegionServer","responsesize":16777,"method":"Multi"}
2014-07-22 13:04:49,164 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478371,"queuetimems":0,"class":"HRegionServer","responsesize":16991,"method":"Multi"}
2014-07-22 13:04:49,171 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10897,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478274,"queuetimems":0,"class":"HRegionServer","responsesize":16836,"method":"Multi"}
2014-07-22 13:04:49,172 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477741,"queuetimems":0,"class":"HRegionServer","responsesize":17102,"method":"Multi"}
2014-07-22 13:04:49,175 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10964,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478211,"queuetimems":1,"class":"HRegionServer","responsesize":16843,"method":"Multi"}
2014-07-22 13:04:49,179 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10711,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478468,"queuetimems":0,"class":"HRegionServer","responsesize":17061,"method":"Multi"}
2014-07-22 13:04:49,179 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11176,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478003,"queuetimems":0,"class":"HRegionServer","responsesize":16886,"method":"Multi"}
2014-07-22 13:04:49,180 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478179,"queuetimems":0,"class":"HRegionServer","responsesize":16703,"method":"Multi"}
2014-07-22 13:04:49,180 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11352,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477828,"queuetimems":0,"class":"HRegionServer","responsesize":16989,"method":"Multi"}
2014-07-22 13:04:49,181 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11036,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478144,"queuetimems":0,"class":"HRegionServer","responsesize":17230,"method":"Multi"}
2014-07-22 13:04:49,182 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478340,"queuetimems":0,"class":"HRegionServer","responsesize":17071,"method":"Multi"}
2014-07-22 13:04:49,182 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477788,"queuetimems":1,"class":"HRegionServer","responsesize":17036,"method":"Multi"}
2014-07-22 13:04:49,182 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10874,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478308,"queuetimems":1,"class":"HRegionServer","responsesize":17429,"method":"Multi"}
2014-07-22 13:04:49,183 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11077,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478106,"queuetimems":0,"class":"HRegionServer","responsesize":17481,"method":"Multi"}
2014-07-22 13:04:49,184 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059477953,"queuetimems":0,"class":"HRegionServer","responsesize":16991,"method":"Multi"}
2014-07-22 13:04:49,184 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478037,"queuetimems":0,"class":"HRegionServer","responsesize":16867,"method":"Multi"}
2014-07-22 13:04:49,184 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11112,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478072,"queuetimems":0,"class":"HRegionServer","responsesize":16878,"method":"Multi"}
2014-07-22 13:04:49,184 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478436,"queuetimems":0,"class":"HRegionServer","responsesize":17006,"method":"Multi"}
2014-07-22 13:04:49,205 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478500,"queuetimems":0,"class":"HRegionServer","responsesize":16955,"method":"Multi"}
2014-07-22 13:04:49,246 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478567,"queuetimems":0,"class":"HRegionServer","responsesize":16842,"method":"Multi"}
2014-07-22 13:04:49,250 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059478243,"queuetimems":1,"class":"HRegionServer","responsesize":17055,"method":"Multi"}
2014-07-22 13:04:50,226 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:50,266 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44599 synced till here 44596
2014-07-22 13:04:50,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059488807 with entries=113, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059490227
2014-07-22 13:04:50,312 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:51,851 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:51,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44694 synced till here 44691
2014-07-22 13:04:51,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059490227 with entries=95, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059491851
2014-07-22 13:04:51,937 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:53,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:53,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44781 synced till here 44779
2014-07-22 13:04:53,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059491851 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059493568
2014-07-22 13:04:53,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:54,782 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:55,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44880 synced till here 44879
2014-07-22 13:04:55,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059493568 with entries=99, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059494784
2014-07-22 13:04:55,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:56,101 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8757, memsize=452.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d6dbb9ef6f0c45e795dbd7c71ba3af40
2014-07-22 13:04:56,367 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,368 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,371 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,399 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,408 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,422 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,443 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,553 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,603 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,841 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:04:56,843 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d6dbb9ef6f0c45e795dbd7c71ba3af40 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d6dbb9ef6f0c45e795dbd7c71ba3af40
2014-07-22 13:04:56,853 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d6dbb9ef6f0c45e795dbd7c71ba3af40, entries=1648650, sequenceid=8757, filesize=117.4m
2014-07-22 13:04:56,853 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~782.0m/819941840, currentsize=398.6m/418013280 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 29586ms, sequenceid=8757, compaction requested=true
2014-07-22 13:04:56,854 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:04:56,854 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 13:04:56,854 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-07-22 13:04:56,854 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,854 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 13:04:56,854 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 251ms
2014-07-22 13:04:56,854 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,854 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:04:56,854 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 834.2m
2014-07-22 13:04:56,855 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:04:56,855 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 302ms
2014-07-22 13:04:56,855 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:04:56,855 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,855 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 413ms
2014-07-22 13:04:56,855 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,860 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 438ms
2014-07-22 13:04:56,860 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,860 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 452ms
2014-07-22 13:04:56,860 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,860 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 461ms
2014-07-22 13:04:56,860 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,860 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 489ms
2014-07-22 13:04:56,860 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,865 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 497ms
2014-07-22 13:04:56,865 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,865 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 498ms
2014-07-22 13:04:56,865 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:04:56,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:56,991 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:04:57,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 44977 synced till here 44969
2014-07-22 13:04:57,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059494784 with entries=97, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059496906
2014-07-22 13:04:57,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:58,480 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:04:58,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:04:58,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45067 synced till here 45056
2014-07-22 13:04:58,767 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059496906 with entries=90, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059498650
2014-07-22 13:04:58,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:04:59,941 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:00,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45178 synced till here 45174
2014-07-22 13:05:00,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059498650 with entries=111, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059499941
2014-07-22 13:05:00,737 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:05:02,573 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:02,641 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45271 synced till here 45255
2014-07-22 13:05:02,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059499941 with entries=93, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059502573
2014-07-22 13:05:02,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:05:04,390 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:04,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45365 synced till here 45360
2014-07-22 13:05:04,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059502573 with entries=94, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059504390
2014-07-22 13:05:04,781 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=45, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:05:06,682 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:06,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45453 synced till here 45451
2014-07-22 13:05:06,722 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059504390 with entries=88, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059506682
2014-07-22 13:05:06,723 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=46, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:05:08,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:08,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45539 synced till here 45537
2014-07-22 13:05:08,328 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,331 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,331 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059506682 with entries=86, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059508250
2014-07-22 13:05:08,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=47, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:05:08,362 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,381 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,382 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,400 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,405 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,406 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,406 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,407 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,408 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,411 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,454 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,485 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,516 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,547 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,604 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,676 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,820 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,918 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,951 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:08,982 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:09,014 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:09,046 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:09,078 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:09,108 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:09,140 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,664 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,695 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,728 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,760 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,796 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,839 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,898 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,944 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:10,981 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:11,021 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:11,056 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,174 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,185 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,205 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,242 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,564 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,602 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,636 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:12,667 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:13,282 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=8868, memsize=426.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/60e0724fcf684e51be3f0a0a7e0b0218
2014-07-22 13:05:13,304 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/60e0724fcf684e51be3f0a0a7e0b0218 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/60e0724fcf684e51be3f0a0a7e0b0218
2014-07-22 13:05:13,328 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:05:13,329 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/60e0724fcf684e51be3f0a0a7e0b0218, entries=1553130, sequenceid=8868, filesize=110.6m
2014-07-22 13:05:13,329 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~823.9m/863974000, currentsize=406.9m/426714400 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 28223ms, sequenceid=8868, compaction requested=true
2014-07-22 13:05:13,330 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:05:13,330 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 13:05:13,330 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 13:05:13,330 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 13:05:13,330 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,330 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 430.8m
2014-07-22 13:05:13,330 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 663ms
2014-07-22 13:05:13,330 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:05:13,330 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,330 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:05:13,331 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:05:13,331 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 695ms
2014-07-22 13:05:13,331 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,332 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 729ms
2014-07-22 13:05:13,332 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,332 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 768ms
2014-07-22 13:05:13,332 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,332 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1090ms
2014-07-22 13:05:13,332 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,332 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1127ms
2014-07-22 13:05:13,332 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,335 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1149ms
2014-07-22 13:05:13,335 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,335 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1161ms
2014-07-22 13:05:13,335 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,343 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2287ms
2014-07-22 13:05:13,343 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,344 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2323ms
2014-07-22 13:05:13,344 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,345 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2363ms
2014-07-22 13:05:13,345 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,345 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2401ms
2014-07-22 13:05:13,345 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,345 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2447ms
2014-07-22 13:05:13,345 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,348 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2509ms
2014-07-22 13:05:13,348 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,348 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2552ms
2014-07-22 13:05:13,348 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,349 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2590ms
2014-07-22 13:05:13,350 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,351 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2623ms
2014-07-22 13:05:13,351 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,352 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2656ms
2014-07-22 13:05:13,352 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,352 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2689ms
2014-07-22 13:05:13,352 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,353 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4212ms
2014-07-22 13:05:13,353 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,354 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4245ms
2014-07-22 13:05:13,354 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,354 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4276ms
2014-07-22 13:05:13,354 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,355 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4309ms
2014-07-22 13:05:13,355 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,355 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4341ms
2014-07-22 13:05:13,355 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,355 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4373ms
2014-07-22 13:05:13,356 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,357 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4406ms
2014-07-22 13:05:13,358 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,358 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4440ms
2014-07-22 13:05:13,359 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,360 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4539ms
2014-07-22 13:05:13,360 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,361 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4685ms
2014-07-22 13:05:13,361 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,362 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4758ms
2014-07-22 13:05:13,362 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,364 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4816ms
2014-07-22 13:05:13,364 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,364 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4848ms
2014-07-22 13:05:13,364 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,365 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4880ms
2014-07-22 13:05:13,365 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,366 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4911ms
2014-07-22 13:05:13,366 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,366 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4955ms
2014-07-22 13:05:13,366 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,368 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4960ms
2014-07-22 13:05:13,368 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,369 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4961ms
2014-07-22 13:05:13,369 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,369 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4963ms
2014-07-22 13:05:13,369 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,370 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4963ms
2014-07-22 13:05:13,370 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,371 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4966ms
2014-07-22 13:05:13,372 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,372 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4972ms
2014-07-22 13:05:13,372 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,372 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4990ms
2014-07-22 13:05:13,372 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,384 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:05:13,384 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,384 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5022ms
2014-07-22 13:05:13,384 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,385 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5054ms
2014-07-22 13:05:13,385 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:13,558 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:05:13,885 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:05:14,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:14,120 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45626 synced till here 45618
2014-07-22 13:05:14,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059508250 with entries=87, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059514028
2014-07-22 13:05:15,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:16,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45734 synced till here 45719
2014-07-22 13:05:16,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059514028 with entries=108, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059515927
2014-07-22 13:05:17,613 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:17,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45850 synced till here 45842
2014-07-22 13:05:17,766 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059515927 with entries=116, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059517614
2014-07-22 13:05:18,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:19,550 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 45955 synced till here 45937
2014-07-22 13:05:19,707 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059517614 with entries=105, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059518983
2014-07-22 13:05:20,887 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46046 synced till here 46043
2014-07-22 13:05:21,000 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059518983 with entries=91, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059520887
2014-07-22 13:05:22,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:22,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46135 synced till here 46131
2014-07-22 13:05:22,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059520887 with entries=89, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059522437
2014-07-22 13:05:22,756 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,758 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,758 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,758 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,769 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,803 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,803 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,804 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,814 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,823 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,835 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,836 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,874 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,905 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,934 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:22,978 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,027 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,073 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,125 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,157 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,189 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,242 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,274 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,330 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,363 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,396 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,452 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,491 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,530 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,566 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,603 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,636 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:23,949 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:24,161 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:24,403 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:24,622 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:24,973 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:25,067 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:25,113 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:25,153 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,333 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,346 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,362 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,396 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,418 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9025, memsize=428.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/d2a42121a136415ea7f35ba03e3a4767
2014-07-22 13:05:26,427 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:26,433 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/d2a42121a136415ea7f35ba03e3a4767 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/d2a42121a136415ea7f35ba03e3a4767
2014-07-22 13:05:26,442 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/d2a42121a136415ea7f35ba03e3a4767, entries=1559880, sequenceid=9025, filesize=111.1m
2014-07-22 13:05:26,442 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~834.2m/874675360, currentsize=378.5m/396932400 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 29588ms, sequenceid=9025, compaction requested=true
2014-07-22 13:05:26,443 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:05:26,444 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-07-22 13:05:26,444 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 13:05:26,444 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 987.9m
2014-07-22 13:05:26,444 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,444 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 13:05:26,444 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 48ms
2014-07-22 13:05:26,444 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,444 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:05:26,444 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:05:26,444 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 82ms
2014-07-22 13:05:26,444 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,445 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:05:26,445 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 99ms
2014-07-22 13:05:26,445 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,445 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 112ms
2014-07-22 13:05:26,445 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,454 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1301ms
2014-07-22 13:05:26,454 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,454 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1341ms
2014-07-22 13:05:26,454 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,454 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1387ms
2014-07-22 13:05:26,454 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,454 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1481ms
2014-07-22 13:05:26,455 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,455 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1833ms
2014-07-22 13:05:26,455 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,456 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2053ms
2014-07-22 13:05:26,456 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,458 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2297ms
2014-07-22 13:05:26,458 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,458 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2509ms
2014-07-22 13:05:26,458 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,458 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2822ms
2014-07-22 13:05:26,458 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,458 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2855ms
2014-07-22 13:05:26,459 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,459 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2893ms
2014-07-22 13:05:26,459 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,459 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2929ms
2014-07-22 13:05:26,459 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,459 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2968ms
2014-07-22 13:05:26,459 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,463 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3012ms
2014-07-22 13:05:26,463 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,463 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3067ms
2014-07-22 13:05:26,464 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,464 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3101ms
2014-07-22 13:05:26,464 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,465 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3135ms
2014-07-22 13:05:26,465 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,467 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3193ms
2014-07-22 13:05:26,467 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,467 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3225ms
2014-07-22 13:05:26,467 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,467 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3279ms
2014-07-22 13:05:26,467 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,468 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3312ms
2014-07-22 13:05:26,468 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,469 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3343ms
2014-07-22 13:05:26,469 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,469 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3396ms
2014-07-22 13:05:26,469 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,470 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3443ms
2014-07-22 13:05:26,470 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,470 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3492ms
2014-07-22 13:05:26,470 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,470 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3536ms
2014-07-22 13:05:26,470 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,472 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3567ms
2014-07-22 13:05:26,472 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,474 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3600ms
2014-07-22 13:05:26,474 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,475 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3640ms
2014-07-22 13:05:26,475 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,475 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3640ms
2014-07-22 13:05:26,475 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,475 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3652ms
2014-07-22 13:05:26,475 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,475 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3661ms
2014-07-22 13:05:26,476 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,477 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3674ms
2014-07-22 13:05:26,477 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,477 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3674ms
2014-07-22 13:05:26,477 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,500 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-22 13:05:26,500 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,503 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3734ms
2014-07-22 13:05:26,503 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,503 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3745ms
2014-07-22 13:05:26,503 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,503 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3745ms
2014-07-22 13:05:26,503 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,503 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3746ms
2014-07-22 13:05:26,504 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,504 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3748ms
2014-07-22 13:05:26,504 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:26,808 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:05:27,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:27,281 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46236 synced till here 46235
2014-07-22 13:05:27,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059522437 with entries=101, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059527193
2014-07-22 13:05:27,473 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:05:29,499 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:29,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46348 synced till here 46315
2014-07-22 13:05:29,559 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9113, memsize=224.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/af4b19d3fcb44cb586596c3f81cdbfcd
2014-07-22 13:05:29,573 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/af4b19d3fcb44cb586596c3f81cdbfcd as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/af4b19d3fcb44cb586596c3f81cdbfcd
2014-07-22 13:05:29,583 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/af4b19d3fcb44cb586596c3f81cdbfcd, entries=815930, sequenceid=9113, filesize=58.1m
2014-07-22 13:05:29,584 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~430.8m/451756480, currentsize=63.3m/66391040 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 16254ms, sequenceid=9113, compaction requested=true
2014-07-22 13:05:29,584 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:05:29,584 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 2000 blocking
2014-07-22 13:05:29,585 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 791.8m
2014-07-22 13:05:29,585 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-22 13:05:29,585 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:05:29,585 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:05:29,585 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:05:29,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059527193 with entries=112, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059529499
2014-07-22 13:05:29,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059412658
2014-07-22 13:05:29,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059415235
2014-07-22 13:05:29,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059416688
2014-07-22 13:05:29,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059419619
2014-07-22 13:05:29,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059421837
2014-07-22 13:05:29,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059424119
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059425008
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059426772
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059428034
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059430272
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059432745
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059436184
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059438019
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059439438
2014-07-22 13:05:29,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059441015
2014-07-22 13:05:29,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059442405
2014-07-22 13:05:29,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059444324
2014-07-22 13:05:29,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059445227
2014-07-22 13:05:29,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059447409
2014-07-22 13:05:29,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059449013
2014-07-22 13:05:29,750 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059450806
2014-07-22 13:05:31,732 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:31,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46468 synced till here 46450
2014-07-22 13:05:31,940 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059529499 with entries=120, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059531733
2014-07-22 13:05:31,977 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:05:33,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:33,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46587 synced till here 46558
2014-07-22 13:05:35,317 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1148ms
GC pool 'ParNew' had collection(s): count=1 time=1231ms
2014-07-22 13:05:35,432 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059531733 with entries=119, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059533880
2014-07-22 13:05:37,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:37,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46713 synced till here 46691
2014-07-22 13:05:37,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059533880 with entries=126, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059537419
2014-07-22 13:05:38,448 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:38,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46827 synced till here 46809
2014-07-22 13:05:39,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059537419 with entries=114, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059538448
2014-07-22 13:05:40,254 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:41,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 46959 synced till here 46929
2014-07-22 13:05:41,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059538448 with entries=132, filesize=106.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059540254
2014-07-22 13:05:42,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:43,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47086 synced till here 47074
2014-07-22 13:05:43,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059540254 with entries=127, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059542547
2014-07-22 13:05:43,844 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,845 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,846 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,847 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,847 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,847 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,850 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,850 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,850 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,850 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,851 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:43,852 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,056 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,089 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,122 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,155 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,188 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,220 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,254 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,287 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:44,320 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:46,052 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:46,704 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,486 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,518 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,551 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,593 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,634 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,670 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,711 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,750 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,803 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:47,834 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,583 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,598 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,613 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,643 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,674 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,709 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,738 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:48,845 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:48,846 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:48,847 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:48,847 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:48,847 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:48,848 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:48,850 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:48,850 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:05:48,851 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:48,851 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:48,851 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:48,853 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:49,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:49,089 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:49,123 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:49,155 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:49,189 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:49,221 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:49,255 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:49,287 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:49,321 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:50,083 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:50,120 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:50,153 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:50,186 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:50,219 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:50,251 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:51,052 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:51,704 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,080 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:52,112 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:52,144 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:05:52,488 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:52,519 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,551 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:52,596 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,636 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:52,671 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,712 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,751 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:52,804 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:52,835 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:53,584 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:53,599 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:53,613 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:53,644 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:53,675 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:53,709 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:53,739 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:53,846 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:53,847 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:05:53,847 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:53,847 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:05:53,848 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:53,849 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:05:53,851 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:05:53,851 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:53,852 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:53,853 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:05:53,853 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-07-22 13:05:53,854 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:05:54,057 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,090 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:05:54,124 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,157 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,189 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:05:54,222 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,255 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,288 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:54,321 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:55,085 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:55,121 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:55,154 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:55,187 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:55,219 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:05:55,252 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:05:56,053 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:05:56,198 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9316, memsize=395.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/10c08eb99e7e4e1a841eb4705f39b1fe
2014-07-22 13:05:56,211 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/10c08eb99e7e4e1a841eb4705f39b1fe as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/10c08eb99e7e4e1a841eb4705f39b1fe
2014-07-22 13:05:56,221 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/10c08eb99e7e4e1a841eb4705f39b1fe, entries=1440420, sequenceid=9316, filesize=102.5m
2014-07-22 13:05:56,222 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~855.0m/896495920, currentsize=210.2m/220445920 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 26637ms, sequenceid=9316, compaction requested=true
2014-07-22 13:05:56,222 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:05:56,223 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 13:05:56,223 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 13:05:56,223 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10171ms
2014-07-22 13:05:56,223 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:05:56,223 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:05:56,223 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 898.8m
2014-07-22 13:05:56,223 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:05:56,223 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,224 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5973ms
2014-07-22 13:05:56,224 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,224 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6005ms
2014-07-22 13:05:56,224 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,224 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6038ms
2014-07-22 13:05:56,224 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,225 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6072ms
2014-07-22 13:05:56,225 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,227 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6106ms
2014-07-22 13:05:56,227 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,229 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6146ms
2014-07-22 13:05:56,229 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,230 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11909ms
2014-07-22 13:05:56,230 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,230 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11943ms
2014-07-22 13:05:56,230 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,230 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11976ms
2014-07-22 13:05:56,230 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,231 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12011ms
2014-07-22 13:05:56,231 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,231 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12044ms
2014-07-22 13:05:56,231 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,232 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12077ms
2014-07-22 13:05:56,232 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,232 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12110ms
2014-07-22 13:05:56,232 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,232 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12143ms
2014-07-22 13:05:56,232 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,233 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12177ms
2014-07-22 13:05:56,234 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,234 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12382ms
2014-07-22 13:05:56,234 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,235 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12388ms
2014-07-22 13:05:56,235 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,235 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12385ms
2014-07-22 13:05:56,235 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,235 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12385ms
2014-07-22 13:05:56,235 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,236 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12386ms
2014-07-22 13:05:56,236 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,237 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12387ms
2014-07-22 13:05:56,237 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,240 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12392ms
2014-07-22 13:05:56,240 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,240 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12393ms
2014-07-22 13:05:56,240 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,240 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12393ms
2014-07-22 13:05:56,240 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,241 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12395ms
2014-07-22 13:05:56,241 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,241 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12396ms
2014-07-22 13:05:56,241 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,243 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12399ms
2014-07-22 13:05:56,243 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,243 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7505ms
2014-07-22 13:05:56,244 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,249 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7541ms
2014-07-22 13:05:56,249 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,250 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7575ms
2014-07-22 13:05:56,250 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,254 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7611ms
2014-07-22 13:05:56,254 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,255 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7643ms
2014-07-22 13:05:56,255 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,256 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7658ms
2014-07-22 13:05:56,256 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,257 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7674ms
2014-07-22 13:05:56,257 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,257 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8423ms
2014-07-22 13:05:56,258 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,259 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8455ms
2014-07-22 13:05:56,259 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,264 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8514ms
2014-07-22 13:05:56,264 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,265 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8553ms
2014-07-22 13:05:56,265 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,265 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8595ms
2014-07-22 13:05:56,265 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,265 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8631ms
2014-07-22 13:05:56,265 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,266 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8673ms
2014-07-22 13:05:56,266 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,269 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8719ms
2014-07-22 13:05:56,269 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,272 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8754ms
2014-07-22 13:05:56,273 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,273 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8787ms
2014-07-22 13:05:56,273 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,275 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4129ms
2014-07-22 13:05:56,275 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,275 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4164ms
2014-07-22 13:05:56,275 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,276 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4196ms
2014-07-22 13:05:56,276 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,276 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9572ms
2014-07-22 13:05:56,276 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:05:56,320 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13771,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542548,"queuetimems":1033,"class":"HRegionServer","responsesize":16826,"method":"Multi"}
2014-07-22 13:05:56,321 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13773,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542548,"queuetimems":1068,"class":"HRegionServer","responsesize":16652,"method":"Multi"}
2014-07-22 13:05:56,340 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13989,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542351,"queuetimems":906,"class":"HRegionServer","responsesize":16773,"method":"Multi"}
2014-07-22 13:05:57,444 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:57,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47195 synced till here 47169
2014-07-22 13:05:57,602 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9251, memsize=447.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0e3d0e84a80a4b12adb2e37c0b21c737
2014-07-22 13:05:57,633 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0e3d0e84a80a4b12adb2e37c0b21c737 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0e3d0e84a80a4b12adb2e37c0b21c737
2014-07-22 13:05:57,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059542547 with entries=109, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059557444
2014-07-22 13:05:57,665 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0e3d0e84a80a4b12adb2e37c0b21c737, entries=1628900, sequenceid=9251, filesize=116.0m
2014-07-22 13:05:57,665 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~987.9m/1035916000, currentsize=312.7m/327877120 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 31221ms, sequenceid=9251, compaction requested=true
2014-07-22 13:05:57,666 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:05:57,666 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 13:05:57,666 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 13:05:57,666 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 730.7m
2014-07-22 13:05:57,666 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:05:57,666 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:05:57,666 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:05:57,712 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15160,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542552,"queuetimems":939,"class":"HRegionServer","responsesize":16664,"method":"Multi"}
2014-07-22 13:05:57,713 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15153,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542559,"queuetimems":734,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-22 13:05:57,712 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542553,"queuetimems":869,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-07-22 13:05:57,712 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542548,"queuetimems":1000,"class":"HRegionServer","responsesize":17168,"method":"Multi"}
2014-07-22 13:05:57,714 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15160,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542553,"queuetimems":834,"class":"HRegionServer","responsesize":16712,"method":"Multi"}
2014-07-22 13:05:57,714 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542554,"queuetimems":794,"class":"HRegionServer","responsesize":17578,"method":"Multi"}
2014-07-22 13:05:57,713 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542549,"queuetimems":969,"class":"HRegionServer","responsesize":16887,"method":"Multi"}
2014-07-22 13:05:57,713 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15154,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542559,"queuetimems":690,"class":"HRegionServer","responsesize":17306,"method":"Multi"}
2014-07-22 13:05:57,715 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059542559,"queuetimems":658,"class":"HRegionServer","responsesize":17039,"method":"Multi"}
2014-07-22 13:05:58,109 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:05:58,253 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:05:58,278 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12228,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059546050,"queuetimems":0,"class":"HRegionServer","responsesize":17004,"method":"Multi"}
2014-07-22 13:05:58,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:05:58,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47315 synced till here 47275
2014-07-22 13:05:58,502 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:05:59,389 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:05:59,434 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059557444 with entries=120, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059558335
2014-07-22 13:05:59,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059452397
2014-07-22 13:05:59,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059453940
2014-07-22 13:05:59,434 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059455062
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059456343
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059457848
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059460341
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059462450
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059464996
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059469250
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059471386
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059472370
2014-07-22 13:05:59,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059473993
2014-07-22 13:05:59,451 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548641,"queuetimems":0,"class":"HRegionServer","responsesize":16932,"method":"Multi"}
2014-07-22 13:05:59,452 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15331,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544121,"queuetimems":0,"class":"HRegionServer","responsesize":16945,"method":"Multi"}
2014-07-22 13:05:59,452 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548596,"queuetimems":0,"class":"HRegionServer","responsesize":17039,"method":"Multi"}
2014-07-22 13:05:59,455 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059546702,"queuetimems":0,"class":"HRegionServer","responsesize":17006,"method":"Multi"}
2014-07-22 13:05:59,480 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548705,"queuetimems":0,"class":"HRegionServer","responsesize":17168,"method":"Multi"}
2014-07-22 13:05:59,481 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11998,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547483,"queuetimems":0,"class":"HRegionServer","responsesize":16652,"method":"Multi"}
2014-07-22 13:05:59,928 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12301,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547626,"queuetimems":0,"class":"HRegionServer","responsesize":16887,"method":"Multi"}
2014-07-22 13:05:59,951 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547792,"queuetimems":1,"class":"HRegionServer","responsesize":17299,"method":"Multi"}
2014-07-22 13:05:59,951 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12203,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547748,"queuetimems":0,"class":"HRegionServer","responsesize":17302,"method":"Multi"}
2014-07-22 13:05:59,951 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15798,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544153,"queuetimems":0,"class":"HRegionServer","responsesize":17082,"method":"Multi"}
2014-07-22 13:05:59,993 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12444,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547548,"queuetimems":0,"class":"HRegionServer","responsesize":17306,"method":"Multi"}
2014-07-22 13:05:59,993 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544319,"queuetimems":1,"class":"HRegionServer","responsesize":17141,"method":"Multi"}
2014-07-22 13:06:00,019 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11437,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548581,"queuetimems":1,"class":"HRegionServer","responsesize":16664,"method":"Multi"}
2014-07-22 13:06:00,019 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544087,"queuetimems":0,"class":"HRegionServer","responsesize":16630,"method":"Multi"}
2014-07-22 13:06:00,019 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11281,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548737,"queuetimems":1,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-22 13:06:00,020 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11346,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548673,"queuetimems":1,"class":"HRegionServer","responsesize":16826,"method":"Multi"}
2014-07-22 13:06:00,020 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547589,"queuetimems":1,"class":"HRegionServer","responsesize":15863,"method":"Multi"}
2014-07-22 13:06:00,020 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12503,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547516,"queuetimems":0,"class":"HRegionServer","responsesize":16712,"method":"Multi"}
2014-07-22 13:06:00,019 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547831,"queuetimems":1,"class":"HRegionServer","responsesize":16773,"method":"Multi"}
2014-07-22 13:06:00,020 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544054,"queuetimems":0,"class":"HRegionServer","responsesize":16921,"method":"Multi"}
2014-07-22 13:06:00,019 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15733,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544285,"queuetimems":0,"class":"HRegionServer","responsesize":17192,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059548611,"queuetimems":1,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544252,"queuetimems":0,"class":"HRegionServer","responsesize":16976,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12313,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547708,"queuetimems":1,"class":"HRegionServer","responsesize":16974,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15803,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544218,"queuetimems":0,"class":"HRegionServer","responsesize":17033,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059547667,"queuetimems":0,"class":"HRegionServer","responsesize":17578,"method":"Multi"}
2014-07-22 13:06:00,021 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059544186,"queuetimems":0,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:06:00,428 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:00,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47424 synced till here 47419
2014-07-22 13:06:00,565 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059558335 with entries=109, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059560429
2014-07-22 13:06:01,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:01,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47514 synced till here 47507
2014-07-22 13:06:02,050 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059560429 with entries=90, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059561872
2014-07-22 13:06:03,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:03,617 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47603 synced till here 47599
2014-07-22 13:06:03,673 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059561872 with entries=89, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059563569
2014-07-22 13:06:05,083 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:05,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059563569 with entries=117, filesize=85.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059565083
2014-07-22 13:06:07,007 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:07,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47808 synced till here 47803
2014-07-22 13:06:07,103 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059565083 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059567008
2014-07-22 13:06:08,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:08,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 47898 synced till here 47891
2014-07-22 13:06:08,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059567008 with entries=90, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059568699
2014-07-22 13:06:09,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:10,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059568699 with entries=94, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059569725
2014-07-22 13:06:11,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:12,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48099 synced till here 48094
2014-07-22 13:06:12,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059569725 with entries=107, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059571291
2014-07-22 13:06:13,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:13,387 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48192 synced till here 48184
2014-07-22 13:06:13,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059571291 with entries=93, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059573365
2014-07-22 13:06:15,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:15,638 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48306 synced till here 48305
2014-07-22 13:06:15,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059573365 with entries=114, filesize=83.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059575242
2014-07-22 13:06:16,568 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:06:17,318 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:17,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48394 synced till here 48388
2014-07-22 13:06:17,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059575242 with entries=88, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059577319
2014-07-22 13:06:18,603 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9503, memsize=241.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b7b3fc34058c47c8bd1af9a4668d51f8
2014-07-22 13:06:18,618 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b7b3fc34058c47c8bd1af9a4668d51f8 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b7b3fc34058c47c8bd1af9a4668d51f8
2014-07-22 13:06:18,637 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b7b3fc34058c47c8bd1af9a4668d51f8, entries=878200, sequenceid=9503, filesize=62.6m
2014-07-22 13:06:18,637 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~732.3m/767833760, currentsize=349.3m/366262080 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 20971ms, sequenceid=9503, compaction requested=true
2014-07-22 13:06:18,638 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:06:18,638 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 13:06:18,638 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 13:06:18,638 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:06:18,638 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 707.8m
2014-07-22 13:06:18,638 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:06:18,639 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:06:18,661 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:06:18,833 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:18,867 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48485 synced till here 48477
2014-07-22 13:06:18,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059577319 with entries=91, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059578834
2014-07-22 13:06:19,288 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:06:20,143 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:20,276 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48585 synced till here 48582
2014-07-22 13:06:20,317 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059578834 with entries=100, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059580144
2014-07-22 13:06:21,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:21,990 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48692 synced till here 48683
2014-07-22 13:06:22,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059580144 with entries=107, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059581803
2014-07-22 13:06:24,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:24,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48783 synced till here 48776
2014-07-22 13:06:24,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059581803 with entries=91, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059584046
2014-07-22 13:06:25,875 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:25,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 48886 synced till here 48870
2014-07-22 13:06:26,020 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059584046 with entries=103, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059585876
2014-07-22 13:06:28,020 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:28,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49003 synced till here 48991
2014-07-22 13:06:28,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059585876 with entries=117, filesize=85.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059588021
2014-07-22 13:06:29,908 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1397ms
GC pool 'ParNew' had collection(s): count=1 time=1444ms
2014-07-22 13:06:30,195 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,195 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,195 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,197 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,197 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,199 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,203 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,205 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,209 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,213 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,213 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,213 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,241 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,278 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,317 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,362 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,362 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,362 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,363 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,364 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,365 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,452 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,485 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,509 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:30,511 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,512 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,512 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,512 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,513 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,513 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,514 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,515 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,516 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,516 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,517 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,519 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,520 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,520 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,520 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,520 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,521 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,521 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,523 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,525 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,526 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,526 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,526 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,526 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,527 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,547 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,581 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:30,934 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9456, memsize=356.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/1ec4c93921124c7bbcf4da2e1743660e
2014-07-22 13:06:30,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059588021 with entries=74, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059590509
2014-07-22 13:06:30,994 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/1ec4c93921124c7bbcf4da2e1743660e as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/1ec4c93921124c7bbcf4da2e1743660e
2014-07-22 13:06:31,008 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/1ec4c93921124c7bbcf4da2e1743660e, entries=1296360, sequenceid=9456, filesize=92.3m
2014-07-22 13:06:31,009 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~898.8m/942461280, currentsize=607.2m/636675760 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 34786ms, sequenceid=9456, compaction requested=true
2014-07-22 13:06:31,011 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:06:31,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 13:06:31,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 13:06:31,011 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 430ms
2014-07-22 13:06:31,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:06:31,011 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,011 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:06:31,011 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 464ms
2014-07-22 13:06:31,012 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:06:31,011 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 821.9m
2014-07-22 13:06:31,012 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,012 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 485ms
2014-07-22 13:06:31,012 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,012 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 486ms
2014-07-22 13:06:31,013 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,013 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-22 13:06:31,013 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,019 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 494ms
2014-07-22 13:06:31,019 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,019 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 494ms
2014-07-22 13:06:31,019 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,020 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 495ms
2014-07-22 13:06:31,020 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,020 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 497ms
2014-07-22 13:06:31,020 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,020 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 499ms
2014-07-22 13:06:31,020 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,021 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 500ms
2014-07-22 13:06:31,021 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,034 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 514ms
2014-07-22 13:06:31,039 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,041 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 521ms
2014-07-22 13:06:31,041 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,041 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 521ms
2014-07-22 13:06:31,042 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,042 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 523ms
2014-07-22 13:06:31,042 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,042 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 523ms
2014-07-22 13:06:31,042 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,043 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 526ms
2014-07-22 13:06:31,063 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,063 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 547ms
2014-07-22 13:06:31,063 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,064 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 548ms
2014-07-22 13:06:31,064 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,072 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 557ms
2014-07-22 13:06:31,072 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,072 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 560ms
2014-07-22 13:06:31,072 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,072 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 559ms
2014-07-22 13:06:31,072 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,073 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 560ms
2014-07-22 13:06:31,073 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,073 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 561ms
2014-07-22 13:06:31,073 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,074 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 562ms
2014-07-22 13:06:31,074 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,074 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 562ms
2014-07-22 13:06:31,074 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,074 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 563ms
2014-07-22 13:06:31,074 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,085 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 600ms
2014-07-22 13:06:31,085 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,093 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 641ms
2014-07-22 13:06:31,093 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,101 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 737ms
2014-07-22 13:06:31,101 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,101 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 737ms
2014-07-22 13:06:31,102 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,107 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 744ms
2014-07-22 13:06:31,108 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,117 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 755ms
2014-07-22 13:06:31,117 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,117 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 755ms
2014-07-22 13:06:31,118 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,118 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 757ms
2014-07-22 13:06:31,118 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,118 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 801ms
2014-07-22 13:06:31,118 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,118 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 840ms
2014-07-22 13:06:31,118 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,126 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 885ms
2014-07-22 13:06:31,126 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,127 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 918ms
2014-07-22 13:06:31,127 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,127 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 918ms
2014-07-22 13:06:31,127 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,137 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 924ms
2014-07-22 13:06:31,137 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,141 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 933ms
2014-07-22 13:06:31,141 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,141 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 936ms
2014-07-22 13:06:31,141 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,145 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 942ms
2014-07-22 13:06:31,145 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,146 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 947ms
2014-07-22 13:06:31,146 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,153 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 956ms
2014-07-22 13:06:31,153 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,154 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 957ms
2014-07-22 13:06:31,154 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,154 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 959ms
2014-07-22 13:06:31,154 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,165 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 970ms
2014-07-22 13:06:31,165 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:31,166 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 972ms
2014-07-22 13:06:31,166 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:32,510 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1101ms
GC pool 'ParNew' had collection(s): count=1 time=1183ms
2014-07-22 13:06:32,894 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:06:33,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:33,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49179 synced till here 49168
2014-07-22 13:06:33,254 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059590509 with entries=102, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059593036
2014-07-22 13:06:33,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059476104
2014-07-22 13:06:33,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059485292
2014-07-22 13:06:33,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059487232
2014-07-22 13:06:33,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059488807
2014-07-22 13:06:33,257 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059490227
2014-07-22 13:06:33,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059491851
2014-07-22 13:06:33,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059493568
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059494784
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059496906
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059498650
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059499941
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059502573
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059504390
2014-07-22 13:06:33,271 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059506682
2014-07-22 13:06:33,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:33,431 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:06:35,120 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:35,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49289 synced till here 49260
2014-07-22 13:06:35,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059593036 with entries=110, filesize=85.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059595121
2014-07-22 13:06:35,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:37,336 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1030ms
GC pool 'ParNew' had collection(s): count=1 time=1266ms
2014-07-22 13:06:37,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:37,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49410 synced till here 49392
2014-07-22 13:06:37,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059595121 with entries=121, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059597785
2014-07-22 13:06:37,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:39,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:40,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49519 synced till here 49506
2014-07-22 13:06:40,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059597785 with entries=109, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059599993
2014-07-22 13:06:40,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:41,845 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:41,892 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49611 synced till here 49596
2014-07-22 13:06:42,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059599993 with entries=92, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059601845
2014-07-22 13:06:42,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:42,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:42,700 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49714 synced till here 49703
2014-07-22 13:06:44,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059601845 with entries=103, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059602677
2014-07-22 13:06:44,486 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:06:44,655 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,656 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,657 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,662 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,704 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,849 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,850 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,850 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,850 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,850 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,851 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,851 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,851 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,852 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,852 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,852 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,852 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,853 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,853 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,876 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,908 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,941 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:44,974 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,007 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,039 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,072 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,102 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,135 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,914 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:45,946 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,818 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,852 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,883 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,914 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,946 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:46,982 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:47,010 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:47,111 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9713, memsize=290.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/ec1540735c404c8ea851e82ddab983b2
2014-07-22 13:06:47,126 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/ec1540735c404c8ea851e82ddab983b2 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/ec1540735c404c8ea851e82ddab983b2
2014-07-22 13:06:47,134 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/ec1540735c404c8ea851e82ddab983b2, entries=1056770, sequenceid=9713, filesize=75.2m
2014-07-22 13:06:47,134 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~712.4m/746986160, currentsize=398.4m/417744400 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 28496ms, sequenceid=9713, compaction requested=true
2014-07-22 13:06:47,134 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:06:47,135 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 13:06:47,135 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 125ms
2014-07-22 13:06:47,135 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 13:06:47,135 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 387.7m
2014-07-22 13:06:47,135 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,135 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:06:47,135 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:06:47,135 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 153ms
2014-07-22 13:06:47,135 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,135 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:06:47,136 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 190ms
2014-07-22 13:06:47,136 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,136 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 222ms
2014-07-22 13:06:47,136 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,143 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 260ms
2014-07-22 13:06:47,143 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,144 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 292ms
2014-07-22 13:06:47,144 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,144 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 326ms
2014-07-22 13:06:47,144 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,144 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1198ms
2014-07-22 13:06:47,144 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,144 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1230ms
2014-07-22 13:06:47,144 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,145 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2010ms
2014-07-22 13:06:47,145 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,145 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2043ms
2014-07-22 13:06:47,145 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,146 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2074ms
2014-07-22 13:06:47,146 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,146 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2107ms
2014-07-22 13:06:47,146 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,146 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2139ms
2014-07-22 13:06:47,146 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,146 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2172ms
2014-07-22 13:06:47,146 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,157 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2216ms
2014-07-22 13:06:47,157 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,157 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2249ms
2014-07-22 13:06:47,157 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,164 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2288ms
2014-07-22 13:06:47,164 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,164 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2455ms
2014-07-22 13:06:47,165 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,165 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2427ms
2014-07-22 13:06:47,165 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,165 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2427ms
2014-07-22 13:06:47,165 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,165 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2423ms
2014-07-22 13:06:47,165 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,165 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2416ms
2014-07-22 13:06:47,165 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,173 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2424ms
2014-07-22 13:06:47,173 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,173 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2423ms
2014-07-22 13:06:47,173 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,174 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2404ms
2014-07-22 13:06:47,174 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,174 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2399ms
2014-07-22 13:06:47,174 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,174 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2399ms
2014-07-22 13:06:47,174 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,175 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2395ms
2014-07-22 13:06:47,175 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,202 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2389ms
2014-07-22 13:06:47,202 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,209 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2390ms
2014-07-22 13:06:47,209 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,209 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2365ms
2014-07-22 13:06:47,209 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,218 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2514ms
2014-07-22 13:06:47,218 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,220 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2558ms
2014-07-22 13:06:47,221 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,221 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2564ms
2014-07-22 13:06:47,221 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,221 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2565ms
2014-07-22 13:06:47,222 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,222 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2567ms
2014-07-22 13:06:47,222 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:47,353 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:06:48,810 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1104ms
GC pool 'ParNew' had collection(s): count=1 time=1336ms
2014-07-22 13:06:48,863 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:06:49,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:49,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49809 synced till here 49793
2014-07-22 13:06:49,229 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059602677 with entries=95, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059609057
2014-07-22 13:06:51,024 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1213ms
GC pool 'ParNew' had collection(s): count=1 time=1337ms
2014-07-22 13:06:51,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:51,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 49914 synced till here 49888
2014-07-22 13:06:51,489 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059609057 with entries=105, filesize=80.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059611268
2014-07-22 13:06:53,295 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:53,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50037 synced till here 50013
2014-07-22 13:06:53,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059611268 with entries=123, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059613296
2014-07-22 13:06:54,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:54,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059613296 with entries=89, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059614702
2014-07-22 13:06:56,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:56,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50211 synced till here 50210
2014-07-22 13:06:56,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059614702 with entries=85, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059616346
2014-07-22 13:06:57,542 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,549 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,585 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,586 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,587 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,588 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,597 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,609 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,620 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:57,624 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:06:58,586 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9948, memsize=144.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/a9893981fc5146c3b38cd9d61521d52c
2014-07-22 13:06:58,608 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/a9893981fc5146c3b38cd9d61521d52c as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/a9893981fc5146c3b38cd9d61521d52c
2014-07-22 13:06:58,627 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/a9893981fc5146c3b38cd9d61521d52c, entries=524350, sequenceid=9948, filesize=37.3m
2014-07-22 13:06:58,627 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~387.7m/406486800, currentsize=51.1m/53582720 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 11492ms, sequenceid=9948, compaction requested=true
2014-07-22 13:06:58,627 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:06:58,627 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 2000 blocking
2014-07-22 13:06:58,628 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-22 13:06:58,628 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1004ms
2014-07-22 13:06:58,628 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:06:58,628 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 912.4m
2014-07-22 13:06:58,628 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,628 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:06:58,628 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:06:58,629 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1009ms
2014-07-22 13:06:58,629 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,641 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1032ms
2014-07-22 13:06:58,641 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,642 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1046ms
2014-07-22 13:06:58,642 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,642 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1054ms
2014-07-22 13:06:58,642 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,642 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1055ms
2014-07-22 13:06:58,642 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,649 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1063ms
2014-07-22 13:06:58,649 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,649 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1064ms
2014-07-22 13:06:58,650 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,653 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1104ms
2014-07-22 13:06:58,653 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,655 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1113ms
2014-07-22 13:06:58,655 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:06:58,842 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:06:58,858 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50296 synced till here 50294
2014-07-22 13:06:58,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059616346 with entries=85, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059618842
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059508250
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059514028
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059515927
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059517614
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059518983
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059520887
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059522437
2014-07-22 13:06:58,881 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059527193
2014-07-22 13:06:59,984 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:07:00,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:00,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50384 synced till here 50382
2014-07-22 13:07:00,361 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059618842 with entries=88, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059620278
2014-07-22 13:07:01,897 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:02,042 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50481 synced till here 50476
2014-07-22 13:07:02,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059620278 with entries=97, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059621897
2014-07-22 13:07:03,616 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,618 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,633 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,665 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,669 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,674 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,675 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,684 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,687 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:03,693 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,694 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,696 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50566 synced till here 50564
2014-07-22 13:07:03,719 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,753 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,758 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,761 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059621897 with entries=85, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059623688
2014-07-22 13:07:03,791 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,795 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,826 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,840 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,840 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,842 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,863 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,898 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,931 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:03,969 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:04,003 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:04,038 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:04,077 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:04,691 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=9851, memsize=411.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e76b8b5df7964ec0a502ac9572ff80cf
2014-07-22 13:07:04,710 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e76b8b5df7964ec0a502ac9572ff80cf as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e76b8b5df7964ec0a502ac9572ff80cf
2014-07-22 13:07:04,723 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e76b8b5df7964ec0a502ac9572ff80cf, entries=1498590, sequenceid=9851, filesize=106.7m
2014-07-22 13:07:04,723 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~821.9m/861836160, currentsize=457.9m/480132720 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 33712ms, sequenceid=9851, compaction requested=true
2014-07-22 13:07:04,724 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:07:04,724 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 13:07:04,724 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 647ms
2014-07-22 13:07:04,724 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 13:07:04,724 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,724 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:07:04,724 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 686ms
2014-07-22 13:07:04,724 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,724 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 1.0g
2014-07-22 13:07:04,724 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 721ms
2014-07-22 13:07:04,724 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,725 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 757ms
2014-07-22 13:07:04,725 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,725 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 794ms
2014-07-22 13:07:04,725 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,725 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 827ms
2014-07-22 13:07:04,725 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,726 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 864ms
2014-07-22 13:07:04,726 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,726 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 884ms
2014-07-22 13:07:04,726 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,726 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 886ms
2014-07-22 13:07:04,726 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,729 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 889ms
2014-07-22 13:07:04,729 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,724 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:07:04,730 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:07:04,733 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 907ms
2014-07-22 13:07:04,733 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,737 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 942ms
2014-07-22 13:07:04,737 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,737 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 946ms
2014-07-22 13:07:04,737 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,738 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 977ms
2014-07-22 13:07:04,738 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,738 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 980ms
2014-07-22 13:07:04,738 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,738 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 985ms
2014-07-22 13:07:04,738 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,742 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1023ms
2014-07-22 13:07:04,742 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,743 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1047ms
2014-07-22 13:07:04,743 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,743 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1049ms
2014-07-22 13:07:04,743 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,743 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1050ms
2014-07-22 13:07:04,743 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,743 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1059ms
2014-07-22 13:07:04,743 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,743 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1068ms
2014-07-22 13:07:04,743 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,745 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1071ms
2014-07-22 13:07:04,745 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,745 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1076ms
2014-07-22 13:07:04,745 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,749 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1084ms
2014-07-22 13:07:04,749 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,750 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1117ms
2014-07-22 13:07:04,751 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,751 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1133ms
2014-07-22 13:07:04,751 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,759 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1143ms
2014-07-22 13:07:04,759 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:04,967 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:07:06,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:06,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50706 synced till here 50686
2014-07-22 13:07:06,834 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059623688 with entries=140, filesize=108.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059626542
2014-07-22 13:07:06,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059529499
2014-07-22 13:07:06,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059531733
2014-07-22 13:07:06,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059533880
2014-07-22 13:07:06,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059537419
2014-07-22 13:07:06,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059538448
2014-07-22 13:07:06,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059540254
2014-07-22 13:07:06,837 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:07:07,944 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:08,060 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50812 synced till here 50810
2014-07-22 13:07:08,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059626542 with entries=106, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059627944
2014-07-22 13:07:10,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:10,055 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059627944 with entries=87, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059630014
2014-07-22 13:07:12,207 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1102ms
GC pool 'ParNew' had collection(s): count=1 time=1349ms
2014-07-22 13:07:12,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:12,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 50988 synced till here 50981
2014-07-22 13:07:12,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059630014 with entries=89, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059632336
2014-07-22 13:07:13,888 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:13,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51078 synced till here 51070
2014-07-22 13:07:14,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059632336 with entries=90, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059633889
2014-07-22 13:07:16,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:16,212 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,213 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,213 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,214 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,216 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,216 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,218 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51177 synced till here 51165
2014-07-22 13:07:16,252 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,263 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,265 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,266 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,268 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,268 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,268 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,289 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,319 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,320 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,320 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,320 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,420 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,456 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059633889 with entries=99, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059636194
2014-07-22 13:07:16,524 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,558 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,575 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,575 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,575 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,576 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,583 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,597 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,602 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,603 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,603 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,603 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,603 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,604 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,607 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,630 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,662 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,693 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:16,724 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,681 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,714 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,745 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,776 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,808 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,843 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,879 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:18,913 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:20,587 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:20,626 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:21,213 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,213 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,213 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,214 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,217 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,217 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,218 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,252 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,264 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,265 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,267 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,269 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,269 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,269 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,289 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,319 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,320 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,320 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,321 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,421 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,456 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,525 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,558 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,575 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,575 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,576 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:07:21,576 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,583 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,597 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,603 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,603 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,603 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,603 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,604 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,604 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,607 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:21,631 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:21,663 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:22,751 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6027ms
2014-07-22 13:07:22,751 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6058ms
2014-07-22 13:07:23,682 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:23,714 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:23,745 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:23,777 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:23,809 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:23,844 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:23,880 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:23,913 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:25,588 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:07:25,626 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:07:26,213 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,214 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,214 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,214 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:07:26,217 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,218 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,218 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:07:26,253 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:07:26,264 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,266 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,267 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,269 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,269 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,270 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,290 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:07:26,320 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,320 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:07:26,321 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,322 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,421 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,457 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,525 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,559 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,575 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,576 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,577 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,577 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,584 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,597 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,603 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,604 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,604 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,604 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,605 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,605 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:26,608 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,631 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:07:26,664 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:07:27,751 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11027ms
2014-07-22 13:07:27,752 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11059ms
2014-07-22 13:07:27,774 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10092, memsize=474.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b88f1a63a99343cc9617974dbfc9b46a
2014-07-22 13:07:27,793 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/b88f1a63a99343cc9617974dbfc9b46a as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b88f1a63a99343cc9617974dbfc9b46a
2014-07-22 13:07:27,808 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/b88f1a63a99343cc9617974dbfc9b46a, entries=1725800, sequenceid=10092, filesize=122.9m
2014-07-22 13:07:27,809 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~912.4m/956691680, currentsize=300.4m/314999280 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 29180ms, sequenceid=10092, compaction requested=true
2014-07-22 13:07:27,809 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:07:27,809 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 13:07:27,809 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11116ms
2014-07-22 13:07:27,810 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 13:07:27,810 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,810 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 838.5m
2014-07-22 13:07:27,810 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11086ms
2014-07-22 13:07:27,810 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,810 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:07:27,810 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:07:27,810 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:07:27,810 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11148ms
2014-07-22 13:07:27,811 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,813 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11183ms
2014-07-22 13:07:27,813 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,813 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11206ms
2014-07-22 13:07:27,814 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,814 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11211ms
2014-07-22 13:07:27,814 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,814 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11211ms
2014-07-22 13:07:27,814 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,817 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11215ms
2014-07-22 13:07:27,817 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,821 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11219ms
2014-07-22 13:07:27,821 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,821 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11219ms
2014-07-22 13:07:27,821 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,822 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11220ms
2014-07-22 13:07:27,822 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,833 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11237ms
2014-07-22 13:07:27,833 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,836 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11252ms
2014-07-22 13:07:27,836 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,836 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11261ms
2014-07-22 13:07:27,836 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,837 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11263ms
2014-07-22 13:07:27,837 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,837 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11263ms
2014-07-22 13:07:27,838 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,838 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11264ms
2014-07-22 13:07:27,838 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,838 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11280ms
2014-07-22 13:07:27,838 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,838 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11314ms
2014-07-22 13:07:27,838 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,855 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11399ms
2014-07-22 13:07:27,856 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,856 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11436ms
2014-07-22 13:07:27,856 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,860 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11540ms
2014-07-22 13:07:27,860 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,861 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11541ms
2014-07-22 13:07:27,861 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,861 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11541ms
2014-07-22 13:07:27,861 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,864 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11545ms
2014-07-22 13:07:27,865 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,866 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11577ms
2014-07-22 13:07:27,867 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,867 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11599ms
2014-07-22 13:07:27,867 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,867 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11599ms
2014-07-22 13:07:27,867 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,867 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11599ms
2014-07-22 13:07:27,867 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,868 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11602ms
2014-07-22 13:07:27,868 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,873 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11608ms
2014-07-22 13:07:27,873 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,873 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11610ms
2014-07-22 13:07:27,873 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,874 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11622ms
2014-07-22 13:07:27,874 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,874 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11656ms
2014-07-22 13:07:27,874 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,874 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11658ms
2014-07-22 13:07:27,874 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,874 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11658ms
2014-07-22 13:07:27,874 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,876 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11662ms
2014-07-22 13:07:27,876 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,876 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11663ms
2014-07-22 13:07:27,877 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,885 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11673ms
2014-07-22 13:07:27,885 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,885 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11673ms
2014-07-22 13:07:27,885 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,889 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7263ms
2014-07-22 13:07:27,889 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,890 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7303ms
2014-07-22 13:07:27,890 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,890 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8977ms
2014-07-22 13:07:27,890 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,890 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9011ms
2014-07-22 13:07:27,890 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,901 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9058ms
2014-07-22 13:07:27,901 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,902 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634224,"queuetimems":0,"class":"HRegionServer","responsesize":17267,"method":"Multi"}
2014-07-22 13:07:27,902 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059633982,"queuetimems":1,"class":"HRegionServer","responsesize":17881,"method":"Multi"}
2014-07-22 13:07:27,903 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059633892,"queuetimems":1,"class":"HRegionServer","responsesize":16860,"method":"Multi"}
2014-07-22 13:07:27,909 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13963,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059633946,"queuetimems":1,"class":"HRegionServer","responsesize":16536,"method":"Multi"}
2014-07-22 13:07:27,909 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634337,"queuetimems":1,"class":"HRegionServer","responsesize":16832,"method":"Multi"}
2014-07-22 13:07:27,909 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13761,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634148,"queuetimems":0,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 13:07:27,909 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9101ms
2014-07-22 13:07:27,909 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,910 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9134ms
2014-07-22 13:07:27,910 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:27,917 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9172ms
2014-07-22 13:07:27,917 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:29,055 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10341ms
2014-07-22 13:07:29,055 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:29,067 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10386ms
2014-07-22 13:07:29,067 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:29,078 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14970,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634107,"queuetimems":0,"class":"HRegionServer","responsesize":17658,"method":"Multi"}
2014-07-22 13:07:29,536 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:07:29,655 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634418,"queuetimems":2,"class":"HRegionServer","responsesize":17021,"method":"Multi"}
2014-07-22 13:07:29,666 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15138,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634528,"queuetimems":1,"class":"HRegionServer","responsesize":17053,"method":"Multi"}
2014-07-22 13:07:29,666 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634457,"queuetimems":1,"class":"HRegionServer","responsesize":17024,"method":"Multi"}
2014-07-22 13:07:29,674 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15111,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634562,"queuetimems":0,"class":"HRegionServer","responsesize":16692,"method":"Multi"}
2014-07-22 13:07:29,675 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634598,"queuetimems":0,"class":"HRegionServer","responsesize":17048,"method":"Multi"}
2014-07-22 13:07:29,681 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059633914,"queuetimems":2,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:07:29,682 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634381,"queuetimems":0,"class":"HRegionServer","responsesize":17023,"method":"Multi"}
2014-07-22 13:07:29,686 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634491,"queuetimems":0,"class":"HRegionServer","responsesize":16875,"method":"Multi"}
2014-07-22 13:07:29,690 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15391,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634298,"queuetimems":1,"class":"HRegionServer","responsesize":17265,"method":"Multi"}
2014-07-22 13:07:29,690 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15428,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634262,"queuetimems":0,"class":"HRegionServer","responsesize":16852,"method":"Multi"}
2014-07-22 13:07:29,690 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15503,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634187,"queuetimems":1,"class":"HRegionServer","responsesize":16801,"method":"Multi"}
2014-07-22 13:07:29,704 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:29,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51303 synced till here 51265
2014-07-22 13:07:29,887 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15251,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634635,"queuetimems":0,"class":"HRegionServer","responsesize":17045,"method":"Multi"}
2014-07-22 13:07:29,986 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:07:30,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059636194 with entries=126, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059649704
2014-07-22 13:07:30,278 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14117,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636161,"queuetimems":1,"class":"HRegionServer","responsesize":17267,"method":"Multi"}
2014-07-22 13:07:30,278 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14138,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636140,"queuetimems":1,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:07:30,278 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636194,"queuetimems":0,"class":"HRegionServer","responsesize":17040,"method":"Multi"}
2014-07-22 13:07:30,298 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14170,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636127,"queuetimems":1,"class":"HRegionServer","responsesize":16873,"method":"Multi"}
2014-07-22 13:07:31,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:31,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51430 synced till here 51392
2014-07-22 13:07:31,823 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17126,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634696,"queuetimems":0,"class":"HRegionServer","responsesize":16981,"method":"Multi"}
2014-07-22 13:07:31,823 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634853,"queuetimems":0,"class":"HRegionServer","responsesize":17199,"method":"Multi"}
2014-07-22 13:07:31,823 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16895,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634928,"queuetimems":0,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-07-22 13:07:31,823 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636658,"queuetimems":0,"class":"HRegionServer","responsesize":16801,"method":"Multi"}
2014-07-22 13:07:31,824 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634735,"queuetimems":0,"class":"HRegionServer","responsesize":17237,"method":"Multi"}
2014-07-22 13:07:31,824 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634803,"queuetimems":0,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 13:07:31,824 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636594,"queuetimems":2,"class":"HRegionServer","responsesize":16536,"method":"Multi"}
2014-07-22 13:07:31,824 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16934,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059634890,"queuetimems":0,"class":"HRegionServer","responsesize":16983,"method":"Multi"}
2014-07-22 13:07:31,834 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636249,"queuetimems":0,"class":"HRegionServer","responsesize":16860,"method":"Multi"}
2014-07-22 13:07:31,842 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636689,"queuetimems":0,"class":"HRegionServer","responsesize":16852,"method":"Multi"}
2014-07-22 13:07:31,883 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638841,"queuetimems":0,"class":"HRegionServer","responsesize":17040,"method":"Multi"}
2014-07-22 13:07:31,896 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638806,"queuetimems":0,"class":"HRegionServer","responsesize":16983,"method":"Multi"}
2014-07-22 13:07:31,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059649704 with entries=127, filesize=98.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059651586
2014-07-22 13:07:32,010 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636417,"queuetimems":0,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:07:32,011 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13134,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638877,"queuetimems":0,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:07:32,156 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13381,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638775,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-07-22 13:07:32,156 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636555,"queuetimems":0,"class":"HRegionServer","responsesize":17265,"method":"Multi"}
2014-07-22 13:07:32,156 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13477,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638679,"queuetimems":0,"class":"HRegionServer","responsesize":16860,"method":"Multi"}
2014-07-22 13:07:32,156 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636287,"queuetimems":1,"class":"HRegionServer","responsesize":17050,"method":"Multi"}
2014-07-22 13:07:32,158 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636626,"queuetimems":0,"class":"HRegionServer","responsesize":17267,"method":"Multi"}
2014-07-22 13:07:32,156 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638909,"queuetimems":0,"class":"HRegionServer","responsesize":17267,"method":"Multi"}
2014-07-22 13:07:32,167 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059640585,"queuetimems":0,"class":"HRegionServer","responsesize":17881,"method":"Multi"}
2014-07-22 13:07:32,170 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13428,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638741,"queuetimems":0,"class":"HRegionServer","responsesize":16873,"method":"Multi"}
2014-07-22 13:07:32,174 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13462,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059638711,"queuetimems":1,"class":"HRegionServer","responsesize":17050,"method":"Multi"}
2014-07-22 13:07:32,853 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16333,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636520,"queuetimems":1,"class":"HRegionServer","responsesize":17658,"method":"Multi"}
2014-07-22 13:07:32,856 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059640624,"queuetimems":1,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:07:32,856 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16401,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636454,"queuetimems":1,"class":"HRegionServer","responsesize":17881,"method":"Multi"}
2014-07-22 13:07:32,982 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059636720,"queuetimems":0,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 13:07:33,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:33,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51599 synced till here 51580
2014-07-22 13:07:35,014 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059651586 with entries=169, filesize=121.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059653480
2014-07-22 13:07:35,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:35,844 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51710 synced till here 51701
2014-07-22 13:07:35,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059653480 with entries=111, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059655803
2014-07-22 13:07:38,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:38,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51809 synced till here 51805
2014-07-22 13:07:38,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059655803 with entries=99, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059658155
2014-07-22 13:07:39,261 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,279 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,293 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,309 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,311 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,312 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,324 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,326 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,331 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,344 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,400 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,431 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,464 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10141, memsize=535.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/13e5131a3ce54caa891d4b54fd036ea3
2014-07-22 13:07:39,477 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/13e5131a3ce54caa891d4b54fd036ea3 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/13e5131a3ce54caa891d4b54fd036ea3
2014-07-22 13:07:39,481 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:39,492 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/13e5131a3ce54caa891d4b54fd036ea3, entries=1948460, sequenceid=10141, filesize=138.7m
2014-07-22 13:07:39,492 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1110216240, currentsize=393.2m/412279280 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 34768ms, sequenceid=10141, compaction requested=true
2014-07-22 13:07:39,493 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:07:39,493 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 13:07:39,493 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-07-22 13:07:39,493 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,493 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 13:07:39,493 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 62ms
2014-07-22 13:07:39,493 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:07:39,493 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:07:39,493 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 861.9m
2014-07-22 13:07:39,493 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:07:39,493 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,494 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 94ms
2014-07-22 13:07:39,494 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,494 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 150ms
2014-07-22 13:07:39,494 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,497 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 166ms
2014-07-22 13:07:39,497 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,498 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 171ms
2014-07-22 13:07:39,498 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,498 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 174ms
2014-07-22 13:07:39,498 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,498 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-07-22 13:07:39,498 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,498 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 187ms
2014-07-22 13:07:39,498 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,498 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 189ms
2014-07-22 13:07:39,498 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,507 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 214ms
2014-07-22 13:07:39,507 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,507 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 228ms
2014-07-22 13:07:39,507 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,508 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 247ms
2014-07-22 13:07:39,508 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:39,569 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:07:39,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:39,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51901 synced till here 51891
2014-07-22 13:07:39,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059658155 with entries=92, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059659758
2014-07-22 13:07:39,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059542547
2014-07-22 13:07:39,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059557444
2014-07-22 13:07:39,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059558335
2014-07-22 13:07:39,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059560429
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059561872
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059563569
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059565083
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059567008
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059568699
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059569725
2014-07-22 13:07:39,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059571291
2014-07-22 13:07:39,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059573365
2014-07-22 13:07:39,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059575242
2014-07-22 13:07:40,243 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:07:41,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:41,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 51995 synced till here 51986
2014-07-22 13:07:41,467 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059659758 with entries=94, filesize=69.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059661368
2014-07-22 13:07:43,055 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:43,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52087 synced till here 52079
2014-07-22 13:07:43,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059661368 with entries=92, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663056
2014-07-22 13:07:43,939 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:43,953 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52174 synced till here 52168
2014-07-22 13:07:43,983 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663056 with entries=87, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663940
2014-07-22 13:07:45,685 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:45,786 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52270 synced till here 52262
2014-07-22 13:07:45,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663940 with entries=96, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059665685
2014-07-22 13:07:47,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:47,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52360 synced till here 52354
2014-07-22 13:07:47,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059665685 with entries=90, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059667426
2014-07-22 13:07:49,617 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:49,795 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:07:49,849 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52467 synced till here 52463
2014-07-22 13:07:49,894 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059667426 with entries=107, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059669618
2014-07-22 13:07:51,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:51,666 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52562 synced till here 52559
2014-07-22 13:07:51,677 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059669618 with entries=95, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059671651
2014-07-22 13:07:53,215 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1083ms
GC pool 'ParNew' had collection(s): count=1 time=1174ms
2014-07-22 13:07:53,709 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:53,717 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,718 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,722 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52653 synced till here 52650
2014-07-22 13:07:53,735 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,740 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,750 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,753 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,754 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059671651 with entries=91, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059673713
2014-07-22 13:07:53,795 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,795 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,795 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,807 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,812 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,812 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,813 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,816 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:53,839 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:07:55,016 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10253, memsize=360.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5a87e259d11d4fd0bd2ae615cd30f391
2014-07-22 13:07:55,039 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5a87e259d11d4fd0bd2ae615cd30f391 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5a87e259d11d4fd0bd2ae615cd30f391
2014-07-22 13:07:55,055 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5a87e259d11d4fd0bd2ae615cd30f391, entries=1313260, sequenceid=10253, filesize=93.5m
2014-07-22 13:07:55,055 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~838.5m/879283280, currentsize=457.2m/479449360 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 27245ms, sequenceid=10253, compaction requested=true
2014-07-22 13:07:55,056 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:07:55,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 13:07:55,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 13:07:55,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:07:55,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:07:55,056 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:07:55,056 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 745.5m
2014-07-22 13:07:55,057 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1217ms
2014-07-22 13:07:55,057 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,057 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1241ms
2014-07-22 13:07:55,057 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,057 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1244ms
2014-07-22 13:07:55,057 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,058 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1245ms
2014-07-22 13:07:55,058 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,058 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1246ms
2014-07-22 13:07:55,058 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,059 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1252ms
2014-07-22 13:07:55,059 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,060 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1265ms
2014-07-22 13:07:55,060 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,060 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1265ms
2014-07-22 13:07:55,060 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,060 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1265ms
2014-07-22 13:07:55,060 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,060 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1306ms
2014-07-22 13:07:55,060 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,060 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1307ms
2014-07-22 13:07:55,061 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,061 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1312ms
2014-07-22 13:07:55,061 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,061 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1321ms
2014-07-22 13:07:55,062 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,063 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1328ms
2014-07-22 13:07:55,064 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,064 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1342ms
2014-07-22 13:07:55,064 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,069 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1350ms
2014-07-22 13:07:55,085 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,085 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1368ms
2014-07-22 13:07:55,085 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:07:55,283 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:07:56,176 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:07:56,260 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:56,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52805 synced till here 52804
2014-07-22 13:07:56,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059673713 with entries=152, filesize=109.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059676261
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059577319
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059578834
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059580144
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059581803
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059584046
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059585876
2014-07-22 13:07:56,979 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059588021
2014-07-22 13:07:58,585 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:58,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52891 synced till here 52887
2014-07-22 13:07:58,665 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059676261 with entries=86, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059678585
2014-07-22 13:07:59,839 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:07:59,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 52976 synced till here 52971
2014-07-22 13:07:59,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059678585 with entries=85, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059679840
2014-07-22 13:08:01,110 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:01,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53078 synced till here 53062
2014-07-22 13:08:01,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059679840 with entries=102, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059681110
2014-07-22 13:08:03,670 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:03,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53176 synced till here 53153
2014-07-22 13:08:03,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059681110 with entries=98, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059683671
2014-07-22 13:08:04,341 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10406, memsize=294.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/6f40b352c9194d34b646e604bf7bc523
2014-07-22 13:08:04,363 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/6f40b352c9194d34b646e604bf7bc523 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/6f40b352c9194d34b646e604bf7bc523
2014-07-22 13:08:04,382 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/6f40b352c9194d34b646e604bf7bc523, entries=1073900, sequenceid=10406, filesize=76.5m
2014-07-22 13:08:04,383 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~861.9m/903731040, currentsize=428.1m/448923360 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 24890ms, sequenceid=10406, compaction requested=true
2014-07-22 13:08:04,383 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:08:04,384 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 13:08:04,384 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 13:08:04,384 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 817.7m
2014-07-22 13:08:04,384 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:08:04,384 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:08:04,384 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:08:05,037 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:08:05,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:05,394 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:08:05,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53328 synced till here 53327
2014-07-22 13:08:05,777 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059683671 with entries=152, filesize=108.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059685240
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059590509
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059593036
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059595121
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059597785
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059599993
2014-07-22 13:08:05,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059601845
2014-07-22 13:08:05,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:07,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:07,462 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53445 synced till here 53444
2014-07-22 13:08:07,480 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059685240 with entries=117, filesize=85.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059687092
2014-07-22 13:08:07,483 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:08,729 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:08,754 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53533 synced till here 53531
2014-07-22 13:08:08,783 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059687092 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059688730
2014-07-22 13:08:08,788 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:10,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:10,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53655 synced till here 53651
2014-07-22 13:08:10,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059688730 with entries=122, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059690240
2014-07-22 13:08:10,693 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:12,364 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:12,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059690240 with entries=96, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059692365
2014-07-22 13:08:12,538 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:13,903 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:13,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 53835 synced till here 53833
2014-07-22 13:08:13,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059692365 with entries=84, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059693904
2014-07-22 13:08:13,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:15,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:15,344 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,345 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,346 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,346 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,346 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,394 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,398 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,399 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,404 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,415 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,444 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,497 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,553 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,615 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059693904 with entries=91, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059695313
2014-07-22 13:08:15,619 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:08:15,786 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,837 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,884 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,936 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:15,982 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,032 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,080 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,132 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,186 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,227 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,266 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,301 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,359 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,429 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,472 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,517 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,793 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:16,841 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,147 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,163 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,204 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,233 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,264 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,295 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,327 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,359 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,390 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,428 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,465 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,500 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,539 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,569 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,600 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,633 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,664 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:18,697 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:20,723 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5108ms
2014-07-22 13:08:20,723 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5319ms
2014-07-22 13:08:20,723 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5377ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5379ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5309ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5280ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5227ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5171ms
2014-07-22 13:08:20,724 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5380ms
2014-07-22 13:08:20,725 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5378ms
2014-07-22 13:08:20,725 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5379ms
2014-07-22 13:08:20,725 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5332ms
2014-07-22 13:08:20,725 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5326ms
2014-07-22 13:08:20,725 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5327ms
2014-07-22 13:08:20,734 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10573, memsize=399.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/21d750e69426425489b7f770fd972d5f
2014-07-22 13:08:20,762 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/21d750e69426425489b7f770fd972d5f as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/21d750e69426425489b7f770fd972d5f
2014-07-22 13:08:20,784 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/21d750e69426425489b7f770fd972d5f, entries=1453730, sequenceid=10573, filesize=103.5m
2014-07-22 13:08:20,784 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~745.5m/781729200, currentsize=388.4m/407238800 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 25728ms, sequenceid=10573, compaction requested=true
2014-07-22 13:08:20,785 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:08:20,785 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 13:08:20,785 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5387ms
2014-07-22 13:08:20,785 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,785 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 13:08:20,785 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:08:20,785 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:08:20,785 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:08:20,786 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:08:20,786 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,789 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5390ms
2014-07-22 13:08:20,789 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5396ms
2014-07-22 13:08:20,789 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,790 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5444ms
2014-07-22 13:08:20,790 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,792 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 393.3m
2014-07-22 13:08:20,793 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5447ms
2014-07-22 13:08:20,793 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,799 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5455ms
2014-07-22 13:08:20,799 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,799 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5246ms
2014-07-22 13:08:20,799 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,799 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5302ms
2014-07-22 13:08:20,799 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,806 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5362ms
2014-07-22 13:08:20,806 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,809 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5394ms
2014-07-22 13:08:20,809 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,809 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5464ms
2014-07-22 13:08:20,809 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,819 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5473ms
2014-07-22 13:08:20,820 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,820 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5416ms
2014-07-22 13:08:20,820 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,820 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5205ms
2014-07-22 13:08:20,822 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,822 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2125ms
2014-07-22 13:08:20,822 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,825 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2161ms
2014-07-22 13:08:20,825 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,830 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2197ms
2014-07-22 13:08:20,830 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,831 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2231ms
2014-07-22 13:08:20,831 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,832 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2263ms
2014-07-22 13:08:20,832 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,833 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2294ms
2014-07-22 13:08:20,833 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,833 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2333ms
2014-07-22 13:08:20,833 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,834 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2369ms
2014-07-22 13:08:20,835 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,835 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2407ms
2014-07-22 13:08:20,835 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,835 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2445ms
2014-07-22 13:08:20,835 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,836 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2477ms
2014-07-22 13:08:20,836 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,840 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2513ms
2014-07-22 13:08:20,840 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,840 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2545ms
2014-07-22 13:08:20,840 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,840 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2576ms
2014-07-22 13:08:20,840 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,840 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2607ms
2014-07-22 13:08:20,840 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,841 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2638ms
2014-07-22 13:08:20,841 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,841 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2678ms
2014-07-22 13:08:20,841 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,845 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-22 13:08:20,845 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,853 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2706ms
2014-07-22 13:08:20,853 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,853 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4012ms
2014-07-22 13:08:20,854 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,854 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4061ms
2014-07-22 13:08:20,854 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,854 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4337ms
2014-07-22 13:08:20,854 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,859 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4387ms
2014-07-22 13:08:20,859 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,859 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4430ms
2014-07-22 13:08:20,859 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,859 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4500ms
2014-07-22 13:08:20,859 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,862 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4561ms
2014-07-22 13:08:20,862 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,862 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4596ms
2014-07-22 13:08:20,862 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,862 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4635ms
2014-07-22 13:08:20,863 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,872 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4687ms
2014-07-22 13:08:20,872 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,877 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4746ms
2014-07-22 13:08:20,877 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,878 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4798ms
2014-07-22 13:08:20,878 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,878 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4846ms
2014-07-22 13:08:20,878 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,878 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4896ms
2014-07-22 13:08:20,878 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,879 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4943ms
2014-07-22 13:08:20,879 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:20,879 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4995ms
2014-07-22 13:08:20,879 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:21,117 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:08:21,240 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:08:22,755 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1031ms
GC pool 'ParNew' had collection(s): count=1 time=1417ms
2014-07-22 13:08:23,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:23,392 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54049 synced till here 54003
2014-07-22 13:08:23,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059695313 with entries=123, filesize=99.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059703226
2014-07-22 13:08:25,762 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10370,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695391,"queuetimems":0,"class":"HRegionServer","responsesize":17002,"method":"Multi"}
2014-07-22 13:08:25,762 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695342,"queuetimems":1,"class":"HRegionServer","responsesize":17119,"method":"Multi"}
2014-07-22 13:08:25,762 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695494,"queuetimems":1,"class":"HRegionServer","responsesize":16971,"method":"Multi"}
2014-07-22 13:08:25,799 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10248,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695550,"queuetimems":1,"class":"HRegionServer","responsesize":17400,"method":"Multi"}
2014-07-22 13:08:25,807 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10197,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695609,"queuetimems":1,"class":"HRegionServer","responsesize":17000,"method":"Multi"}
2014-07-22 13:08:25,808 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695441,"queuetimems":0,"class":"HRegionServer","responsesize":16664,"method":"Multi"}
2014-07-22 13:08:25,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:26,007 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54166 synced till here 54133
2014-07-22 13:08:26,018 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695934,"queuetimems":1,"class":"HRegionServer","responsesize":17194,"method":"Multi"}
2014-07-22 13:08:27,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059703226 with entries=117, filesize=89.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059705904
2014-07-22 13:08:27,714 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696512,"queuetimems":0,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-07-22 13:08:27,714 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695883,"queuetimems":1,"class":"HRegionServer","responsesize":16763,"method":"Multi"}
2014-07-22 13:08:27,723 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11370,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696353,"queuetimems":1,"class":"HRegionServer","responsesize":16604,"method":"Multi"}
2014-07-22 13:08:27,734 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11265,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696468,"queuetimems":0,"class":"HRegionServer","responsesize":17272,"method":"Multi"}
2014-07-22 13:08:27,735 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059695835,"queuetimems":1,"class":"HRegionServer","responsesize":17260,"method":"Multi"}
2014-07-22 13:08:27,742 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10902,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696839,"queuetimems":0,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-07-22 13:08:27,846 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11421,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696424,"queuetimems":0,"class":"HRegionServer","responsesize":17369,"method":"Multi"}
2014-07-22 13:08:27,854 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696030,"queuetimems":1,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:08:27,966 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11703,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696263,"queuetimems":1,"class":"HRegionServer","responsesize":17155,"method":"Multi"}
2014-07-22 13:08:27,972 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11673,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696299,"queuetimems":0,"class":"HRegionServer","responsesize":16707,"method":"Multi"}
2014-07-22 13:08:27,975 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11184,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696790,"queuetimems":0,"class":"HRegionServer","responsesize":16844,"method":"Multi"}
2014-07-22 13:08:27,978 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696128,"queuetimems":0,"class":"HRegionServer","responsesize":16839,"method":"Multi"}
2014-07-22 13:08:27,978 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11900,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059696078,"queuetimems":0,"class":"HRegionServer","responsesize":17516,"method":"Multi"}
2014-07-22 13:08:29,363 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1134ms
GC pool 'ParNew' had collection(s): count=1 time=1136ms
2014-07-22 13:08:29,502 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:29,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54285 synced till here 54271
2014-07-22 13:08:29,938 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059705904 with entries=119, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059709502
2014-07-22 13:08:32,023 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1159ms
GC pool 'ParNew' had collection(s): count=1 time=1448ms
2014-07-22 13:08:32,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:32,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54402 synced till here 54374
2014-07-22 13:08:32,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059709502 with entries=117, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059712293
2014-07-22 13:08:34,200 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,201 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,203 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,203 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,274 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,274 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,274 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,278 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,278 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,282 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,327 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,327 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,331 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,332 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,332 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,332 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,336 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,337 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,337 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,337 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,338 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,341 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,341 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,342 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,342 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,342 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,342 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,343 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,343 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,344 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,344 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,346 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:34,469 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,469 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,469 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,470 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,471 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,472 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,473 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,474 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,474 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,474 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,480 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,481 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,482 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,482 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,482 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,484 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,484 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,485 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:34,491 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059712293 with entries=106, filesize=72.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059714464
2014-07-22 13:08:37,157 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10675, memsize=448.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/332c108060f04ee7885e09a692085cd0
2014-07-22 13:08:37,175 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/332c108060f04ee7885e09a692085cd0 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/332c108060f04ee7885e09a692085cd0
2014-07-22 13:08:37,192 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/332c108060f04ee7885e09a692085cd0, entries=1634620, sequenceid=10675, filesize=116.5m
2014-07-22 13:08:37,192 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~817.7m/857374480, currentsize=404.7m/424326800 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 32808ms, sequenceid=10675, compaction requested=true
2014-07-22 13:08:37,192 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:08:37,193 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 13:08:37,193 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2708ms
2014-07-22 13:08:37,193 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 1.0g
2014-07-22 13:08:37,193 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 13:08:37,193 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,193 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:08:37,193 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:08:37,193 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2709ms
2014-07-22 13:08:37,193 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,193 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:08:37,193 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2710ms
2014-07-22 13:08:37,194 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,194 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 13:08:37,194 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,194 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2712ms
2014-07-22 13:08:37,194 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,199 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2717ms
2014-07-22 13:08:37,200 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,200 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2719ms
2014-07-22 13:08:37,200 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,200 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2720ms
2014-07-22 13:08:37,200 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,200 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2726ms
2014-07-22 13:08:37,200 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,200 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2726ms
2014-07-22 13:08:37,200 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,201 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2728ms
2014-07-22 13:08:37,201 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,201 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2728ms
2014-07-22 13:08:37,201 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,202 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2730ms
2014-07-22 13:08:37,202 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,202 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2731ms
2014-07-22 13:08:37,202 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,205 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2736ms
2014-07-22 13:08:37,205 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,205 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2736ms
2014-07-22 13:08:37,206 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,206 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2737ms
2014-07-22 13:08:37,206 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,208 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2739ms
2014-07-22 13:08:37,208 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,209 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2864ms
2014-07-22 13:08:37,209 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,209 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2865ms
2014-07-22 13:08:37,209 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,209 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2865ms
2014-07-22 13:08:37,209 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,209 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2866ms
2014-07-22 13:08:37,209 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,210 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2867ms
2014-07-22 13:08:37,210 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,210 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2868ms
2014-07-22 13:08:37,210 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,217 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2875ms
2014-07-22 13:08:37,217 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,217 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2875ms
2014-07-22 13:08:37,217 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,218 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2877ms
2014-07-22 13:08:37,218 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,218 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2877ms
2014-07-22 13:08:37,218 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,218 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2877ms
2014-07-22 13:08:37,218 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,223 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2886ms
2014-07-22 13:08:37,223 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,223 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2886ms
2014-07-22 13:08:37,223 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,223 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2886ms
2014-07-22 13:08:37,224 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,224 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2888ms
2014-07-22 13:08:37,224 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,229 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2893ms
2014-07-22 13:08:37,229 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,229 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2897ms
2014-07-22 13:08:37,229 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,240 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2908ms
2014-07-22 13:08:37,240 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,240 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2908ms
2014-07-22 13:08:37,240 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,240 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2909ms
2014-07-22 13:08:37,241 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,249 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2923ms
2014-07-22 13:08:37,249 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,249 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2923ms
2014-07-22 13:08:37,250 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,257 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2975ms
2014-07-22 13:08:37,257 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,257 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2979ms
2014-07-22 13:08:37,257 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,261 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2983ms
2014-07-22 13:08:37,261 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,261 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2987ms
2014-07-22 13:08:37,261 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,262 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2988ms
2014-07-22 13:08:37,262 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,269 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2996ms
2014-07-22 13:08:37,269 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,277 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3074ms
2014-07-22 13:08:37,277 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,285 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3082ms
2014-07-22 13:08:37,285 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,285 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3084ms
2014-07-22 13:08:37,286 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:37,286 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3086ms
2014-07-22 13:08:37,286 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:08:38,994 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1297ms
GC pool 'ParNew' had collection(s): count=1 time=1585ms
2014-07-22 13:08:39,030 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059707979,"queuetimems":2629,"class":"HRegionServer","responsesize":16867,"method":"Multi"}
2014-07-22 13:08:39,119 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059707735,"queuetimems":4453,"class":"HRegionServer","responsesize":16664,"method":"Multi"}
2014-07-22 13:08:39,126 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059707854,"queuetimems":4082,"class":"HRegionServer","responsesize":17058,"method":"Multi"}
2014-07-22 13:08:39,401 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:08:39,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:39,791 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:08:39,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54632 synced till here 54596
2014-07-22 13:08:40,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059714464 with entries=124, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059719730
2014-07-22 13:08:41,917 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10787, memsize=232.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/22d31cb45a814ca0aa969c82055936a7
2014-07-22 13:08:41,944 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/22d31cb45a814ca0aa969c82055936a7 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/22d31cb45a814ca0aa969c82055936a7
2014-07-22 13:08:41,958 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/22d31cb45a814ca0aa969c82055936a7, entries=845020, sequenceid=10787, filesize=60.2m
2014-07-22 13:08:41,959 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~393.3m/412455200, currentsize=61.5m/64468080 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 21167ms, sequenceid=10787, compaction requested=true
2014-07-22 13:08:41,959 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 2000 blocking
2014-07-22 13:08:41,959 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-22 13:08:41,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:08:41,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:08:41,960 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:08:41,960 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:08:41,960 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 884.1m
2014-07-22 13:08:42,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:42,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54742 synced till here 54714
2014-07-22 13:08:42,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059719730 with entries=110, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059722299
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059602677
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059609057
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059611268
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059613296
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059614702
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059616346
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059618842
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059620278
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059621897
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059623688
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059626542
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059627944
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059630014
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059632336
2014-07-22 13:08:42,501 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059633889
2014-07-22 13:08:44,221 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:08:44,394 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:44,497 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54872 synced till here 54865
2014-07-22 13:08:44,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059722299 with entries=130, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059724394
2014-07-22 13:08:46,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:46,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 54995 synced till here 54968
2014-07-22 13:08:46,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059724394 with entries=123, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059726547
2014-07-22 13:08:48,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:48,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55135 synced till here 55131
2014-07-22 13:08:49,042 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059726547 with entries=140, filesize=93.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059728717
2014-07-22 13:08:50,397 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:50,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55252 synced till here 55251
2014-07-22 13:08:50,480 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059728717 with entries=117, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059730397
2014-07-22 13:08:56,529 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:08:56,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059730397 with entries=91, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059736530
2014-07-22 13:08:57,146 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:57,147 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:57,147 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:57,148 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:57,151 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,004 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,019 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,019 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,020 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,021 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,032 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,033 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,045 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,046 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,047 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,078 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,099 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,125 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,227 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,256 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,287 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,321 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,351 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,383 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,413 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,446 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,480 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,511 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,544 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,574 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,606 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:58,638 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:59,365 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:08:59,399 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:01,318 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:01,354 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:01,388 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:01,420 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:02,147 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:02,147 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:02,148 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:02,148 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:02,151 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,005 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,019 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,020 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,020 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,022 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,033 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,033 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,045 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,046 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,048 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,078 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,100 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,126 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,228 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,257 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,288 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,321 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,352 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,384 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,413 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,420 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:03,446 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,455 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:03,481 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,511 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,544 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:03,575 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,607 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:03,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:04,366 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:04,399 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:05,776 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:05,805 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:05,836 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:05,869 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:06,319 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:06,354 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:06,389 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:06,421 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:07,148 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:07,149 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:09:07,149 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 13:09:07,149 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:07,152 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:07,527 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:07,558 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:08,005 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,020 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,020 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,021 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,022 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,033 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,035 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,046 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,047 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,048 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,079 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,101 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,126 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,202 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:08,228 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,257 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,269 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:08,288 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,302 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:08,322 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,352 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,370 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:09:08,384 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,414 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:09:08,421 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:09:08,448 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,456 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:09:08,481 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,512 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:09:08,545 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:09:08,576 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:09:08,607 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:08,640 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:09:09,367 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:09:09,399 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:09:10,354 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10986, memsize=574.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/2210066f9cc04450b9423d4454a4b59d
2014-07-22 13:09:10,372 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/2210066f9cc04450b9423d4454a4b59d as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/2210066f9cc04450b9423d4454a4b59d
2014-07-22 13:09:10,395 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/2210066f9cc04450b9423d4454a4b59d, entries=2092780, sequenceid=10986, filesize=149.0m
2014-07-22 13:09:10,396 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~897.9m/941544880, currentsize=199.2m/208888400 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 28436ms, sequenceid=10986, compaction requested=true
2014-07-22 13:09:10,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:09:10,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 13:09:10,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 13:09:10,397 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10998ms
2014-07-22 13:09:10,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:09:10,397 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 848.8m
2014-07-22 13:09:10,397 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:09:10,398 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11033ms
2014-07-22 13:09:10,398 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:09:10,398 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,398 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11760ms
2014-07-22 13:09:10,398 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,398 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11792ms
2014-07-22 13:09:10,399 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,399 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11825ms
2014-07-22 13:09:10,399 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,400 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11856ms
2014-07-22 13:09:10,400 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,400 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11889ms
2014-07-22 13:09:10,401 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,401 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11921ms
2014-07-22 13:09:10,413 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,413 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6958ms
2014-07-22 13:09:10,414 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,414 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11968ms
2014-07-22 13:09:10,414 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,414 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6994ms
2014-07-22 13:09:10,414 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,414 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12001ms
2014-07-22 13:09:10,414 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,414 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12031ms
2014-07-22 13:09:10,415 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,417 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2047ms
2014-07-22 13:09:10,417 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,420 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12069ms
2014-07-22 13:09:10,420 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,420 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12100ms
2014-07-22 13:09:10,420 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,421 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2119ms
2014-07-22 13:09:10,421 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,421 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12134ms
2014-07-22 13:09:10,421 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,422 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2152ms
2014-07-22 13:09:10,422 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,424 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12168ms
2014-07-22 13:09:10,424 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,424 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12197ms
2014-07-22 13:09:10,424 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,424 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2222ms
2014-07-22 13:09:10,424 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,425 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12300ms
2014-07-22 13:09:10,425 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,426 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12327ms
2014-07-22 13:09:10,426 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,428 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12350ms
2014-07-22 13:09:10,428 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,430 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12383ms
2014-07-22 13:09:10,430 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,430 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12384ms
2014-07-22 13:09:10,430 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,432 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12388ms
2014-07-22 13:09:10,432 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,432 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12399ms
2014-07-22 13:09:10,432 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,436 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12404ms
2014-07-22 13:09:10,436 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,445 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12424ms
2014-07-22 13:09:10,445 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,445 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12426ms
2014-07-22 13:09:10,445 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,445 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12426ms
2014-07-22 13:09:10,445 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,447 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12427ms
2014-07-22 13:09:10,447 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,447 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12443ms
2014-07-22 13:09:10,447 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,448 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2889ms
2014-07-22 13:09:10,448 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,449 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2922ms
2014-07-22 13:09:10,449 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,452 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13302ms
2014-07-22 13:09:10,452 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,453 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13305ms
2014-07-22 13:09:10,453 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,453 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13307ms
2014-07-22 13:09:10,453 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,461 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13314ms
2014-07-22 13:09:10,461 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,465 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13319ms
2014-07-22 13:09:10,465 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,465 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9045ms
2014-07-22 13:09:10,465 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,465 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9077ms
2014-07-22 13:09:10,466 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,466 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9112ms
2014-07-22 13:09:10,466 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,466 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9148ms
2014-07-22 13:09:10,466 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,466 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4597ms
2014-07-22 13:09:10,466 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,467 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4631ms
2014-07-22 13:09:10,468 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,469 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4663ms
2014-07-22 13:09:10,469 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,469 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4693ms
2014-07-22 13:09:10,469 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:09:10,538 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13942,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736595,"queuetimems":0,"class":"HRegionServer","responsesize":16664,"method":"Multi"}
2014-07-22 13:09:11,665 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736560,"queuetimems":0,"class":"HRegionServer","responsesize":17196,"method":"Multi"}
2014-07-22 13:09:11,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:11,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55464 synced till here 55426
2014-07-22 13:09:11,861 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15339,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736521,"queuetimems":0,"class":"HRegionServer","responsesize":17173,"method":"Multi"}
2014-07-22 13:09:12,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059736530 with entries=121, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059751668
2014-07-22 13:09:12,341 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15639,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736702,"queuetimems":0,"class":"HRegionServer","responsesize":16920,"method":"Multi"}
2014-07-22 13:09:12,342 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736667,"queuetimems":0,"class":"HRegionServer","responsesize":16628,"method":"Multi"}
2014-07-22 13:09:12,342 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736737,"queuetimems":1,"class":"HRegionServer","responsesize":17194,"method":"Multi"}
2014-07-22 13:09:12,341 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736631,"queuetimems":0,"class":"HRegionServer","responsesize":16425,"method":"Multi"}
2014-07-22 13:09:12,355 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:09:12,490 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736825,"queuetimems":0,"class":"HRegionServer","responsesize":16859,"method":"Multi"}
2014-07-22 13:09:12,490 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736898,"queuetimems":0,"class":"HRegionServer","responsesize":16867,"method":"Multi"}
2014-07-22 13:09:12,669 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736861,"queuetimems":1,"class":"HRegionServer","responsesize":17400,"method":"Multi"}
2014-07-22 13:09:13,968 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15900,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736774,"queuetimems":0,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:09:14,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:14,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55582 synced till here 55549
2014-07-22 13:09:14,282 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=10915, memsize=612.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/11f99b89330c4f6da361d13f8131008a
2014-07-22 13:09:14,293 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/11f99b89330c4f6da361d13f8131008a as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/11f99b89330c4f6da361d13f8131008a
2014-07-22 13:09:14,301 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/11f99b89330c4f6da361d13f8131008a, entries=2230330, sequenceid=10915, filesize=158.8m
2014-07-22 13:09:14,302 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1.0g/1074597920, currentsize=306.4m/321233760 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 37108ms, sequenceid=10915, compaction requested=true
2014-07-22 13:09:14,302 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:09:14,302 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 13:09:14,302 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 740.1m
2014-07-22 13:09:14,302 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 13:09:14,302 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:09:14,302 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:09:14,302 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:09:14,332 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:09:14,334 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17252,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059737081,"queuetimems":0,"class":"HRegionServer","responsesize":16831,"method":"Multi"}
2014-07-22 13:09:14,334 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17378,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059736955,"queuetimems":1,"class":"HRegionServer","responsesize":16763,"method":"Multi"}
2014-07-22 13:09:14,346 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14982,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059739363,"queuetimems":0,"class":"HRegionServer","responsesize":16997,"method":"Multi"}
2014-07-22 13:09:14,350 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15712,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738637,"queuetimems":1,"class":"HRegionServer","responsesize":16707,"method":"Multi"}
2014-07-22 13:09:14,350 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15840,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738509,"queuetimems":0,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-07-22 13:09:14,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059751668 with entries=118, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059754123
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059636194
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059649704
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059651586
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059653480
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059655803
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059658155
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059659758
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059661368
2014-07-22 13:09:14,378 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663056
2014-07-22 13:09:14,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059663940
2014-07-22 13:09:14,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059665685
2014-07-22 13:09:14,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059667426
2014-07-22 13:09:14,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059669618
2014-07-22 13:09:14,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059671651
2014-07-22 13:09:14,401 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:09:14,490 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059739395,"queuetimems":0,"class":"HRegionServer","responsesize":16831,"method":"Multi"}
2014-07-22 13:09:14,492 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738604,"queuetimems":0,"class":"HRegionServer","responsesize":17260,"method":"Multi"}
2014-07-22 13:09:14,500 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15927,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738572,"queuetimems":0,"class":"HRegionServer","responsesize":17260,"method":"Multi"}
2014-07-22 13:09:14,515 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16035,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738479,"queuetimems":1,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-22 13:09:14,608 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738541,"queuetimems":1,"class":"HRegionServer","responsesize":17369,"method":"Multi"}
2014-07-22 13:09:14,884 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16473,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738411,"queuetimems":0,"class":"HRegionServer","responsesize":17000,"method":"Multi"}
2014-07-22 13:09:14,886 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16505,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738380,"queuetimems":1,"class":"HRegionServer","responsesize":16604,"method":"Multi"}
2014-07-22 13:09:15,185 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:15,187 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11768,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059743419,"queuetimems":1,"class":"HRegionServer","responsesize":16997,"method":"Multi"}
2014-07-22 13:09:15,187 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16743,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738444,"queuetimems":0,"class":"HRegionServer","responsesize":17272,"method":"Multi"}
2014-07-22 13:09:15,187 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738318,"queuetimems":1,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-07-22 13:09:15,188 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:09:15,202 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13815,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059741386,"queuetimems":1,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 13:09:15,203 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13784,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059741418,"queuetimems":0,"class":"HRegionServer","responsesize":16793,"method":"Multi"}
2014-07-22 13:09:15,202 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16978,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738223,"queuetimems":1,"class":"HRegionServer","responsesize":17153,"method":"Multi"}
2014-07-22 13:09:15,202 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059741316,"queuetimems":0,"class":"HRegionServer","responsesize":16764,"method":"Multi"}
2014-07-22 13:09:15,214 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738349,"queuetimems":0,"class":"HRegionServer","responsesize":17058,"method":"Multi"}
2014-07-22 13:09:15,214 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738285,"queuetimems":0,"class":"HRegionServer","responsesize":17155,"method":"Multi"}
2014-07-22 13:09:15,214 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17171,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738042,"queuetimems":0,"class":"HRegionServer","responsesize":16926,"method":"Multi"}
2014-07-22 13:09:15,214 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16960,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738254,"queuetimems":0,"class":"HRegionServer","responsesize":16844,"method":"Multi"}
2014-07-22 13:09:15,224 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13872,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059741352,"queuetimems":0,"class":"HRegionServer","responsesize":16926,"method":"Multi"}
2014-07-22 13:09:16,247 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55719 synced till here 55706
2014-07-22 13:09:16,292 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19138,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059737154,"queuetimems":0,"class":"HRegionServer","responsesize":16997,"method":"Multi"}
2014-07-22 13:09:16,292 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18171,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738121,"queuetimems":1,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-07-22 13:09:16,295 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18276,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738019,"queuetimems":0,"class":"HRegionServer","responsesize":16793,"method":"Multi"}
2014-07-22 13:09:16,295 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059738076,"queuetimems":1,"class":"HRegionServer","responsesize":16764,"method":"Multi"}
2014-07-22 13:09:16,295 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059743451,"queuetimems":0,"class":"HRegionServer","responsesize":16831,"method":"Multi"}
2014-07-22 13:09:16,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059754123 with entries=137, filesize=88.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059755185
2014-07-22 13:09:17,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:17,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55802 synced till here 55801
2014-07-22 13:09:17,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059755185 with entries=83, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059757057
2014-07-22 13:09:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:09:19,794 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:20,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059757057 with entries=102, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059759794
2014-07-22 13:09:22,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:22,643 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 55990 synced till here 55986
2014-07-22 13:09:22,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059759794 with entries=86, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059762614
2014-07-22 13:09:23,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:23,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059762614 with entries=86, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059763923
2014-07-22 13:09:25,348 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:25,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56163 synced till here 56159
2014-07-22 13:09:25,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059763923 with entries=87, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059765348
2014-07-22 13:09:26,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:26,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56252 synced till here 56250
2014-07-22 13:09:26,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059765348 with entries=89, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059766153
2014-07-22 13:09:28,217 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:28,662 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56372 synced till here 56371
2014-07-22 13:09:28,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059766153 with entries=120, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059768218
2014-07-22 13:09:30,188 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:31,216 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56485 synced till here 56478
2014-07-22 13:09:31,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059768218 with entries=113, filesize=84.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059770188
2014-07-22 13:09:31,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:32,905 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56595 synced till here 56593
2014-07-22 13:09:32,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059770188 with entries=110, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059771964
2014-07-22 13:09:33,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:33,863 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:09:33,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56682 synced till here 56679
2014-07-22 13:09:35,289 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059771964 with entries=87, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059773850
2014-07-22 13:09:36,109 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:36,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56772 synced till here 56759
2014-07-22 13:09:36,444 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11164, memsize=279.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c6f54d7ca618463ba4bc05b4e699a9a3
2014-07-22 13:09:37,686 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059773850 with entries=90, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059776110
2014-07-22 13:09:37,704 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c6f54d7ca618463ba4bc05b4e699a9a3 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c6f54d7ca618463ba4bc05b4e699a9a3
2014-07-22 13:09:37,717 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c6f54d7ca618463ba4bc05b4e699a9a3, entries=1017210, sequenceid=11164, filesize=72.4m
2014-07-22 13:09:37,718 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~749.3m/785707760, currentsize=348.5m/365378080 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 23416ms, sequenceid=11164, compaction requested=true
2014-07-22 13:09:37,721 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:09:37,722 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 13:09:37,722 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 696.9m
2014-07-22 13:09:37,722 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 13:09:37,722 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:09:37,722 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:09:37,722 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:09:37,810 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:09:38,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:38,635 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 56890 synced till here 56863
2014-07-22 13:09:39,850 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1164ms
GC pool 'ParNew' had collection(s): count=1 time=1187ms
2014-07-22 13:09:40,069 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059776110 with entries=118, filesize=90.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059778596
2014-07-22 13:09:40,133 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:09:42,083 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1231ms
GC pool 'ParNew' had collection(s): count=1 time=1306ms
2014-07-22 13:09:42,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:42,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57009 synced till here 56984
2014-07-22 13:09:43,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059778596 with entries=119, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059782285
2014-07-22 13:09:44,814 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1153ms
GC pool 'ParNew' had collection(s): count=1 time=1374ms
2014-07-22 13:09:45,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:45,373 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059782285 with entries=111, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059785266
2014-07-22 13:09:47,607 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:47,750 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57244 synced till here 57195
2014-07-22 13:09:47,835 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11124, memsize=392.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/04d6ed0a0e774b3eb8691555a0d43efb
2014-07-22 13:09:47,847 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/04d6ed0a0e774b3eb8691555a0d43efb as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/04d6ed0a0e774b3eb8691555a0d43efb
2014-07-22 13:09:47,857 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/04d6ed0a0e774b3eb8691555a0d43efb, entries=1429450, sequenceid=11124, filesize=101.7m
2014-07-22 13:09:47,858 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~848.8m/889987440, currentsize=587.8m/616326400 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 37461ms, sequenceid=11124, compaction requested=true
2014-07-22 13:09:47,858 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:09:47,858 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 13:09:47,859 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 13:09:47,859 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 753.0m
2014-07-22 13:09:47,859 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:09:47,859 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:09:47,859 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:09:48,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059785266 with entries=124, filesize=95.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059787608
2014-07-22 13:09:48,100 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059673713
2014-07-22 13:09:48,100 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059676261
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059678585
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059679840
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059681110
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059683671
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059685240
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059687092
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059688730
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059690240
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059692365
2014-07-22 13:09:48,101 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059693904
2014-07-22 13:09:48,220 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:09:50,090 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:09:50,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:50,526 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57365 synced till here 57332
2014-07-22 13:09:50,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059787608 with entries=121, filesize=98.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059790505
2014-07-22 13:09:52,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:52,903 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57476 synced till here 57473
2014-07-22 13:09:52,962 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059790505 with entries=111, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059792807
2014-07-22 13:09:54,810 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:54,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57595 synced till here 57575
2014-07-22 13:09:55,081 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059792807 with entries=119, filesize=94.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059794811
2014-07-22 13:09:55,082 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:09:56,888 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:09:57,008 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57732 synced till here 57719
2014-07-22 13:09:57,088 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059794811 with entries=137, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059796888
2014-07-22 13:09:57,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:10:19,891 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 24013ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:10:19,892 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 24014ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:10:19,894 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 22015ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=22302ms
2014-07-22 13:10:19,894 WARN  [regionserver60020] util.Sleeper: We slept 25019ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:10:19,990 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23525,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796465,"queuetimems":0,"class":"HRegionServer","responsesize":16879,"method":"Multi"}
2014-07-22 13:10:19,991 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19309 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:19,993 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,234 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23743,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796490,"queuetimems":0,"class":"HRegionServer","responsesize":16903,"method":"Multi"}
2014-07-22 13:10:20,234 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19308 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,234 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,238 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796728,"queuetimems":0,"class":"HRegionServer","responsesize":17496,"method":"Multi"}
2014-07-22 13:10:20,238 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19302 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,238 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23652,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796623,"queuetimems":0,"class":"HRegionServer","responsesize":17338,"method":"Multi"}
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23986,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796290,"queuetimems":3404,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23582,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796693,"queuetimems":0,"class":"HRegionServer","responsesize":17391,"method":"Multi"}
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796453,"queuetimems":2202,"class":"HRegionServer","responsesize":16842,"method":"Multi"}
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19305 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23823,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796452,"queuetimems":2363,"class":"HRegionServer","responsesize":17029,"method":"Multi"}
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19303 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796453,"queuetimems":2299,"class":"HRegionServer","responsesize":17284,"method":"Multi"}
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,276 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19284 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19297 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19293 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,277 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,278 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19294 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,278 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24000,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796283,"queuetimems":3431,"class":"HRegionServer","responsesize":16945,"method":"Multi"}
2014-07-22 13:10:20,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19285 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23861,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796457,"queuetimems":27,"class":"HRegionServer","responsesize":17362,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796657,"queuetimems":0,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796453,"queuetimems":2267,"class":"HRegionServer","responsesize":17140,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23862,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796456,"queuetimems":65,"class":"HRegionServer","responsesize":16463,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796555,"queuetimems":0,"class":"HRegionServer","responsesize":17082,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796453,"queuetimems":2233,"class":"HRegionServer","responsesize":17478,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796522,"queuetimems":0,"class":"HRegionServer","responsesize":17234,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796453,"queuetimems":2332,"class":"HRegionServer","responsesize":17065,"method":"Multi"}
2014-07-22 13:10:20,318 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24036,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54750","starttimems":1406059796282,"queuetimems":3488,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-07-22 13:10:20,319 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19310 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,319 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19286 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19298 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19307 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19295 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19306 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19301 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,320 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19296 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,321 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,321 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19304 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,321 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:20,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059796888 with entries=88, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059820375
2014-07-22 13:10:20,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:10:20,497 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19311 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,497 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:20,502 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19314 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54750: output error
2014-07-22 13:10:20,502 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:22,799 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:22,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 57913 synced till here 57901
2014-07-22 13:10:22,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059820375 with entries=93, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059822800
2014-07-22 13:10:22,931 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:10:24,477 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11380, memsize=288.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/53155e11263e4a40ba8d8cca6dc56eb6
2014-07-22 13:10:24,490 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/53155e11263e4a40ba8d8cca6dc56eb6 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/53155e11263e4a40ba8d8cca6dc56eb6
2014-07-22 13:10:24,514 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/53155e11263e4a40ba8d8cca6dc56eb6, entries=1050840, sequenceid=11380, filesize=74.8m
2014-07-22 13:10:24,515 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~717.2m/752080480, currentsize=358.7m/376112880 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 46793ms, sequenceid=11380, compaction requested=true
2014-07-22 13:10:24,515 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:10:24,515 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 13:10:24,515 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 13:10:24,515 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 378.8m
2014-07-22 13:10:24,516 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:10:24,516 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:10:24,516 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:10:24,622 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:10:24,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:24,762 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58019 synced till here 58002
2014-07-22 13:10:24,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059822800 with entries=106, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059824723
2014-07-22 13:10:24,921 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:10:26,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:26,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58109 synced till here 58106
2014-07-22 13:10:26,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059824723 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059826313
2014-07-22 13:10:27,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:27,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58196 synced till here 58195
2014-07-22 13:10:27,805 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059826313 with entries=87, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059827747
2014-07-22 13:10:28,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:28,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58282 synced till here 58281
2014-07-22 13:10:28,575 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059827747 with entries=86, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059828527
2014-07-22 13:10:30,307 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11593, memsize=91.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/60424198fd5c47ae82fc6bdabfbd8656
2014-07-22 13:10:30,326 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/60424198fd5c47ae82fc6bdabfbd8656 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/60424198fd5c47ae82fc6bdabfbd8656
2014-07-22 13:10:30,346 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/60424198fd5c47ae82fc6bdabfbd8656, entries=332460, sequenceid=11593, filesize=23.7m
2014-07-22 13:10:30,347 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~378.8m/397185920, currentsize=37.8m/39601760 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 5832ms, sequenceid=11593, compaction requested=true
2014-07-22 13:10:30,348 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:10:30,348 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 2000 blocking
2014-07-22 13:10:30,348 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-22 13:10:30,348 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 832.2m
2014-07-22 13:10:30,348 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:10:30,348 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:10:30,348 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:10:30,660 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11477, memsize=298.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/33262a20963e4b9481ee7563e3eeec9f
2014-07-22 13:10:30,679 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/33262a20963e4b9481ee7563e3eeec9f as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/33262a20963e4b9481ee7563e3eeec9f
2014-07-22 13:10:30,694 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/33262a20963e4b9481ee7563e3eeec9f, entries=1087510, sequenceid=11477, filesize=77.4m
2014-07-22 13:10:30,695 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~754.7m/791353600, currentsize=363.1m/380695680 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 42836ms, sequenceid=11477, compaction requested=true
2014-07-22 13:10:30,695 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:10:30,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 13:10:30,695 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 907.8m
2014-07-22 13:10:30,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 13:10:30,695 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:10:30,696 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:10:30,696 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:10:30,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:30,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059828527 with entries=84, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059830940
2014-07-22 13:10:30,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059695313
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059703226
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059705904
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059709502
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059712293
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059714464
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059719730
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059722299
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059724394
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059726547
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059728717
2014-07-22 13:10:30,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059730397
2014-07-22 13:10:31,005 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:10:31,025 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:10:31,754 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:10:32,144 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:32,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58471 synced till here 58468
2014-07-22 13:10:32,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059830940 with entries=105, filesize=78.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059832144
2014-07-22 13:10:33,784 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:34,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059832144 with entries=101, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059833784
2014-07-22 13:10:36,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:36,618 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58694 synced till here 58678
2014-07-22 13:10:37,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059833784 with entries=122, filesize=98.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059836338
2014-07-22 13:10:46,363 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 7329ms
No GCs detected
2014-07-22 13:10:51,253 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1386ms
GC pool 'ParNew' had collection(s): count=1 time=1724ms
2014-07-22 13:10:51,678 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15576,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836101,"queuetimems":0,"class":"HRegionServer","responsesize":17528,"method":"Multi"}
2014-07-22 13:10:51,679 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19664 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:51,682 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:51,742 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836273,"queuetimems":1,"class":"HRegionServer","responsesize":16991,"method":"Multi"}
2014-07-22 13:10:51,742 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19680 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:51,742 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:51,759 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15426,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836333,"queuetimems":0,"class":"HRegionServer","responsesize":17401,"method":"Multi"}
2014-07-22 13:10:51,759 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19678 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:51,760 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:51,802 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836156,"queuetimems":0,"class":"HRegionServer","responsesize":17216,"method":"Multi"}
2014-07-22 13:10:51,803 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19684 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:51,803 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:51,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:51,859 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15344,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836515,"queuetimems":1,"class":"HRegionServer","responsesize":16769,"method":"Multi"}
2014-07-22 13:10:51,882 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19673 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:51,882 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:51,894 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58798 synced till here 58779
2014-07-22 13:10:52,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059836338 with entries=104, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059851856
2014-07-22 13:10:52,627 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836578,"queuetimems":0,"class":"HRegionServer","responsesize":17118,"method":"Multi"}
2014-07-22 13:10:52,628 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19671 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:52,628 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:52,628 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836412,"queuetimems":4,"class":"HRegionServer","responsesize":16973,"method":"Multi"}
2014-07-22 13:10:52,628 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19676 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:52,629 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:53,254 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15634,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837619,"queuetimems":0,"class":"HRegionServer","responsesize":16347,"method":"Multi"}
2014-07-22 13:10:53,254 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19695 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:53,254 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837960,"queuetimems":0,"class":"HRegionServer","responsesize":17180,"method":"Multi"}
2014-07-22 13:10:53,255 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19702 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:53,255 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:53,256 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837725,"queuetimems":1,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-07-22 13:10:53,256 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19688 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:53,256 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:53,254 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15326,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837927,"queuetimems":1,"class":"HRegionServer","responsesize":17512,"method":"Multi"}
2014-07-22 13:10:53,257 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19705 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:53,257 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:53,254 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:54,969 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1208ms
GC pool 'ParNew' had collection(s): count=1 time=1444ms
2014-07-22 13:10:55,112 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838099,"queuetimems":0,"class":"HRegionServer","responsesize":16562,"method":"Multi"}
2014-07-22 13:10:55,112 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837769,"queuetimems":0,"class":"HRegionServer","responsesize":16746,"method":"Multi"}
2014-07-22 13:10:55,112 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837997,"queuetimems":1,"class":"HRegionServer","responsesize":16675,"method":"Multi"}
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19713 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19701 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19686 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,113 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,114 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837634,"queuetimems":1,"class":"HRegionServer","responsesize":17087,"method":"Multi"}
2014-07-22 13:10:55,114 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19694 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,114 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,417 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:55,427 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17395,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838031,"queuetimems":0,"class":"HRegionServer","responsesize":16834,"method":"Multi"}
2014-07-22 13:10:55,428 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19698 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,428 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,434 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838409,"queuetimems":1,"class":"HRegionServer","responsesize":17001,"method":"Multi"}
2014-07-22 13:10:55,434 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837866,"queuetimems":0,"class":"HRegionServer","responsesize":17290,"method":"Multi"}
2014-07-22 13:10:55,435 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838067,"queuetimems":1,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:10:55,435 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17601,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837833,"queuetimems":0,"class":"HRegionServer","responsesize":16826,"method":"Multi"}
2014-07-22 13:10:55,434 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837801,"queuetimems":0,"class":"HRegionServer","responsesize":16902,"method":"Multi"}
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838451,"queuetimems":0,"class":"HRegionServer","responsesize":17118,"method":"Multi"}
2014-07-22 13:10:55,434 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19716 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,435 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059837667,"queuetimems":0,"class":"HRegionServer","responsesize":17436,"method":"Multi"}
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19718 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19711 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,460 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,461 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19709 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,461 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19696 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19706 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19692 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,468 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,470 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059836623,"queuetimems":0,"class":"HRegionServer","responsesize":17186,"method":"Multi"}
2014-07-22 13:10:55,470 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19669 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,470 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,506 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 58906 synced till here 58889
2014-07-22 13:10:55,693 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17173,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838519,"queuetimems":0,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-07-22 13:10:55,693 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19729 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,693 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,696 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17206,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838487,"queuetimems":0,"class":"HRegionServer","responsesize":17504,"method":"Multi"}
2014-07-22 13:10:55,696 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19730 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,696 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,697 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17077,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838619,"queuetimems":0,"class":"HRegionServer","responsesize":17186,"method":"Multi"}
2014-07-22 13:10:55,697 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17145,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838552,"queuetimems":1,"class":"HRegionServer","responsesize":16973,"method":"Multi"}
2014-07-22 13:10:55,697 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19726 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,697 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,698 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19728 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,698 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,700 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19725 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,700 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,701 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17114,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54753","starttimems":1406059838586,"queuetimems":0,"class":"HRegionServer","responsesize":17528,"method":"Multi"}
2014-07-22 13:10:55,701 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19727 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54753: output error
2014-07-22 13:10:55,701 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:10:55,743 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059851856 with entries=108, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059855418
2014-07-22 13:10:58,985 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:10:59,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59018 synced till here 58992
2014-07-22 13:10:59,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059855418 with entries=112, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059858986
2014-07-22 13:11:01,850 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1376ms
GC pool 'ParNew' had collection(s): count=1 time=1537ms
2014-07-22 13:11:02,363 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:02,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59126 synced till here 59101
2014-07-22 13:11:02,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059858986 with entries=108, filesize=77.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059862365
2014-07-22 13:11:05,027 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:05,042 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59236 synced till here 59215
2014-07-22 13:11:05,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059862365 with entries=110, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059865028
2014-07-22 13:11:07,615 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:07,672 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59358 synced till here 59334
2014-07-22 13:11:07,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059865028 with entries=122, filesize=89.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059867615
2014-07-22 13:11:09,571 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,571 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,572 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,572 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,572 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,598 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,678 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,724 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,767 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,832 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,836 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,841 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,845 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,845 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,846 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,848 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,850 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,850 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,851 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,851 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,851 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,852 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,855 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,856 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,892 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,918 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:09,919 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,920 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,922 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,923 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,925 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,931 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:09,939 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:10,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059867615 with entries=85, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059869919
2014-07-22 13:11:10,803 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:10,835 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:10,887 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:10,926 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:10,960 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,634 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,668 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,703 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,736 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,771 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,823 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,862 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,905 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:11,941 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:11:12,412 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11703, memsize=382.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/de645b37b9ea4eb88fb2aa2c791f0f08
2014-07-22 13:11:12,433 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/de645b37b9ea4eb88fb2aa2c791f0f08 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/de645b37b9ea4eb88fb2aa2c791f0f08
2014-07-22 13:11:12,449 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/de645b37b9ea4eb88fb2aa2c791f0f08, entries=1391950, sequenceid=11703, filesize=99.2m
2014-07-22 13:11:12,449 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~833.8m/874297520, currentsize=343.7m/360391280 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 42101ms, sequenceid=11703, compaction requested=true
2014-07-22 13:11:12,450 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:12,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 13:11:12,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 13:11:12,450 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 509ms
2014-07-22 13:11:12,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:12,450 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 813.0m
2014-07-22 13:11:12,450 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,450 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:12,451 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:11:12,451 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 546ms
2014-07-22 13:11:12,451 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,451 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 589ms
2014-07-22 13:11:12,451 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,451 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 629ms
2014-07-22 13:11:12,451 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,452 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 681ms
2014-07-22 13:11:12,452 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,454 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 717ms
2014-07-22 13:11:12,454 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,454 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 751ms
2014-07-22 13:11:12,454 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,457 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 789ms
2014-07-22 13:11:12,457 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,458 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 823ms
2014-07-22 13:11:12,458 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,461 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1501ms
2014-07-22 13:11:12,461 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,461 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1535ms
2014-07-22 13:11:12,462 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,462 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1575ms
2014-07-22 13:11:12,462 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,465 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1630ms
2014-07-22 13:11:12,466 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,466 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1663ms
2014-07-22 13:11:12,466 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,466 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2527ms
2014-07-22 13:11:12,466 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,466 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2536ms
2014-07-22 13:11:12,466 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,469 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2544ms
2014-07-22 13:11:12,469 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,473 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2550ms
2014-07-22 13:11:12,473 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,474 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2551ms
2014-07-22 13:11:12,474 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,474 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2555ms
2014-07-22 13:11:12,474 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,489 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2570ms
2014-07-22 13:11:12,489 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,489 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2597ms
2014-07-22 13:11:12,489 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,490 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2633ms
2014-07-22 13:11:12,490 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,490 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2635ms
2014-07-22 13:11:12,490 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,492 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2640ms
2014-07-22 13:11:12,492 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,503 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2650ms
2014-07-22 13:11:12,503 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,503 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2652ms
2014-07-22 13:11:12,503 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,503 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2653ms
2014-07-22 13:11:12,503 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,505 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2655ms
2014-07-22 13:11:12,505 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,506 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2655ms
2014-07-22 13:11:12,506 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,506 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2658ms
2014-07-22 13:11:12,506 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,508 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2662ms
2014-07-22 13:11:12,508 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,510 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2665ms
2014-07-22 13:11:12,510 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,511 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2666ms
2014-07-22 13:11:12,511 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,513 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2672ms
2014-07-22 13:11:12,513 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,566 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2681ms
2014-07-22 13:11:12,566 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,573 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2741ms
2014-07-22 13:11:12,574 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,574 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2807ms
2014-07-22 13:11:12,574 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,575 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2851ms
2014-07-22 13:11:12,575 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,575 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2898ms
2014-07-22 13:11:12,575 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,576 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2978ms
2014-07-22 13:11:12,576 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,577 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3004ms
2014-07-22 13:11:12,577 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,579 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3007ms
2014-07-22 13:11:12,579 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,579 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3008ms
2014-07-22 13:11:12,579 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,579 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3008ms
2014-07-22 13:11:12,580 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:12,580 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3010ms
2014-07-22 13:11:12,580 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:11:14,388 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1096ms
GC pool 'ParNew' had collection(s): count=1 time=1311ms
2014-07-22 13:11:14,466 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11710, memsize=382.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/85a7bee22ecd4cd2b4c810b8bd5e4c73
2014-07-22 13:11:14,481 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/85a7bee22ecd4cd2b4c810b8bd5e4c73 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/85a7bee22ecd4cd2b4c810b8bd5e4c73
2014-07-22 13:11:14,491 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/85a7bee22ecd4cd2b4c810b8bd5e4c73, entries=1391840, sequenceid=11710, filesize=99.1m
2014-07-22 13:11:14,492 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~907.8m/951865760, currentsize=380.3m/398797040 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 43797ms, sequenceid=11710, compaction requested=true
2014-07-22 13:11:14,492 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:14,492 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 13:11:14,492 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 13:11:14,492 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 705.9m
2014-07-22 13:11:14,492 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:14,492 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:14,492 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:11:14,496 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:11:14,505 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:11:14,711 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:14,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:15,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59549 synced till here 59530
2014-07-22 13:11:15,252 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059869919 with entries=106, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059874984
2014-07-22 13:11:15,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059736530
2014-07-22 13:11:15,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059751668
2014-07-22 13:11:15,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059754123
2014-07-22 13:11:15,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059755185
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059757057
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059759794
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059762614
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059763923
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059765348
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059766153
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059768218
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059770188
2014-07-22 13:11:15,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059771964
2014-07-22 13:11:15,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059773850
2014-07-22 13:11:15,284 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:17,830 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:17,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59663 synced till here 59632
2014-07-22 13:11:17,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059874984 with entries=114, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059877831
2014-07-22 13:11:19,461 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:19,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59755 synced till here 59750
2014-07-22 13:11:19,521 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059877831 with entries=92, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059879462
2014-07-22 13:11:21,640 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:21,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59841 synced till here 59840
2014-07-22 13:11:21,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059879462 with entries=86, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059881641
2014-07-22 13:11:25,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:25,667 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 59930 synced till here 59928
2014-07-22 13:11:25,725 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059881641 with entries=89, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059885428
2014-07-22 13:11:28,630 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:28,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60016 synced till here 60012
2014-07-22 13:11:28,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059885428 with entries=86, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059888630
2014-07-22 13:11:30,437 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:30,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60106 synced till here 60102
2014-07-22 13:11:30,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059888630 with entries=90, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059890437
2014-07-22 13:11:31,452 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11932, memsize=263.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/35faf6f67343459b8ea3e6885d1e99b7
2014-07-22 13:11:31,464 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/35faf6f67343459b8ea3e6885d1e99b7 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/35faf6f67343459b8ea3e6885d1e99b7
2014-07-22 13:11:31,473 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/35faf6f67343459b8ea3e6885d1e99b7, entries=959690, sequenceid=11932, filesize=68.3m
2014-07-22 13:11:31,474 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~710.6m/745119680, currentsize=197.5m/207055040 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 16982ms, sequenceid=11932, compaction requested=true
2014-07-22 13:11:31,474 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:31,474 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 13:11:31,474 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 13:11:31,474 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 550.7m
2014-07-22 13:11:31,475 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:31,475 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:31,475 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:11:32,550 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=11907, memsize=262.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/dcdf754644dc427ba011afaa8f766f28
2014-07-22 13:11:32,564 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/dcdf754644dc427ba011afaa8f766f28 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/dcdf754644dc427ba011afaa8f766f28
2014-07-22 13:11:32,581 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/dcdf754644dc427ba011afaa8f766f28, entries=955740, sequenceid=11907, filesize=68.1m
2014-07-22 13:11:32,581 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~813.0m/852483120, currentsize=223.5m/234323920 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 20131ms, sequenceid=11907, compaction requested=true
2014-07-22 13:11:32,581 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:32,582 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 13:11:32,582 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 552.7m
2014-07-22 13:11:32,582 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 13:11:32,582 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:32,582 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:32,582 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:11:32,742 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:33,360 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:33,454 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:33,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60200 synced till here 60190
2014-07-22 13:11:33,547 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059890437 with entries=94, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059893454
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059776110
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059778596
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059782285
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059785266
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059787608
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059790505
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059792807
2014-07-22 13:11:33,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059794811
2014-07-22 13:11:33,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059796888
2014-07-22 13:11:33,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059820375
2014-07-22 13:11:34,794 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:11:35,320 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:35,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60306 synced till here 60304
2014-07-22 13:11:35,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059893454 with entries=106, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059895320
2014-07-22 13:11:35,695 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:11:37,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:37,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60410 synced till here 60404
2014-07-22 13:11:37,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059895320 with entries=104, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059897421
2014-07-22 13:11:39,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:39,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60505 synced till here 60497
2014-07-22 13:11:39,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059897421 with entries=95, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059899826
2014-07-22 13:11:44,337 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12066, memsize=222.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/def31eea00444f15b71636bdc4dd214a
2014-07-22 13:11:44,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:44,357 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/def31eea00444f15b71636bdc4dd214a as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/def31eea00444f15b71636bdc4dd214a
2014-07-22 13:11:44,360 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60595 synced till here 60594
2014-07-22 13:11:44,368 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/def31eea00444f15b71636bdc4dd214a, entries=808650, sequenceid=12066, filesize=57.6m
2014-07-22 13:11:44,368 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~554.9m/581803920, currentsize=145.2m/152244720 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 12894ms, sequenceid=12066, compaction requested=true
2014-07-22 13:11:44,369 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:44,369 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 13:11:44,369 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 13:11:44,369 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 365.0m
2014-07-22 13:11:44,369 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:44,369 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:44,369 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:11:44,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059899826 with entries=90, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059904341
2014-07-22 13:11:44,643 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:46,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:46,183 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:11:46,377 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12064, memsize=229.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/4cbf45fad99541e39506757d20f1137d
2014-07-22 13:11:46,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059904341 with entries=107, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059906086
2014-07-22 13:11:46,452 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/4cbf45fad99541e39506757d20f1137d as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/4cbf45fad99541e39506757d20f1137d
2014-07-22 13:11:46,475 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/4cbf45fad99541e39506757d20f1137d, entries=836430, sequenceid=12064, filesize=59.6m
2014-07-22 13:11:46,475 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~557.6m/584650400, currentsize=165.8m/173807440 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 13893ms, sequenceid=12064, compaction requested=true
2014-07-22 13:11:46,476 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:46,476 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 13:11:46,476 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 381.5m
2014-07-22 13:11:46,476 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 13:11:46,476 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:46,476 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:46,476 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:11:46,763 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:11:52,048 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:53,719 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1283ms
GC pool 'ParNew' had collection(s): count=1 time=1541ms
2014-07-22 13:11:53,727 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60795 synced till here 60781
2014-07-22 13:11:53,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059906086 with entries=93, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059912049
2014-07-22 13:11:55,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:56,560 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 60896 synced till here 60891
2014-07-22 13:11:56,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059912049 with entries=101, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059915496
2014-07-22 13:11:57,260 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:11:59,627 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1570ms
GC pool 'ParNew' had collection(s): count=1 time=1739ms
2014-07-22 13:11:59,630 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:11:59,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:11:59,813 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12144, memsize=250.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/232cffd2f9914bdab56f07da4c397b3b
2014-07-22 13:11:59,864 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/232cffd2f9914bdab56f07da4c397b3b as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/232cffd2f9914bdab56f07da4c397b3b
2014-07-22 13:11:59,881 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/232cffd2f9914bdab56f07da4c397b3b, entries=911920, sequenceid=12144, filesize=64.9m
2014-07-22 13:11:59,882 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~365.0m/382698240, currentsize=113.1m/118632560 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15513ms, sequenceid=12144, compaction requested=true
2014-07-22 13:11:59,883 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:11:59,883 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 13:11:59,883 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 281.9m
2014-07-22 13:11:59,883 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 13:11:59,883 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:11:59,883 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:11:59,883 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:11:59,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61014 synced till here 60986
2014-07-22 13:12:00,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059915496 with entries=118, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059919769
2014-07-22 13:12:00,413 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:03,947 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2818ms
GC pool 'ParNew' had collection(s): count=1 time=3160ms
2014-07-22 13:12:04,530 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:04,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61131 synced till here 61103
2014-07-22 13:12:04,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059919769 with entries=117, filesize=89.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059924531
2014-07-22 13:12:07,312 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2358ms
GC pool 'ParNew' had collection(s): count=1 time=2394ms
2014-07-22 13:12:08,165 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:08,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61254 synced till here 61230
2014-07-22 13:12:08,430 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059924531 with entries=123, filesize=90.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059928165
2014-07-22 13:12:09,688 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12178, memsize=273.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/f1bcb474f8a04662898dc1d7e27861de
2014-07-22 13:12:09,705 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/f1bcb474f8a04662898dc1d7e27861de as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/f1bcb474f8a04662898dc1d7e27861de
2014-07-22 13:12:09,767 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/f1bcb474f8a04662898dc1d7e27861de, entries=996570, sequenceid=12178, filesize=70.9m
2014-07-22 13:12:09,768 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~381.5m/400032320, currentsize=174.4m/182857280 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 23292ms, sequenceid=12178, compaction requested=true
2014-07-22 13:12:09,769 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:09,769 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 13:12:09,769 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 13:12:09,769 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:09,769 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:09,769 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:12:09,769 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 369.0m
2014-07-22 13:12:10,446 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:10,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:10,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61388 synced till here 61357
2014-07-22 13:12:11,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059928165 with entries=134, filesize=94.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059930553
2014-07-22 13:12:14,025 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:14,065 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:12:14,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61538 synced till here 61508
2014-07-22 13:12:14,481 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:12:14,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059930553 with entries=150, filesize=115.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059934026
2014-07-22 13:12:16,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:16,806 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61709 synced till here 61673
2014-07-22 13:12:18,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059934026 with entries=171, filesize=127.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059936430
2014-07-22 13:12:18,192 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12196, memsize=146.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/eb5116ff41704014a3107c52a4442e7d
2014-07-22 13:12:18,215 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/eb5116ff41704014a3107c52a4442e7d as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/eb5116ff41704014a3107c52a4442e7d
2014-07-22 13:12:18,227 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/eb5116ff41704014a3107c52a4442e7d, entries=533410, sequenceid=12196, filesize=38.0m
2014-07-22 13:12:18,228 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~283.8m/297540960, currentsize=60.4m/63324800 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 18345ms, sequenceid=12196, compaction requested=true
2014-07-22 13:12:18,229 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:18,229 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 2000 blocking
2014-07-22 13:12:18,229 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-22 13:12:18,229 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 497.6m
2014-07-22 13:12:18,229 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:18,229 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:18,229 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:12:18,879 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:18,922 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61809 synced till here 61800
2014-07-22 13:12:19,150 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:19,196 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059936430 with entries=100, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059938879
2014-07-22 13:12:19,197 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059822800
2014-07-22 13:12:19,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059824723
2014-07-22 13:12:19,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059826313
2014-07-22 13:12:19,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059827747
2014-07-22 13:12:19,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059828527
2014-07-22 13:12:19,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059830940
2014-07-22 13:12:19,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059832144
2014-07-22 13:12:19,200 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059833784
2014-07-22 13:12:19,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059836338
2014-07-22 13:12:19,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059851856
2014-07-22 13:12:19,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059855418
2014-07-22 13:12:19,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059858986
2014-07-22 13:12:19,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059862365
2014-07-22 13:12:19,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059865028
2014-07-22 13:12:19,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059867615
2014-07-22 13:12:19,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059869919
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059874984
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059877831
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059879462
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059881641
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059885428
2014-07-22 13:12:19,203 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059888630
2014-07-22 13:12:20,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:20,616 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61910 synced till here 61906
2014-07-22 13:12:20,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059938879 with entries=101, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059940595
2014-07-22 13:12:22,064 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:22,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 61997 synced till here 61996
2014-07-22 13:12:22,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059940595 with entries=87, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059942064
2014-07-22 13:12:23,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:24,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62100 synced till here 62096
2014-07-22 13:12:24,156 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059942064 with entries=103, filesize=77.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059943723
2014-07-22 13:12:25,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:25,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059943723 with entries=85, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059945664
2014-07-22 13:12:26,926 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12320, memsize=222.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/04e14104394042b3a8bba531c27148ce
2014-07-22 13:12:26,944 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/04e14104394042b3a8bba531c27148ce as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/04e14104394042b3a8bba531c27148ce
2014-07-22 13:12:26,959 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/04e14104394042b3a8bba531c27148ce, entries=809320, sequenceid=12320, filesize=57.7m
2014-07-22 13:12:26,960 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~392.2m/411202560, currentsize=276.1m/289493520 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 17191ms, sequenceid=12320, compaction requested=true
2014-07-22 13:12:26,960 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:26,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 13:12:26,960 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 13:12:26,961 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 516.4m
2014-07-22 13:12:26,961 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:26,961 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:26,961 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:12:27,146 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:12:27,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:28,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62284 synced till here 62280
2014-07-22 13:12:28,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059945664 with entries=99, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059947160
2014-07-22 13:12:28,534 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:29,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:29,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62377 synced till here 62369
2014-07-22 13:12:29,630 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059947160 with entries=93, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059949452
2014-07-22 13:12:31,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:31,544 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62480 synced till here 62470
2014-07-22 13:12:32,327 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059949452 with entries=103, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059951481
2014-07-22 13:12:33,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:34,908 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62584 synced till here 62565
2014-07-22 13:12:35,027 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12401, memsize=214.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/ce09e851506249da96a8da14c87c3e0e
2014-07-22 13:12:35,043 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/ce09e851506249da96a8da14c87c3e0e as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/ce09e851506249da96a8da14c87c3e0e
2014-07-22 13:12:35,061 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/ce09e851506249da96a8da14c87c3e0e, entries=779220, sequenceid=12401, filesize=55.5m
2014-07-22 13:12:35,061 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~520.7m/545959040, currentsize=241.1m/252780560 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 16832ms, sequenceid=12401, compaction requested=true
2014-07-22 13:12:35,062 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:35,062 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 13:12:35,062 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 13:12:35,062 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 581.6m
2014-07-22 13:12:35,062 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:35,062 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:35,062 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:12:35,086 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059951481 with entries=104, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059953844
2014-07-22 13:12:35,086 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059890437
2014-07-22 13:12:35,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059893454
2014-07-22 13:12:35,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059895320
2014-07-22 13:12:35,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059897421
2014-07-22 13:12:35,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059899826
2014-07-22 13:12:35,782 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:12:37,131 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:37,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62695 synced till here 62665
2014-07-22 13:12:37,269 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:37,456 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059953844 with entries=111, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059957132
2014-07-22 13:12:39,358 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:39,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62816 synced till here 62779
2014-07-22 13:12:39,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059957132 with entries=121, filesize=87.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059959359
2014-07-22 13:12:41,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:41,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 62941 synced till here 62916
2014-07-22 13:12:41,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059959359 with entries=125, filesize=93.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059961613
2014-07-22 13:12:43,739 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1078ms
GC pool 'ParNew' had collection(s): count=1 time=1554ms
2014-07-22 13:12:44,280 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12480, memsize=169.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/d5889ca539ed4b80a7f4de7f4074b66a
2014-07-22 13:12:44,304 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/d5889ca539ed4b80a7f4de7f4074b66a as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/d5889ca539ed4b80a7f4de7f4074b66a
2014-07-22 13:12:44,324 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/d5889ca539ed4b80a7f4de7f4074b66a, entries=616750, sequenceid=12480, filesize=44.0m
2014-07-22 13:12:44,324 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~519.5m/544691600, currentsize=216.0m/226467360 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 17364ms, sequenceid=12480, compaction requested=true
2014-07-22 13:12:44,325 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 13:12:44,325 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:44,325 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 13:12:44,325 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:44,325 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:44,325 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:12:44,325 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 529.5m
2014-07-22 13:12:44,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:44,353 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63065 synced till here 63033
2014-07-22 13:12:47,865 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3125ms
GC pool 'ParNew' had collection(s): count=1 time=3137ms
2014-07-22 13:12:47,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059961613 with entries=124, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059964333
2014-07-22 13:12:47,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059904341
2014-07-22 13:12:48,150 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21040 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21006 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21045 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21041 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21011 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,152 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21039 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21014 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21033 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21037 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,153 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,299 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21034 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,299 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,306 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21031 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,306 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21035 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21032 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21036 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,310 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,414 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:48,597 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21030 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21028 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21022 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21025 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,599 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,600 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21026 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,600 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,606 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21059 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,606 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,613 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:12:48,614 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21061 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,614 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,614 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21027 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,614 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63183 synced till here 63169
2014-07-22 13:12:48,764 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21056 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,764 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,764 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21029 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,764 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059964333 with entries=118, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059968598
2014-07-22 13:12:48,881 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21060 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,881 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,890 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21054 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,890 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,960 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21057 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,960 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21023 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21052 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21078 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21076 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,961 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,963 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21024 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,963 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:48,963 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21058 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:48,963 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:50,806 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1439ms
GC pool 'ParNew' had collection(s): count=1 time=1832ms
2014-07-22 13:12:51,046 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21082 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,046 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,047 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21074 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,047 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,047 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21081 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,047 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,048 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21083 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,048 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21049 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21080 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21077 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,049 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,050 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21055 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,050 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,050 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21079 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,050 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21075 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21051 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21084 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21053 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21050 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,051 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,096 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21073 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,096 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,096 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21072 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,097 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,135 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21071 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54754: output error
2014-07-22 13:12:51,135 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:12:51,358 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:51,400 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63285 synced till here 63279
2014-07-22 13:12:51,490 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059968598 with entries=102, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059971359
2014-07-22 13:12:52,915 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:52,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63377 synced till here 63365
2014-07-22 13:12:53,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059971359 with entries=92, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059972915
2014-07-22 13:12:54,803 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:54,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63465 synced till here 63463
2014-07-22 13:12:54,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059972915 with entries=88, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059974803
2014-07-22 13:12:57,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:57,076 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63553 synced till here 63546
2014-07-22 13:12:57,118 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059974803 with entries=88, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059977029
2014-07-22 13:12:57,848 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12562, memsize=225.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/bca8c377db794e9d9f51eb6e7676ca32
2014-07-22 13:12:57,887 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/bca8c377db794e9d9f51eb6e7676ca32 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/bca8c377db794e9d9f51eb6e7676ca32
2014-07-22 13:12:57,943 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/bca8c377db794e9d9f51eb6e7676ca32, entries=820560, sequenceid=12562, filesize=58.4m
2014-07-22 13:12:57,944 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~595.7m/624618480, currentsize=314.9m/330212720 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 22882ms, sequenceid=12562, compaction requested=true
2014-07-22 13:12:57,945 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:12:57,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 13:12:57,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 13:12:57,945 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 555.4m
2014-07-22 13:12:57,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:12:57,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:12:57,945 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:12:57,951 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:12:58,855 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:58,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63646 synced till here 63637
2014-07-22 13:12:58,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059977029 with entries=93, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059978855
2014-07-22 13:12:58,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059906086
2014-07-22 13:12:58,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059912049
2014-07-22 13:12:58,951 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059915496
2014-07-22 13:12:59,328 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:12:59,420 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:12:59,709 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:12:59,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63736 synced till here 63735
2014-07-22 13:12:59,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059978855 with entries=90, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059979710
2014-07-22 13:13:01,633 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:01,669 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63823 synced till here 63822
2014-07-22 13:13:01,680 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059979710 with entries=87, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059981634
2014-07-22 13:13:02,870 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12679, memsize=211.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/66befb7c41c54d1e9855e937852761b0
2014-07-22 13:13:02,897 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/66befb7c41c54d1e9855e937852761b0 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/66befb7c41c54d1e9855e937852761b0
2014-07-22 13:13:02,913 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/66befb7c41c54d1e9855e937852761b0, entries=769780, sequenceid=12679, filesize=54.8m
2014-07-22 13:13:02,914 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~556.0m/582968000, currentsize=208.6m/218698800 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 18589ms, sequenceid=12679, compaction requested=true
2014-07-22 13:13:02,914 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:02,914 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 13:13:02,914 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 495.4m
2014-07-22 13:13:02,915 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 13:13:02,915 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:02,915 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:02,915 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:13:03,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:03,332 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63909 synced till here 63906
2014-07-22 13:13:03,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059981634 with entries=86, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059983316
2014-07-22 13:13:03,636 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:04,978 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:05,002 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 63998 synced till here 63994
2014-07-22 13:13:05,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059983316 with entries=89, filesize=66.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059984979
2014-07-22 13:13:05,150 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:13:06,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:06,299 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64085 synced till here 64082
2014-07-22 13:13:06,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059984979 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059986282
2014-07-22 13:13:07,881 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:07,904 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64171 synced till here 64170
2014-07-22 13:13:07,928 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059986282 with entries=86, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059987882
2014-07-22 13:13:09,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:09,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64260 synced till here 64253
2014-07-22 13:13:09,380 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059987882 with entries=89, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059989292
2014-07-22 13:13:10,976 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:11,009 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64348 synced till here 64344
2014-07-22 13:13:11,107 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059989292 with entries=88, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059990976
2014-07-22 13:13:12,273 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:12,299 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64437 synced till here 64433
2014-07-22 13:13:12,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059990976 with entries=89, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059992274
2014-07-22 13:13:12,482 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:13:12,685 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12765, memsize=239.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/af84f01209a048e3bcfbee30ee526d3e
2014-07-22 13:13:12,705 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/af84f01209a048e3bcfbee30ee526d3e as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/af84f01209a048e3bcfbee30ee526d3e
2014-07-22 13:13:12,718 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/af84f01209a048e3bcfbee30ee526d3e, entries=871610, sequenceid=12765, filesize=62.1m
2014-07-22 13:13:12,719 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~563.1m/590498560, currentsize=248.9m/260996880 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 14774ms, sequenceid=12765, compaction requested=true
2014-07-22 13:13:12,719 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:12,719 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 30 store files, 0 compacting, 30 eligible, 2000 blocking
2014-07-22 13:13:12,719 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 568.0m
2014-07-22 13:13:12,720 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 30 files from compaction candidates
2014-07-22 13:13:12,720 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:12,720 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:12,720 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:13:12,779 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:13:13,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:14,085 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:14,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64535 synced till here 64523
2014-07-22 13:13:14,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059992274 with entries=98, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059994083
2014-07-22 13:13:14,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:13:14,614 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12801, memsize=178.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5431edefac66429f91d636feb42e87ea
2014-07-22 13:13:14,628 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5431edefac66429f91d636feb42e87ea as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5431edefac66429f91d636feb42e87ea
2014-07-22 13:13:14,645 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5431edefac66429f91d636feb42e87ea, entries=649040, sequenceid=12801, filesize=46.3m
2014-07-22 13:13:14,646 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~496.9m/521082960, currentsize=219.3m/229944080 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 11732ms, sequenceid=12801, compaction requested=true
2014-07-22 13:13:14,646 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:14,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 13:13:14,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 13:13:14,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:14,646 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 339.5m
2014-07-22 13:13:14,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:14,647 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:13:14,915 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:16,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:16,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64640 synced till here 64637
2014-07-22 13:13:16,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059994083 with entries=105, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059996125
2014-07-22 13:13:17,519 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:13:17,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:17,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64727 synced till here 64726
2014-07-22 13:13:17,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059996125 with entries=87, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059997723
2014-07-22 13:13:19,590 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:19,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64815 synced till here 64807
2014-07-22 13:13:19,726 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059997723 with entries=88, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059999591
2014-07-22 13:13:20,904 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:20,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 64905 synced till here 64903
2014-07-22 13:13:20,979 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059999591 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060000905
2014-07-22 13:13:21,980 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12919, memsize=140.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/c5b23ffb75174fc48cbc9fa0a2d95035
2014-07-22 13:13:22,035 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/c5b23ffb75174fc48cbc9fa0a2d95035 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/c5b23ffb75174fc48cbc9fa0a2d95035
2014-07-22 13:13:22,090 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/c5b23ffb75174fc48cbc9fa0a2d95035, entries=511020, sequenceid=12919, filesize=36.4m
2014-07-22 13:13:22,090 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~339.5m/356041840, currentsize=38.4m/40226960 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 7444ms, sequenceid=12919, compaction requested=true
2014-07-22 13:13:22,091 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:22,091 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 2000 blocking
2014-07-22 13:13:22,091 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-22 13:13:22,091 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:22,091 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 551.9m
2014-07-22 13:13:22,091 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:22,091 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:13:22,538 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:22,710 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:22,746 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060000905 with entries=85, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060002710
2014-07-22 13:13:22,746 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059919769
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059924531
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059928165
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059930553
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059934026
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059936430
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059938879
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059940595
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059942064
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059943723
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059945664
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059947160
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059949452
2014-07-22 13:13:22,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059951481
2014-07-22 13:13:24,751 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:24,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65081 synced till here 65075
2014-07-22 13:13:24,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060002710 with entries=91, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060004751
2014-07-22 13:13:26,821 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:26,837 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65167 synced till here 65164
2014-07-22 13:13:26,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060004751 with entries=86, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060006822
2014-07-22 13:13:27,876 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=12931, memsize=284.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/4db41c6b9dde4f1da8d0ca183b0582b2
2014-07-22 13:13:27,914 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/4db41c6b9dde4f1da8d0ca183b0582b2 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/4db41c6b9dde4f1da8d0ca183b0582b2
2014-07-22 13:13:27,939 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/4db41c6b9dde4f1da8d0ca183b0582b2, entries=1036200, sequenceid=12931, filesize=73.8m
2014-07-22 13:13:27,939 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~572.9m/600710080, currentsize=229.5m/240637680 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 15220ms, sequenceid=12931, compaction requested=true
2014-07-22 13:13:27,940 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:27,940 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 13:13:27,940 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 13:13:27,940 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 478.3m
2014-07-22 13:13:27,940 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:27,940 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:27,941 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:13:28,644 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:29,456 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:29,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65263 synced till here 65253
2014-07-22 13:13:29,622 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060006822 with entries=96, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060009456
2014-07-22 13:13:29,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059953844
2014-07-22 13:13:29,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059957132
2014-07-22 13:13:29,623 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059959359
2014-07-22 13:13:30,713 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:13:31,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:31,248 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65353 synced till here 65352
2014-07-22 13:13:31,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060009456 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060011227
2014-07-22 13:13:32,757 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:32,782 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65447 synced till here 65437
2014-07-22 13:13:32,875 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060011227 with entries=94, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060012758
2014-07-22 13:13:34,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:35,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65566 synced till here 65565
2014-07-22 13:13:35,229 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060012758 with entries=119, filesize=85.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060014777
2014-07-22 13:13:36,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:36,557 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65660 synced till here 65647
2014-07-22 13:13:36,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060014777 with entries=94, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060016467
2014-07-22 13:13:38,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:38,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65749 synced till here 65748
2014-07-22 13:13:38,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060016467 with entries=89, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060018754
2014-07-22 13:13:40,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:40,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 65839 synced till here 65827
2014-07-22 13:13:40,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060018754 with entries=90, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060020293
2014-07-22 13:13:42,660 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13035, memsize=422.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/58374c00849344eab14e7d612405d529
2014-07-22 13:13:42,690 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/58374c00849344eab14e7d612405d529 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/58374c00849344eab14e7d612405d529
2014-07-22 13:13:42,710 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/58374c00849344eab14e7d612405d529, entries=1538690, sequenceid=13035, filesize=109.6m
2014-07-22 13:13:42,710 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~551.9m/578702000, currentsize=273.9m/287243120 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 20619ms, sequenceid=13035, compaction requested=true
2014-07-22 13:13:42,711 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:42,711 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 13:13:42,711 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 13:13:42,711 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:42,711 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 619.7m
2014-07-22 13:13:42,711 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:42,711 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:13:43,168 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:43,367 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:13:44,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:44,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060020293 with entries=145, filesize=103.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060024506
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059961613
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059964333
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059968598
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059971359
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059972915
2014-07-22 13:13:44,991 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059974803
2014-07-22 13:13:47,472 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13074, memsize=424.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2207436ef3a64f45869c593d85d4e32b
2014-07-22 13:13:47,484 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/2207436ef3a64f45869c593d85d4e32b as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2207436ef3a64f45869c593d85d4e32b
2014-07-22 13:13:47,495 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/2207436ef3a64f45869c593d85d4e32b, entries=1544510, sequenceid=13074, filesize=110.0m
2014-07-22 13:13:47,495 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~478.3m/501556880, currentsize=267.9m/280885760 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 19555ms, sequenceid=13074, compaction requested=true
2014-07-22 13:13:47,496 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:13:47,496 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 31 store files, 0 compacting, 31 eligible, 2000 blocking
2014-07-22 13:13:47,496 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 492.0m
2014-07-22 13:13:47,496 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 31 files from compaction candidates
2014-07-22 13:13:47,496 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:13:47,496 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:13:47,497 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:13:47,498 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:13:47,565 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:47,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66071 synced till here 66066
2014-07-22 13:13:47,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060024506 with entries=87, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060027565
2014-07-22 13:13:47,658 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059977029
2014-07-22 13:13:47,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059978855
2014-07-22 13:13:47,660 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059979710
2014-07-22 13:13:48,502 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:13:49,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:49,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66164 synced till here 66158
2014-07-22 13:13:49,814 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060027565 with entries=93, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060029711
2014-07-22 13:13:51,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:51,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060029711 with entries=110, filesize=79.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060031451
2014-07-22 13:13:54,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:54,276 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66364 synced till here 66350
2014-07-22 13:13:54,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060031451 with entries=90, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060034248
2014-07-22 13:13:55,942 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:56,232 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66477 synced till here 66476
2014-07-22 13:13:56,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060034248 with entries=113, filesize=79.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060035943
2014-07-22 13:13:57,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:13:57,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66578 synced till here 66574
2014-07-22 13:13:57,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060035943 with entries=101, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060037713
2014-07-22 13:14:00,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:00,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66668 synced till here 66664
2014-07-22 13:14:00,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060037713 with entries=90, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060040437
2014-07-22 13:14:02,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:02,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66779 synced till here 66774
2014-07-22 13:14:02,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060040437 with entries=111, filesize=82.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060042269
2014-07-22 13:14:04,660 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:04,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66873 synced till here 66857
2014-07-22 13:14:04,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060042269 with entries=94, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060044661
2014-07-22 13:14:06,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:08,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 66987 synced till here 66969
2014-07-22 13:14:08,529 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060044661 with entries=114, filesize=86.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060046963
2014-07-22 13:14:09,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:10,439 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67096 synced till here 67077
2014-07-22 13:14:10,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060046963 with entries=109, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060049256
2014-07-22 13:14:11,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:12,703 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67214 synced till here 67205
2014-07-22 13:14:12,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060049256 with entries=118, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060051395
2014-07-22 13:14:13,546 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:13,616 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13201, memsize=528.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/7f66de4f300a478f8d3d059db92a9d4d
2014-07-22 13:14:13,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67316 synced till here 67312
2014-07-22 13:14:13,661 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:14:13,689 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/7f66de4f300a478f8d3d059db92a9d4d as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/7f66de4f300a478f8d3d059db92a9d4d
2014-07-22 13:14:13,694 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060051395 with entries=102, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060053547
2014-07-22 13:14:13,744 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/7f66de4f300a478f8d3d059db92a9d4d, entries=1923440, sequenceid=13201, filesize=137.0m
2014-07-22 13:14:13,745 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~619.7m/649805760, currentsize=455.8m/477898960 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 31034ms, sequenceid=13201, compaction requested=true
2014-07-22 13:14:13,746 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:14:13,746 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 13:14:13,746 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 730.7m
2014-07-22 13:14:13,746 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 13:14:13,746 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:14:13,746 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:14:13,746 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:14:15,017 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13252, memsize=419.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d8a81a1db2cb4caeb63ec0bd0bd86929
2014-07-22 13:14:15,033 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/d8a81a1db2cb4caeb63ec0bd0bd86929 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d8a81a1db2cb4caeb63ec0bd0bd86929
2014-07-22 13:14:15,055 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/d8a81a1db2cb4caeb63ec0bd0bd86929, entries=1525460, sequenceid=13252, filesize=108.6m
2014-07-22 13:14:15,058 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~496.7m/520783200, currentsize=394.0m/413133520 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 27562ms, sequenceid=13252, compaction requested=true
2014-07-22 13:14:15,058 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:14:15,058 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 13:14:15,058 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 13:14:15,058 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 669.1m
2014-07-22 13:14:15,058 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:14:15,059 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:14:15,059 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:14:15,087 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:14:15,253 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:14:15,565 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:14:15,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:15,714 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67407 synced till here 67395
2014-07-22 13:14:15,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060053547 with entries=91, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060055681
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059981634
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059983316
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059984979
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059986282
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059987882
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059989292
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059990976
2014-07-22 13:14:15,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059992274
2014-07-22 13:14:15,902 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:14:17,515 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1153ms
GC pool 'ParNew' had collection(s): count=1 time=1498ms
2014-07-22 13:14:18,161 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:18,196 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67501 synced till here 67491
2014-07-22 13:14:18,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060055681 with entries=94, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060058162
2014-07-22 13:14:19,514 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:14:19,909 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67604 synced till here 67597
2014-07-22 13:14:19,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060058162 with entries=103, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060059515
2014-07-22 13:14:21,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:21,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67695 synced till here 67683
2014-07-22 13:14:21,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060059515 with entries=91, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060061494
2014-07-22 13:14:23,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:23,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67784 synced till here 67780
2014-07-22 13:14:23,516 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060061494 with entries=89, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060063464
2014-07-22 13:14:23,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:24,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:24,688 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 67871 synced till here 67870
2014-07-22 13:14:24,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060063464 with entries=87, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060064663
2014-07-22 13:14:24,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:25,829 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:25,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060064663 with entries=87, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060065829
2014-07-22 13:14:25,903 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:27,883 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:28,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68050 synced till here 68044
2014-07-22 13:14:28,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060065829 with entries=92, filesize=69.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060067883
2014-07-22 13:14:28,588 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:29,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:29,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68138 synced till here 68129
2014-07-22 13:14:29,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060067883 with entries=88, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060069301
2014-07-22 13:14:29,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:30,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:30,867 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68248 synced till here 68246
2014-07-22 13:14:30,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060069301 with entries=110, filesize=79.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060070676
2014-07-22 13:14:30,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:32,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:32,373 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060070676 with entries=90, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060072187
2014-07-22 13:14:32,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:34,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:34,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68430 synced till here 68420
2014-07-22 13:14:35,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060072187 with entries=92, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060074921
2014-07-22 13:14:35,035 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:14:36,461 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,462 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,465 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,468 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,470 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,484 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,484 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,508 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,510 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,512 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,516 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,516 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,738 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,787 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,830 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,885 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,941 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:36,972 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,022 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,069 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,106 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,145 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,188 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,237 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,274 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,322 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,363 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,409 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:14:37,516 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13509, memsize=377.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/64901e84244e4f61bdafa17805b4ab8b
2014-07-22 13:14:37,540 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/64901e84244e4f61bdafa17805b4ab8b as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/64901e84244e4f61bdafa17805b4ab8b
2014-07-22 13:14:37,557 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/64901e84244e4f61bdafa17805b4ab8b, entries=1375490, sequenceid=13509, filesize=97.9m
2014-07-22 13:14:37,558 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~675.2m/707947040, currentsize=357.4m/374770480 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 22500ms, sequenceid=13509, compaction requested=true
2014-07-22 13:14:37,559 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:14:37,559 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 150ms
2014-07-22 13:14:37,559 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 32 store files, 0 compacting, 32 eligible, 2000 blocking
2014-07-22 13:14:37,559 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,559 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 32 files from compaction candidates
2014-07-22 13:14:37,559 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 196ms
2014-07-22 13:14:37,559 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,559 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 237ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,560 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 286ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,560 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 324ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,560 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 372ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,560 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 415ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,560 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 454ms
2014-07-22 13:14:37,560 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,561 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 492ms
2014-07-22 13:14:37,561 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,561 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 539ms
2014-07-22 13:14:37,561 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,559 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 365.5m
2014-07-22 13:14:37,570 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 598ms
2014-07-22 13:14:37,559 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:14:37,570 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,570 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:14:37,571 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:14:37,575 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 634ms
2014-07-22 13:14:37,575 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,575 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 690ms
2014-07-22 13:14:37,575 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,579 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 749ms
2014-07-22 13:14:37,579 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,583 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 796ms
2014-07-22 13:14:37,583 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,583 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 845ms
2014-07-22 13:14:37,583 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,583 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1067ms
2014-07-22 13:14:37,583 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,584 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1068ms
2014-07-22 13:14:37,584 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,584 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1073ms
2014-07-22 13:14:37,584 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,584 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1074ms
2014-07-22 13:14:37,584 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,585 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1077ms
2014-07-22 13:14:37,585 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,589 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1105ms
2014-07-22 13:14:37,589 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,589 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1105ms
2014-07-22 13:14:37,589 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,590 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1119ms
2014-07-22 13:14:37,590 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,590 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1122ms
2014-07-22 13:14:37,590 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,591 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1126ms
2014-07-22 13:14:37,591 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,591 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1130ms
2014-07-22 13:14:37,591 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,591 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1130ms
2014-07-22 13:14:37,591 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:14:37,915 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:37,941 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:14:37,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68537 synced till here 68521
2014-07-22 13:14:39,145 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1058ms
GC pool 'ParNew' had collection(s): count=1 time=1157ms
2014-07-22 13:14:39,190 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:14:39,285 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13509, memsize=434.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/67b59ada5ace46eb92fc40581b7c5962
2014-07-22 13:14:39,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060074921 with entries=107, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060077916
2014-07-22 13:14:39,330 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/67b59ada5ace46eb92fc40581b7c5962 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/67b59ada5ace46eb92fc40581b7c5962
2014-07-22 13:14:39,352 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/67b59ada5ace46eb92fc40581b7c5962, entries=1583160, sequenceid=13509, filesize=112.7m
2014-07-22 13:14:39,352 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~733.8m/769477760, currentsize=402.5m/422000560 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 25606ms, sequenceid=13509, compaction requested=true
2014-07-22 13:14:39,353 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 13:14:39,353 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 13:14:39,353 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:14:39,353 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:14:39,353 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:14:39,354 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:14:39,354 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 766.9m
2014-07-22 13:14:39,430 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:14:40,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:40,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68634 synced till here 68611
2014-07-22 13:14:41,985 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1339ms
GC pool 'ParNew' had collection(s): count=1 time=1750ms
2014-07-22 13:14:42,077 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060077916 with entries=97, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060080084
2014-07-22 13:14:42,362 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:14:42,993 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:44,944 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1457ms
GC pool 'ParNew' had collection(s): count=1 time=1859ms
2014-07-22 13:14:44,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68755 synced till here 68709
2014-07-22 13:14:45,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060080084 with entries=121, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060082993
2014-07-22 13:14:47,240 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:47,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68876 synced till here 68837
2014-07-22 13:14:47,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060082993 with entries=121, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060087240
2014-07-22 13:14:49,196 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1018ms
GC pool 'ParNew' had collection(s): count=1 time=1052ms
2014-07-22 13:14:49,551 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:49,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 68991 synced till here 68952
2014-07-22 13:14:50,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060087240 with entries=115, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060089551
2014-07-22 13:14:53,834 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:53,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69126 synced till here 69125
2014-07-22 13:14:54,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060089551 with entries=135, filesize=87.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060093834
2014-07-22 13:14:56,690 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1359ms
GC pool 'ParNew' had collection(s): count=1 time=1609ms
2014-07-22 13:14:57,084 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:14:57,791 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69295 synced till here 69266
2014-07-22 13:14:59,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060093834 with entries=169, filesize=133.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060097084
2014-07-22 13:15:01,891 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1345ms
GC pool 'ParNew' had collection(s): count=1 time=1758ms
2014-07-22 13:15:01,961 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13699, memsize=225.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/cce475d4c6724a97a138725865d2420f
2014-07-22 13:15:01,977 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/cce475d4c6724a97a138725865d2420f as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/cce475d4c6724a97a138725865d2420f
2014-07-22 13:15:01,987 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/cce475d4c6724a97a138725865d2420f, entries=819820, sequenceid=13699, filesize=58.4m
2014-07-22 13:15:01,988 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~365.5m/383225760, currentsize=79.6m/83434560 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 24429ms, sequenceid=13699, compaction requested=true
2014-07-22 13:15:01,989 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:15:01,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 2000 blocking
2014-07-22 13:15:01,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-22 13:15:01,989 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 1.1g
2014-07-22 13:15:01,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:15:01,989 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:15:01,990 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:15:01,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:02,033 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69423 synced till here 69391
2014-07-22 13:15:02,526 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060097084 with entries=128, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060101990
2014-07-22 13:15:02,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059994083
2014-07-22 13:15:02,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059996125
2014-07-22 13:15:02,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059997723
2014-07-22 13:15:02,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406059999591
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060000905
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060002710
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060004751
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060006822
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060009456
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060011227
2014-07-22 13:15:02,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060012758
2014-07-22 13:15:02,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060014777
2014-07-22 13:15:02,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060016467
2014-07-22 13:15:02,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060018754
2014-07-22 13:15:04,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:04,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69555 synced till here 69510
2014-07-22 13:15:04,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060101990 with entries=132, filesize=102.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060104471
2014-07-22 13:15:05,265 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:15:07,489 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:07,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69686 synced till here 69655
2014-07-22 13:15:07,773 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060104471 with entries=131, filesize=91.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060107490
2014-07-22 13:15:11,253 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2922ms
GC pool 'ParNew' had collection(s): count=1 time=3361ms
2014-07-22 13:15:11,386 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23286 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,388 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,388 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23285 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,388 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,388 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23280 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,388 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,856 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23277 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,856 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,898 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23370 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,899 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23363 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23367 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23366 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:11,900 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:12,015 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23358 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23352 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23362 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23373 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,016 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23365 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,017 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,017 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23364 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,017 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,017 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23355 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,017 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23361 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23359 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23360 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,018 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,036 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69794 synced till here 69778
2014-07-22 13:15:12,093 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23353 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,093 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,123 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23350 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,123 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,139 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060107490 with entries=108, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060112011
2014-07-22 13:15:12,246 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23348 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,246 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,247 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23346 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23345 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23347 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23351 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23344 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,247 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23349 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23354 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23341 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23339 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,248 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,278 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23338 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,278 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,279 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23340 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,279 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,279 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,279 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23337 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,279 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,281 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,281 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,282 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,283 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,285 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,322 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23330 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,322 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,323 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23335 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,323 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,323 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23333 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:12,323 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:12,326 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,326 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,326 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,327 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,327 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,327 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,328 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,329 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:12,440 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,801 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,839 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,876 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,925 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,965 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:13,995 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,762 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1507ms
GC pool 'ParNew' had collection(s): count=1 time=1736ms
2014-07-22 13:15:15,769 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,803 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,815 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,848 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,894 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,930 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,962 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:15,994 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,229 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,247 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,262 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,279 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,281 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,282 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,282 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,284 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,285 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,295 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,327 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,327 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,328 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,328 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:15:17,329 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:15:17,329 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:15:17,330 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,330 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:15:17,330 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:17,359 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,394 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,427 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:17,441 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:17,457 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:18,801 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:18,840 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:18,855 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13748, memsize=382.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a2fd7ef62547441890b57039efc33ff3
2014-07-22 13:15:18,868 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a2fd7ef62547441890b57039efc33ff3 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a2fd7ef62547441890b57039efc33ff3
2014-07-22 13:15:18,877 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:18,877 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a2fd7ef62547441890b57039efc33ff3, entries=1393080, sequenceid=13748, filesize=99.3m
2014-07-22 13:15:18,877 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~770.2m/807563760, currentsize=412.1m/432119520 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 39523ms, sequenceid=13748, compaction requested=true
2014-07-22 13:15:18,878 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:15:18,878 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:15:18,878 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 772.3m
2014-07-22 13:15:18,878 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 13:15:18,878 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,878 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 13:15:18,879 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5040ms
2014-07-22 13:15:18,879 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:15:18,879 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,879 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:15:18,879 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5079ms
2014-07-22 13:15:18,879 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:15:18,879 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,880 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1423ms
2014-07-22 13:15:18,880 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,880 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6440ms
2014-07-22 13:15:18,880 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,880 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1454ms
2014-07-22 13:15:18,880 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,880 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1486ms
2014-07-22 13:15:18,880 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,889 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1530ms
2014-07-22 13:15:18,889 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,889 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6560ms
2014-07-22 13:15:18,890 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,890 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6563ms
2014-07-22 13:15:18,890 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,890 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6562ms
2014-07-22 13:15:18,890 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,901 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6575ms
2014-07-22 13:15:18,901 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,901 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6575ms
2014-07-22 13:15:18,901 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,902 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6576ms
2014-07-22 13:15:18,902 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,902 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1575ms
2014-07-22 13:15:18,902 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,902 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6576ms
2014-07-22 13:15:18,902 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,902 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6576ms
2014-07-22 13:15:18,902 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,913 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1618ms
2014-07-22 13:15:18,913 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,921 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6637ms
2014-07-22 13:15:18,921 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,921 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6638ms
2014-07-22 13:15:18,921 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,922 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6639ms
2014-07-22 13:15:18,922 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,922 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6641ms
2014-07-22 13:15:18,922 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,922 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6641ms
2014-07-22 13:15:18,922 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,922 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6643ms
2014-07-22 13:15:18,922 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,925 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:18,926 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,930 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1668ms
2014-07-22 13:15:18,930 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,930 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6684ms
2014-07-22 13:15:18,930 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,930 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1701ms
2014-07-22 13:15:18,930 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,941 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2947ms
2014-07-22 13:15:18,941 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,941 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2979ms
2014-07-22 13:15:18,941 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,941 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3011ms
2014-07-22 13:15:18,941 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,941 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3048ms
2014-07-22 13:15:18,942 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,942 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3094ms
2014-07-22 13:15:18,942 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,945 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3129ms
2014-07-22 13:15:18,945 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,946 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3143ms
2014-07-22 13:15:18,946 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,947 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3178ms
2014-07-22 13:15:18,948 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,948 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4953ms
2014-07-22 13:15:18,948 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,948 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4983ms
2014-07-22 13:15:18,948 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:18,957 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107495,"queuetimems":7880,"class":"HRegionServer","responsesize":17197,"method":"Multi"}
2014-07-22 13:15:18,957 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23334 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:18,957 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:18,981 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107494,"queuetimems":7980,"class":"HRegionServer","responsesize":16974,"method":"Multi"}
2014-07-22 13:15:18,981 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23336 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:18,981 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,065 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107723,"queuetimems":7679,"class":"HRegionServer","responsesize":17435,"method":"Multi"}
2014-07-22 13:15:19,066 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23398 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,066 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,237 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107729,"queuetimems":5827,"class":"HRegionServer","responsesize":17389,"method":"Multi"}
2014-07-22 13:15:19,237 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23393 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,237 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,406 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107723,"queuetimems":7645,"class":"HRegionServer","responsesize":16950,"method":"Multi"}
2014-07-22 13:15:19,406 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107522,"queuetimems":7629,"class":"HRegionServer","responsesize":17137,"method":"Multi"}
2014-07-22 13:15:19,407 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23395 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,407 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,407 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23324 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,407 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,407 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11910,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107497,"queuetimems":7734,"class":"HRegionServer","responsesize":16970,"method":"Multi"}
2014-07-22 13:15:19,408 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23327 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,408 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,409 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107730,"queuetimems":5809,"class":"HRegionServer","responsesize":17028,"method":"Multi"}
2014-07-22 13:15:19,410 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107723,"queuetimems":7597,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 13:15:19,410 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11883,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107527,"queuetimems":7595,"class":"HRegionServer","responsesize":17043,"method":"Multi"}
2014-07-22 13:15:19,411 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107722,"queuetimems":7754,"class":"HRegionServer","responsesize":16640,"method":"Multi"}
2014-07-22 13:15:19,411 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11889,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107522,"queuetimems":7656,"class":"HRegionServer","responsesize":16297,"method":"Multi"}
2014-07-22 13:15:19,410 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23392 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,415 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23325 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23322 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23391 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23323 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23394 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,416 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,411 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11905,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54755","starttimems":1406060107506,"queuetimems":7695,"class":"HRegionServer","responsesize":17215,"method":"Multi"}
2014-07-22 13:15:19,417 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23326 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54755: output error
2014-07-22 13:15:19,417 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:15:19,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:19,462 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:15:19,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69889 synced till here 69888
2014-07-22 13:15:19,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060112011 with entries=95, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060119430
2014-07-22 13:15:20,636 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:15:21,083 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:21,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 69987 synced till here 69966
2014-07-22 13:15:21,269 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060119430 with entries=98, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060121084
2014-07-22 13:15:22,916 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:22,939 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70083 synced till here 70074
2014-07-22 13:15:23,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060121084 with entries=96, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060122916
2014-07-22 13:15:26,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:26,810 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060122916 with entries=92, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060126765
2014-07-22 13:15:29,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:29,663 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70263 synced till here 70261
2014-07-22 13:15:29,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060126765 with entries=88, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060129594
2014-07-22 13:15:31,049 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:31,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70351 synced till here 70347
2014-07-22 13:15:31,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060129594 with entries=88, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060131050
2014-07-22 13:15:31,548 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,557 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,569 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,580 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,597 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,612 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,625 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,637 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,691 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,728 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:31,883 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,530 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,566 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,600 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,632 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,664 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,696 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,728 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,762 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,795 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,827 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,864 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,904 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,939 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:32,970 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:33,011 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:33,055 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:33,090 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:33,128 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:33,160 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,264 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,285 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,320 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,351 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,385 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,424 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:34,457 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:35,652 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:35,685 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:35,719 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:36,549 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,557 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:36,569 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:36,580 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:36,598 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,612 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,626 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,638 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,692 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:36,729 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:36,883 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:37,530 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:37,567 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:37,601 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:37,863 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5037ms
2014-07-22 13:15:37,863 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5231ms
2014-07-22 13:15:37,863 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5199ms
2014-07-22 13:15:37,864 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-07-22 13:15:37,864 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5136ms
2014-07-22 13:15:37,864 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:15:37,865 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5103ms
2014-07-22 13:15:37,865 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5070ms
2014-07-22 13:15:37,866 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14007, memsize=279.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7062e59fe3f44f5e8662fedce65af0f6
2014-07-22 13:15:37,889 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7062e59fe3f44f5e8662fedce65af0f6 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7062e59fe3f44f5e8662fedce65af0f6
2014-07-22 13:15:37,894 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:15:37,905 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:15:37,912 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7062e59fe3f44f5e8662fedce65af0f6, entries=1018920, sequenceid=14007, filesize=72.6m
2014-07-22 13:15:37,913 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~772.3m/809860560, currentsize=171.1m/179395920 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 19035ms, sequenceid=14007, compaction requested=true
2014-07-22 13:15:37,913 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:15:37,913 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5009ms
2014-07-22 13:15:37,913 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 13:15:37,914 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 951.8m
2014-07-22 13:15:37,914 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 13:15:37,913 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,914 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:15:37,914 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:15:37,914 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:15:37,914 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20ms
2014-07-22 13:15:37,914 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,916 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5121ms
2014-07-22 13:15:37,916 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,918 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5156ms
2014-07-22 13:15:37,918 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,918 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5054ms
2014-07-22 13:15:37,918 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,918 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5190ms
2014-07-22 13:15:37,918 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,918 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5222ms
2014-07-22 13:15:37,919 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,921 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5257ms
2014-07-22 13:15:37,921 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,921 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5289ms
2014-07-22 13:15:37,921 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,921 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5095ms
2014-07-22 13:15:37,922 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,922 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5322ms
2014-07-22 13:15:37,922 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,922 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5356ms
2014-07-22 13:15:37,922 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,922 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5392ms
2014-07-22 13:15:37,922 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,926 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6043ms
2014-07-22 13:15:37,926 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,926 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6198ms
2014-07-22 13:15:37,926 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,932 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6241ms
2014-07-22 13:15:37,932 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,933 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6296ms
2014-07-22 13:15:37,933 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,933 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6308ms
2014-07-22 13:15:37,934 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,936 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6325ms
2014-07-22 13:15:37,936 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,936 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6340ms
2014-07-22 13:15:37,936 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,945 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6365ms
2014-07-22 13:15:37,945 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,945 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6376ms
2014-07-22 13:15:37,945 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,949 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5010ms
2014-07-22 13:15:37,949 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,953 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6396ms
2014-07-22 13:15:37,953 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,953 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6405ms
2014-07-22 13:15:37,954 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,954 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2235ms
2014-07-22 13:15:37,954 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,954 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2269ms
2014-07-22 13:15:37,954 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,954 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2302ms
2014-07-22 13:15:37,954 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,954 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3497ms
2014-07-22 13:15:37,954 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,955 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3531ms
2014-07-22 13:15:37,955 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,960 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3575ms
2014-07-22 13:15:37,960 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,960 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3609ms
2014-07-22 13:15:37,960 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,960 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3640ms
2014-07-22 13:15:37,960 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,961 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3676ms
2014-07-22 13:15:37,961 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,964 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3701ms
2014-07-22 13:15:37,964 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,966 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4806ms
2014-07-22 13:15:37,966 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,967 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4838ms
2014-07-22 13:15:37,967 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,969 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4878ms
2014-07-22 13:15:37,969 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,971 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4916ms
2014-07-22 13:15:37,972 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,973 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4961ms
2014-07-22 13:15:37,973 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:37,973 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:15:37,973 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:15:38,443 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:38,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70461 synced till here 70450
2014-07-22 13:15:40,114 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1250ms
GC pool 'ParNew' had collection(s): count=1 time=1563ms
2014-07-22 13:15:40,139 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060131050 with entries=110, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060138443
2014-07-22 13:15:40,571 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:15:40,967 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:41,029 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70575 synced till here 70541
2014-07-22 13:15:41,137 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=13905, memsize=484.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/774016eb450743519718023c6e4d6ddd
2014-07-22 13:15:41,153 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/774016eb450743519718023c6e4d6ddd as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/774016eb450743519718023c6e4d6ddd
2014-07-22 13:15:41,165 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/774016eb450743519718023c6e4d6ddd, entries=1765120, sequenceid=13905, filesize=125.8m
2014-07-22 13:15:41,166 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.1g/1141931680, currentsize=332.6m/348766880 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 39177ms, sequenceid=13905, compaction requested=true
2014-07-22 13:15:41,166 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:15:41,167 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 33 store files, 0 compacting, 33 eligible, 2000 blocking
2014-07-22 13:15:41,167 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 33 files from compaction candidates
2014-07-22 13:15:41,167 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 605.1m
2014-07-22 13:15:41,167 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:15:41,167 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:15:41,167 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:15:41,233 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:15:41,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060138443 with entries=114, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060140967
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060020293
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060024506
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060027565
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060029711
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060031451
2014-07-22 13:15:41,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060034248
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060035943
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060037713
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060040437
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060042269
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060044661
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060046963
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060049256
2014-07-22 13:15:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060051395
2014-07-22 13:15:42,625 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:15:42,690 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060132598,"queuetimems":0,"class":"HRegionServer","responsesize":17203,"method":"Multi"}
2014-07-22 13:15:42,726 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060131636,"queuetimems":1,"class":"HRegionServer","responsesize":17043,"method":"Multi"}
2014-07-22 13:15:42,802 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10141,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060132661,"queuetimems":0,"class":"HRegionServer","responsesize":17113,"method":"Multi"}
2014-07-22 13:15:42,803 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11225,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060131578,"queuetimems":1,"class":"HRegionServer","responsesize":16858,"method":"Multi"}
2014-07-22 13:15:42,804 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10240,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060132564,"queuetimems":0,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 13:15:42,806 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10277,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060132528,"queuetimems":1,"class":"HRegionServer","responsesize":17242,"method":"Multi"}
2014-07-22 13:15:42,859 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:42,916 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11230,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060131686,"queuetimems":1,"class":"HRegionServer","responsesize":17177,"method":"Multi"}
2014-07-22 13:15:42,936 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11210,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060131726,"queuetimems":0,"class":"HRegionServer","responsesize":16779,"method":"Multi"}
2014-07-22 13:15:42,940 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060131881,"queuetimems":1,"class":"HRegionServer","responsesize":17010,"method":"Multi"}
2014-07-22 13:15:43,021 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060140967 with entries=115, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060142859
2014-07-22 13:15:43,264 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:15:44,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:44,740 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70777 synced till here 70771
2014-07-22 13:15:44,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060142859 with entries=87, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060144714
2014-07-22 13:15:45,858 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:45,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70864 synced till here 70863
2014-07-22 13:15:45,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060144714 with entries=87, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060145859
2014-07-22 13:15:46,718 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:47,889 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 70955 synced till here 70953
2014-07-22 13:15:47,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060145859 with entries=91, filesize=67.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060146718
2014-07-22 13:15:48,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:48,680 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71042 synced till here 71037
2014-07-22 13:15:49,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060146718 with entries=87, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060148651
2014-07-22 13:15:50,712 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:51,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71148 synced till here 71124
2014-07-22 13:15:52,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060148651 with entries=106, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060150712
2014-07-22 13:15:54,356 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1010ms
GC pool 'ParNew' had collection(s): count=1 time=1090ms
2014-07-22 13:15:54,361 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:54,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71260 synced till here 71243
2014-07-22 13:15:54,553 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060150712 with entries=112, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060154362
2014-07-22 13:15:54,907 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:15:56,581 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:56,604 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71364 synced till here 71350
2014-07-22 13:15:56,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060154362 with entries=104, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060156582
2014-07-22 13:15:58,493 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:15:58,828 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71517 synced till here 71468
2014-07-22 13:15:59,133 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060156582 with entries=153, filesize=112.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060158493
2014-07-22 13:15:59,247 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14168, memsize=143.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/25cae37a06724ddd84461bad4aed58b2
2014-07-22 13:15:59,280 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/25cae37a06724ddd84461bad4aed58b2 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/25cae37a06724ddd84461bad4aed58b2
2014-07-22 13:15:59,313 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/25cae37a06724ddd84461bad4aed58b2, entries=521140, sequenceid=14168, filesize=37.1m
2014-07-22 13:15:59,314 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~648.7m/680183680, currentsize=279.0m/292525680 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 18147ms, sequenceid=14168, compaction requested=true
2014-07-22 13:15:59,314 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:15:59,314 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 13:15:59,315 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 13:15:59,315 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:15:59,315 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 645.5m
2014-07-22 13:15:59,315 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:15:59,315 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:16:00,830 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:16:01,347 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:01,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71630 synced till here 71612
2014-07-22 13:16:01,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060158493 with entries=113, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060161348
2014-07-22 13:16:01,835 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:03,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:03,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71738 synced till here 71712
2014-07-22 13:16:04,178 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060161348 with entries=108, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060163696
2014-07-22 13:16:05,870 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1227ms
GC pool 'ParNew' had collection(s): count=1 time=1528ms
2014-07-22 13:16:06,663 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:06,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71858 synced till here 71845
2014-07-22 13:16:06,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060163696 with entries=120, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060166664
2014-07-22 13:16:08,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:08,867 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 71986 synced till here 71936
2014-07-22 13:16:10,233 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060166664 with entries=128, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060168609
2014-07-22 13:16:11,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:11,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72125 synced till here 72093
2014-07-22 13:16:11,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060168609 with entries=139, filesize=98.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060171243
2014-07-22 13:16:15,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:15,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72221 synced till here 72218
2014-07-22 13:16:15,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060171243 with entries=96, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060175256
2014-07-22 13:16:16,649 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,660 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,667 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,698 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,698 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,701 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,714 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,737 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,737 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,739 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,739 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,776 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,776 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,778 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,803 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,803 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,804 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,821 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,822 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,827 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:16,830 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,830 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:16:16,988 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060175256 with entries=77, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060176827
2014-07-22 13:16:17,236 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14125, memsize=346.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bb8be64ef6104f749f2bc5778f7ad76c
2014-07-22 13:16:17,276 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bb8be64ef6104f749f2bc5778f7ad76c as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bb8be64ef6104f749f2bc5778f7ad76c
2014-07-22 13:16:17,313 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bb8be64ef6104f749f2bc5778f7ad76c, entries=1260710, sequenceid=14125, filesize=89.8m
2014-07-22 13:16:17,313 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~951.8m/998058960, currentsize=598.0m/627092320 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 39400ms, sequenceid=14125, compaction requested=true
2014-07-22 13:16:17,314 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:16:17,314 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 13:16:17,314 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 484ms
2014-07-22 13:16:17,314 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,314 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 13:16:17,314 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 484ms
2014-07-22 13:16:17,314 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:16:17,314 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 764.2m
2014-07-22 13:16:17,314 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,314 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:16:17,317 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 496ms
2014-07-22 13:16:17,317 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,317 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 496ms
2014-07-22 13:16:17,318 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,318 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 515ms
2014-07-22 13:16:17,318 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,318 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 515ms
2014-07-22 13:16:17,318 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,318 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 515ms
2014-07-22 13:16:17,318 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,319 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 541ms
2014-07-22 13:16:17,319 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,319 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 543ms
2014-07-22 13:16:17,319 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,319 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 543ms
2014-07-22 13:16:17,319 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,317 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:16:17,323 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 13:16:17,323 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,323 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 584ms
2014-07-22 13:16:17,323 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,324 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 587ms
2014-07-22 13:16:17,324 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,324 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 587ms
2014-07-22 13:16:17,324 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,337 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 623ms
2014-07-22 13:16:17,337 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,339 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 638ms
2014-07-22 13:16:17,350 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,350 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 652ms
2014-07-22 13:16:17,350 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,350 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 652ms
2014-07-22 13:16:17,350 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,350 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 683ms
2014-07-22 13:16:17,350 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,352 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 692ms
2014-07-22 13:16:17,352 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,353 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 704ms
2014-07-22 13:16:17,353 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:16:17,531 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:16:18,592 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:18,758 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:18,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72390 synced till here 72389
2014-07-22 13:16:18,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060176827 with entries=92, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060178759
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060053547
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060055681
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060058162
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060059515
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060061494
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060063464
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060064663
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060065829
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060067883
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060069301
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060070676
2014-07-22 13:16:18,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060072187
2014-07-22 13:16:18,898 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:16:20,755 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:20,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060178759 with entries=85, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060180755
2014-07-22 13:16:20,802 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:16:22,024 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14336, memsize=248.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/ae442509900542f6a8141c3b28a5485b
2014-07-22 13:16:22,047 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/ae442509900542f6a8141c3b28a5485b as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/ae442509900542f6a8141c3b28a5485b
2014-07-22 13:16:22,070 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/ae442509900542f6a8141c3b28a5485b, entries=903470, sequenceid=14336, filesize=64.3m
2014-07-22 13:16:22,075 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~665.6m/697964240, currentsize=311.6m/326691600 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 22760ms, sequenceid=14336, compaction requested=true
2014-07-22 13:16:22,077 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:16:22,078 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 13:16:22,078 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 13:16:22,078 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:16:22,078 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:16:22,078 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:16:22,081 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 378.5m
2014-07-22 13:16:22,091 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:16:22,397 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:22,415 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72561 synced till here 72560
2014-07-22 13:16:22,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060180755 with entries=86, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060182398
2014-07-22 13:16:23,003 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:24,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:24,035 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060182398 with entries=84, filesize=61.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060184001
2014-07-22 13:16:25,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:26,196 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060184001 with entries=103, filesize=76.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060185798
2014-07-22 13:16:28,043 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14515, memsize=88.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/e37a493eda634ca8918adf1439d1b27b
2014-07-22 13:16:28,067 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/e37a493eda634ca8918adf1439d1b27b as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/e37a493eda634ca8918adf1439d1b27b
2014-07-22 13:16:28,090 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/e37a493eda634ca8918adf1439d1b27b, entries=320610, sequenceid=14515, filesize=22.9m
2014-07-22 13:16:28,090 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~378.5m/396854640, currentsize=25.8m/27035360 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 6010ms, sequenceid=14515, compaction requested=true
2014-07-22 13:16:28,091 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 19 store files, 0 compacting, 19 eligible, 2000 blocking
2014-07-22 13:16:28,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 19 files from compaction candidates
2014-07-22 13:16:28,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:16:28,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:16:28,092 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:16:28,092 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:16:28,092 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 677.3m
2014-07-22 13:16:28,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:28,195 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72836 synced till here 72830
2014-07-22 13:16:28,269 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060185798 with entries=88, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060188149
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060074921
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060077916
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060080084
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060082993
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060087240
2014-07-22 13:16:28,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060089551
2014-07-22 13:16:28,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060093834
2014-07-22 13:16:28,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060097084
2014-07-22 13:16:28,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060101990
2014-07-22 13:16:28,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060104471
2014-07-22 13:16:28,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060107490
2014-07-22 13:16:29,780 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:29,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:30,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 72942 synced till here 72920
2014-07-22 13:16:30,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060188149 with entries=106, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060189996
2014-07-22 13:16:31,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:31,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73041 synced till here 73037
2014-07-22 13:16:32,010 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060189996 with entries=99, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060191948
2014-07-22 13:16:33,351 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:33,369 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73130 synced till here 73128
2014-07-22 13:16:33,388 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060191948 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060193351
2014-07-22 13:16:33,424 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14504, memsize=248.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/84932db26432490486a7be939e46c3c1
2014-07-22 13:16:33,442 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/84932db26432490486a7be939e46c3c1 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/84932db26432490486a7be939e46c3c1
2014-07-22 13:16:33,452 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/84932db26432490486a7be939e46c3c1, entries=905010, sequenceid=14504, filesize=64.5m
2014-07-22 13:16:33,453 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~764.2m/801364480, currentsize=251.6m/263791680 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 16139ms, sequenceid=14504, compaction requested=true
2014-07-22 13:16:33,453 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:16:33,453 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 34 store files, 0 compacting, 34 eligible, 2000 blocking
2014-07-22 13:16:33,453 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 34 files from compaction candidates
2014-07-22 13:16:33,453 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 847.0m
2014-07-22 13:16:33,454 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:16:33,454 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:16:33,454 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:16:33,561 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:16:34,677 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:34,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:34,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73217 synced till here 73213
2014-07-22 13:16:34,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060193351 with entries=87, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060194901
2014-07-22 13:16:34,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060112011
2014-07-22 13:16:34,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060119430
2014-07-22 13:16:34,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060121084
2014-07-22 13:16:34,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060122916
2014-07-22 13:16:34,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060126765
2014-07-22 13:16:34,971 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060129594
2014-07-22 13:16:36,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:36,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73305 synced till here 73304
2014-07-22 13:16:36,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060194901 with entries=88, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060196851
2014-07-22 13:16:37,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:38,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73402 synced till here 73399
2014-07-22 13:16:38,497 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060196851 with entries=97, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060197877
2014-07-22 13:16:39,446 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:39,478 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73489 synced till here 73488
2014-07-22 13:16:39,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060197877 with entries=87, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060199446
2014-07-22 13:16:40,757 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:42,171 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73612 synced till here 73609
2014-07-22 13:16:42,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060199446 with entries=123, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060200758
2014-07-22 13:16:43,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:43,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73719 synced till here 73717
2014-07-22 13:16:43,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060200758 with entries=107, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060203057
2014-07-22 13:16:44,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:44,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73811 synced till here 73804
2014-07-22 13:16:44,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060203057 with entries=92, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060204631
2014-07-22 13:16:46,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:46,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 73909 synced till here 73892
2014-07-22 13:16:47,552 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1188ms
GC pool 'ParNew' had collection(s): count=1 time=1309ms
2014-07-22 13:16:47,564 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060204631 with entries=98, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060206112
2014-07-22 13:16:48,266 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14610, memsize=284.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/677982edb84848c484e6ab5feba87bc9
2014-07-22 13:16:48,290 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/677982edb84848c484e6ab5feba87bc9 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/677982edb84848c484e6ab5feba87bc9
2014-07-22 13:16:48,320 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/677982edb84848c484e6ab5feba87bc9, entries=1036210, sequenceid=14610, filesize=73.8m
2014-07-22 13:16:48,321 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~683.5m/716698720, currentsize=353.3m/370459200 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 20229ms, sequenceid=14610, compaction requested=true
2014-07-22 13:16:48,322 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:16:48,323 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 13:16:48,323 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 746.6m
2014-07-22 13:16:48,323 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 13:16:48,323 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:16:48,323 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:16:48,323 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:16:48,415 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:48,417 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:16:48,477 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74016 synced till here 73993
2014-07-22 13:16:49,557 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1004ms
GC pool 'ParNew' had collection(s): count=1 time=1028ms
2014-07-22 13:16:49,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060206112 with entries=107, filesize=81.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060208416
2014-07-22 13:16:49,664 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060140967
2014-07-22 13:16:50,316 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:16:50,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:51,600 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74114 synced till here 74095
2014-07-22 13:16:51,602 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1044ms
GC pool 'ParNew' had collection(s): count=1 time=1089ms
2014-07-22 13:16:51,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060208416 with entries=98, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060210420
2014-07-22 13:16:52,438 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:52,459 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74210 synced till here 74190
2014-07-22 13:16:53,946 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1343ms
GC pool 'ParNew' had collection(s): count=1 time=1373ms
2014-07-22 13:16:53,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060210420 with entries=96, filesize=79.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060212439
2014-07-22 13:16:55,992 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1045ms
GC pool 'ParNew' had collection(s): count=1 time=1048ms
2014-07-22 13:16:56,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:56,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74328 synced till here 74320
2014-07-22 13:16:56,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060212439 with entries=118, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060216109
2014-07-22 13:16:58,173 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1179ms
GC pool 'ParNew' had collection(s): count=1 time=1085ms
2014-07-22 13:16:58,308 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:16:58,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74453 synced till here 74426
2014-07-22 13:16:58,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060216109 with entries=125, filesize=91.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060218309
2014-07-22 13:17:00,798 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1123ms
GC pool 'ParNew' had collection(s): count=1 time=1405ms
2014-07-22 13:17:00,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:00,948 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74572 synced till here 74549
2014-07-22 13:17:01,076 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060218309 with entries=119, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060220867
2014-07-22 13:17:01,920 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:02,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74681 synced till here 74669
2014-07-22 13:17:02,758 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14672, memsize=325.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/764855789e404d649887ce1b84f19e69
2014-07-22 13:17:02,768 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/764855789e404d649887ce1b84f19e69 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/764855789e404d649887ce1b84f19e69
2014-07-22 13:17:02,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060220867 with entries=109, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060221920
2014-07-22 13:17:02,780 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/764855789e404d649887ce1b84f19e69, entries=1184340, sequenceid=14672, filesize=84.3m
2014-07-22 13:17:02,794 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~848.6m/889868560, currentsize=484.8m/508390000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 29341ms, sequenceid=14672, compaction requested=true
2014-07-22 13:17:02,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 13:17:02,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 13:17:02,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:17:02,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:17:02,795 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:17:02,795 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:17:02,795 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 741.2m
2014-07-22 13:17:03,020 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:17:03,380 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:17:03,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:04,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74792 synced till here 74786
2014-07-22 13:17:04,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060221920 with entries=111, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060223885
2014-07-22 13:17:04,517 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060131050
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060138443
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060142859
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060144714
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060145859
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060146718
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060148651
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060150712
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060154362
2014-07-22 13:17:04,518 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060156582
2014-07-22 13:17:05,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:05,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74879 synced till here 74877
2014-07-22 13:17:05,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060223885 with entries=87, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060225302
2014-07-22 13:17:06,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:07,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 74979 synced till here 74958
2014-07-22 13:17:08,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060225302 with entries=100, filesize=76.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060227166
2014-07-22 13:17:09,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:09,380 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75091 synced till here 75071
2014-07-22 13:17:10,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060227166 with entries=112, filesize=85.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060229337
2014-07-22 13:17:11,341 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:12,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75204 synced till here 75183
2014-07-22 13:17:12,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060229337 with entries=113, filesize=83.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060231341
2014-07-22 13:17:14,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:14,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75312 synced till here 75281
2014-07-22 13:17:14,445 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060231341 with entries=108, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234156
2014-07-22 13:17:14,801 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:17:14,989 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:15,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75423 synced till here 75402
2014-07-22 13:17:15,974 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234156 with entries=111, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234989
2014-07-22 13:17:16,591 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,593 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,598 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,616 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,653 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,704 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,781 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,781 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,781 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,782 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,782 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,782 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,782 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,782 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,783 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:16,783 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:18,198 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,198 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,199 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,199 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,199 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,200 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,200 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,200 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,200 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,202 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,202 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,203 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:18,215 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234989 with entries=94, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060238195
2014-07-22 13:17:18,979 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,023 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,063 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,093 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,128 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,166 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,204 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,239 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,276 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,313 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,349 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,388 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,426 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,463 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,502 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,540 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,577 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,616 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,655 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,697 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,735 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:19,777 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:21,591 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:21,593 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:21,598 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:21,616 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:21,653 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:21,781 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5077ms
2014-07-22 13:17:21,782 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5071ms
2014-07-22 13:17:21,783 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5076ms
2014-07-22 13:17:21,783 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5076ms
2014-07-22 13:17:21,783 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5076ms
2014-07-22 13:17:21,784 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5074ms
2014-07-22 13:17:21,784 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5075ms
2014-07-22 13:17:21,784 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5075ms
2014-07-22 13:17:21,784 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5075ms
2014-07-22 13:17:21,784 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5077ms
2014-07-22 13:17:21,786 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5078ms
2014-07-22 13:17:21,839 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14826, memsize=425.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0648f1e900b645f8a191685dd7c46957
2014-07-22 13:17:21,851 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0648f1e900b645f8a191685dd7c46957 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0648f1e900b645f8a191685dd7c46957
2014-07-22 13:17:21,860 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0648f1e900b645f8a191685dd7c46957, entries=1550190, sequenceid=14826, filesize=110.5m
2014-07-22 13:17:21,861 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~759.1m/796022560, currentsize=469.2m/491977120 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 33538ms, sequenceid=14826, compaction requested=true
2014-07-22 13:17:21,861 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:17:21,861 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 13:17:21,861 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 13:17:21,861 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-22 13:17:21,862 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 828.7m
2014-07-22 13:17:21,862 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:17:21,862 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,862 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:17:21,862 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5155ms
2014-07-22 13:17:21,862 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,862 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:17:21,863 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-22 13:17:21,863 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,863 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-22 13:17:21,863 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,863 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-22 13:17:21,863 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,863 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-07-22 13:17:21,863 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,865 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5158ms
2014-07-22 13:17:21,865 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,865 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5158ms
2014-07-22 13:17:21,866 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,866 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-07-22 13:17:21,866 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,868 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5158ms
2014-07-22 13:17:21,868 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,868 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5164ms
2014-07-22 13:17:21,868 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,868 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5216ms
2014-07-22 13:17:21,868 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,878 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5262ms
2014-07-22 13:17:21,878 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,878 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5280ms
2014-07-22 13:17:21,878 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,879 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5286ms
2014-07-22 13:17:21,879 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,881 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5290ms
2014-07-22 13:17:21,881 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,881 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2104ms
2014-07-22 13:17:21,881 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,881 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2146ms
2014-07-22 13:17:21,881 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,881 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2184ms
2014-07-22 13:17:21,882 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,882 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2227ms
2014-07-22 13:17:21,882 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,893 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2277ms
2014-07-22 13:17:21,893 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,893 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2316ms
2014-07-22 13:17:21,893 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,894 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2354ms
2014-07-22 13:17:21,894 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,894 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2392ms
2014-07-22 13:17:21,894 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,894 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2431ms
2014-07-22 13:17:21,895 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,897 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2471ms
2014-07-22 13:17:21,897 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,929 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2541ms
2014-07-22 13:17:21,930 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,930 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2581ms
2014-07-22 13:17:21,930 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,930 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2617ms
2014-07-22 13:17:21,930 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,940 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2664ms
2014-07-22 13:17:21,940 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,940 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2701ms
2014-07-22 13:17:21,940 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,941 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2737ms
2014-07-22 13:17:21,941 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,949 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2783ms
2014-07-22 13:17:21,949 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,950 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2821ms
2014-07-22 13:17:21,950 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,953 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2860ms
2014-07-22 13:17:21,953 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,954 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2892ms
2014-07-22 13:17:21,955 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,955 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2932ms
2014-07-22 13:17:21,955 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,961 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2982ms
2014-07-22 13:17:21,961 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,961 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3758ms
2014-07-22 13:17:21,962 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,966 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3763ms
2014-07-22 13:17:21,966 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,966 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3764ms
2014-07-22 13:17:21,967 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,977 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3777ms
2014-07-22 13:17:21,977 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,978 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3778ms
2014-07-22 13:17:21,978 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,978 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3778ms
2014-07-22 13:17:21,978 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,978 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3778ms
2014-07-22 13:17:21,978 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,989 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3790ms
2014-07-22 13:17:21,989 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,998 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3798ms
2014-07-22 13:17:21,998 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,998 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3800ms
2014-07-22 13:17:21,998 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:21,999 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3801ms
2014-07-22 13:17:21,999 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:22,005 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3807ms
2014-07-22 13:17:22,005 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:23,273 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1079ms
GC pool 'ParNew' had collection(s): count=1 time=1113ms
2014-07-22 13:17:23,280 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:17:23,787 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:23,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75622 synced till here 75617
2014-07-22 13:17:23,963 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060238195 with entries=105, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060243788
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060158493
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060161348
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060163696
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060166664
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060168609
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060171243
2014-07-22 13:17:23,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060175256
2014-07-22 13:17:24,050 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:17:25,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:25,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75745 synced till here 75737
2014-07-22 13:17:26,048 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060243788 with entries=123, filesize=98.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060245768
2014-07-22 13:17:27,297 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:27,481 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75881 synced till here 75872
2014-07-22 13:17:27,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060245768 with entries=136, filesize=88.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060247297
2014-07-22 13:17:28,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:29,011 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 75973 synced till here 75965
2014-07-22 13:17:29,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060247297 with entries=92, filesize=70.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060248985
2014-07-22 13:17:30,654 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:30,692 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76066 synced till here 76065
2014-07-22 13:17:30,718 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060248985 with entries=93, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060250655
2014-07-22 13:17:30,731 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,755 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,755 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,762 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,793 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,828 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:30,908 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:31,030 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:31,054 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=14981, memsize=412.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/b32c57d700f545bdbf9d543088b6d2c1
2014-07-22 13:17:31,070 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/b32c57d700f545bdbf9d543088b6d2c1 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/b32c57d700f545bdbf9d543088b6d2c1
2014-07-22 13:17:31,073 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:31,091 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/b32c57d700f545bdbf9d543088b6d2c1, entries=1501560, sequenceid=14981, filesize=107.0m
2014-07-22 13:17:31,091 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~741.2m/777239360, currentsize=423.5m/444048800 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 28296ms, sequenceid=14981, compaction requested=true
2014-07-22 13:17:31,092 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:17:31,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 35 store files, 0 compacting, 35 eligible, 2000 blocking
2014-07-22 13:17:31,092 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 19ms
2014-07-22 13:17:31,092 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 35 files from compaction candidates
2014-07-22 13:17:31,092 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 914.6m
2014-07-22 13:17:31,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:17:31,092 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 62ms
2014-07-22 13:17:31,092 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,092 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:17:31,092 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:17:31,093 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-07-22 13:17:31,093 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,093 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 265ms
2014-07-22 13:17:31,097 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,097 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 304ms
2014-07-22 13:17:31,097 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,097 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 335ms
2014-07-22 13:17:31,097 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,097 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 342ms
2014-07-22 13:17:31,098 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,101 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 346ms
2014-07-22 13:17:31,101 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,106 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 374ms
2014-07-22 13:17:31,106 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:31,174 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:17:31,913 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:31,986 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:17:32,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76158 synced till here 76154
2014-07-22 13:17:32,455 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060250655 with entries=92, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060251914
2014-07-22 13:17:32,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060176827
2014-07-22 13:17:32,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060178759
2014-07-22 13:17:32,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:17:33,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:33,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060251914 with entries=84, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060253426
2014-07-22 13:17:33,470 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:17:34,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:35,120 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76362 synced till here 76353
2014-07-22 13:17:36,353 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1128ms
GC pool 'ParNew' had collection(s): count=1 time=1178ms
2014-07-22 13:17:36,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060253426 with entries=120, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060254775
2014-07-22 13:17:36,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:17:37,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:37,123 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76464 synced till here 76450
2014-07-22 13:17:38,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060254775 with entries=102, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060257072
2014-07-22 13:17:38,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:17:38,848 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:38,884 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76568 synced till here 76564
2014-07-22 13:17:38,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060257072 with entries=104, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060258849
2014-07-22 13:17:38,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:17:39,940 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,943 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,947 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,950 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,952 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,969 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:39,982 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,007 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,021 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,021 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,060 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,110 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,142 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,174 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,205 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,235 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,267 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,785 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,937 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:40,975 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:41,167 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:41,299 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:41,410 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:41,506 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:41,815 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,031 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,076 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,108 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,142 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,182 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,219 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,259 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,302 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,350 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,391 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:42,426 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,510 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,528 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,544 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,578 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,611 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,645 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:43,675 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,023 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,056 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,087 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,120 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,154 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,186 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,221 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:44,940 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:44,944 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:44,947 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:44,951 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:44,952 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:44,969 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:44,982 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:45,007 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,021 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,021 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,060 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:45,111 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:45,143 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,174 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:17:45,205 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,236 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,268 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,786 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:17:45,809 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15147, memsize=336.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/29125e394bc94285843c12fd84efeb32
2014-07-22 13:17:45,836 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/29125e394bc94285843c12fd84efeb32 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/29125e394bc94285843c12fd84efeb32
2014-07-22 13:17:45,850 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/29125e394bc94285843c12fd84efeb32, entries=1224850, sequenceid=15147, filesize=87.3m
2014-07-22 13:17:45,851 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~828.7m/868907680, currentsize=350.3m/367322880 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 23989ms, sequenceid=15147, compaction requested=true
2014-07-22 13:17:45,852 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:17:45,852 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 13:17:45,852 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5067ms
2014-07-22 13:17:45,852 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 13:17:45,852 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,852 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:17:45,852 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5585ms
2014-07-22 13:17:45,852 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,852 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 377.9m
2014-07-22 13:17:45,852 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5617ms
2014-07-22 13:17:45,852 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,853 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-07-22 13:17:45,853 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,861 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5688ms
2014-07-22 13:17:45,861 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,861 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5719ms
2014-07-22 13:17:45,861 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,862 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5752ms
2014-07-22 13:17:45,862 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,862 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5803ms
2014-07-22 13:17:45,862 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,862 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5841ms
2014-07-22 13:17:45,862 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,866 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5845ms
2014-07-22 13:17:45,866 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,867 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5859ms
2014-07-22 13:17:45,867 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,852 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:17:45,868 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:17:45,869 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-07-22 13:17:45,869 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,873 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5904ms
2014-07-22 13:17:45,873 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,874 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5921ms
2014-07-22 13:17:45,874 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,874 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5924ms
2014-07-22 13:17:45,874 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,874 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5927ms
2014-07-22 13:17:45,874 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,875 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5931ms
2014-07-22 13:17:45,875 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,897 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5956ms
2014-07-22 13:17:45,897 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,898 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1677ms
2014-07-22 13:17:45,898 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,899 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1713ms
2014-07-22 13:17:45,899 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,899 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1745ms
2014-07-22 13:17:45,899 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,902 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1782ms
2014-07-22 13:17:45,902 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,902 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1815ms
2014-07-22 13:17:45,902 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,906 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1851ms
2014-07-22 13:17:45,906 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,906 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1883ms
2014-07-22 13:17:45,907 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,908 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2232ms
2014-07-22 13:17:45,908 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,908 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2263ms
2014-07-22 13:17:45,908 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,909 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2297ms
2014-07-22 13:17:45,909 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,910 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2331ms
2014-07-22 13:17:45,910 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,911 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2367ms
2014-07-22 13:17:45,911 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,918 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2389ms
2014-07-22 13:17:45,918 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,919 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2408ms
2014-07-22 13:17:45,919 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,920 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3494ms
2014-07-22 13:17:45,920 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,921 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3530ms
2014-07-22 13:17:45,921 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,922 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3572ms
2014-07-22 13:17:45,922 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,923 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3621ms
2014-07-22 13:17:45,923 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,927 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3668ms
2014-07-22 13:17:45,927 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,927 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3709ms
2014-07-22 13:17:45,927 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,927 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3745ms
2014-07-22 13:17:45,928 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,928 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3786ms
2014-07-22 13:17:45,928 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,929 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3820ms
2014-07-22 13:17:45,929 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,930 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3853ms
2014-07-22 13:17:45,930 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,931 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3899ms
2014-07-22 13:17:45,931 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,933 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4118ms
2014-07-22 13:17:45,933 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,934 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4427ms
2014-07-22 13:17:45,934 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,934 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4524ms
2014-07-22 13:17:45,935 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,935 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4636ms
2014-07-22 13:17:45,935 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,935 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4768ms
2014-07-22 13:17:45,935 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,936 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4960ms
2014-07-22 13:17:45,936 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:45,937 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4999ms
2014-07-22 13:17:45,937 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:46,049 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:17:46,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:46,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76690 synced till here 76664
2014-07-22 13:17:46,373 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:17:47,698 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1194ms
GC pool 'ParNew' had collection(s): count=1 time=1223ms
2014-07-22 13:17:47,743 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060258849 with entries=122, filesize=91.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060266215
2014-07-22 13:17:48,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:49,520 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76807 synced till here 76776
2014-07-22 13:17:49,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060266215 with entries=117, filesize=96.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060268335
2014-07-22 13:17:49,982 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060259950,"queuetimems":1,"class":"HRegionServer","responsesize":17779,"method":"Multi"}
2014-07-22 13:17:50,538 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:51,350 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 76936 synced till here 76894
2014-07-22 13:17:51,539 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10131,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060261408,"queuetimems":0,"class":"HRegionServer","responsesize":16895,"method":"Multi"}
2014-07-22 13:17:51,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060268335 with entries=129, filesize=85.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060270539
2014-07-22 13:17:51,697 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060261504,"queuetimems":1,"class":"HRegionServer","responsesize":16768,"method":"Multi"}
2014-07-22 13:17:51,722 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10787,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060260935,"queuetimems":0,"class":"HRegionServer","responsesize":16945,"method":"Multi"}
2014-07-22 13:17:51,722 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060261165,"queuetimems":0,"class":"HRegionServer","responsesize":17002,"method":"Multi"}
2014-07-22 13:17:53,397 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:53,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77054 synced till here 77013
2014-07-22 13:17:53,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060270539 with entries=118, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060273397
2014-07-22 13:17:55,934 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1083ms
GC pool 'ParNew' had collection(s): count=1 time=1242ms
2014-07-22 13:17:56,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:56,542 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77187 synced till here 77173
2014-07-22 13:17:56,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060273397 with entries=133, filesize=95.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060276415
2014-07-22 13:17:57,218 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,220 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,221 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,222 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,223 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,223 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,227 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,227 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,230 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,234 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,234 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,237 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,238 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,243 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,243 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,244 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,246 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,341 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,388 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,396 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,397 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,397 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,397 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,398 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,398 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,399 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,399 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,400 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,400 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,400 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,401 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,426 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,447 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15261, memsize=319.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5e563f08f2cd49c3a72dfd70e2f21937
2014-07-22 13:17:57,464 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,465 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,465 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5e563f08f2cd49c3a72dfd70e2f21937 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5e563f08f2cd49c3a72dfd70e2f21937
2014-07-22 13:17:57,466 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,466 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,469 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:57,470 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,384 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,384 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,385 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,385 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,398 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:17:58,400 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5e563f08f2cd49c3a72dfd70e2f21937, entries=1164320, sequenceid=15261, filesize=83.0m
2014-07-22 13:17:58,400 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~914.6m/958985040, currentsize=396.2m/415433680 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 27308ms, sequenceid=15261, compaction requested=true
2014-07-22 13:17:58,401 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:17:58,401 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3ms
2014-07-22 13:17:58,401 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,401 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 13:17:58,401 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 1016.7m
2014-07-22 13:17:58,401 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-07-22 13:17:58,402 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,402 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 13:17:58,402 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-07-22 13:17:58,402 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,402 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-07-22 13:17:58,402 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,402 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-07-22 13:17:58,402 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,405 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 935ms
2014-07-22 13:17:58,405 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,405 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 936ms
2014-07-22 13:17:58,405 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,406 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 940ms
2014-07-22 13:17:58,406 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,406 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 940ms
2014-07-22 13:17:58,406 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,406 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 941ms
2014-07-22 13:17:58,406 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,406 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 942ms
2014-07-22 13:17:58,406 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,407 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 981ms
2014-07-22 13:17:58,407 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,407 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1006ms
2014-07-22 13:17:58,407 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,407 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1007ms
2014-07-22 13:17:58,407 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,402 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:17:58,409 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:17:58,410 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:17:58,409 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1009ms
2014-07-22 13:17:58,417 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,433 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:58,440 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1041ms
2014-07-22 13:17:58,440 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,440 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1041ms
2014-07-22 13:17:58,440 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,440 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1041ms
2014-07-22 13:17:58,440 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,441 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1043ms
2014-07-22 13:17:58,441 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,441 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1044ms
2014-07-22 13:17:58,441 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,441 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1044ms
2014-07-22 13:17:58,441 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,445 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1048ms
2014-07-22 13:17:58,445 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,445 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1048ms
2014-07-22 13:17:58,446 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,446 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1050ms
2014-07-22 13:17:58,446 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,446 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1058ms
2014-07-22 13:17:58,446 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,455 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1114ms
2014-07-22 13:17:58,455 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,457 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1211ms
2014-07-22 13:17:58,457 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,457 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1213ms
2014-07-22 13:17:58,457 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,458 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1224ms
2014-07-22 13:17:58,458 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,460 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1217ms
2014-07-22 13:17:58,460 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,461 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1223ms
2014-07-22 13:17:58,461 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,462 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1225ms
2014-07-22 13:17:58,462 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,462 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1228ms
2014-07-22 13:17:58,463 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,463 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1230ms
2014-07-22 13:17:58,463 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,469 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1239ms
2014-07-22 13:17:58,469 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,469 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1242ms
2014-07-22 13:17:58,470 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,470 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1243ms
2014-07-22 13:17:58,470 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,473 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1250ms
2014-07-22 13:17:58,474 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,474 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1251ms
2014-07-22 13:17:58,474 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,474 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1252ms
2014-07-22 13:17:58,474 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,474 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1254ms
2014-07-22 13:17:58,474 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,474 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1254ms
2014-07-22 13:17:58,474 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,475 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1257ms
2014-07-22 13:17:58,475 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:17:58,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77285 synced till here 77277
2014-07-22 13:17:58,644 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060276415 with entries=98, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060278434
2014-07-22 13:17:58,810 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:17:59,452 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:17:59,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77392 synced till here 77370
2014-07-22 13:18:01,030 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1145ms
GC pool 'ParNew' had collection(s): count=1 time=1462ms
2014-07-22 13:18:01,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060278434 with entries=107, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060279453
2014-07-22 13:18:01,299 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:01,851 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:01,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77493 synced till here 77477
2014-07-22 13:18:02,537 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060279453 with entries=101, filesize=70.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060281852
2014-07-22 13:18:04,080 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:04,106 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77584 synced till here 77579
2014-07-22 13:18:04,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060281852 with entries=91, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060284081
2014-07-22 13:18:04,557 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15331, memsize=163.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9389158538a34ebcba7944c6fd4d7a9e
2014-07-22 13:18:04,581 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/9389158538a34ebcba7944c6fd4d7a9e as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9389158538a34ebcba7944c6fd4d7a9e
2014-07-22 13:18:04,596 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/9389158538a34ebcba7944c6fd4d7a9e, entries=596440, sequenceid=15331, filesize=42.5m
2014-07-22 13:18:04,597 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~377.9m/396296400, currentsize=91.1m/95536640 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 18745ms, sequenceid=15331, compaction requested=true
2014-07-22 13:18:04,597 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:18:04,597 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 20 store files, 0 compacting, 20 eligible, 2000 blocking
2014-07-22 13:18:04,598 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 20 files from compaction candidates
2014-07-22 13:18:04,598 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 899.2m
2014-07-22 13:18:04,598 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:18:04,598 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:18:04,598 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:18:05,803 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:05,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77672 synced till here 77670
2014-07-22 13:18:05,906 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060284081 with entries=88, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060285804
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060180755
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060182398
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060184001
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060185798
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060188149
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060189996
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060191948
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060193351
2014-07-22 13:18:05,906 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060194901
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060196851
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060197877
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060199446
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060200758
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060203057
2014-07-22 13:18:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060204631
2014-07-22 13:18:05,981 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:07,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:07,628 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77761 synced till here 77754
2014-07-22 13:18:07,724 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060285804 with entries=89, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060287609
2014-07-22 13:18:08,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:09,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77849 synced till here 77844
2014-07-22 13:18:09,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060287609 with entries=88, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060288630
2014-07-22 13:18:10,345 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:10,395 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 77939 synced till here 77934
2014-07-22 13:18:10,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060288630 with entries=90, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060290345
2014-07-22 13:18:11,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:11,890 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78024 synced till here 78021
2014-07-22 13:18:11,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060290345 with entries=85, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060291876
2014-07-22 13:18:13,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:13,590 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78117 synced till here 78108
2014-07-22 13:18:13,635 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060291876 with entries=93, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060293542
2014-07-22 13:18:15,349 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:15,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78203 synced till here 78202
2014-07-22 13:18:15,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060293542 with entries=86, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060295349
2014-07-22 13:18:15,436 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:15,838 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:15,892 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:15,944 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:15,996 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,052 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,091 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,138 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,178 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,229 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,276 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,317 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,383 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,422 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,478 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,579 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:16,647 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:17,760 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:17,809 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:17,883 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:17,949 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,008 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,057 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,091 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,124 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,156 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,188 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,245 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,301 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,335 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,388 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:18,541 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:20,942 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-22 13:18:20,942 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-22 13:18:20,942 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5104ms
2014-07-22 13:18:20,944 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:20,950 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:20,975 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:20,991 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:20,997 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,023 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,053 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:21,056 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,089 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,091 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:21,121 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,139 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,153 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,179 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,186 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,217 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,230 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:21,253 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,277 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,286 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,317 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,318 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,351 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,383 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,383 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,417 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,423 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,448 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,479 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:21,482 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:21,580 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:21,648 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:18:22,708 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15479, memsize=275.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/770451787d5348fdbffdc429826a5425
2014-07-22 13:18:22,725 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/770451787d5348fdbffdc429826a5425 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/770451787d5348fdbffdc429826a5425
2014-07-22 13:18:22,738 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/770451787d5348fdbffdc429826a5425, entries=1003050, sequenceid=15479, filesize=71.5m
2014-07-22 13:18:22,739 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1016.7m/1066083840, currentsize=302.6m/317343840 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 24338ms, sequenceid=15479, compaction requested=true
2014-07-22 13:18:22,739 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:18:22,739 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 13:18:22,739 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6092ms
2014-07-22 13:18:22,739 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 13:18:22,739 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,739 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:18:22,740 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6161ms
2014-07-22 13:18:22,740 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,739 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 844.8m
2014-07-22 13:18:22,740 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:18:22,740 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:18:22,741 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1259ms
2014-07-22 13:18:22,741 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,743 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6265ms
2014-07-22 13:18:22,743 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,743 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1295ms
2014-07-22 13:18:22,743 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,745 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6323ms
2014-07-22 13:18:22,745 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,745 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1329ms
2014-07-22 13:18:22,745 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,746 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1363ms
2014-07-22 13:18:22,746 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,746 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6364ms
2014-07-22 13:18:22,746 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,747 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1396ms
2014-07-22 13:18:22,747 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,753 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1435ms
2014-07-22 13:18:22,753 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,754 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6438ms
2014-07-22 13:18:22,754 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,754 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1468ms
2014-07-22 13:18:22,754 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,754 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6478ms
2014-07-22 13:18:22,754 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,757 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1504ms
2014-07-22 13:18:22,757 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,757 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6528ms
2014-07-22 13:18:22,758 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,758 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1541ms
2014-07-22 13:18:22,758 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,758 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1573ms
2014-07-22 13:18:22,758 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,758 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6580ms
2014-07-22 13:18:22,758 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,760 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:18:22,760 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,765 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1612ms
2014-07-22 13:18:22,765 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,765 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6627ms
2014-07-22 13:18:22,765 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,765 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1644ms
2014-07-22 13:18:22,765 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,766 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6674ms
2014-07-22 13:18:22,766 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,766 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1677ms
2014-07-22 13:18:22,766 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,766 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1710ms
2014-07-22 13:18:22,766 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,767 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6715ms
2014-07-22 13:18:22,767 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,767 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1744ms
2014-07-22 13:18:22,767 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,767 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6771ms
2014-07-22 13:18:22,767 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,767 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1776ms
2014-07-22 13:18:22,767 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,767 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1792ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,768 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1818ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,768 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6824ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,768 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6930ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,768 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7332ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,768 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6876ms
2014-07-22 13:18:22,768 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,769 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4228ms
2014-07-22 13:18:22,769 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,770 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4382ms
2014-07-22 13:18:22,770 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,771 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4436ms
2014-07-22 13:18:22,771 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,771 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4470ms
2014-07-22 13:18:22,771 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,773 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4528ms
2014-07-22 13:18:22,773 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,773 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4585ms
2014-07-22 13:18:22,773 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,773 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4617ms
2014-07-22 13:18:22,773 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,773 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4650ms
2014-07-22 13:18:22,773 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,774 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4682ms
2014-07-22 13:18:22,774 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,774 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4717ms
2014-07-22 13:18:22,774 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,774 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4766ms
2014-07-22 13:18:22,774 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,777 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4828ms
2014-07-22 13:18:22,777 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,777 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4894ms
2014-07-22 13:18:22,777 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:22,778 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4969ms
2014-07-22 13:18:22,778 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:24,606 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1163ms
GC pool 'ParNew' had collection(s): count=1 time=1132ms
2014-07-22 13:18:24,669 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:24,676 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:18:25,010 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:25,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78319 synced till here 78289
2014-07-22 13:18:25,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060295349 with entries=116, filesize=91.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060305010
2014-07-22 13:18:25,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060206112
2014-07-22 13:18:25,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060208416
2014-07-22 13:18:25,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060210420
2014-07-22 13:18:25,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060212439
2014-07-22 13:18:25,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060216109
2014-07-22 13:18:25,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060218309
2014-07-22 13:18:25,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060220867
2014-07-22 13:18:25,492 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15563, memsize=277.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e90148cb29e74145af4400a87dc0f204
2014-07-22 13:18:26,605 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e90148cb29e74145af4400a87dc0f204 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e90148cb29e74145af4400a87dc0f204
2014-07-22 13:18:26,617 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e90148cb29e74145af4400a87dc0f204, entries=1011190, sequenceid=15563, filesize=72.0m
2014-07-22 13:18:26,618 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~900.9m/944611760, currentsize=262.4m/275181680 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 22020ms, sequenceid=15563, compaction requested=true
2014-07-22 13:18:26,619 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:18:26,619 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 36 store files, 0 compacting, 36 eligible, 2000 blocking
2014-07-22 13:18:26,619 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 36 files from compaction candidates
2014-07-22 13:18:26,619 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 737.5m
2014-07-22 13:18:26,619 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:18:26,619 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:18:26,619 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:18:26,907 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10328,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296578,"queuetimems":1,"class":"HRegionServer","responsesize":16963,"method":"Multi"}
2014-07-22 13:18:26,958 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:18:27,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:27,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78437 synced till here 78399
2014-07-22 13:18:27,324 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060295890,"queuetimems":1,"class":"HRegionServer","responsesize":16805,"method":"Multi"}
2014-07-22 13:18:27,330 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11154,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296176,"queuetimems":0,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-22 13:18:27,330 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10910,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296420,"queuetimems":0,"class":"HRegionServer","responsesize":17174,"method":"Multi"}
2014-07-22 13:18:27,335 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060295836,"queuetimems":0,"class":"HRegionServer","responsesize":17078,"method":"Multi"}
2014-07-22 13:18:27,444 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060305010 with entries=118, filesize=90.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060307058
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060221920
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060223885
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060225302
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060227166
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060229337
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060231341
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234156
2014-07-22 13:18:27,444 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060234989
2014-07-22 13:18:27,608 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:27,641 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060295995,"queuetimems":0,"class":"HRegionServer","responsesize":16645,"method":"Multi"}
2014-07-22 13:18:27,646 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11272,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296374,"queuetimems":0,"class":"HRegionServer","responsesize":16871,"method":"Multi"}
2014-07-22 13:18:27,662 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11719,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060295942,"queuetimems":1,"class":"HRegionServer","responsesize":16786,"method":"Multi"}
2014-07-22 13:18:27,670 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296089,"queuetimems":0,"class":"HRegionServer","responsesize":16911,"method":"Multi"}
2014-07-22 13:18:27,674 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11624,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296050,"queuetimems":1,"class":"HRegionServer","responsesize":17031,"method":"Multi"}
2014-07-22 13:18:27,678 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11451,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296227,"queuetimems":0,"class":"HRegionServer","responsesize":16903,"method":"Multi"}
2014-07-22 13:18:27,718 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11581,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296136,"queuetimems":0,"class":"HRegionServer","responsesize":16781,"method":"Multi"}
2014-07-22 13:18:27,724 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11450,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296274,"queuetimems":0,"class":"HRegionServer","responsesize":16973,"method":"Multi"}
2014-07-22 13:18:27,728 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11252,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060296476,"queuetimems":0,"class":"HRegionServer","responsesize":17013,"method":"Multi"}
2014-07-22 13:18:28,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:29,061 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78559 synced till here 78543
2014-07-22 13:18:29,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060307058 with entries=122, filesize=73.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060308996
2014-07-22 13:18:31,040 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:31,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78677 synced till here 78645
2014-07-22 13:18:31,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060308996 with entries=118, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060311041
2014-07-22 13:18:33,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:33,301 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78807 synced till here 78772
2014-07-22 13:18:33,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060311041 with entries=130, filesize=84.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060313230
2014-07-22 13:18:35,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:35,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 78927 synced till here 78902
2014-07-22 13:18:35,907 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060313230 with entries=120, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060315578
2014-07-22 13:18:37,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:37,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79049 synced till here 79011
2014-07-22 13:18:38,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060315578 with entries=122, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060317765
2014-07-22 13:18:40,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:40,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79154 synced till here 79151
2014-07-22 13:18:40,046 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060317765 with entries=105, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060320001
2014-07-22 13:18:43,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:43,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79245 synced till here 79236
2014-07-22 13:18:43,889 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060320001 with entries=91, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060323769
2014-07-22 13:18:45,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:45,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79337 synced till here 79333
2014-07-22 13:18:45,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060323769 with entries=92, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060325154
2014-07-22 13:18:45,870 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:18:46,735 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:46,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79425 synced till here 79424
2014-07-22 13:18:46,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060325154 with entries=88, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060326735
2014-07-22 13:18:47,548 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:47,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79525 synced till here 79519
2014-07-22 13:18:48,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060326735 with entries=100, filesize=76.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060327549
2014-07-22 13:18:49,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:49,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79614 synced till here 79610
2014-07-22 13:18:49,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060327549 with entries=89, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060329336
2014-07-22 13:18:50,788 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,792 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,792 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,792 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,794 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,796 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,799 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,824 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,825 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,825 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,826 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,847 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,848 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,858 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,870 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,902 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,945 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:50,993 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:51,069 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:51,150 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:18:51,152 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15688, memsize=371.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/8d5a3ca80d2645c880a2549eef604a0a
2014-07-22 13:18:51,170 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/8d5a3ca80d2645c880a2549eef604a0a as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/8d5a3ca80d2645c880a2549eef604a0a
2014-07-22 13:18:51,182 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/8d5a3ca80d2645c880a2549eef604a0a, entries=1353650, sequenceid=15688, filesize=96.5m
2014-07-22 13:18:51,183 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~844.8m/885810960, currentsize=451.7m/473678400 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 28444ms, sequenceid=15688, compaction requested=true
2014-07-22 13:18:51,184 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:18:51,184 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 13:18:51,184 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34ms
2014-07-22 13:18:51,184 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,185 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 13:18:51,185 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 743.7m
2014-07-22 13:18:51,185 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 116ms
2014-07-22 13:18:51,185 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,185 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:18:51,185 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 193ms
2014-07-22 13:18:51,185 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:18:51,185 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,186 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:18:51,189 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 244ms
2014-07-22 13:18:51,189 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,193 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 291ms
2014-07-22 13:18:51,193 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,194 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 324ms
2014-07-22 13:18:51,194 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,194 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 336ms
2014-07-22 13:18:51,194 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,194 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 346ms
2014-07-22 13:18:51,194 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,205 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 358ms
2014-07-22 13:18:51,205 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,205 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 379ms
2014-07-22 13:18:51,206 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,206 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 381ms
2014-07-22 13:18:51,206 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,213 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 388ms
2014-07-22 13:18:51,230 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,230 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 406ms
2014-07-22 13:18:51,230 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,230 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 431ms
2014-07-22 13:18:51,230 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,241 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 445ms
2014-07-22 13:18:51,241 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,241 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 447ms
2014-07-22 13:18:51,242 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,242 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 450ms
2014-07-22 13:18:51,242 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,242 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 450ms
2014-07-22 13:18:51,242 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,253 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 461ms
2014-07-22 13:18:51,253 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,253 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 465ms
2014-07-22 13:18:51,254 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:18:51,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:51,512 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:18:51,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79712 synced till here 79709
2014-07-22 13:18:52,792 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1070ms
GC pool 'ParNew' had collection(s): count=1 time=1170ms
2014-07-22 13:18:52,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060329336 with entries=98, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060331501
2014-07-22 13:18:52,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060238195
2014-07-22 13:18:52,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060243788
2014-07-22 13:18:52,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060245768
2014-07-22 13:18:52,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060247297
2014-07-22 13:18:52,839 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060248985
2014-07-22 13:18:53,224 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:53,514 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15742, memsize=359.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ca9a1256800e4b86b84523fb565c5a50
2014-07-22 13:18:53,592 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ca9a1256800e4b86b84523fb565c5a50 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ca9a1256800e4b86b84523fb565c5a50
2014-07-22 13:18:53,799 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ca9a1256800e4b86b84523fb565c5a50, entries=1308960, sequenceid=15742, filesize=93.3m
2014-07-22 13:18:53,800 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~740.6m/776561680, currentsize=421.8m/442278640 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 27181ms, sequenceid=15742, compaction requested=true
2014-07-22 13:18:53,800 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:18:53,800 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 13:18:53,800 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 13:18:53,800 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 671.3m
2014-07-22 13:18:53,800 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:18:53,800 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:18:53,801 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:18:53,825 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:18:55,303 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:55,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79844 synced till here 79813
2014-07-22 13:18:55,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060331501 with entries=132, filesize=102.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060335303
2014-07-22 13:18:55,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060250655
2014-07-22 13:18:55,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060251914
2014-07-22 13:18:55,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060253426
2014-07-22 13:18:55,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060254775
2014-07-22 13:18:55,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060257072
2014-07-22 13:18:55,943 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:18:57,444 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1369ms
GC pool 'ParNew' had collection(s): count=1 time=1435ms
2014-07-22 13:18:57,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:57,962 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 79958 synced till here 79950
2014-07-22 13:18:58,042 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060335303 with entries=114, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060337882
2014-07-22 13:18:59,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:18:59,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80074 synced till here 80043
2014-07-22 13:19:00,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060337882 with entries=116, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060339679
2014-07-22 13:19:00,023 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:01,626 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1187ms
GC pool 'ParNew' had collection(s): count=1 time=1279ms
2014-07-22 13:19:02,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:02,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80197 synced till here 80175
2014-07-22 13:19:02,561 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060339679 with entries=123, filesize=90.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060342235
2014-07-22 13:19:02,562 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:04,215 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1087ms
GC pool 'ParNew' had collection(s): count=1 time=1581ms
2014-07-22 13:19:04,935 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:05,022 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80323 synced till here 80288
2014-07-22 13:19:06,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060342235 with entries=126, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060344935
2014-07-22 13:19:06,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:08,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:09,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80446 synced till here 80432
2014-07-22 13:19:09,246 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060344935 with entries=123, filesize=93.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060348967
2014-07-22 13:19:09,247 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:09,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:10,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80567 synced till here 80540
2014-07-22 13:19:11,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060348967 with entries=121, filesize=87.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060349972
2014-07-22 13:19:11,069 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:11,922 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:13,148 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1157ms
GC pool 'ParNew' had collection(s): count=1 time=1207ms
2014-07-22 13:19:13,199 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80682 synced till here 80665
2014-07-22 13:19:13,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060349972 with entries=115, filesize=83.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060351922
2014-07-22 13:19:13,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:14,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:14,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80791 synced till here 80788
2014-07-22 13:19:14,363 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060351922 with entries=109, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060354115
2014-07-22 13:19:14,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:19:15,949 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:16,177 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:16,482 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,475 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,507 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,541 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,571 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,602 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,635 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,665 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,699 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,730 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,763 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,796 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,828 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,860 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:17,892 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,041 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,073 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,106 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,136 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,170 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,201 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,234 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,266 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,302 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:18,337 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:19:19,988 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:20,036 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:19:20,471 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=15963, memsize=346.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/4ea4e32dfbce473386a362f60cc54e89
2014-07-22 13:19:20,482 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/4ea4e32dfbce473386a362f60cc54e89 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/4ea4e32dfbce473386a362f60cc54e89
2014-07-22 13:19:20,490 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/4ea4e32dfbce473386a362f60cc54e89, entries=1262530, sequenceid=15963, filesize=89.9m
2014-07-22 13:19:20,491 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~743.7m/779842000, currentsize=372.1m/390168240 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 29306ms, sequenceid=15963, compaction requested=true
2014-07-22 13:19:20,491 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:20,491 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 13:19:20,491 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 456ms
2014-07-22 13:19:20,491 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,491 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 13:19:20,491 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 393.5m
2014-07-22 13:19:20,491 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:20,491 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 503ms
2014-07-22 13:19:20,492 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:20,492 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,492 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:19:20,493 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2156ms
2014-07-22 13:19:20,493 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,493 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2191ms
2014-07-22 13:19:20,494 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,494 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2228ms
2014-07-22 13:19:20,494 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,494 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2260ms
2014-07-22 13:19:20,494 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,509 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2308ms
2014-07-22 13:19:20,509 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,516 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2346ms
2014-07-22 13:19:20,516 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,517 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2381ms
2014-07-22 13:19:20,517 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,517 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2411ms
2014-07-22 13:19:20,517 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,517 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2444ms
2014-07-22 13:19:20,517 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,518 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2477ms
2014-07-22 13:19:20,518 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,518 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2626ms
2014-07-22 13:19:20,518 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,521 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2661ms
2014-07-22 13:19:20,521 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,521 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2693ms
2014-07-22 13:19:20,522 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,522 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2726ms
2014-07-22 13:19:20,522 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,522 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2759ms
2014-07-22 13:19:20,522 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:20,528 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2798ms
2014-07-22 13:19:21,358 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,358 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3659ms
2014-07-22 13:19:21,358 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,362 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3697ms
2014-07-22 13:19:21,363 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,363 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3728ms
2014-07-22 13:19:21,363 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,364 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3761ms
2014-07-22 13:19:21,364 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,364 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3793ms
2014-07-22 13:19:21,364 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,364 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3823ms
2014-07-22 13:19:21,364 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,365 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3858ms
2014-07-22 13:19:21,365 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,373 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3898ms
2014-07-22 13:19:21,374 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,374 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4892ms
2014-07-22 13:19:21,374 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,374 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5197ms
2014-07-22 13:19:21,374 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,374 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5426ms
2014-07-22 13:19:21,374 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:19:21,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:21,815 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80893 synced till here 80886
2014-07-22 13:19:21,894 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:19:21,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060354115 with entries=102, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060361776
2014-07-22 13:19:22,018 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:19:22,617 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:22,639 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 80984 synced till here 80972
2014-07-22 13:19:22,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060361776 with entries=91, filesize=66.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060362618
2014-07-22 13:19:23,623 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16011, memsize=362.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8964480a9dda43fb99edc6522b4c742b
2014-07-22 13:19:23,634 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8964480a9dda43fb99edc6522b4c742b as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8964480a9dda43fb99edc6522b4c742b
2014-07-22 13:19:23,648 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8964480a9dda43fb99edc6522b4c742b, entries=1319230, sequenceid=16011, filesize=94.0m
2014-07-22 13:19:23,648 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~688.4m/721872560, currentsize=359.4m/376902800 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 29848ms, sequenceid=16011, compaction requested=true
2014-07-22 13:19:23,649 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:23,649 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 37 store files, 0 compacting, 37 eligible, 2000 blocking
2014-07-22 13:19:23,649 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 37 files from compaction candidates
2014-07-22 13:19:23,649 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 865.3m
2014-07-22 13:19:23,649 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:23,649 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:23,649 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:19:23,749 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:19:24,377 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:19:24,844 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:24,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81074 synced till here 81072
2014-07-22 13:19:24,930 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060362618 with entries=90, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060364844
2014-07-22 13:19:26,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:26,543 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81177 synced till here 81176
2014-07-22 13:19:26,572 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060364844 with entries=103, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060366265
2014-07-22 13:19:28,210 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:28,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81263 synced till here 81262
2014-07-22 13:19:28,250 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060366265 with entries=86, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060368210
2014-07-22 13:19:28,321 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16179, memsize=116.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/43d10451ed6f4c1ca7b4a529989b0da6
2014-07-22 13:19:28,336 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/43d10451ed6f4c1ca7b4a529989b0da6 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/43d10451ed6f4c1ca7b4a529989b0da6
2014-07-22 13:19:28,349 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/43d10451ed6f4c1ca7b4a529989b0da6, entries=424980, sequenceid=16179, filesize=30.3m
2014-07-22 13:19:28,350 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~393.5m/412649440, currentsize=38.3m/40152400 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 7859ms, sequenceid=16179, compaction requested=true
2014-07-22 13:19:28,350 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:28,350 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 21 store files, 0 compacting, 21 eligible, 2000 blocking
2014-07-22 13:19:28,350 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 21 files from compaction candidates
2014-07-22 13:19:28,350 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 866.2m
2014-07-22 13:19:28,350 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:28,351 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:28,351 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:19:29,549 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:19:29,562 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:29,592 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060368210 with entries=84, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060369562
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060258849
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060266215
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060268335
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060270539
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060273397
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060276415
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060278434
2014-07-22 13:19:29,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060279453
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060281852
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060284081
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060285804
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060287609
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060288630
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060290345
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060291876
2014-07-22 13:19:29,594 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060293542
2014-07-22 13:19:31,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:31,245 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81434 synced till here 81430
2014-07-22 13:19:31,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060369562 with entries=87, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060371220
2014-07-22 13:19:33,280 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:33,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81523 synced till here 81521
2014-07-22 13:19:33,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060371220 with entries=89, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060373281
2014-07-22 13:19:35,064 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:35,091 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060373281 with entries=85, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060375065
2014-07-22 13:19:37,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:38,525 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16251, memsize=188.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/482a49b60c4146cfa11160ef1faa8fe7
2014-07-22 13:19:38,543 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81726 synced till here 81721
2014-07-22 13:19:38,547 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/482a49b60c4146cfa11160ef1faa8fe7 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/482a49b60c4146cfa11160ef1faa8fe7
2014-07-22 13:19:38,559 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/482a49b60c4146cfa11160ef1faa8fe7, entries=686690, sequenceid=16251, filesize=49.0m
2014-07-22 13:19:38,559 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~865.3m/907318880, currentsize=219.8m/230483040 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 14910ms, sequenceid=16251, compaction requested=true
2014-07-22 13:19:38,560 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:38,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 13:19:38,560 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 640.8m
2014-07-22 13:19:38,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 13:19:38,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:38,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:38,560 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:19:38,585 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060375065 with entries=118, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060377130
2014-07-22 13:19:38,585 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060295349
2014-07-22 13:19:39,172 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:19:39,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:39,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81815 synced till here 81810
2014-07-22 13:19:39,316 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060377130 with entries=89, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060379245
2014-07-22 13:19:40,349 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:19:40,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:40,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 81909 synced till here 81898
2014-07-22 13:19:41,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060379245 with entries=94, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060380906
2014-07-22 13:19:42,243 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1062ms
GC pool 'ParNew' had collection(s): count=1 time=1092ms
2014-07-22 13:19:42,870 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:42,900 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82009 synced till here 81997
2014-07-22 13:19:42,973 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060380906 with entries=100, filesize=72.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060382871
2014-07-22 13:19:44,700 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1455ms
GC pool 'ParNew' had collection(s): count=1 time=1504ms
2014-07-22 13:19:45,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:45,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82108 synced till here 82093
2014-07-22 13:19:45,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060382871 with entries=99, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060385223
2014-07-22 13:19:47,079 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:47,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82217 synced till here 82193
2014-07-22 13:19:47,355 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060385223 with entries=109, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060387079
2014-07-22 13:19:48,683 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1070ms
GC pool 'ParNew' had collection(s): count=1 time=1117ms
2014-07-22 13:19:49,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:49,335 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82329 synced till here 82301
2014-07-22 13:19:49,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060387079 with entries=112, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060389161
2014-07-22 13:19:50,789 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1105ms
GC pool 'ParNew' had collection(s): count=1 time=1118ms
2014-07-22 13:19:51,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:51,205 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82424 synced till here 82423
2014-07-22 13:19:51,230 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060389161 with entries=95, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060391161
2014-07-22 13:19:52,869 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16306, memsize=259.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2a7d8c94e29d4574a0b262fe7a468a05
2014-07-22 13:19:52,883 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2a7d8c94e29d4574a0b262fe7a468a05 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2a7d8c94e29d4574a0b262fe7a468a05
2014-07-22 13:19:52,896 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2a7d8c94e29d4574a0b262fe7a468a05, entries=945180, sequenceid=16306, filesize=67.4m
2014-07-22 13:19:52,897 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~867.7m/909890320, currentsize=382.4m/400990000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 24547ms, sequenceid=16306, compaction requested=true
2014-07-22 13:19:52,899 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:52,899 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 13:19:52,899 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 822.5m
2014-07-22 13:19:52,899 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 13:19:52,899 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:52,899 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:52,899 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:19:52,912 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:19:52,954 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:52,975 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82510 synced till here 82509
2014-07-22 13:19:52,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060391161 with entries=86, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060392954
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060305010
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060307058
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060308996
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060311041
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060313230
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060315578
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060317765
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060320001
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060323769
2014-07-22 13:19:52,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060325154
2014-07-22 13:19:53,000 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060326735
2014-07-22 13:19:53,000 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060327549
2014-07-22 13:19:53,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:54,360 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:19:54,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82598 synced till here 82596
2014-07-22 13:19:54,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060392954 with entries=88, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060393692
2014-07-22 13:19:55,198 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:55,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82691 synced till here 82681
2014-07-22 13:19:56,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060393692 with entries=93, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060395199
2014-07-22 13:19:56,938 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:56,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82794 synced till here 82780
2014-07-22 13:19:57,038 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060395199 with entries=103, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060396939
2014-07-22 13:19:58,409 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:19:58,774 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16378, memsize=227.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/791974eb3ab54dd881cf568959967a73
2014-07-22 13:19:59,564 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 82927 synced till here 82917
2014-07-22 13:19:59,604 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/791974eb3ab54dd881cf568959967a73 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/791974eb3ab54dd881cf568959967a73
2014-07-22 13:19:59,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060396939 with entries=133, filesize=98.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060398410
2014-07-22 13:19:59,664 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/791974eb3ab54dd881cf568959967a73, entries=826570, sequenceid=16378, filesize=58.9m
2014-07-22 13:19:59,665 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~642.2m/673436880, currentsize=370.4m/388423920 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 21105ms, sequenceid=16378, compaction requested=true
2014-07-22 13:19:59,666 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:19:59,667 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 13:19:59,667 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 592.9m
2014-07-22 13:19:59,668 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 13:19:59,668 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:19:59,669 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:19:59,669 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:19:59,780 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:20:00,468 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:00,471 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:20:01,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83025 synced till here 83009
2014-07-22 13:20:01,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060398410 with entries=98, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060400469
2014-07-22 13:20:01,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060329336
2014-07-22 13:20:02,591 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:02,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83128 synced till here 83126
2014-07-22 13:20:02,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060400469 with entries=103, filesize=73.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060402592
2014-07-22 13:20:04,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:04,377 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83213 synced till here 83212
2014-07-22 13:20:04,428 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060402592 with entries=85, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060404164
2014-07-22 13:20:05,923 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:05,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83301 synced till here 83296
2014-07-22 13:20:06,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060404164 with entries=88, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060405924
2014-07-22 13:20:07,437 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:07,745 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060405924 with entries=105, filesize=76.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060407437
2014-07-22 13:20:09,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:09,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83492 synced till here 83488
2014-07-22 13:20:09,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060407437 with entries=86, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060409273
2014-07-22 13:20:11,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:11,038 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83578 synced till here 83577
2014-07-22 13:20:11,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060409273 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060411018
2014-07-22 13:20:11,225 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:20:12,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:12,580 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83664 synced till here 83662
2014-07-22 13:20:12,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060411018 with entries=86, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060412562
2014-07-22 13:20:13,408 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16545, memsize=267.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c1fa97dc6e2b414382a904f77eb2300f
2014-07-22 13:20:13,420 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/c1fa97dc6e2b414382a904f77eb2300f as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c1fa97dc6e2b414382a904f77eb2300f
2014-07-22 13:20:13,433 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/c1fa97dc6e2b414382a904f77eb2300f, entries=975220, sequenceid=16545, filesize=69.5m
2014-07-22 13:20:13,433 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~828.8m/869042640, currentsize=374.0m/392178800 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 20534ms, sequenceid=16545, compaction requested=true
2014-07-22 13:20:13,434 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:20:13,434 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 38 store files, 0 compacting, 38 eligible, 2000 blocking
2014-07-22 13:20:13,434 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 761.7m
2014-07-22 13:20:13,434 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 38 files from compaction candidates
2014-07-22 13:20:13,434 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:20:13,434 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:20:13,435 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:20:13,440 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:20:13,593 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:13,641 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83756 synced till here 83746
2014-07-22 13:20:14,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060412562 with entries=92, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060413593
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060331501
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060335303
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060337882
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060339679
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060342235
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060344935
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060348967
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060349972
2014-07-22 13:20:14,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060351922
2014-07-22 13:20:14,329 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060354115
2014-07-22 13:20:14,779 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:20:15,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:15,257 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060413593 with entries=95, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060415137
2014-07-22 13:20:16,801 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:17,054 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 83960 synced till here 83958
2014-07-22 13:20:17,067 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060415137 with entries=109, filesize=79.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060416801
2014-07-22 13:20:18,518 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:18,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060416801 with entries=84, filesize=61.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060418518
2014-07-22 13:20:18,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:18,878 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16636, memsize=330.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/9ec6aea0077c4093b86664b7b58d9d27
2014-07-22 13:20:18,898 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/9ec6aea0077c4093b86664b7b58d9d27 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/9ec6aea0077c4093b86664b7b58d9d27
2014-07-22 13:20:18,916 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/9ec6aea0077c4093b86664b7b58d9d27, entries=1203650, sequenceid=16636, filesize=85.7m
2014-07-22 13:20:18,916 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~595.9m/624873520, currentsize=345.1m/361825440 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 19249ms, sequenceid=16636, compaction requested=true
2014-07-22 13:20:18,917 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:20:18,917 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 13:20:18,917 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 720.8m
2014-07-22 13:20:18,917 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 13:20:18,917 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:20:18,917 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:20:18,917 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:20:19,159 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:20:19,469 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:20:20,959 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:20,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84133 synced till here 84121
2014-07-22 13:20:21,218 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060418518 with entries=89, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060420961
2014-07-22 13:20:21,218 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:22,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:22,890 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84229 synced till here 84222
2014-07-22 13:20:22,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060420961 with entries=96, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060422861
2014-07-22 13:20:22,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:24,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:24,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84329 synced till here 84313
2014-07-22 13:20:24,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060422861 with entries=100, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060424748
2014-07-22 13:20:24,892 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:26,418 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1009ms
GC pool 'ParNew' had collection(s): count=1 time=1136ms
2014-07-22 13:20:26,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:26,753 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84426 synced till here 84410
2014-07-22 13:20:26,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060424748 with entries=97, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060426721
2014-07-22 13:20:26,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:28,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:28,915 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84539 synced till here 84536
2014-07-22 13:20:28,996 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060426721 with entries=113, filesize=85.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060428676
2014-07-22 13:20:29,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:31,299 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:31,336 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84646 synced till here 84628
2014-07-22 13:20:31,741 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060428676 with entries=107, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060431300
2014-07-22 13:20:31,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:33,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:33,436 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84744 synced till here 84727
2014-07-22 13:20:33,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060431300 with entries=98, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060433414
2014-07-22 13:20:33,587 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:35,697 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1675ms
GC pool 'ParNew' had collection(s): count=1 time=1750ms
2014-07-22 13:20:36,022 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:36,055 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84840 synced till here 84834
2014-07-22 13:20:36,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060433414 with entries=96, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060436022
2014-07-22 13:20:36,106 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:38,072 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:38,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 84934 synced till here 84927
2014-07-22 13:20:38,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060436022 with entries=94, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060438072
2014-07-22 13:20:38,186 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:57,884 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 21466ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:20:57,885 WARN  [regionserver60020] util.Sleeper: We slept 19158ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:20:57,884 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 21467ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-22 13:20:57,885 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 18941ms
GC pool 'ParNew' had collection(s): count=1 time=2674ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=16322ms
2014-07-22 13:20:57,940 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:20:57,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85023 synced till here 85016
2014-07-22 13:20:58,010 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438143,"queuetimems":1,"class":"HRegionServer","responsesize":16615,"method":"Multi"}
2014-07-22 13:20:58,011 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28055 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,012 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060438072 with entries=89, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060457940
2014-07-22 13:20:58,016 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:20:58,074 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438172,"queuetimems":1,"class":"HRegionServer","responsesize":16983,"method":"Multi"}
2014-07-22 13:20:58,074 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438208,"queuetimems":1,"class":"HRegionServer","responsesize":16868,"method":"Multi"}
2014-07-22 13:20:58,074 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28054 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,074 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,075 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28053 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,075 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,118 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19878,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438239,"queuetimems":1,"class":"HRegionServer","responsesize":16249,"method":"Multi"}
2014-07-22 13:20:58,118 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28052 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,118 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,122 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438373,"queuetimems":1,"class":"HRegionServer","responsesize":16357,"method":"Multi"}
2014-07-22 13:20:58,122 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28074 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,122 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,137 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19807,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438330,"queuetimems":5,"class":"HRegionServer","responsesize":15214,"method":"Multi"}
2014-07-22 13:20:58,137 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438427,"queuetimems":1,"class":"HRegionServer","responsesize":16847,"method":"Multi"}
2014-07-22 13:20:58,138 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28075 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,138 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,138 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28072 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,138 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,206 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438570,"queuetimems":0,"class":"HRegionServer","responsesize":16816,"method":"Multi"}
2014-07-22 13:20:58,206 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19742,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438464,"queuetimems":1,"class":"HRegionServer","responsesize":16892,"method":"Multi"}
2014-07-22 13:20:58,206 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19671,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438535,"queuetimems":1,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-07-22 13:20:58,206 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28076 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,207 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,207 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28078 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,207 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,207 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28071 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,207 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,231 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19611,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438619,"queuetimems":0,"class":"HRegionServer","responsesize":16905,"method":"Multi"}
2014-07-22 13:20:58,231 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28081 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,231 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438660,"queuetimems":0,"class":"HRegionServer","responsesize":17006,"method":"Multi"}
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19545,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438700,"queuetimems":0,"class":"HRegionServer","responsesize":17232,"method":"Multi"}
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28083 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28085 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,246 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,248 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438735,"queuetimems":0,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-07-22 13:20:58,248 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28089 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,248 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,270 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:20:58,280 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:20:58,281 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438769,"queuetimems":1,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:20:58,282 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28087 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:20:58,282 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:20:58,302 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:20:59,192 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:20:59,226 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:20:59,252 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:01,992 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16800, memsize=483.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/df91a20fb345464fa203433bdb60605a
2014-07-22 13:21:02,002 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/df91a20fb345464fa203433bdb60605a as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/df91a20fb345464fa203433bdb60605a
2014-07-22 13:21:02,024 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/df91a20fb345464fa203433bdb60605a, entries=1759910, sequenceid=16800, filesize=125.3m
2014-07-22 13:21:02,025 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~764.8m/801988400, currentsize=406.6m/426351120 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 48591ms, sequenceid=16800, compaction requested=true
2014-07-22 13:21:02,025 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:21:02,025 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 13:21:02,025 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2773ms
2014-07-22 13:21:02,026 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,026 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 394.5m
2014-07-22 13:21:02,026 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2800ms
2014-07-22 13:21:02,026 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,026 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 13:21:02,026 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:21:02,026 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:21:02,027 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:21:02,028 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2836ms
2014-07-22 13:21:02,028 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,028 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3726ms
2014-07-22 13:21:02,028 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,028 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3748ms
2014-07-22 13:21:02,028 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,029 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3759ms
2014-07-22 13:21:02,029 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:02,038 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23206,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54756","starttimems":1406060438832,"queuetimems":0,"class":"HRegionServer","responsesize":16871,"method":"Multi"}
2014-07-22 13:21:02,039 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28090 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:21:02,039 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:21:02,095 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:21:02,112 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28094 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.53:54756: output error
2014-07-22 13:21:02,112 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:21:02,112 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28102 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.53:54756: output error
2014-07-22 13:21:02,113 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-22 13:21:02,307 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:21:04,100 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=16844, memsize=462.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c8bd33a5286a433f8dffc36de192a9f2
2014-07-22 13:21:04,109 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c8bd33a5286a433f8dffc36de192a9f2 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c8bd33a5286a433f8dffc36de192a9f2
2014-07-22 13:21:04,117 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c8bd33a5286a433f8dffc36de192a9f2, entries=1684670, sequenceid=16844, filesize=119.9m
2014-07-22 13:21:04,117 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~720.8m/755855360, currentsize=322.7m/338396160 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 45200ms, sequenceid=16844, compaction requested=true
2014-07-22 13:21:04,118 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:21:04,118 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 13:21:04,118 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 13:21:04,118 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 797.6m
2014-07-22 13:21:04,118 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:21:04,118 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:21:04,119 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:21:04,688 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:21:05,546 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:21:05,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:06,003 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85144 synced till here 85132
2014-07-22 13:21:06,062 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060457940 with entries=121, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060465728
2014-07-22 13:21:07,250 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:07,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060465728 with entries=112, filesize=79.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060467251
2014-07-22 13:21:08,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:08,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85342 synced till here 85337
2014-07-22 13:21:08,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060467251 with entries=86, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060468760
2014-07-22 13:21:09,986 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:10,290 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85467 synced till here 85459
2014-07-22 13:21:10,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060468760 with entries=125, filesize=93.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060469986
2014-07-22 13:21:11,485 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17021, memsize=229.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/72f4a46261cf42e597da671a345e7ec4
2014-07-22 13:21:11,499 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/72f4a46261cf42e597da671a345e7ec4 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/72f4a46261cf42e597da671a345e7ec4
2014-07-22 13:21:11,522 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/72f4a46261cf42e597da671a345e7ec4, entries=834360, sequenceid=17021, filesize=59.4m
2014-07-22 13:21:11,522 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~394.5m/413655520, currentsize=44.3m/46426240 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 9496ms, sequenceid=17021, compaction requested=true
2014-07-22 13:21:11,523 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:21:11,523 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 22 store files, 0 compacting, 22 eligible, 2000 blocking
2014-07-22 13:21:11,523 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 807.5m
2014-07-22 13:21:11,523 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 22 files from compaction candidates
2014-07-22 13:21:11,523 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:21:11,523 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:21:11,523 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:21:11,627 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:11,641 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85555 synced till here 85553
2014-07-22 13:21:11,653 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060469986 with entries=88, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060471627
2014-07-22 13:21:11,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060361776
2014-07-22 13:21:11,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060362618
2014-07-22 13:21:11,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060364844
2014-07-22 13:21:11,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060366265
2014-07-22 13:21:11,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060368210
2014-07-22 13:21:11,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060369562
2014-07-22 13:21:11,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060371220
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060373281
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060375065
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060377130
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060379245
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060380906
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060382871
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060385223
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060387079
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060389161
2014-07-22 13:21:11,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060391161
2014-07-22 13:21:12,632 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:21:13,043 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:13,073 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85645 synced till here 85638
2014-07-22 13:21:13,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060471627 with entries=90, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060473044
2014-07-22 13:21:14,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:14,923 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85751 synced till here 85746
2014-07-22 13:21:15,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060473044 with entries=106, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060474717
2014-07-22 13:21:16,820 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:16,837 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85848 synced till here 85833
2014-07-22 13:21:16,977 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060474717 with entries=97, filesize=74.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060476821
2014-07-22 13:21:18,498 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1140ms
GC pool 'ParNew' had collection(s): count=1 time=1232ms
2014-07-22 13:21:18,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:19,013 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 85949 synced till here 85932
2014-07-22 13:21:19,195 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060476821 with entries=101, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060478947
2014-07-22 13:21:20,562 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1063ms
GC pool 'ParNew' had collection(s): count=1 time=1113ms
2014-07-22 13:21:21,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:21,117 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86061 synced till here 86034
2014-07-22 13:21:21,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060478947 with entries=112, filesize=84.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060481078
2014-07-22 13:21:23,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:24,781 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1300ms
GC pool 'ParNew' had collection(s): count=1 time=1361ms
2014-07-22 13:21:24,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86194 synced till here 86169
2014-07-22 13:21:25,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060481078 with entries=133, filesize=98.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060483352
2014-07-22 13:21:27,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:27,124 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86313 synced till here 86293
2014-07-22 13:21:27,309 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060483352 with entries=119, filesize=90.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060487048
2014-07-22 13:21:29,434 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:29,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86433 synced till here 86408
2014-07-22 13:21:29,620 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060487048 with entries=120, filesize=85.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060489435
2014-07-22 13:21:29,903 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,904 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,904 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,904 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,905 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,905 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,905 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,907 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,908 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:29,946 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,004 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,005 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,007 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,007 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,008 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,008 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,008 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,008 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,009 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,009 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,009 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,010 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,010 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,012 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,384 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,417 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,453 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,487 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,526 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,558 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,591 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,625 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,663 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,699 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,734 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,776 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,821 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,864 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:30,903 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,797 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,806 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,841 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,876 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,908 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,944 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:31,974 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:32,007 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:32,040 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:32,072 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:32,104 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:34,680 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17058, memsize=514.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5512f1c1064e44888ee86567be27ac83
2014-07-22 13:21:34,692 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5512f1c1064e44888ee86567be27ac83 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5512f1c1064e44888ee86567be27ac83
2014-07-22 13:21:34,707 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5512f1c1064e44888ee86567be27ac83, entries=1873100, sequenceid=17058, filesize=133.3m
2014-07-22 13:21:34,708 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~797.6m/836373440, currentsize=450.3m/472178160 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 30590ms, sequenceid=17058, compaction requested=true
2014-07-22 13:21:34,709 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:21:34,709 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 39 store files, 0 compacting, 39 eligible, 2000 blocking
2014-07-22 13:21:34,709 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 39 files from compaction candidates
2014-07-22 13:21:34,709 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2605ms
2014-07-22 13:21:34,709 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:21:34,709 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,709 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 867.5m
2014-07-22 13:21:34,709 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:21:34,709 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2637ms
2014-07-22 13:21:34,709 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,709 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:21:34,710 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2670ms
2014-07-22 13:21:34,710 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,710 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2703ms
2014-07-22 13:21:34,710 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,710 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2736ms
2014-07-22 13:21:34,710 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,710 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2766ms
2014-07-22 13:21:34,711 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,711 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2803ms
2014-07-22 13:21:34,711 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,713 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2836ms
2014-07-22 13:21:34,713 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,714 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2872ms
2014-07-22 13:21:34,714 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,714 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2908ms
2014-07-22 13:21:34,714 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,722 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2924ms
2014-07-22 13:21:34,722 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,722 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3819ms
2014-07-22 13:21:34,723 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,724 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3859ms
2014-07-22 13:21:34,724 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,724 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3903ms
2014-07-22 13:21:34,724 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,724 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3948ms
2014-07-22 13:21:34,724 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,724 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3990ms
2014-07-22 13:21:34,724 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,725 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4026ms
2014-07-22 13:21:34,725 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,725 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4062ms
2014-07-22 13:21:34,725 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,728 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4103ms
2014-07-22 13:21:34,728 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,728 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4137ms
2014-07-22 13:21:34,728 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,728 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4170ms
2014-07-22 13:21:34,729 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,730 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4203ms
2014-07-22 13:21:34,730 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,731 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4244ms
2014-07-22 13:21:34,731 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,732 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4278ms
2014-07-22 13:21:34,732 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,732 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4315ms
2014-07-22 13:21:34,733 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,734 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4349ms
2014-07-22 13:21:34,734 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,734 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4723ms
2014-07-22 13:21:34,735 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,735 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4725ms
2014-07-22 13:21:34,735 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,736 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4726ms
2014-07-22 13:21:34,736 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,737 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4729ms
2014-07-22 13:21:34,737 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,737 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4729ms
2014-07-22 13:21:34,737 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,741 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4732ms
2014-07-22 13:21:34,742 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,742 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4735ms
2014-07-22 13:21:34,742 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,742 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4735ms
2014-07-22 13:21:34,742 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,742 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4735ms
2014-07-22 13:21:34,743 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,757 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4750ms
2014-07-22 13:21:34,757 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,758 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4750ms
2014-07-22 13:21:34,758 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,759 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4752ms
2014-07-22 13:21:34,759 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,759 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4754ms
2014-07-22 13:21:34,759 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,760 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4755ms
2014-07-22 13:21:34,760 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,770 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4823ms
2014-07-22 13:21:34,770 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,771 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4863ms
2014-07-22 13:21:34,775 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,775 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4868ms
2014-07-22 13:21:34,775 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,775 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4870ms
2014-07-22 13:21:34,775 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,785 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4880ms
2014-07-22 13:21:34,785 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,786 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4881ms
2014-07-22 13:21:34,786 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,786 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4882ms
2014-07-22 13:21:34,786 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,792 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4888ms
2014-07-22 13:21:34,792 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,792 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4888ms
2014-07-22 13:21:34,792 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:34,793 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4890ms
2014-07-22 13:21:34,793 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:35,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:35,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86547 synced till here 86525
2014-07-22 13:21:35,420 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:21:35,563 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:21:36,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060489435 with entries=114, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060495228
2014-07-22 13:21:36,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060392954
2014-07-22 13:21:36,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060393692
2014-07-22 13:21:36,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060395199
2014-07-22 13:21:36,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060396939
2014-07-22 13:21:37,626 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060487534,"queuetimems":0,"class":"HRegionServer","responsesize":17401,"method":"Multi"}
2014-07-22 13:21:37,626 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060487271,"queuetimems":21,"class":"HRegionServer","responsesize":16598,"method":"Multi"}
2014-07-22 13:21:37,633 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10391,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060487242,"queuetimems":30,"class":"HRegionServer","responsesize":16930,"method":"Multi"}
2014-07-22 13:21:37,633 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060487590,"queuetimems":0,"class":"HRegionServer","responsesize":17805,"method":"Multi"}
2014-07-22 13:21:38,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:38,169 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86675 synced till here 86641
2014-07-22 13:21:39,738 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060495228 with entries=128, filesize=96.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060498155
2014-07-22 13:21:40,523 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10141,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490382,"queuetimems":0,"class":"HRegionServer","responsesize":16605,"method":"Multi"}
2014-07-22 13:21:40,525 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490485,"queuetimems":0,"class":"HRegionServer","responsesize":16570,"method":"Multi"}
2014-07-22 13:21:40,655 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:40,656 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490451,"queuetimems":0,"class":"HRegionServer","responsesize":17594,"method":"Multi"}
2014-07-22 13:21:40,656 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490557,"queuetimems":0,"class":"HRegionServer","responsesize":16652,"method":"Multi"}
2014-07-22 13:21:40,657 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10033,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490623,"queuetimems":0,"class":"HRegionServer","responsesize":17077,"method":"Multi"}
2014-07-22 13:21:40,657 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490525,"queuetimems":1,"class":"HRegionServer","responsesize":17189,"method":"Multi"}
2014-07-22 13:21:40,657 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10241,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490415,"queuetimems":0,"class":"HRegionServer","responsesize":16721,"method":"Multi"}
2014-07-22 13:21:40,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 86799 synced till here 86783
2014-07-22 13:21:40,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060498155 with entries=124, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060500655
2014-07-22 13:21:40,906 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060490901,"queuetimems":0,"class":"HRegionServer","responsesize":16662,"method":"Multi"}
2014-07-22 13:21:42,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:42,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060500655 with entries=94, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060502353
2014-07-22 13:21:43,870 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:44,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87023 synced till here 87020
2014-07-22 13:21:44,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060502353 with entries=130, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060503871
2014-07-22 13:21:45,691 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,691 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,695 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,717 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,728 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,739 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,740 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,741 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,743 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,779 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,816 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,922 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:45,987 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:46,203 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:47,755 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17160, memsize=542.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3b08b3af940f4479bf669a2ec8bd348e
2014-07-22 13:21:47,777 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3b08b3af940f4479bf669a2ec8bd348e as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3b08b3af940f4479bf669a2ec8bd348e
2014-07-22 13:21:47,796 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3b08b3af940f4479bf669a2ec8bd348e, entries=1975000, sequenceid=17160, filesize=140.6m
2014-07-22 13:21:47,797 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~810.7m/850111600, currentsize=475.3m/498342320 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 36274ms, sequenceid=17160, compaction requested=true
2014-07-22 13:21:47,797 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:21:47,798 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 13:21:47,798 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1595ms
2014-07-22 13:21:47,798 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 13:21:47,798 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 936.7m
2014-07-22 13:21:47,798 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:21:47,798 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:21:47,798 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:21:47,798 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,799 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1811ms
2014-07-22 13:21:47,799 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,799 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1877ms
2014-07-22 13:21:47,799 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,800 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1983ms
2014-07-22 13:21:47,800 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,800 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2021ms
2014-07-22 13:21:47,800 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,800 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2057ms
2014-07-22 13:21:47,800 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,800 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2059ms
2014-07-22 13:21:47,801 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,810 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2070ms
2014-07-22 13:21:47,810 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,810 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2072ms
2014-07-22 13:21:47,810 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,810 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2082ms
2014-07-22 13:21:47,810 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,811 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2094ms
2014-07-22 13:21:47,811 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,819 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2125ms
2014-07-22 13:21:47,819 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,819 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2128ms
2014-07-22 13:21:47,819 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,821 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2129ms
2014-07-22 13:21:47,821 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:21:47,945 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:21:48,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:48,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87119 synced till here 87106
2014-07-22 13:21:49,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060503871 with entries=96, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060508078
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060398410
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060400469
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060402592
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060404164
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060405924
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060407437
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060409273
2014-07-22 13:21:49,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060411018
2014-07-22 13:21:49,572 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:21:49,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:49,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87212 synced till here 87210
2014-07-22 13:21:49,918 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060508078 with entries=93, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060509807
2014-07-22 13:21:51,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:51,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87297 synced till here 87293
2014-07-22 13:21:51,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060509807 with entries=85, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060511203
2014-07-22 13:21:52,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:53,096 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87398 synced till here 87397
2014-07-22 13:21:53,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060511203 with entries=101, filesize=74.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060512869
2014-07-22 13:21:54,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:54,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87483 synced till here 87481
2014-07-22 13:21:54,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060512869 with entries=85, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060514381
2014-07-22 13:21:56,012 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:56,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87575 synced till here 87567
2014-07-22 13:21:56,158 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060514381 with entries=92, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060516012
2014-07-22 13:21:58,078 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:21:58,198 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,200 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,200 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,200 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,219 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,238 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,240 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,241 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,243 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,244 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,254 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,262 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,263 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,264 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,265 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,278 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,278 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,282 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,284 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,337 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,369 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,402 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,433 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,465 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,700 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,700 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,700 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,700 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,700 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,701 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,701 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,703 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060516012 with entries=114, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060518079
2014-07-22 13:21:58,723 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,757 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,785 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,818 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,852 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,884 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:21:58,916 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,678 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,711 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,743 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,774 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,807 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,838 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,868 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,904 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:00,943 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:01,000 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:01,056 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:01,093 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:02,643 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17361, memsize=390.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2f75ac65751546fc91b3c16049e09ae3
2014-07-22 13:22:02,654 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2f75ac65751546fc91b3c16049e09ae3 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2f75ac65751546fc91b3c16049e09ae3
2014-07-22 13:22:02,662 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2f75ac65751546fc91b3c16049e09ae3, entries=1419870, sequenceid=17361, filesize=101.1m
2014-07-22 13:22:02,663 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~867.5m/909662000, currentsize=365.8m/383587600 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 27954ms, sequenceid=17361, compaction requested=true
2014-07-22 13:22:02,663 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:02,663 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 13:22:02,664 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 13:22:02,664 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1571ms
2014-07-22 13:22:02,664 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:02,664 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,664 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:02,664 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 805.7m
2014-07-22 13:22:02,664 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1608ms
2014-07-22 13:22:02,664 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,664 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:22:02,665 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1664ms
2014-07-22 13:22:02,665 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,665 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1722ms
2014-07-22 13:22:02,665 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,665 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1761ms
2014-07-22 13:22:02,665 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,665 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1797ms
2014-07-22 13:22:02,666 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,666 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1828ms
2014-07-22 13:22:02,666 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,673 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1867ms
2014-07-22 13:22:02,673 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,674 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1900ms
2014-07-22 13:22:02,674 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,675 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1931ms
2014-07-22 13:22:02,675 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,676 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1964ms
2014-07-22 13:22:02,676 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,683 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2006ms
2014-07-22 13:22:02,683 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,684 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3767ms
2014-07-22 13:22:02,684 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,689 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3805ms
2014-07-22 13:22:02,689 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,689 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3837ms
2014-07-22 13:22:02,689 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,691 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3871ms
2014-07-22 13:22:02,691 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,697 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3912ms
2014-07-22 13:22:02,697 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,697 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3940ms
2014-07-22 13:22:02,697 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,698 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3974ms
2014-07-22 13:22:02,698 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,701 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4202ms
2014-07-22 13:22:02,701 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,702 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4170ms
2014-07-22 13:22:02,702 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,702 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4139ms
2014-07-22 13:22:02,702 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,703 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4110ms
2014-07-22 13:22:02,703 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,707 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4083ms
2014-07-22 13:22:02,707 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,708 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4051ms
2014-07-22 13:22:02,708 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,708 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4020ms
2014-07-22 13:22:02,708 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,708 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4243ms
2014-07-22 13:22:02,708 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,713 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4279ms
2014-07-22 13:22:02,713 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,714 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4311ms
2014-07-22 13:22:02,714 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,714 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4345ms
2014-07-22 13:22:02,714 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,715 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4378ms
2014-07-22 13:22:02,715 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,722 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4437ms
2014-07-22 13:22:02,722 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,722 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4440ms
2014-07-22 13:22:02,723 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,723 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4445ms
2014-07-22 13:22:02,723 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,723 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4445ms
2014-07-22 13:22:02,723 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,724 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4459ms
2014-07-22 13:22:02,724 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,724 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4460ms
2014-07-22 13:22:02,724 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,725 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4462ms
2014-07-22 13:22:02,725 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,726 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4464ms
2014-07-22 13:22:02,726 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,737 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4483ms
2014-07-22 13:22:02,737 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,738 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4494ms
2014-07-22 13:22:02,738 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,738 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4496ms
2014-07-22 13:22:02,738 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,738 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4498ms
2014-07-22 13:22:02,738 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,750 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4510ms
2014-07-22 13:22:02,768 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,769 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4531ms
2014-07-22 13:22:02,770 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,770 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4551ms
2014-07-22 13:22:02,771 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,777 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4576ms
2014-07-22 13:22:02,777 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,777 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4577ms
2014-07-22 13:22:02,777 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,777 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4577ms
2014-07-22 13:22:02,777 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,778 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4579ms
2014-07-22 13:22:02,778 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:02,880 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:22:03,431 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:03,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:03,621 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87809 synced till here 87778
2014-07-22 13:22:05,409 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060518079 with entries=120, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060523577
2014-07-22 13:22:05,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060412562
2014-07-22 13:22:05,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060413593
2014-07-22 13:22:05,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060415137
2014-07-22 13:22:05,409 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060416801
2014-07-22 13:22:07,582 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:07,685 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 87937 synced till here 87908
2014-07-22 13:22:07,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060523577 with entries=128, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060527583
2014-07-22 13:22:07,944 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:22:09,536 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1239ms
GC pool 'ParNew' had collection(s): count=1 time=1281ms
2014-07-22 13:22:09,851 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:09,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88076 synced till here 88040
2014-07-22 13:22:10,377 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060527583 with entries=139, filesize=99.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060529851
2014-07-22 13:22:12,369 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:12,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88181 synced till here 88164
2014-07-22 13:22:12,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060529851 with entries=105, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060532369
2014-07-22 13:22:14,725 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:14,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88319 synced till here 88294
2014-07-22 13:22:15,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060532369 with entries=138, filesize=101.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060534826
2014-07-22 13:22:15,295 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,297 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,297 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,297 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,298 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,298 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:15,298 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,518 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,518 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,518 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,518 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,519 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,519 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,519 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,520 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,520 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,521 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,521 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,522 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,522 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,522 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,522 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,523 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,523 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,524 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,524 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,524 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,524 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,524 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,525 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,525 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,525 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,525 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,528 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,528 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,529 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,531 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,560 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,560 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,560 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,561 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,561 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,561 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,561 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,562 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,563 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,563 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,564 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,564 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:16,565 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:18,362 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17448, memsize=332.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/278a5c5061754f5f8cd26ca0d3ef7023
2014-07-22 13:22:18,379 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/278a5c5061754f5f8cd26ca0d3ef7023 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/278a5c5061754f5f8cd26ca0d3ef7023
2014-07-22 13:22:18,391 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/278a5c5061754f5f8cd26ca0d3ef7023, entries=1211370, sequenceid=17448, filesize=86.3m
2014-07-22 13:22:18,392 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~936.7m/982168000, currentsize=391.9m/410888800 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 30594ms, sequenceid=17448, compaction requested=true
2014-07-22 13:22:18,392 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:18,392 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1827ms
2014-07-22 13:22:18,392 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 13:22:18,392 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,392 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 878.8m
2014-07-22 13:22:18,392 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 13:22:18,392 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1828ms
2014-07-22 13:22:18,393 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:18,393 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,393 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:18,393 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1830ms
2014-07-22 13:22:18,393 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,393 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:22:18,393 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1830ms
2014-07-22 13:22:18,393 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,393 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1830ms
2014-07-22 13:22:18,394 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,394 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1833ms
2014-07-22 13:22:18,394 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,401 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1840ms
2014-07-22 13:22:18,401 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,402 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1841ms
2014-07-22 13:22:18,402 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,409 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1848ms
2014-07-22 13:22:18,409 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,413 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1853ms
2014-07-22 13:22:18,413 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,413 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1853ms
2014-07-22 13:22:18,414 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,414 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1854ms
2014-07-22 13:22:18,414 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,414 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1854ms
2014-07-22 13:22:18,414 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,414 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1883ms
2014-07-22 13:22:18,414 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,418 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1889ms
2014-07-22 13:22:18,419 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,429 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1901ms
2014-07-22 13:22:18,429 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,429 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1901ms
2014-07-22 13:22:18,430 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,430 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1905ms
2014-07-22 13:22:18,430 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,437 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1912ms
2014-07-22 13:22:18,437 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,437 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1912ms
2014-07-22 13:22:18,437 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,438 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1919ms
2014-07-22 13:22:18,438 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,438 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1914ms
2014-07-22 13:22:18,438 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,438 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1917ms
2014-07-22 13:22:18,438 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,445 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1923ms
2014-07-22 13:22:18,445 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,445 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1930ms
2014-07-22 13:22:18,445 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,453 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1929ms
2014-07-22 13:22:18,453 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,458 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1935ms
2014-07-22 13:22:18,458 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,458 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1935ms
2014-07-22 13:22:18,458 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,458 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1936ms
2014-07-22 13:22:18,458 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,458 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1936ms
2014-07-22 13:22:18,458 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,459 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1936ms
2014-07-22 13:22:18,459 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,459 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1937ms
2014-07-22 13:22:18,459 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,464 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1943ms
2014-07-22 13:22:18,464 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,464 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1943ms
2014-07-22 13:22:18,464 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,464 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1948ms
2014-07-22 13:22:18,464 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,473 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1956ms
2014-07-22 13:22:18,473 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,473 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1956ms
2014-07-22 13:22:18,473 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,474 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1957ms
2014-07-22 13:22:18,474 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,481 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1963ms
2014-07-22 13:22:18,481 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,481 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1963ms
2014-07-22 13:22:18,481 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,485 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1967ms
2014-07-22 13:22:18,485 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,485 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1967ms
2014-07-22 13:22:18,485 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,486 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3189ms
2014-07-22 13:22:18,486 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,487 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3189ms
2014-07-22 13:22:18,487 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,487 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3189ms
2014-07-22 13:22:18,487 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,491 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3194ms
2014-07-22 13:22:18,491 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,497 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3200ms
2014-07-22 13:22:18,497 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,497 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3200ms
2014-07-22 13:22:18,497 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,498 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3202ms
2014-07-22 13:22:18,498 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,498 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3203ms
2014-07-22 13:22:18,498 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:18,584 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:22:18,669 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10623,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060528045,"queuetimems":2408,"class":"HRegionServer","responsesize":15862,"method":"Multi"}
2014-07-22 13:22:18,921 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:18,952 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88433 synced till here 88407
2014-07-22 13:22:19,133 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060534826 with entries=114, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060538922
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060418518
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060420961
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060422861
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060424748
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060426721
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060428676
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060431300
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060433414
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060436022
2014-07-22 13:22:19,134 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060438072
2014-07-22 13:22:20,088 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:20,440 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:20,459 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88536 synced till here 88523
2014-07-22 13:22:20,690 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060538922 with entries=103, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060540441
2014-07-22 13:22:22,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:22,345 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88648 synced till here 88635
2014-07-22 13:22:22,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060540441 with entries=112, filesize=79.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060542187
2014-07-22 13:22:22,371 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:23,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:23,826 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88741 synced till here 88739
2014-07-22 13:22:23,841 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060542187 with entries=93, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060543800
2014-07-22 13:22:23,842 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:24,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:25,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88827 synced till here 88826
2014-07-22 13:22:25,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060543800 with entries=86, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060544894
2014-07-22 13:22:25,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:26,458 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:27,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 88930 synced till here 88926
2014-07-22 13:22:27,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060544894 with entries=103, filesize=78.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060546459
2014-07-22 13:22:27,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:28,390 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:28,415 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89017 synced till here 89016
2014-07-22 13:22:28,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060546459 with entries=87, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060548391
2014-07-22 13:22:28,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:28,644 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,675 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,688 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,698 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,700 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,710 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,715 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,716 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,766 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,803 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:29,835 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:30,422 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:22:31,608 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17582, memsize=396.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8e4c0b592d894bc1aa10ce493da851dd
2014-07-22 13:22:31,628 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/8e4c0b592d894bc1aa10ce493da851dd as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8e4c0b592d894bc1aa10ce493da851dd
2014-07-22 13:22:31,641 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/8e4c0b592d894bc1aa10ce493da851dd, entries=1442450, sequenceid=17582, filesize=102.7m
2014-07-22 13:22:31,641 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~805.7m/844841200, currentsize=414.0m/434104960 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 28977ms, sequenceid=17582, compaction requested=true
2014-07-22 13:22:31,642 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:31,642 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 40 store files, 0 compacting, 40 eligible, 2000 blocking
2014-07-22 13:22:31,642 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1220ms
2014-07-22 13:22:31,642 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,642 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 40 files from compaction candidates
2014-07-22 13:22:31,642 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 774.6m
2014-07-22 13:22:31,642 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:31,642 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:31,642 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:22:31,645 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1810ms
2014-07-22 13:22:31,645 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,645 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1842ms
2014-07-22 13:22:31,645 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,646 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1880ms
2014-07-22 13:22:31,646 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,646 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1930ms
2014-07-22 13:22:31,646 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,646 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1931ms
2014-07-22 13:22:31,646 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,646 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1936ms
2014-07-22 13:22:31,646 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,647 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1947ms
2014-07-22 13:22:31,647 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,647 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1949ms
2014-07-22 13:22:31,647 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,649 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1961ms
2014-07-22 13:22:31,649 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,650 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1975ms
2014-07-22 13:22:31,651 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,651 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3007ms
2014-07-22 13:22:31,651 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:22:31,758 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:22:32,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:32,423 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:32,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89116 synced till here 89102
2014-07-22 13:22:32,625 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060548391 with entries=99, filesize=74.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060552242
2014-07-22 13:22:32,626 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:34,494 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:34,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89203 synced till here 89196
2014-07-22 13:22:34,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060552242 with entries=87, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060554494
2014-07-22 13:22:34,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:36,117 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:36,132 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89292 synced till here 89289
2014-07-22 13:22:36,164 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060554494 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060556118
2014-07-22 13:22:36,165 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:37,541 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:37,700 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89393 synced till here 89390
2014-07-22 13:22:38,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060556118 with entries=101, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060557541
2014-07-22 13:22:38,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:39,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:40,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89529 synced till here 89527
2014-07-22 13:22:40,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060557541 with entries=136, filesize=100.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060559623
2014-07-22 13:22:40,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:22:40,181 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17727, memsize=275.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/fcffd0f99a6743aebb15605efd709269
2014-07-22 13:22:40,193 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/fcffd0f99a6743aebb15605efd709269 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/fcffd0f99a6743aebb15605efd709269
2014-07-22 13:22:40,203 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/fcffd0f99a6743aebb15605efd709269, entries=1002530, sequenceid=17727, filesize=71.4m
2014-07-22 13:22:40,204 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~878.8m/921469840, currentsize=355.9m/373167760 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 21812ms, sequenceid=17727, compaction requested=true
2014-07-22 13:22:40,205 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:40,205 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 13:22:40,205 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 13:22:40,205 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 416.6m
2014-07-22 13:22:40,205 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:40,205 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:40,205 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:22:40,214 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:22:41,040 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:41,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:41,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89620 synced till here 89614
2014-07-22 13:22:41,415 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060559623 with entries=91, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060561346
2014-07-22 13:22:42,706 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:42,731 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89710 synced till here 89708
2014-07-22 13:22:42,759 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060561346 with entries=90, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060562706
2014-07-22 13:22:44,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:44,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89853 synced till here 89852
2014-07-22 13:22:44,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060562706 with entries=143, filesize=105.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060564222
2014-07-22 13:22:46,885 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:48,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 89981 synced till here 89972
2014-07-22 13:22:48,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060564222 with entries=128, filesize=97.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060566886
2014-07-22 13:22:48,981 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:48,997 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90069 synced till here 90068
2014-07-22 13:22:49,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060566886 with entries=88, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060568982
2014-07-22 13:22:50,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:50,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90155 synced till here 90154
2014-07-22 13:22:50,447 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060568982 with entries=86, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060570410
2014-07-22 13:22:51,887 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:52,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90263 synced till here 90261
2014-07-22 13:22:52,335 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060570410 with entries=108, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060571887
2014-07-22 13:22:52,525 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17915, memsize=183.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/011be0641cc84fcdbeaa634184223c66
2014-07-22 13:22:52,547 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/011be0641cc84fcdbeaa634184223c66 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/011be0641cc84fcdbeaa634184223c66
2014-07-22 13:22:52,568 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/011be0641cc84fcdbeaa634184223c66, entries=669770, sequenceid=17915, filesize=47.7m
2014-07-22 13:22:52,569 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~418.0m/438269040, currentsize=69.3m/72656240 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 12364ms, sequenceid=17915, compaction requested=true
2014-07-22 13:22:52,569 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:52,570 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 23 store files, 0 compacting, 23 eligible, 2000 blocking
2014-07-22 13:22:52,570 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 23 files from compaction candidates
2014-07-22 13:22:52,570 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 992.7m
2014-07-22 13:22:52,570 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:52,570 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:52,570 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:22:53,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:53,864 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90350 synced till here 90347
2014-07-22 13:22:53,911 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060571887 with entries=87, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060573846
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060457940
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060465728
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060467251
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060468760
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060469986
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060471627
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060473044
2014-07-22 13:22:53,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060474717
2014-07-22 13:22:53,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060476821
2014-07-22 13:22:53,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060478947
2014-07-22 13:22:53,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060481078
2014-07-22 13:22:53,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060483352
2014-07-22 13:22:53,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060487048
2014-07-22 13:22:54,202 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:55,194 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:55,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90441 synced till here 90435
2014-07-22 13:22:55,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060573846 with entries=91, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060575194
2014-07-22 13:22:55,294 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=17863, memsize=328.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/877ee32937704fe6b3fddf7c29884e25
2014-07-22 13:22:55,307 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/877ee32937704fe6b3fddf7c29884e25 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/877ee32937704fe6b3fddf7c29884e25
2014-07-22 13:22:55,334 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/877ee32937704fe6b3fddf7c29884e25, entries=1194310, sequenceid=17863, filesize=85.1m
2014-07-22 13:22:55,335 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~774.6m/812277200, currentsize=437.7m/458951920 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 23693ms, sequenceid=17863, compaction requested=true
2014-07-22 13:22:55,336 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:22:55,336 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 849.3m
2014-07-22 13:22:55,336 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 13:22:55,336 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 13:22:55,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:22:55,337 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:22:55,337 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:22:55,338 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:22:56,330 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:56,346 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:22:56,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90530 synced till here 90527
2014-07-22 13:22:56,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060575194 with entries=89, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060576330
2014-07-22 13:22:56,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060489435
2014-07-22 13:22:56,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060495228
2014-07-22 13:22:56,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060498155
2014-07-22 13:22:56,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060500655
2014-07-22 13:22:56,442 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060502353
2014-07-22 13:22:57,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:57,316 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90620 synced till here 90615
2014-07-22 13:22:57,975 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060576330 with entries=90, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060577262
2014-07-22 13:22:59,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:22:59,457 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90713 synced till here 90704
2014-07-22 13:22:59,570 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060577262 with entries=93, filesize=69.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060579299
2014-07-22 13:23:01,178 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:01,195 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90800 synced till here 90797
2014-07-22 13:23:01,223 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060579299 with entries=87, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060581179
2014-07-22 13:23:02,719 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:02,752 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90889 synced till here 90884
2014-07-22 13:23:02,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060581179 with entries=89, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060582719
2014-07-22 13:23:04,491 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:04,764 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 90995 synced till here 90988
2014-07-22 13:23:04,904 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060582719 with entries=106, filesize=78.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060584492
2014-07-22 13:23:06,979 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:07,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91092 synced till here 91071
2014-07-22 13:23:07,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060584492 with entries=97, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060586979
2014-07-22 13:23:09,191 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:09,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91204 synced till here 91180
2014-07-22 13:23:09,407 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,408 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,408 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,408 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,409 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,409 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,410 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,412 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,413 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,413 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,413 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,413 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,414 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,414 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,415 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,416 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,417 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,419 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,439 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,440 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,440 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,441 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,441 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,444 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060586979 with entries=112, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060589191
2014-07-22 13:23:09,481 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,512 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:09,547 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,692 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1039ms
GC pool 'ParNew' had collection(s): count=1 time=1118ms
2014-07-22 13:23:10,692 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,693 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,693 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,694 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,694 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,696 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,700 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,700 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,701 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,710 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,724 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,724 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,724 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,724 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,724 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,725 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,726 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,727 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,728 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,761 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,793 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,826 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:10,860 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:14,592 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1399ms
GC pool 'ParNew' had collection(s): count=1 time=1560ms
2014-07-22 13:23:14,593 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5030ms
2014-07-22 13:23:14,593 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5152ms
2014-07-22 13:23:14,594 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5113ms
2014-07-22 13:23:14,595 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5083ms
2014-07-22 13:23:14,595 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-22 13:23:14,595 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-22 13:23:14,595 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5187ms
2014-07-22 13:23:14,596 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-22 13:23:14,596 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-22 13:23:14,597 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5187ms
2014-07-22 13:23:14,598 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-22 13:23:14,598 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5188ms
2014-07-22 13:23:14,598 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5186ms
2014-07-22 13:23:14,598 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 13:23:14,598 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5186ms
2014-07-22 13:23:14,599 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 13:23:14,599 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5186ms
2014-07-22 13:23:14,599 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 13:23:14,599 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 13:23:14,600 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5186ms
2014-07-22 13:23:14,600 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5184ms
2014-07-22 13:23:14,601 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5183ms
2014-07-22 13:23:14,601 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5162ms
2014-07-22 13:23:14,601 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5182ms
2014-07-22 13:23:14,602 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5163ms
2014-07-22 13:23:14,602 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5163ms
2014-07-22 13:23:14,602 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5162ms
2014-07-22 13:23:15,693 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,693 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,694 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,694 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,695 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,696 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,700 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,700 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,701 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,711 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,724 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,724 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,724 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,725 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,725 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:15,726 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,726 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,727 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,728 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,762 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,793 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,827 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:15,860 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:20,540 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11100ms
2014-07-22 13:23:20,541 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11102ms
2014-07-22 13:23:20,542 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11102ms
2014-07-22 13:23:20,543 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11128ms
2014-07-22 13:23:20,544 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11100ms
2014-07-22 13:23:20,544 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11130ms
2014-07-22 13:23:20,546 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11128ms
2014-07-22 13:23:20,546 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11130ms
2014-07-22 13:23:20,548 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11128ms
2014-07-22 13:23:20,548 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11109ms
2014-07-22 13:23:20,549 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11107ms
2014-07-22 13:23:20,549 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11069ms
2014-07-22 13:23:20,549 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11037ms
2014-07-22 13:23:20,550 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11003ms
2014-07-22 13:23:20,550 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11143ms
2014-07-22 13:23:20,550 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-07-22 13:23:20,551 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-07-22 13:23:20,551 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11143ms
2014-07-22 13:23:20,552 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11143ms
2014-07-22 13:23:20,552 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-07-22 13:23:20,552 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11143ms
2014-07-22 13:23:20,553 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11140ms
2014-07-22 13:23:20,554 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11139ms
2014-07-22 13:23:20,554 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-07-22 13:23:20,554 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-07-22 13:23:20,555 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11143ms
2014-07-22 13:23:20,556 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-07-22 13:23:20,693 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,694 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:20,694 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,694 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:23:20,695 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,697 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,700 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:23:20,701 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,702 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,711 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,724 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,725 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,726 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:20,727 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:20,728 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 13:23:20,728 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,729 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:20,729 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-22 13:23:20,729 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,762 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,794 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,827 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:20,861 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:25,439 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18135, memsize=513.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/eea7778d58fd482c80154f5371419b64
2014-07-22 13:23:25,453 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/eea7778d58fd482c80154f5371419b64 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/eea7778d58fd482c80154f5371419b64
2014-07-22 13:23:25,462 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/eea7778d58fd482c80154f5371419b64, entries=1868660, sequenceid=18135, filesize=133.0m
2014-07-22 13:23:25,463 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~854.0m/895496560, currentsize=247.6m/259616160 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 30127ms, sequenceid=18135, compaction requested=true
2014-07-22 13:23:25,463 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:23:25,463 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 13:23:25,463 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 13:23:25,463 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14603ms
2014-07-22 13:23:25,464 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:23:25,464 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 887.0m
2014-07-22 13:23:25,464 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,464 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:23:25,464 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14638ms
2014-07-22 13:23:25,464 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:23:25,464 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,464 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14671ms
2014-07-22 13:23:25,464 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,465 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14704ms
2014-07-22 13:23:25,465 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,467 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14739ms
2014-07-22 13:23:25,467 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,467 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14742ms
2014-07-22 13:23:25,467 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,467 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14741ms
2014-07-22 13:23:25,467 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,468 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14741ms
2014-07-22 13:23:25,468 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,469 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14745ms
2014-07-22 13:23:25,469 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,476 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14752ms
2014-07-22 13:23:25,476 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,476 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14752ms
2014-07-22 13:23:25,476 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,477 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14752ms
2014-07-22 13:23:25,477 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,482 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14758ms
2014-07-22 13:23:25,482 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,482 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14772ms
2014-07-22 13:23:25,483 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,489 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14788ms
2014-07-22 13:23:25,489 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,490 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14790ms
2014-07-22 13:23:25,490 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,490 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14790ms
2014-07-22 13:23:25,490 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,490 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14794ms
2014-07-22 13:23:25,490 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,520 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14826ms
2014-07-22 13:23:25,521 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,522 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14827ms
2014-07-22 13:23:25,522 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,523 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14829ms
2014-07-22 13:23:25,523 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,523 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14831ms
2014-07-22 13:23:25,523 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,523 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14831ms
2014-07-22 13:23:25,523 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,532 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16118ms
2014-07-22 13:23:25,532 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,534 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16122ms
2014-07-22 13:23:25,534 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,534 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16121ms
2014-07-22 13:23:25,534 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,535 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16121ms
2014-07-22 13:23:25,535 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,535 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16121ms
2014-07-22 13:23:25,535 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,536 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16124ms
2014-07-22 13:23:25,536 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,542 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16101ms
2014-07-22 13:23:25,543 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,544 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16130ms
2014-07-22 13:23:25,544 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,545 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16100ms
2014-07-22 13:23:25,545 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,546 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16129ms
2014-07-22 13:23:25,546 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,547 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16131ms
2014-07-22 13:23:25,547 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,548 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16129ms
2014-07-22 13:23:25,548 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,549 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16108ms
2014-07-22 13:23:25,549 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,550 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16069ms
2014-07-22 13:23:25,551 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,551 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16143ms
2014-07-22 13:23:25,552 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,552 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16144ms
2014-07-22 13:23:25,552 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,552 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16143ms
2014-07-22 13:23:25,552 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,553 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16144ms
2014-07-22 13:23:25,553 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,558 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16118ms
2014-07-22 13:23:25,558 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,558 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16148ms
2014-07-22 13:23:25,559 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,559 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16151ms
2014-07-22 13:23:25,559 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,560 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16152ms
2014-07-22 13:23:25,560 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,564 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16017ms
2014-07-22 13:23:25,564 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,570 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16057ms
2014-07-22 13:23:25,570 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,570 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16131ms
2014-07-22 13:23:25,570 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,571 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16157ms
2014-07-22 13:23:25,571 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:25,572 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16132ms
2014-07-22 13:23:25,572 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:23:27,093 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20330,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586761,"queuetimems":1,"class":"HRegionServer","responsesize":16579,"method":"Multi"}
2014-07-22 13:23:27,093 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586835,"queuetimems":1,"class":"HRegionServer","responsesize":17035,"method":"Multi"}
2014-07-22 13:23:27,093 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20296,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586795,"queuetimems":1,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:23:27,093 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20375,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586718,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:23:27,094 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20204,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586888,"queuetimems":0,"class":"HRegionServer","responsesize":16591,"method":"Multi"}
2014-07-22 13:23:27,113 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:23:27,279 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587222,"queuetimems":0,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-07-22 13:23:27,294 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587530,"queuetimems":1,"class":"HRegionServer","responsesize":17025,"method":"Multi"}
2014-07-22 13:23:27,298 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588688,"queuetimems":0,"class":"HRegionServer","responsesize":16651,"method":"Multi"}
2014-07-22 13:23:27,298 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20044,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587253,"queuetimems":1,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-07-22 13:23:27,428 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:23:27,498 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:27,500 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18837,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588663,"queuetimems":1,"class":"HRegionServer","responsesize":16732,"method":"Multi"}
2014-07-22 13:23:27,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91335 synced till here 91299
2014-07-22 13:23:27,681 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588954,"queuetimems":0,"class":"HRegionServer","responsesize":17258,"method":"Multi"}
2014-07-22 13:23:27,682 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587315,"queuetimems":0,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:23:27,683 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587068,"queuetimems":1,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:23:27,690 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20579,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587111,"queuetimems":1,"class":"HRegionServer","responsesize":16579,"method":"Multi"}
2014-07-22 13:23:27,690 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20406,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587284,"queuetimems":1,"class":"HRegionServer","responsesize":17010,"method":"Multi"}
2014-07-22 13:23:27,691 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587349,"queuetimems":1,"class":"HRegionServer","responsesize":16969,"method":"Multi"}
2014-07-22 13:23:27,691 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20541,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587149,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:23:27,714 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587388,"queuetimems":1,"class":"HRegionServer","responsesize":17398,"method":"Multi"}
2014-07-22 13:23:27,715 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587024,"queuetimems":0,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-22 13:23:27,718 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588909,"queuetimems":0,"class":"HRegionServer","responsesize":16895,"method":"Multi"}
2014-07-22 13:23:27,722 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20736,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586985,"queuetimems":0,"class":"HRegionServer","responsesize":16626,"method":"Multi"}
2014-07-22 13:23:27,726 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20774,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060586951,"queuetimems":1,"class":"HRegionServer","responsesize":16848,"method":"Multi"}
2014-07-22 13:23:27,714 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20222,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060587491,"queuetimems":0,"class":"HRegionServer","responsesize":16919,"method":"Multi"}
2014-07-22 13:23:27,743 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588713,"queuetimems":1,"class":"HRegionServer","responsesize":17184,"method":"Multi"}
2014-07-22 13:23:27,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18966,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588783,"queuetimems":0,"class":"HRegionServer","responsesize":17022,"method":"Multi"}
2014-07-22 13:23:27,750 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18930,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588819,"queuetimems":0,"class":"HRegionServer","responsesize":17034,"method":"Multi"}
2014-07-22 13:23:27,751 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19003,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588747,"queuetimems":0,"class":"HRegionServer","responsesize":16917,"method":"Multi"}
2014-07-22 13:23:27,750 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18886,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588863,"queuetimems":0,"class":"HRegionServer","responsesize":17110,"method":"Multi"}
2014-07-22 13:23:27,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060589191 with entries=131, filesize=95.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060607498
2014-07-22 13:23:29,088 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19774,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589314,"queuetimems":1,"class":"HRegionServer","responsesize":17047,"method":"Multi"}
2014-07-22 13:23:29,296 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589377,"queuetimems":1,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 13:23:29,296 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19952,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589344,"queuetimems":0,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 13:23:29,454 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20232,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589222,"queuetimems":1,"class":"HRegionServer","responsesize":16643,"method":"Multi"}
2014-07-22 13:23:29,456 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20282,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589174,"queuetimems":1,"class":"HRegionServer","responsesize":16784,"method":"Multi"}
2014-07-22 13:23:29,458 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20404,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589053,"queuetimems":1,"class":"HRegionServer","responsesize":17424,"method":"Multi"}
2014-07-22 13:23:29,458 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590858,"queuetimems":0,"class":"HRegionServer","responsesize":16784,"method":"Multi"}
2014-07-22 13:23:29,458 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20191,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589266,"queuetimems":0,"class":"HRegionServer","responsesize":17095,"method":"Multi"}
2014-07-22 13:23:29,458 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20335,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589123,"queuetimems":0,"class":"HRegionServer","responsesize":16969,"method":"Multi"}
2014-07-22 13:23:29,459 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20437,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589021,"queuetimems":0,"class":"HRegionServer","responsesize":17142,"method":"Multi"}
2014-07-22 13:23:29,466 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060588987,"queuetimems":1,"class":"HRegionServer","responsesize":16782,"method":"Multi"}
2014-07-22 13:23:29,677 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:29,693 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91447 synced till here 91419
2014-07-22 13:23:30,157 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060607498 with entries=112, filesize=89.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060609678
2014-07-22 13:23:31,223 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20463,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590759,"queuetimems":1,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-07-22 13:23:31,230 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589542,"queuetimems":1,"class":"HRegionServer","responsesize":17047,"method":"Multi"}
2014-07-22 13:23:31,230 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589477,"queuetimems":0,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-22 13:23:31,242 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21732,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589509,"queuetimems":0,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 13:23:31,242 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20534,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590707,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:23:31,243 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589442,"queuetimems":0,"class":"HRegionServer","responsesize":17071,"method":"Multi"}
2014-07-22 13:23:31,243 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060589411,"queuetimems":1,"class":"HRegionServer","responsesize":17140,"method":"Multi"}
2014-07-22 13:23:31,243 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590695,"queuetimems":1,"class":"HRegionServer","responsesize":16643,"method":"Multi"}
2014-07-22 13:23:31,244 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20518,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590726,"queuetimems":1,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-07-22 13:23:31,245 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590825,"queuetimems":1,"class":"HRegionServer","responsesize":16579,"method":"Multi"}
2014-07-22 13:23:31,245 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060590791,"queuetimems":1,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:23:31,615 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18093, memsize=587.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0b44ca560fdf4312860955cddbf04859
2014-07-22 13:23:31,634 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/0b44ca560fdf4312860955cddbf04859 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0b44ca560fdf4312860955cddbf04859
2014-07-22 13:23:31,644 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/0b44ca560fdf4312860955cddbf04859, entries=2138110, sequenceid=18093, filesize=152.3m
2014-07-22 13:23:31,644 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~994.2m/1042520720, currentsize=368.7m/386647200 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 39074ms, sequenceid=18093, compaction requested=true
2014-07-22 13:23:31,645 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:23:31,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 41 store files, 0 compacting, 41 eligible, 2000 blocking
2014-07-22 13:23:31,646 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 781.3m
2014-07-22 13:23:31,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 41 files from compaction candidates
2014-07-22 13:23:31,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:23:31,646 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:23:31,646 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:23:31,702 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:31,704 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:23:31,749 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91567 synced till here 91536
2014-07-22 13:23:31,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060609678 with entries=120, filesize=84.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060611703
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060503871
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060508078
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060509807
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060511203
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060512869
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060514381
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060516012
2014-07-22 13:23:31,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060518079
2014-07-22 13:23:31,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060523577
2014-07-22 13:23:31,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060527583
2014-07-22 13:23:31,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060529851
2014-07-22 13:23:31,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060532369
2014-07-22 13:23:33,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:33,863 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:23:33,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91703 synced till here 91657
2014-07-22 13:23:34,222 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060611703 with entries=136, filesize=104.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060613761
2014-07-22 13:23:35,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:36,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060613761 with entries=125, filesize=77.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060615828
2014-07-22 13:23:37,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:37,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91942 synced till here 91906
2014-07-22 13:23:38,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060615828 with entries=114, filesize=88.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060617781
2014-07-22 13:23:39,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:39,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92061 synced till here 92017
2014-07-22 13:23:39,953 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060617781 with entries=119, filesize=97.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060619472
2014-07-22 13:23:40,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:41,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060619472 with entries=120, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060620714
2014-07-22 13:23:42,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:42,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92299 synced till here 92263
2014-07-22 13:23:42,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060620714 with entries=118, filesize=92.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060622091
2014-07-22 13:23:44,231 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:44,277 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:23:44,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92445 synced till here 92436
2014-07-22 13:23:44,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060622091 with entries=146, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060624232
2014-07-22 13:23:46,248 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:23:46,281 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92549 synced till here 92527
2014-07-22 13:23:46,435 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060624232 with entries=104, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060626249
2014-07-22 13:23:47,289 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,289 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,290 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,290 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,290 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,303 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,303 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,358 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,378 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,626 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:47,655 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:49,611 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:49,794 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:49,825 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:49,856 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:52,289 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:52,290 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:52,290 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:52,290 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:52,290 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:52,303 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:52,304 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:52,358 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:52,379 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:52,626 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:52,655 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:53,618 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,649 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,681 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,716 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,768 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,800 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,830 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,862 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,893 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,934 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:53,973 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:54,611 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:54,794 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:54,825 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:54,856 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:56,458 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,492 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,526 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,561 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,597 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,625 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,658 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,689 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,722 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,755 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,787 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,819 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,850 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,885 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,916 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,949 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:56,980 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:57,291 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:57,291 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:57,292 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:23:57,292 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 13:23:57,293 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-07-22 13:23:57,303 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:23:57,304 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:57,359 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:57,380 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:57,627 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:57,655 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:23:58,618 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,649 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:58,681 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:58,705 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:58,716 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,739 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:58,769 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:58,770 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:58,800 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:23:58,802 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:23:58,831 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,862 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,893 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,934 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:58,973 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:23:59,612 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:59,795 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:59,825 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:23:59,857 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:23:59,875 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:24:00,401 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18391, memsize=557.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ec0cd813997649f782f735bec0f4dd31
2014-07-22 13:24:00,415 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ec0cd813997649f782f735bec0f4dd31 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ec0cd813997649f782f735bec0f4dd31
2014-07-22 13:24:00,424 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ec0cd813997649f782f735bec0f4dd31, entries=2029510, sequenceid=18391, filesize=144.5m
2014-07-22 13:24:00,424 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~816.5m/856196640, currentsize=277.8m/291317760 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 28778ms, sequenceid=18391, compaction requested=true
2014-07-22 13:24:00,425 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:00,425 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 13:24:00,425 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 13:24:00,425 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:00,425 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 656.5m
2014-07-22 13:24:00,425 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:00,426 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:24:00,426 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 550ms
2014-07-22 13:24:00,426 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,426 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10570ms
2014-07-22 13:24:00,426 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,426 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10601ms
2014-07-22 13:24:00,426 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,426 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10633ms
2014-07-22 13:24:00,426 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,429 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10818ms
2014-07-22 13:24:00,429 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,430 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6457ms
2014-07-22 13:24:00,431 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,431 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6497ms
2014-07-22 13:24:00,431 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,441 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6547ms
2014-07-22 13:24:00,441 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,441 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6579ms
2014-07-22 13:24:00,441 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,442 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6612ms
2014-07-22 13:24:00,442 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,442 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1640ms
2014-07-22 13:24:00,442 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,442 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6643ms
2014-07-22 13:24:00,442 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,444 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1673ms
2014-07-22 13:24:00,444 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,444 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6676ms
2014-07-22 13:24:00,444 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,445 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1705ms
2014-07-22 13:24:00,445 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,450 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6733ms
2014-07-22 13:24:00,450 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,451 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1745ms
2014-07-22 13:24:00,451 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,458 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6777ms
2014-07-22 13:24:00,458 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,458 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6810ms
2014-07-22 13:24:00,458 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,459 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6841ms
2014-07-22 13:24:00,459 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,459 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12804ms
2014-07-22 13:24:00,459 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,459 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12833ms
2014-07-22 13:24:00,459 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,462 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13084ms
2014-07-22 13:24:00,462 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,463 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13105ms
2014-07-22 13:24:00,463 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,463 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13160ms
2014-07-22 13:24:00,463 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,463 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13160ms
2014-07-22 13:24:00,463 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,465 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13176ms
2014-07-22 13:24:00,466 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,466 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13177ms
2014-07-22 13:24:00,467 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,467 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13178ms
2014-07-22 13:24:00,467 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,473 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13184ms
2014-07-22 13:24:00,473 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,477 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13188ms
2014-07-22 13:24:00,477 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,478 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3497ms
2014-07-22 13:24:00,478 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,486 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3536ms
2014-07-22 13:24:00,486 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,490 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3573ms
2014-07-22 13:24:00,490 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,491 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3605ms
2014-07-22 13:24:00,491 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,491 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3641ms
2014-07-22 13:24:00,491 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,491 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3672ms
2014-07-22 13:24:00,492 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,494 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3707ms
2014-07-22 13:24:00,494 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,501 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3746ms
2014-07-22 13:24:00,501 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,506 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3784ms
2014-07-22 13:24:00,507 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,507 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3818ms
2014-07-22 13:24:00,507 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,507 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3849ms
2014-07-22 13:24:00,507 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,508 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3882ms
2014-07-22 13:24:00,508 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,509 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3911ms
2014-07-22 13:24:00,509 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,510 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3949ms
2014-07-22 13:24:00,510 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,513 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3987ms
2014-07-22 13:24:00,514 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,515 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4022ms
2014-07-22 13:24:00,515 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,521 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4063ms
2014-07-22 13:24:00,521 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:24:00,530 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14629,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625900,"queuetimems":0,"class":"HRegionServer","responsesize":17140,"method":"Multi"}
2014-07-22 13:24:00,531 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625663,"queuetimems":0,"class":"HRegionServer","responsesize":17142,"method":"Multi"}
2014-07-22 13:24:00,531 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060626033,"queuetimems":0,"class":"HRegionServer","responsesize":16966,"method":"Multi"}
2014-07-22 13:24:00,532 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625829,"queuetimems":0,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-22 13:24:00,530 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625998,"queuetimems":0,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-22 13:24:00,532 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625776,"queuetimems":0,"class":"HRegionServer","responsesize":17110,"method":"Multi"}
2014-07-22 13:24:00,532 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625934,"queuetimems":0,"class":"HRegionServer","responsesize":17071,"method":"Multi"}
2014-07-22 13:24:00,533 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14838,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625695,"queuetimems":0,"class":"HRegionServer","responsesize":17022,"method":"Multi"}
2014-07-22 13:24:00,540 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060625726,"queuetimems":0,"class":"HRegionServer","responsesize":16591,"method":"Multi"}
2014-07-22 13:24:00,946 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:24:00,997 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:01,739 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:01,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060626249 with entries=95, filesize=66.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060641740
2014-07-22 13:24:02,153 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18298, memsize=676.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3ad05a96edda46f681aa40646bca1ae5
2014-07-22 13:24:02,178 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/3ad05a96edda46f681aa40646bca1ae5 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3ad05a96edda46f681aa40646bca1ae5
2014-07-22 13:24:02,291 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/3ad05a96edda46f681aa40646bca1ae5, entries=2462280, sequenceid=18298, filesize=175.2m
2014-07-22 13:24:02,293 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~887.0m/930094160, currentsize=429.0m/449843600 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 36829ms, sequenceid=18298, compaction requested=true
2014-07-22 13:24:02,293 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:02,293 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 13:24:02,293 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 13:24:02,294 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 721.7m
2014-07-22 13:24:02,294 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:02,294 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:02,294 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:24:02,441 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:24:02,659 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:02,660 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12869,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060629791,"queuetimems":0,"class":"HRegionServer","responsesize":17398,"method":"Multi"}
2014-07-22 13:24:02,660 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12806,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060629854,"queuetimems":1,"class":"HRegionServer","responsesize":16776,"method":"Multi"}
2014-07-22 13:24:02,713 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060641740 with entries=93, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060642659
2014-07-22 13:24:02,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060534826
2014-07-22 13:24:02,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060538922
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060540441
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060542187
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060543800
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060544894
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060546459
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060548391
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060552242
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060554494
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060556118
2014-07-22 13:24:02,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060557541
2014-07-22 13:24:02,727 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12903,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060629823,"queuetimems":1,"class":"HRegionServer","responsesize":17010,"method":"Multi"}
2014-07-22 13:24:03,614 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060627622,"queuetimems":2,"class":"HRegionServer","responsesize":17095,"method":"Multi"}
2014-07-22 13:24:03,620 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:03,656 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14047,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060629609,"queuetimems":0,"class":"HRegionServer","responsesize":16818,"method":"Multi"}
2014-07-22 13:24:03,656 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10041,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060633615,"queuetimems":0,"class":"HRegionServer","responsesize":17184,"method":"Multi"}
2014-07-22 13:24:03,658 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060633647,"queuetimems":1,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-07-22 13:24:03,658 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16005,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060627653,"queuetimems":1,"class":"HRegionServer","responsesize":17047,"method":"Multi"}
2014-07-22 13:24:04,042 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:04,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060642659 with entries=103, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060644043
2014-07-22 13:24:06,446 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:06,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 92926 synced till here 92923
2014-07-22 13:24:06,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060644043 with entries=86, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060646446
2014-07-22 13:24:07,964 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:07,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93013 synced till here 93010
2014-07-22 13:24:08,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060646446 with entries=87, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060647965
2014-07-22 13:24:08,030 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:24:08,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:09,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93102 synced till here 93100
2014-07-22 13:24:09,397 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060647965 with entries=89, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060648944
2014-07-22 13:24:09,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:24:10,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:10,953 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93201 synced till here 93198
2014-07-22 13:24:10,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060648944 with entries=99, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060650220
2014-07-22 13:24:10,995 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:24:11,671 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18567, memsize=177.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7859ab1f393d4ab39b83a5f2b843dea0
2014-07-22 13:24:11,693 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/7859ab1f393d4ab39b83a5f2b843dea0 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7859ab1f393d4ab39b83a5f2b843dea0
2014-07-22 13:24:11,705 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/7859ab1f393d4ab39b83a5f2b843dea0, entries=645590, sequenceid=18567, filesize=46.0m
2014-07-22 13:24:11,705 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~656.5m/688380480, currentsize=212.6m/222902400 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 11280ms, sequenceid=18567, compaction requested=true
2014-07-22 13:24:11,706 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 13:24:11,706 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:11,706 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 13:24:11,706 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:11,706 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 343.6m
2014-07-22 13:24:11,706 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:11,706 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:24:11,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:11,801 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93287 synced till here 93283
2014-07-22 13:24:11,838 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060650220 with entries=86, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060651780
2014-07-22 13:24:12,089 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:12,905 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:12,924 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93374 synced till here 93372
2014-07-22 13:24:12,976 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060651780 with entries=87, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060652905
2014-07-22 13:24:13,918 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:24:14,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:14,349 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93462 synced till here 93460
2014-07-22 13:24:14,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060652905 with entries=88, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060654333
2014-07-22 13:24:15,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:15,898 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18560, memsize=236.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/502b994ade9e45d2b478512d40011b49
2014-07-22 13:24:15,900 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93558 synced till here 93557
2014-07-22 13:24:15,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060654333 with entries=96, filesize=69.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060655734
2014-07-22 13:24:15,913 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/502b994ade9e45d2b478512d40011b49 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/502b994ade9e45d2b478512d40011b49
2014-07-22 13:24:15,924 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/502b994ade9e45d2b478512d40011b49, entries=861250, sequenceid=18560, filesize=61.4m
2014-07-22 13:24:15,925 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~724.8m/759955920, currentsize=291.9m/306030080 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 13631ms, sequenceid=18560, compaction requested=true
2014-07-22 13:24:15,925 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:15,925 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 42 store files, 0 compacting, 42 eligible, 2000 blocking
2014-07-22 13:24:15,925 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 42 files from compaction candidates
2014-07-22 13:24:15,925 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 574.8m
2014-07-22 13:24:15,925 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:15,926 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:15,926 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:24:16,146 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:24:16,384 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:17,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:17,726 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93645 synced till here 93642
2014-07-22 13:24:17,777 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060655734 with entries=87, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060657682
2014-07-22 13:24:18,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:19,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93750 synced till here 93745
2014-07-22 13:24:19,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060657682 with entries=105, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060658651
2014-07-22 13:24:19,725 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:24:20,172 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:20,892 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93847 synced till here 93845
2014-07-22 13:24:20,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060658651 with entries=97, filesize=72.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060660173
2014-07-22 13:24:21,076 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18662, memsize=162.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/7637401f49234d7f9166295fca256b2f
2014-07-22 13:24:21,091 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/7637401f49234d7f9166295fca256b2f as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/7637401f49234d7f9166295fca256b2f
2014-07-22 13:24:21,102 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/7637401f49234d7f9166295fca256b2f, entries=590110, sequenceid=18662, filesize=42.1m
2014-07-22 13:24:21,103 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~344.1m/360855440, currentsize=54.1m/56703040 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 9397ms, sequenceid=18662, compaction requested=true
2014-07-22 13:24:21,104 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:21,104 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 813.8m
2014-07-22 13:24:21,104 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 24 store files, 0 compacting, 24 eligible, 2000 blocking
2014-07-22 13:24:21,105 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 24 files from compaction candidates
2014-07-22 13:24:21,105 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:21,105 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:21,105 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:24:21,749 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:21,776 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 93935 synced till here 93934
2014-07-22 13:24:21,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060660173 with entries=88, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060661750
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060559623
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060561346
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060562706
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060564222
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060566886
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060568982
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060570410
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060571887
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060573846
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060575194
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060576330
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060577262
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060579299
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060581179
2014-07-22 13:24:21,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060582719
2014-07-22 13:24:21,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060584492
2014-07-22 13:24:21,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060586979
2014-07-22 13:24:22,246 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:22,828 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:22,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94018 synced till here 94017
2014-07-22 13:24:22,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060661750 with entries=83, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060662829
2014-07-22 13:24:24,019 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:24,052 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94108 synced till here 94098
2014-07-22 13:24:24,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060662829 with entries=90, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060664020
2014-07-22 13:24:25,101 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:25,132 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94197 synced till here 94196
2014-07-22 13:24:25,175 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060664020 with entries=89, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060665101
2014-07-22 13:24:26,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:26,756 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94292 synced till here 94286
2014-07-22 13:24:26,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060665101 with entries=95, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060666619
2014-07-22 13:24:28,221 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:28,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94381 synced till here 94379
2014-07-22 13:24:28,269 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060666619 with entries=89, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668222
2014-07-22 13:24:28,945 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:28,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94465 synced till here 94463
2014-07-22 13:24:28,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668222 with entries=84, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668946
2014-07-22 13:24:30,304 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:30,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94561 synced till here 94547
2014-07-22 13:24:30,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668946 with entries=96, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060670305
2014-07-22 13:24:31,790 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18771, memsize=233.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2ce3b15841b84ff6af8664b36ba3417b
2014-07-22 13:24:31,819 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/2ce3b15841b84ff6af8664b36ba3417b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2ce3b15841b84ff6af8664b36ba3417b
2014-07-22 13:24:31,863 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/2ce3b15841b84ff6af8664b36ba3417b, entries=851170, sequenceid=18771, filesize=60.6m
2014-07-22 13:24:31,864 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~576.3m/604257840, currentsize=345.9m/362664800 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 15939ms, sequenceid=18771, compaction requested=true
2014-07-22 13:24:31,865 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 13:24:31,866 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 13:24:31,866 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:31,866 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:31,866 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:24:31,871 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:31,872 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 635.5m
2014-07-22 13:24:31,884 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:24:32,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:32,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94667 synced till here 94639
2014-07-22 13:24:33,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060670305 with entries=106, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060672015
2014-07-22 13:24:33,956 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:34,069 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:34,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94776 synced till here 94755
2014-07-22 13:24:34,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060672015 with entries=109, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060674069
2014-07-22 13:24:36,259 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:36,301 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94889 synced till here 94876
2014-07-22 13:24:37,451 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060674069 with entries=113, filesize=85.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060676260
2014-07-22 13:24:38,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:38,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 94986 synced till here 94977
2014-07-22 13:24:38,383 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060676260 with entries=97, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060678249
2014-07-22 13:24:40,178 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:40,205 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95096 synced till here 95079
2014-07-22 13:24:40,308 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060678249 with entries=110, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060680179
2014-07-22 13:24:42,078 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:42,113 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95195 synced till here 95184
2014-07-22 13:24:42,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060680179 with entries=99, filesize=70.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060682078
2014-07-22 13:24:43,653 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1149ms
GC pool 'ParNew' had collection(s): count=1 time=1219ms
2014-07-22 13:24:44,157 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:44,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95293 synced till here 95277
2014-07-22 13:24:44,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060682078 with entries=98, filesize=73.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060684158
2014-07-22 13:24:45,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:45,740 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060684158 with entries=89, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060685695
2014-07-22 13:24:47,141 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:47,166 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95472 synced till here 95467
2014-07-22 13:24:47,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060685695 with entries=90, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060687141
2014-07-22 13:24:47,396 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18827, memsize=314.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/25416f465cdc441fb25279f77024a568
2014-07-22 13:24:47,411 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/25416f465cdc441fb25279f77024a568 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/25416f465cdc441fb25279f77024a568
2014-07-22 13:24:47,421 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/25416f465cdc441fb25279f77024a568, entries=1144480, sequenceid=18827, filesize=81.5m
2014-07-22 13:24:47,421 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~821.6m/861525040, currentsize=498.3m/522481040 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 26317ms, sequenceid=18827, compaction requested=true
2014-07-22 13:24:47,422 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:24:47,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 13:24:47,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 13:24:47,422 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 880.5m
2014-07-22 13:24:47,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:24:47,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:24:47,422 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:24:47,430 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:24:49,179 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1263ms
GC pool 'ParNew' had collection(s): count=1 time=1281ms
2014-07-22 13:24:49,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:49,309 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95563 synced till here 95557
2014-07-22 13:24:49,361 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060687141 with entries=91, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060689263
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060589191
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060607498
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060609678
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060611703
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060613761
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060615828
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060617781
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060619472
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060620714
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060622091
2014-07-22 13:24:49,362 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060624232
2014-07-22 13:24:49,622 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:24:51,128 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:51,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95666 synced till here 95656
2014-07-22 13:24:51,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060689263 with entries=103, filesize=75.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691129
2014-07-22 13:24:51,978 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:53,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95767 synced till here 95757
2014-07-22 13:24:53,329 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691129 with entries=101, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691979
2014-07-22 13:24:54,049 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:54,125 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95869 synced till here 95847
2014-07-22 13:24:55,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691979 with entries=102, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060694049
2014-07-22 13:24:57,252 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:57,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 95973 synced till here 95960
2014-07-22 13:24:57,437 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060694049 with entries=104, filesize=80.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060697252
2014-07-22 13:24:59,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:24:59,276 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:24:59,385 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96121 synced till here 96082
2014-07-22 13:24:59,641 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060697252 with entries=148, filesize=110.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060699096
2014-07-22 13:25:01,172 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1172ms
GC pool 'ParNew' had collection(s): count=1 time=1203ms
2014-07-22 13:25:01,559 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:01,568 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,571 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,571 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,572 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,573 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,583 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,584 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,587 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,587 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,587 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,588 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,588 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,588 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,591 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,591 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,591 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96218 synced till here 96208
2014-07-22 13:25:01,639 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,639 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,639 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,640 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,640 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,641 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,641 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060699096 with entries=97, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060701559
2014-07-22 13:25:01,664 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,681 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,683 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,693 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,693 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,695 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,742 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:01,775 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,236 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,268 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,308 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,340 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,371 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,405 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,436 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,469 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,502 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,534 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,566 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,598 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,634 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,665 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,698 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,733 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,766 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,798 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:02,829 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:06,569 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,572 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,572 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,572 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,573 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,584 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,584 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,587 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,588 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,588 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,589 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,589 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:25:06,589 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,591 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,592 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,592 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,639 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,640 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,640 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,640 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,641 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,641 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,642 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,665 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,681 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,683 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,693 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,694 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,695 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:06,743 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:06,775 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,236 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,268 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,308 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,340 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,371 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,405 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,437 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:07,470 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,503 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,534 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,567 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,599 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:07,634 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:07,665 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,699 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,733 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,767 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,798 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:07,815 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=18983, memsize=475.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/805627bcf942469eaf49982b042aa1d3
2014-07-22 13:25:07,830 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:07,833 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/805627bcf942469eaf49982b042aa1d3 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/805627bcf942469eaf49982b042aa1d3
2014-07-22 13:25:07,843 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/805627bcf942469eaf49982b042aa1d3, entries=1732320, sequenceid=18983, filesize=123.4m
2014-07-22 13:25:07,844 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~644.8m/676088480, currentsize=497.0m/521178320 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 35972ms, sequenceid=18983, compaction requested=true
2014-07-22 13:25:07,845 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:25:07,845 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 13:25:07,845 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5016ms
2014-07-22 13:25:07,846 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,846 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 13:25:07,846 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-22 13:25:07,846 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 852.6m
2014-07-22 13:25:07,846 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,846 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:25:07,846 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:25:07,846 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:25:07,849 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5083ms
2014-07-22 13:25:07,849 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,849 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5116ms
2014-07-22 13:25:07,850 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,850 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5152ms
2014-07-22 13:25:07,850 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,850 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5185ms
2014-07-22 13:25:07,850 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,850 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5217ms
2014-07-22 13:25:07,850 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,850 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5252ms
2014-07-22 13:25:07,850 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,851 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5285ms
2014-07-22 13:25:07,851 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,861 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5327ms
2014-07-22 13:25:07,861 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,861 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5359ms
2014-07-22 13:25:07,861 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,866 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5397ms
2014-07-22 13:25:07,866 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,866 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5430ms
2014-07-22 13:25:07,866 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,867 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5462ms
2014-07-22 13:25:07,867 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,867 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5496ms
2014-07-22 13:25:07,867 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,872 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5532ms
2014-07-22 13:25:07,872 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,872 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5564ms
2014-07-22 13:25:07,872 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,872 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5604ms
2014-07-22 13:25:07,872 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,877 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5641ms
2014-07-22 13:25:07,877 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,877 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6102ms
2014-07-22 13:25:07,878 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,878 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6136ms
2014-07-22 13:25:07,878 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,878 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6183ms
2014-07-22 13:25:07,878 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,878 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6185ms
2014-07-22 13:25:07,878 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,878 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6185ms
2014-07-22 13:25:07,879 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,882 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6199ms
2014-07-22 13:25:07,882 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,882 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6201ms
2014-07-22 13:25:07,882 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,882 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6218ms
2014-07-22 13:25:07,883 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,883 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6242ms
2014-07-22 13:25:07,883 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,897 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6257ms
2014-07-22 13:25:07,897 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,897 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6258ms
2014-07-22 13:25:07,898 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,909 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6270ms
2014-07-22 13:25:07,909 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,909 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6270ms
2014-07-22 13:25:07,909 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,910 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6271ms
2014-07-22 13:25:07,910 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,910 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6271ms
2014-07-22 13:25:07,910 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,910 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6319ms
2014-07-22 13:25:07,910 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,910 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6319ms
2014-07-22 13:25:07,910 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,910 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6319ms
2014-07-22 13:25:07,911 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,911 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6323ms
2014-07-22 13:25:07,911 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,921 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6334ms
2014-07-22 13:25:07,921 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,921 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6333ms
2014-07-22 13:25:07,922 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,922 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6335ms
2014-07-22 13:25:07,922 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,922 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6335ms
2014-07-22 13:25:07,922 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,928 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6341ms
2014-07-22 13:25:07,928 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,928 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6344ms
2014-07-22 13:25:07,928 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,928 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6345ms
2014-07-22 13:25:07,928 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,933 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6360ms
2014-07-22 13:25:07,933 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,933 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6361ms
2014-07-22 13:25:07,934 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,937 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6366ms
2014-07-22 13:25:07,937 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,937 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6366ms
2014-07-22 13:25:07,937 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:07,937 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6369ms
2014-07-22 13:25:07,937 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:08,274 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:25:10,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:10,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96328 synced till here 96293
2014-07-22 13:25:10,294 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10945,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699349,"queuetimems":0,"class":"HRegionServer","responsesize":17555,"method":"Multi"}
2014-07-22 13:25:10,294 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699248,"queuetimems":1,"class":"HRegionServer","responsesize":16668,"method":"Multi"}
2014-07-22 13:25:10,299 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699413,"queuetimems":0,"class":"HRegionServer","responsesize":16799,"method":"Multi"}
2014-07-22 13:25:10,299 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699185,"queuetimems":0,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 13:25:10,299 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699382,"queuetimems":0,"class":"HRegionServer","responsesize":17043,"method":"Multi"}
2014-07-22 13:25:10,300 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12273,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060698027,"queuetimems":0,"class":"HRegionServer","responsesize":17556,"method":"Multi"}
2014-07-22 13:25:10,301 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11152,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699149,"queuetimems":0,"class":"HRegionServer","responsesize":16669,"method":"Multi"}
2014-07-22 13:25:10,301 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11188,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699113,"queuetimems":0,"class":"HRegionServer","responsesize":16838,"method":"Multi"}
2014-07-22 13:25:10,302 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699217,"queuetimems":1,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-22 13:25:10,303 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699059,"queuetimems":1,"class":"HRegionServer","responsesize":17371,"method":"Multi"}
2014-07-22 13:25:10,332 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:25:10,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060701559 with entries=110, filesize=86.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060710060
2014-07-22 13:25:10,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060626249
2014-07-22 13:25:10,576 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10994,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699581,"queuetimems":0,"class":"HRegionServer","responsesize":17219,"method":"Multi"}
2014-07-22 13:25:10,750 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11305,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699444,"queuetimems":0,"class":"HRegionServer","responsesize":16936,"method":"Multi"}
2014-07-22 13:25:10,762 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699319,"queuetimems":0,"class":"HRegionServer","responsesize":16972,"method":"Multi"}
2014-07-22 13:25:10,766 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699769,"queuetimems":1,"class":"HRegionServer","responsesize":16396,"method":"Multi"}
2014-07-22 13:25:10,772 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699823,"queuetimems":0,"class":"HRegionServer","responsesize":17368,"method":"Multi"}
2014-07-22 13:25:10,772 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11224,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699548,"queuetimems":0,"class":"HRegionServer","responsesize":16349,"method":"Multi"}
2014-07-22 13:25:10,772 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699738,"queuetimems":1,"class":"HRegionServer","responsesize":17463,"method":"Multi"}
2014-07-22 13:25:10,773 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699929,"queuetimems":3,"class":"HRegionServer","responsesize":16908,"method":"Multi"}
2014-07-22 13:25:10,773 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11137,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699636,"queuetimems":0,"class":"HRegionServer","responsesize":17345,"method":"Multi"}
2014-07-22 13:25:10,773 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11102,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699671,"queuetimems":0,"class":"HRegionServer","responsesize":17497,"method":"Multi"}
2014-07-22 13:25:10,774 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699499,"queuetimems":0,"class":"HRegionServer","responsesize":16897,"method":"Multi"}
2014-07-22 13:25:10,777 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699890,"queuetimems":0,"class":"HRegionServer","responsesize":17367,"method":"Multi"}
2014-07-22 13:25:10,778 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060699704,"queuetimems":0,"class":"HRegionServer","responsesize":16883,"method":"Multi"}
2014-07-22 13:25:12,093 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:12,094 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10654,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060701440,"queuetimems":1,"class":"HRegionServer","responsesize":16657,"method":"Multi"}
2014-07-22 13:25:12,152 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96449 synced till here 96418
2014-07-22 13:25:12,266 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060701774,"queuetimems":1,"class":"HRegionServer","responsesize":17463,"method":"Multi"}
2014-07-22 13:25:12,314 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702304,"queuetimems":0,"class":"HRegionServer","responsesize":17219,"method":"Multi"}
2014-07-22 13:25:12,303 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060701740,"queuetimems":0,"class":"HRegionServer","responsesize":16883,"method":"Multi"}
2014-07-22 13:25:12,302 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10069,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702232,"queuetimems":0,"class":"HRegionServer","responsesize":16972,"method":"Multi"}
2014-07-22 13:25:12,382 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060710060 with entries=121, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060712093
2014-07-22 13:25:12,681 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10017,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702663,"queuetimems":0,"class":"HRegionServer","responsesize":16409,"method":"Multi"}
2014-07-22 13:25:12,681 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702266,"queuetimems":0,"class":"HRegionServer","responsesize":17371,"method":"Multi"}
2014-07-22 13:25:12,681 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10148,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702532,"queuetimems":1,"class":"HRegionServer","responsesize":16669,"method":"Multi"}
2014-07-22 13:25:12,681 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702402,"queuetimems":0,"class":"HRegionServer","responsesize":16972,"method":"Multi"}
2014-07-22 13:25:12,681 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702631,"queuetimems":0,"class":"HRegionServer","responsesize":17556,"method":"Multi"}
2014-07-22 13:25:12,682 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060702434,"queuetimems":0,"class":"HRegionServer","responsesize":16897,"method":"Multi"}
2014-07-22 13:25:14,041 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:14,105 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96572 synced till here 96553
2014-07-22 13:25:14,288 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060712093 with entries=123, filesize=95.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060714042
2014-07-22 13:25:16,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:16,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96683 synced till here 96659
2014-07-22 13:25:16,536 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,536 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,537 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,537 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,538 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,538 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,538 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,539 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,540 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,540 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,541 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,541 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,541 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,542 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,544 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,544 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,545 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,549 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060714042 with entries=111, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060716345
2014-07-22 13:25:16,569 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,601 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,632 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,633 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,633 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,633 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,633 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,730 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,732 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,732 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,733 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,733 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,733 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,734 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,736 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,736 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,737 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,737 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,737 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,738 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,759 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,791 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:16,820 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:17,714 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:17,746 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:17,780 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:17,812 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:18,857 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:20,616 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:20,653 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:22,031 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5211ms
2014-07-22 13:25:22,031 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5241ms
2014-07-22 13:25:22,032 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5295ms
2014-07-22 13:25:22,032 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5300ms
2014-07-22 13:25:22,032 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5400ms
2014-07-22 13:25:22,032 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5483ms
2014-07-22 13:25:22,033 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5488ms
2014-07-22 13:25:22,033 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5493ms
2014-07-22 13:25:22,034 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5296ms
2014-07-22 13:25:22,034 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5296ms
2014-07-22 13:25:22,034 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5275ms
2014-07-22 13:25:22,035 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5297ms
2014-07-22 13:25:22,036 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5494ms
2014-07-22 13:25:22,036 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5499ms
2014-07-22 13:25:22,036 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5496ms
2014-07-22 13:25:22,036 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5492ms
2014-07-22 13:25:22,037 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5495ms
2014-07-22 13:25:22,037 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5493ms
2014-07-22 13:25:22,037 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5468ms
2014-07-22 13:25:22,038 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5405ms
2014-07-22 13:25:22,038 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5501ms
2014-07-22 13:25:22,038 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5437ms
2014-07-22 13:25:22,039 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5405ms
2014-07-22 13:25:22,039 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5406ms
2014-07-22 13:25:22,039 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5406ms
2014-07-22 13:25:22,039 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5309ms
2014-07-22 13:25:22,040 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5308ms
2014-07-22 13:25:22,040 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5307ms
2014-07-22 13:25:22,040 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5307ms
2014-07-22 13:25:22,041 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5307ms
2014-07-22 13:25:22,041 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5307ms
2014-07-22 13:25:22,041 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-07-22 13:25:22,041 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-07-22 13:25:22,042 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-22 13:25:22,043 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-22 13:25:22,043 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5506ms
2014-07-22 13:25:22,046 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5508ms
2014-07-22 13:25:22,047 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:25:22,049 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:25:22,049 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:25:22,050 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:25:22,215 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:22,715 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:22,747 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:25:22,782 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:22,807 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:25:22,814 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:25:23,472 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19133, memsize=505.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/8acb00e61dd14221990ff5c138f0b8e9
2014-07-22 13:25:23,496 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/8acb00e61dd14221990ff5c138f0b8e9 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/8acb00e61dd14221990ff5c138f0b8e9
2014-07-22 13:25:23,511 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/8acb00e61dd14221990ff5c138f0b8e9, entries=1841070, sequenceid=19133, filesize=131.1m
2014-07-22 13:25:23,511 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~888.1m/931218160, currentsize=373.9m/392080800 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 36089ms, sequenceid=19133, compaction requested=true
2014-07-22 13:25:23,512 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:25:23,512 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 43 store files, 0 compacting, 43 eligible, 2000 blocking
2014-07-22 13:25:23,512 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 43 files from compaction candidates
2014-07-22 13:25:23,513 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:25:23,513 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:25:23,513 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5701ms
2014-07-22 13:25:23,513 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,513 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:25:23,513 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 706ms
2014-07-22 13:25:23,513 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,513 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 881.2m
2014-07-22 13:25:23,514 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5734ms
2014-07-22 13:25:23,514 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,514 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5768ms
2014-07-22 13:25:23,514 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,514 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5800ms
2014-07-22 13:25:23,514 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,515 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1300ms
2014-07-22 13:25:23,515 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,515 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6975ms
2014-07-22 13:25:23,515 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,515 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6976ms
2014-07-22 13:25:23,516 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,517 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6979ms
2014-07-22 13:25:23,517 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,518 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6981ms
2014-07-22 13:25:23,518 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,518 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6982ms
2014-07-22 13:25:23,518 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,518 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6981ms
2014-07-22 13:25:23,518 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,520 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6984ms
2014-07-22 13:25:23,520 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,520 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6984ms
2014-07-22 13:25:23,520 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,521 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6784ms
2014-07-22 13:25:23,521 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,522 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6786ms
2014-07-22 13:25:23,522 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,523 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6788ms
2014-07-22 13:25:23,523 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,523 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6790ms
2014-07-22 13:25:23,524 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,526 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6792ms
2014-07-22 13:25:23,526 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,526 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6793ms
2014-07-22 13:25:23,526 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,531 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6799ms
2014-07-22 13:25:23,531 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,532 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6801ms
2014-07-22 13:25:23,533 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,533 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6900ms
2014-07-22 13:25:23,533 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,536 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6902ms
2014-07-22 13:25:23,536 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,536 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6903ms
2014-07-22 13:25:23,536 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,536 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6935ms
2014-07-22 13:25:23,536 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,536 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6999ms
2014-07-22 13:25:23,536 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,536 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6904ms
2014-07-22 13:25:23,537 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,538 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6969ms
2014-07-22 13:25:23,538 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,538 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6994ms
2014-07-22 13:25:23,539 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,539 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6997ms
2014-07-22 13:25:23,539 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,539 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6995ms
2014-07-22 13:25:23,539 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,540 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7000ms
2014-07-22 13:25:23,541 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,541 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7004ms
2014-07-22 13:25:23,541 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,541 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7000ms
2014-07-22 13:25:23,541 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,590 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6853ms
2014-07-22 13:25:23,590 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,601 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6842ms
2014-07-22 13:25:23,601 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,603 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6863ms
2014-07-22 13:25:23,603 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,603 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6866ms
2014-07-22 13:25:23,603 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,608 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060712706,"queuetimems":2149,"class":"HRegionServer","responsesize":16349,"method":"Multi"}
2014-07-22 13:25:23,609 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7069ms
2014-07-22 13:25:23,609 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,610 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7066ms
2014-07-22 13:25:23,610 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,618 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7068ms
2014-07-22 13:25:23,618 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,625 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6993ms
2014-07-22 13:25:23,625 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,632 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6897ms
2014-07-22 13:25:23,632 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,632 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6895ms
2014-07-22 13:25:23,632 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,633 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6842ms
2014-07-22 13:25:23,633 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,638 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6817ms
2014-07-22 13:25:23,638 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,638 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2985ms
2014-07-22 13:25:23,639 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,645 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3029ms
2014-07-22 13:25:23,645 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,646 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4788ms
2014-07-22 13:25:23,646 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:25:23,714 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11007,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060712706,"queuetimems":837,"class":"HRegionServer","responsesize":17367,"method":"Multi"}
2014-07-22 13:25:23,722 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060712712,"queuetimems":808,"class":"HRegionServer","responsesize":16396,"method":"Multi"}
2014-07-22 13:25:23,729 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060712702,"queuetimems":2221,"class":"HRegionServer","responsesize":16883,"method":"Multi"}
2014-07-22 13:25:23,978 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:25:23,979 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11272,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060712706,"queuetimems":2185,"class":"HRegionServer","responsesize":17463,"method":"Multi"}
2014-07-22 13:25:24,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:24,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96787 synced till here 96778
2014-07-22 13:25:24,303 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060716345 with entries=104, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060724150
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060641740
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060642659
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060644043
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060646446
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060647965
2014-07-22 13:25:24,303 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060648944
2014-07-22 13:25:24,581 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714044,"queuetimems":1997,"class":"HRegionServer","responsesize":16908,"method":"Multi"}
2014-07-22 13:25:24,582 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060713122,"queuetimems":1173,"class":"HRegionServer","responsesize":17368,"method":"Multi"}
2014-07-22 13:25:24,587 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714565,"queuetimems":76,"class":"HRegionServer","responsesize":17345,"method":"Multi"}
2014-07-22 13:25:24,587 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10160,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714426,"queuetimems":0,"class":"HRegionServer","responsesize":17555,"method":"Multi"}
2014-07-22 13:25:24,587 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714153,"queuetimems":0,"class":"HRegionServer","responsesize":17463,"method":"Multi"}
2014-07-22 13:25:24,587 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10542,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714044,"queuetimems":2028,"class":"HRegionServer","responsesize":18104,"method":"Multi"}
2014-07-22 13:25:24,588 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10053,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714534,"queuetimems":77,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 13:25:24,588 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10544,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714044,"queuetimems":1231,"class":"HRegionServer","responsesize":16349,"method":"Multi"}
2014-07-22 13:25:24,589 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714335,"queuetimems":0,"class":"HRegionServer","responsesize":16669,"method":"Multi"}
2014-07-22 13:25:24,590 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10397,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714192,"queuetimems":1,"class":"HRegionServer","responsesize":16883,"method":"Multi"}
2014-07-22 13:25:24,601 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714567,"queuetimems":47,"class":"HRegionServer","responsesize":16585,"method":"Multi"}
2014-07-22 13:25:24,602 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714598,"queuetimems":0,"class":"HRegionServer","responsesize":16972,"method":"Multi"}
2014-07-22 13:25:24,633 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:25:25,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:25:25,563 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714797,"queuetimems":127,"class":"HRegionServer","responsesize":17219,"method":"Multi"}
2014-07-22 13:25:25,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:25,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 96892 synced till here 96876
2014-07-22 13:25:25,733 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10907,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714825,"queuetimems":0,"class":"HRegionServer","responsesize":16897,"method":"Multi"}
2014-07-22 13:25:25,733 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714799,"queuetimems":32,"class":"HRegionServer","responsesize":16877,"method":"Multi"}
2014-07-22 13:25:25,755 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060724150 with entries=105, filesize=76.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060725675
2014-07-22 13:25:25,757 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:25:25,804 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714858,"queuetimems":0,"class":"HRegionServer","responsesize":16668,"method":"Multi"}
2014-07-22 13:25:25,804 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060714798,"queuetimems":83,"class":"HRegionServer","responsesize":16936,"method":"Multi"}
2014-07-22 13:25:28,338 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:28,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97005 synced till here 96993
2014-07-22 13:25:28,595 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060725675 with entries=113, filesize=75.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060728338
2014-07-22 13:25:28,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:25:29,974 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:29,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97094 synced till here 97090
2014-07-22 13:25:30,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060728338 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060729974
2014-07-22 13:25:30,032 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:25:31,164 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:31,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97183 synced till here 97180
2014-07-22 13:25:32,059 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060729974 with entries=89, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060731164
2014-07-22 13:25:32,062 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:25:32,658 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19318, memsize=336.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/899a20e2423a4a7199def75e5839402a
2014-07-22 13:25:32,677 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/899a20e2423a4a7199def75e5839402a as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/899a20e2423a4a7199def75e5839402a
2014-07-22 13:25:32,689 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/899a20e2423a4a7199def75e5839402a, entries=1225780, sequenceid=19318, filesize=87.3m
2014-07-22 13:25:32,689 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~852.6m/893997200, currentsize=302.2m/316908720 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 24843ms, sequenceid=19318, compaction requested=true
2014-07-22 13:25:32,690 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:25:32,690 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 13:25:32,690 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 13:25:32,690 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 372.0m
2014-07-22 13:25:32,690 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:25:32,690 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:25:32,690 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:25:32,809 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:25:32,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:32,941 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97268 synced till here 97267
2014-07-22 13:25:32,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060731164 with entries=85, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060732920
2014-07-22 13:25:33,135 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:25:34,301 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:34,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97356 synced till here 97354
2014-07-22 13:25:34,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060732920 with entries=88, filesize=65.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060734302
2014-07-22 13:25:36,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:36,768 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97455 synced till here 97443
2014-07-22 13:25:36,873 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060734302 with entries=99, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060736737
2014-07-22 13:25:38,639 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:38,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97554 synced till here 97539
2014-07-22 13:25:38,828 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060736737 with entries=99, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060738639
2014-07-22 13:25:40,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:40,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97654 synced till here 97638
2014-07-22 13:25:40,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060738639 with entries=100, filesize=74.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060740630
2014-07-22 13:25:42,435 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:42,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97744 synced till here 97743
2014-07-22 13:25:42,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060740630 with entries=90, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060742435
2014-07-22 13:25:43,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:43,456 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97830 synced till here 97829
2014-07-22 13:25:43,476 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060742435 with entries=86, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060743436
2014-07-22 13:25:45,139 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:45,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 97917 synced till here 97912
2014-07-22 13:25:45,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060743436 with entries=87, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060745140
2014-07-22 13:25:45,366 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19459, memsize=143.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/8826b554b40346e3be48a5149308807f
2014-07-22 13:25:45,382 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/8826b554b40346e3be48a5149308807f as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/8826b554b40346e3be48a5149308807f
2014-07-22 13:25:45,395 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/8826b554b40346e3be48a5149308807f, entries=520920, sequenceid=19459, filesize=37.1m
2014-07-22 13:25:45,396 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~372.0m/390039760, currentsize=62.3m/65339920 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 12706ms, sequenceid=19459, compaction requested=true
2014-07-22 13:25:45,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:25:45,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 25 store files, 0 compacting, 25 eligible, 2000 blocking
2014-07-22 13:25:45,397 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 1008.3m
2014-07-22 13:25:45,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 25 files from compaction candidates
2014-07-22 13:25:45,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:25:45,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:25:45,397 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:25:46,703 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:46,729 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98004 synced till here 98001
2014-07-22 13:25:46,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060745140 with entries=87, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060746704
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060650220
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060651780
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060652905
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060654333
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060655734
2014-07-22 13:25:46,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060657682
2014-07-22 13:25:46,772 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060658651
2014-07-22 13:25:47,210 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:25:48,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:48,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98093 synced till here 98086
2014-07-22 13:25:48,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060746704 with entries=89, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060748266
2014-07-22 13:25:50,456 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19394, memsize=267.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a0e0edce4944400188276fe458efeee1
2014-07-22 13:25:50,477 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/a0e0edce4944400188276fe458efeee1 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a0e0edce4944400188276fe458efeee1
2014-07-22 13:25:50,547 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/a0e0edce4944400188276fe458efeee1, entries=973690, sequenceid=19394, filesize=69.3m
2014-07-22 13:25:50,547 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~881.2m/924035440, currentsize=438.5m/459822880 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 27034ms, sequenceid=19394, compaction requested=true
2014-07-22 13:25:50,549 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:25:50,549 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 13:25:50,549 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 823.2m
2014-07-22 13:25:50,549 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 13:25:50,550 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:25:50,550 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:25:50,550 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:25:50,553 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:25:50,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:50,850 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98198 synced till here 98176
2014-07-22 13:25:51,015 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060748266 with entries=105, filesize=82.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060750799
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060660173
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060661750
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060662829
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060664020
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060665101
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060666619
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668222
2014-07-22 13:25:51,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060668946
2014-07-22 13:25:52,697 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:25:52,998 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:53,080 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98307 synced till here 98286
2014-07-22 13:25:53,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060750799 with entries=109, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060752998
2014-07-22 13:25:55,192 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1626ms
GC pool 'ParNew' had collection(s): count=1 time=1649ms
2014-07-22 13:25:55,679 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:55,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98422 synced till here 98398
2014-07-22 13:25:55,866 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060752998 with entries=115, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060755679
2014-07-22 13:25:57,498 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:57,740 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98535 synced till here 98532
2014-07-22 13:25:57,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060755679 with entries=113, filesize=81.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060757498
2014-07-22 13:25:59,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:25:59,952 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060757498 with entries=103, filesize=74.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060759769
2014-07-22 13:26:01,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:01,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98731 synced till here 98729
2014-07-22 13:26:01,728 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060759769 with entries=93, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060761662
2014-07-22 13:26:03,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:03,380 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060761662 with entries=87, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060763351
2014-07-22 13:26:04,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:04,984 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98911 synced till here 98903
2014-07-22 13:26:05,014 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060763351 with entries=93, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060764224
2014-07-22 13:26:05,171 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,185 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,199 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,227 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,228 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,228 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,230 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,245 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,245 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,245 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,263 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,298 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,331 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,361 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,395 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,428 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,460 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,498 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,537 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,566 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,599 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,632 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,665 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,698 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:05,889 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:06,011 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:06,118 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:06,233 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,383 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,420 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,470 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,493 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,523 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,553 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,594 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,630 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,660 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,692 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,725 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,763 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,807 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,832 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,873 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,918 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:07,962 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:08,008 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:08,068 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:09,512 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1026ms
GC pool 'ParNew' had collection(s): count=1 time=1432ms
2014-07-22 13:26:09,526 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:09,543 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:09,564 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:10,171 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,185 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,199 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,228 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,228 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,228 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,230 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,245 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,245 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,245 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,263 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,298 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,331 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,362 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,395 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,428 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,460 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,498 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,537 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,567 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,599 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,632 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,665 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:10,699 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:10,889 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:11,012 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:11,118 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:11,233 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:12,246 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19638, memsize=320.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/0cd62ec52b7a48d98f55c60fdd18313f
2014-07-22 13:26:12,263 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/0cd62ec52b7a48d98f55c60fdd18313f as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/0cd62ec52b7a48d98f55c60fdd18313f
2014-07-22 13:26:12,279 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/0cd62ec52b7a48d98f55c60fdd18313f, entries=1167050, sequenceid=19638, filesize=83.1m
2014-07-22 13:26:12,280 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1009.9m/1058924000, currentsize=310.2m/325306240 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 26883ms, sequenceid=19638, compaction requested=true
2014-07-22 13:26:12,281 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:26:12,281 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 13:26:12,281 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 13:26:12,281 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:26:12,281 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:26:12,281 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6048ms
2014-07-22 13:26:12,281 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:26:12,281 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,281 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 828.0m
2014-07-22 13:26:12,282 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6163ms
2014-07-22 13:26:12,282 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,282 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6271ms
2014-07-22 13:26:12,282 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,282 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6393ms
2014-07-22 13:26:12,282 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,283 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6584ms
2014-07-22 13:26:12,283 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,283 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6618ms
2014-07-22 13:26:12,283 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,289 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6657ms
2014-07-22 13:26:12,289 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,289 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6690ms
2014-07-22 13:26:12,289 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,289 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6723ms
2014-07-22 13:26:12,289 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,290 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6754ms
2014-07-22 13:26:12,290 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,290 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6792ms
2014-07-22 13:26:12,290 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,290 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6830ms
2014-07-22 13:26:12,290 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,292 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6864ms
2014-07-22 13:26:12,292 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,293 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6898ms
2014-07-22 13:26:12,293 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,294 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6932ms
2014-07-22 13:26:12,294 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,294 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6963ms
2014-07-22 13:26:12,294 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,294 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6997ms
2014-07-22 13:26:12,294 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,296 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7032ms
2014-07-22 13:26:12,296 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,300 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7055ms
2014-07-22 13:26:12,300 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,301 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7056ms
2014-07-22 13:26:12,301 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,302 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7056ms
2014-07-22 13:26:12,302 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,302 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7072ms
2014-07-22 13:26:12,302 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,305 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7077ms
2014-07-22 13:26:12,305 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,314 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7086ms
2014-07-22 13:26:12,314 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,314 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7087ms
2014-07-22 13:26:12,314 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,315 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7116ms
2014-07-22 13:26:12,315 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,315 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7131ms
2014-07-22 13:26:12,315 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,315 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7144ms
2014-07-22 13:26:12,315 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,315 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2751ms
2014-07-22 13:26:12,316 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,316 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2773ms
2014-07-22 13:26:12,316 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,319 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2793ms
2014-07-22 13:26:12,319 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,319 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4252ms
2014-07-22 13:26:12,319 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,320 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4311ms
2014-07-22 13:26:12,320 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,321 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4358ms
2014-07-22 13:26:12,321 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,322 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4404ms
2014-07-22 13:26:12,323 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,324 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4450ms
2014-07-22 13:26:12,324 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,324 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4492ms
2014-07-22 13:26:12,324 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,324 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4517ms
2014-07-22 13:26:12,324 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,324 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4561ms
2014-07-22 13:26:12,325 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,325 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4600ms
2014-07-22 13:26:12,325 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,325 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4633ms
2014-07-22 13:26:12,325 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,326 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4665ms
2014-07-22 13:26:12,326 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,327 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4696ms
2014-07-22 13:26:12,327 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,328 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4733ms
2014-07-22 13:26:12,328 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,328 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4775ms
2014-07-22 13:26:12,328 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,328 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4805ms
2014-07-22 13:26:12,328 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,329 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4836ms
2014-07-22 13:26:12,329 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,330 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4859ms
2014-07-22 13:26:12,330 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,330 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4910ms
2014-07-22 13:26:12,330 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,330 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4947ms
2014-07-22 13:26:12,330 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:12,590 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:26:12,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:14,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 98993 synced till here 98988
2014-07-22 13:26:14,120 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19662, memsize=311.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/848a3d37165e44a69607c56b0007b601
2014-07-22 13:26:14,132 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/848a3d37165e44a69607c56b0007b601 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/848a3d37165e44a69607c56b0007b601
2014-07-22 13:26:14,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060764224 with entries=82, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060772790
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060670305
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060672015
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060674069
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060676260
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060678249
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060680179
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060682078
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060684158
2014-07-22 13:26:14,148 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060685695
2014-07-22 13:26:14,154 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/848a3d37165e44a69607c56b0007b601, entries=1134090, sequenceid=19662, filesize=80.7m
2014-07-22 13:26:14,155 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~824.8m/864855920, currentsize=259.6m/272256080 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 23606ms, sequenceid=19662, compaction requested=true
2014-07-22 13:26:14,156 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:26:14,156 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 44 store files, 0 compacting, 44 eligible, 2000 blocking
2014-07-22 13:26:14,156 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 695.6m
2014-07-22 13:26:14,156 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 44 files from compaction candidates
2014-07-22 13:26:14,156 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:26:14,156 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:26:14,156 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:26:14,341 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:26:14,405 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:26:14,935 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:14,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99115 synced till here 99081
2014-07-22 13:26:16,150 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:26:16,239 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060766116,"queuetimems":1,"class":"HRegionServer","responsesize":16843,"method":"Multi"}
2014-07-22 13:26:16,305 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060772790 with entries=122, filesize=99.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060774936
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060687141
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060689263
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691129
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060691979
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060694049
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060697252
2014-07-22 13:26:16,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060699096
2014-07-22 13:26:16,341 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765496,"queuetimems":0,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-07-22 13:26:16,370 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060766009,"queuetimems":0,"class":"HRegionServer","responsesize":17013,"method":"Multi"}
2014-07-22 13:26:16,843 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11547,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765295,"queuetimems":0,"class":"HRegionServer","responsesize":16783,"method":"Multi"}
2014-07-22 13:26:16,906 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765426,"queuetimems":0,"class":"HRegionServer","responsesize":15664,"method":"Multi"}
2014-07-22 13:26:16,908 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765261,"queuetimems":0,"class":"HRegionServer","responsesize":17180,"method":"Multi"}
2014-07-22 13:26:16,909 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11374,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765535,"queuetimems":1,"class":"HRegionServer","responsesize":17029,"method":"Multi"}
2014-07-22 13:26:17,097 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:17,098 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765225,"queuetimems":0,"class":"HRegionServer","responsesize":16725,"method":"Multi"}
2014-07-22 13:26:17,099 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765458,"queuetimems":0,"class":"HRegionServer","responsesize":16413,"method":"Multi"}
2014-07-22 13:26:17,102 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765393,"queuetimems":0,"class":"HRegionServer","responsesize":16930,"method":"Multi"}
2014-07-22 13:26:17,114 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765565,"queuetimems":0,"class":"HRegionServer","responsesize":16616,"method":"Multi"}
2014-07-22 13:26:17,114 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765596,"queuetimems":0,"class":"HRegionServer","responsesize":16827,"method":"Multi"}
2014-07-22 13:26:17,122 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765359,"queuetimems":0,"class":"HRegionServer","responsesize":16878,"method":"Multi"}
2014-07-22 13:26:17,127 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765663,"queuetimems":0,"class":"HRegionServer","responsesize":17585,"method":"Multi"}
2014-07-22 13:26:17,129 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765696,"queuetimems":0,"class":"HRegionServer","responsesize":17279,"method":"Multi"}
2014-07-22 13:26:17,137 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765887,"queuetimems":1,"class":"HRegionServer","responsesize":17268,"method":"Multi"}
2014-07-22 13:26:17,129 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765630,"queuetimems":0,"class":"HRegionServer","responsesize":16420,"method":"Multi"}
2014-07-22 13:26:17,180 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11850,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060765329,"queuetimems":1,"class":"HRegionServer","responsesize":17113,"method":"Multi"}
2014-07-22 13:26:17,879 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99250 synced till here 99247
2014-07-22 13:26:17,884 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10161,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060767722,"queuetimems":1,"class":"HRegionServer","responsesize":16930,"method":"Multi"}
2014-07-22 13:26:17,917 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060774936 with entries=135, filesize=80.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060777097
2014-07-22 13:26:18,572 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:18,626 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99350 synced till here 99327
2014-07-22 13:26:19,854 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1005ms
GC pool 'ParNew' had collection(s): count=1 time=1099ms
2014-07-22 13:26:19,913 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060777097 with entries=100, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060778573
2014-07-22 13:26:20,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:20,643 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99456 synced till here 99431
2014-07-22 13:26:20,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060778573 with entries=106, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060780624
2014-07-22 13:26:22,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:22,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99575 synced till here 99542
2014-07-22 13:26:22,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060780624 with entries=119, filesize=80.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060782622
2014-07-22 13:26:24,633 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:25,038 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060782622 with entries=125, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060784633
2014-07-22 13:26:26,568 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:26,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99787 synced till here 99779
2014-07-22 13:26:26,670 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060784633 with entries=87, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060786568
2014-07-22 13:26:28,789 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:28,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99890 synced till here 99875
2014-07-22 13:26:28,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060786568 with entries=103, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060788790
2014-07-22 13:26:30,435 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1008ms
GC pool 'ParNew' had collection(s): count=1 time=1103ms
2014-07-22 13:26:30,891 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:30,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 99999 synced till here 99978
2014-07-22 13:26:31,095 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060788790 with entries=109, filesize=83.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060790891
2014-07-22 13:26:32,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:32,953 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100114 synced till here 100083
2014-07-22 13:26:32,990 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:26:33,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060790891 with entries=115, filesize=83.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060792895
2014-07-22 13:26:34,978 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:35,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100226 synced till here 100202
2014-07-22 13:26:35,217 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060792895 with entries=112, filesize=86.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060794979
2014-07-22 13:26:37,300 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,301 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,301 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,305 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,307 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,309 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,309 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,311 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,311 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,311 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,343 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,378 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,422 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,423 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:37,424 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,426 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,426 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,427 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,429 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,429 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,430 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,430 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,430 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,430 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,432 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,543 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,580 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,622 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,651 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060794979 with entries=97, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060797423
2014-07-22 13:26:37,669 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,718 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,751 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,790 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,829 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,868 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,909 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,949 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:37,991 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,029 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,069 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,106 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,146 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,185 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,223 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,260 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,311 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:38,365 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:41,170 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:41,275 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:41,286 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:41,313 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:41,346 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:26:42,301 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,301 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,302 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,306 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,308 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,310 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,310 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,311 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,311 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,311 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,344 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,379 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,423 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,425 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,426 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,427 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,428 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,430 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,430 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,430 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,431 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,431 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:26:42,431 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:26:42,432 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,543 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,582 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,624 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,670 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,719 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,751 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,790 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,832 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,869 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:42,909 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,949 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:42,992 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:43,029 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,070 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,107 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,147 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:43,185 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,224 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:26:43,261 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,312 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:43,365 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:26:44,481 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19846, memsize=394.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/85c848cc2b1f46b086ad2fce029bb0c7
2014-07-22 13:26:44,495 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/85c848cc2b1f46b086ad2fce029bb0c7 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/85c848cc2b1f46b086ad2fce029bb0c7
2014-07-22 13:26:44,503 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/85c848cc2b1f46b086ad2fce029bb0c7, entries=1436690, sequenceid=19846, filesize=102.3m
2014-07-22 13:26:44,503 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~698.7m/732590400, currentsize=428.3m/449066880 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 30347ms, sequenceid=19846, compaction requested=true
2014-07-22 13:26:44,504 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:26:44,505 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 13:26:44,505 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6140ms
2014-07-22 13:26:44,505 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 13:26:44,505 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 741.0m
2014-07-22 13:26:44,505 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:26:44,505 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:26:44,505 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:26:44,505 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,505 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6194ms
2014-07-22 13:26:44,505 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,506 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6246ms
2014-07-22 13:26:44,506 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,506 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6283ms
2014-07-22 13:26:44,506 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,506 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6321ms
2014-07-22 13:26:44,506 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,507 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6361ms
2014-07-22 13:26:44,507 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,508 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6402ms
2014-07-22 13:26:44,508 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,508 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6439ms
2014-07-22 13:26:44,508 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,509 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6480ms
2014-07-22 13:26:44,509 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,510 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6519ms
2014-07-22 13:26:44,510 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,521 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6572ms
2014-07-22 13:26:44,521 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,526 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6617ms
2014-07-22 13:26:44,526 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,526 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6658ms
2014-07-22 13:26:44,526 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,526 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6697ms
2014-07-22 13:26:44,527 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,527 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6737ms
2014-07-22 13:26:44,527 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,529 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6778ms
2014-07-22 13:26:44,529 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,529 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6811ms
2014-07-22 13:26:44,529 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,529 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6861ms
2014-07-22 13:26:44,529 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,537 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6915ms
2014-07-22 13:26:44,537 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,537 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6957ms
2014-07-22 13:26:44,537 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,537 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6995ms
2014-07-22 13:26:44,537 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,537 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7105ms
2014-07-22 13:26:44,537 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,537 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 13:26:44,538 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,538 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7109ms
2014-07-22 13:26:44,538 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,538 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 13:26:44,538 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,538 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7108ms
2014-07-22 13:26:44,538 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,548 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7119ms
2014-07-22 13:26:44,548 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,548 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7119ms
2014-07-22 13:26:44,548 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,548 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7121ms
2014-07-22 13:26:44,548 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,557 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7131ms
2014-07-22 13:26:44,557 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,557 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7131ms
2014-07-22 13:26:44,557 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,563 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7139ms
2014-07-22 13:26:44,563 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,564 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7142ms
2014-07-22 13:26:44,564 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,564 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7186ms
2014-07-22 13:26:44,564 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,564 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7221ms
2014-07-22 13:26:44,564 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,564 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7253ms
2014-07-22 13:26:44,564 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,567 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7256ms
2014-07-22 13:26:44,567 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,573 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7263ms
2014-07-22 13:26:44,573 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,573 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7264ms
2014-07-22 13:26:44,573 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,575 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7266ms
2014-07-22 13:26:44,575 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,575 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7268ms
2014-07-22 13:26:44,575 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,576 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7271ms
2014-07-22 13:26:44,576 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,576 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7275ms
2014-07-22 13:26:44,576 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,603 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7302ms
2014-07-22 13:26:44,604 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,613 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7313ms
2014-07-22 13:26:44,613 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,613 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3267ms
2014-07-22 13:26:44,613 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,614 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3301ms
2014-07-22 13:26:44,614 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,614 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3328ms
2014-07-22 13:26:44,614 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,615 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3340ms
2014-07-22 13:26:44,615 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,615 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3445ms
2014-07-22 13:26:44,615 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:26:44,682 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060794587,"queuetimems":0,"class":"HRegionServer","responsesize":16920,"method":"Multi"}
2014-07-22 13:26:44,684 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10062,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060794621,"queuetimems":0,"class":"HRegionServer","responsesize":17007,"method":"Multi"}
2014-07-22 13:26:44,685 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060794681,"queuetimems":0,"class":"HRegionServer","responsesize":16753,"method":"Multi"}
2014-07-22 13:26:44,686 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10130,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060794555,"queuetimems":0,"class":"HRegionServer","responsesize":17652,"method":"Multi"}
2014-07-22 13:26:44,949 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:26:46,309 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795122,"queuetimems":0,"class":"HRegionServer","responsesize":17368,"method":"Multi"}
2014-07-22 13:26:46,310 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795163,"queuetimems":0,"class":"HRegionServer","responsesize":17367,"method":"Multi"}
2014-07-22 13:26:46,310 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10956,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795354,"queuetimems":1,"class":"HRegionServer","responsesize":17279,"method":"Multi"}
2014-07-22 13:26:46,311 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11223,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795087,"queuetimems":0,"class":"HRegionServer","responsesize":18104,"method":"Multi"}
2014-07-22 13:26:46,318 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795020,"queuetimems":0,"class":"HRegionServer","responsesize":16330,"method":"Multi"}
2014-07-22 13:26:46,318 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11265,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795052,"queuetimems":0,"class":"HRegionServer","responsesize":16121,"method":"Multi"}
2014-07-22 13:26:46,318 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795213,"queuetimems":0,"class":"HRegionServer","responsesize":16821,"method":"Multi"}
2014-07-22 13:26:46,322 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11340,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060794981,"queuetimems":0,"class":"HRegionServer","responsesize":17354,"method":"Multi"}
2014-07-22 13:26:46,309 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060795266,"queuetimems":0,"class":"HRegionServer","responsesize":16881,"method":"Multi"}
2014-07-22 13:26:46,453 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:26:46,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:46,570 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=19850, memsize=392.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/d51bf39a49fb4c21beaa4dacea06ff6e
2014-07-22 13:26:46,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100449 synced till here 100422
2014-07-22 13:26:46,583 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/d51bf39a49fb4c21beaa4dacea06ff6e as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/d51bf39a49fb4c21beaa4dacea06ff6e
2014-07-22 13:26:46,591 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/d51bf39a49fb4c21beaa4dacea06ff6e, entries=1429320, sequenceid=19850, filesize=101.7m
2014-07-22 13:26:46,591 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~828.0m/868253040, currentsize=493.9m/517863600 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 34310ms, sequenceid=19850, compaction requested=true
2014-07-22 13:26:46,592 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:26:46,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 13:26:46,592 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 693.8m
2014-07-22 13:26:46,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 13:26:46,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:26:46,592 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:26:46,592 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:26:46,728 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:26:46,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060797423 with entries=126, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060806567
2014-07-22 13:26:46,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060701559
2014-07-22 13:26:46,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060710060
2014-07-22 13:26:46,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060712093
2014-07-22 13:26:46,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060714042
2014-07-22 13:26:46,794 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060716345
2014-07-22 13:26:46,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060724150
2014-07-22 13:26:46,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060725675
2014-07-22 13:26:46,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060728338
2014-07-22 13:26:46,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060729974
2014-07-22 13:26:48,085 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1029ms
GC pool 'ParNew' had collection(s): count=1 time=1028ms
2014-07-22 13:26:48,502 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:26:48,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:48,684 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100568 synced till here 100539
2014-07-22 13:26:48,856 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060806567 with entries=119, filesize=98.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060808669
2014-07-22 13:26:48,857 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:48,934 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798182,"queuetimems":0,"class":"HRegionServer","responsesize":16821,"method":"Multi"}
2014-07-22 13:26:48,993 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10848,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798144,"queuetimems":1,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-22 13:26:48,993 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11452,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797541,"queuetimems":1,"class":"HRegionServer","responsesize":17469,"method":"Multi"}
2014-07-22 13:26:48,994 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11686,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797308,"queuetimems":1,"class":"HRegionServer","responsesize":16791,"method":"Multi"}
2014-07-22 13:26:48,994 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11380,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797614,"queuetimems":0,"class":"HRegionServer","responsesize":17212,"method":"Multi"}
2014-07-22 13:26:48,993 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10772,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798220,"queuetimems":0,"class":"HRegionServer","responsesize":16926,"method":"Multi"}
2014-07-22 13:26:48,994 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797946,"queuetimems":0,"class":"HRegionServer","responsesize":17041,"method":"Multi"}
2014-07-22 13:26:48,994 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798104,"queuetimems":1,"class":"HRegionServer","responsesize":16330,"method":"Multi"}
2014-07-22 13:26:48,994 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798027,"queuetimems":1,"class":"HRegionServer","responsesize":16753,"method":"Multi"}
2014-07-22 13:26:48,995 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797826,"queuetimems":0,"class":"HRegionServer","responsesize":17004,"method":"Multi"}
2014-07-22 13:26:48,995 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797708,"queuetimems":0,"class":"HRegionServer","responsesize":17652,"method":"Multi"}
2014-07-22 13:26:49,014 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11107,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797906,"queuetimems":0,"class":"HRegionServer","responsesize":17112,"method":"Multi"}
2014-07-22 13:26:49,022 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797376,"queuetimems":1,"class":"HRegionServer","responsesize":16768,"method":"Multi"}
2014-07-22 13:26:49,034 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797866,"queuetimems":1,"class":"HRegionServer","responsesize":16920,"method":"Multi"}
2014-07-22 13:26:49,034 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797989,"queuetimems":1,"class":"HRegionServer","responsesize":17007,"method":"Multi"}
2014-07-22 13:26:49,035 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11616,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797419,"queuetimems":0,"class":"HRegionServer","responsesize":17044,"method":"Multi"}
2014-07-22 13:26:49,035 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060798066,"queuetimems":0,"class":"HRegionServer","responsesize":17117,"method":"Multi"}
2014-07-22 13:26:49,036 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797341,"queuetimems":1,"class":"HRegionServer","responsesize":16927,"method":"Multi"}
2014-07-22 13:26:49,043 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797577,"queuetimems":0,"class":"HRegionServer","responsesize":17217,"method":"Multi"}
2014-07-22 13:26:49,045 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11296,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797748,"queuetimems":1,"class":"HRegionServer","responsesize":16881,"method":"Multi"}
2014-07-22 13:26:49,118 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11451,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797666,"queuetimems":1,"class":"HRegionServer","responsesize":17417,"method":"Multi"}
2014-07-22 13:26:49,159 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11371,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060797787,"queuetimems":1,"class":"HRegionServer","responsesize":16881,"method":"Multi"}
2014-07-22 13:26:50,158 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:50,188 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100691 synced till here 100685
2014-07-22 13:26:50,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060808669 with entries=123, filesize=67.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810158
2014-07-22 13:26:50,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:50,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:51,492 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100775 synced till here 100766
2014-07-22 13:26:51,581 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810158 with entries=84, filesize=68.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810971
2014-07-22 13:26:51,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:52,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:52,698 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810971 with entries=100, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060812661
2014-07-22 13:26:52,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:53,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:53,955 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 100963 synced till here 100961
2014-07-22 13:26:53,988 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060812661 with entries=88, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060813929
2014-07-22 13:26:53,989 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:55,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:55,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060813929 with entries=85, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060815383
2014-07-22 13:26:55,427 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:57,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:57,145 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101135 synced till here 101131
2014-07-22 13:26:57,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060815383 with entries=87, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060817116
2014-07-22 13:26:57,589 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:26:59,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:26:59,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101227 synced till here 101222
2014-07-22 13:26:59,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060817116 with entries=92, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060819193
2014-07-22 13:26:59,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:27:00,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:00,187 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101315 synced till here 101308
2014-07-22 13:27:00,943 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060819193 with entries=88, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060820167
2014-07-22 13:27:00,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:27:01,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:01,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101405 synced till here 101402
2014-07-22 13:27:01,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060820167 with entries=90, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060821612
2014-07-22 13:27:01,672 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:27:01,768 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20111, memsize=190.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/bdf0b9d23dd64bc8aead96e8a34c6ff5
2014-07-22 13:27:01,783 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/bdf0b9d23dd64bc8aead96e8a34c6ff5 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/bdf0b9d23dd64bc8aead96e8a34c6ff5
2014-07-22 13:27:01,794 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/bdf0b9d23dd64bc8aead96e8a34c6ff5, entries=691700, sequenceid=20111, filesize=49.3m
2014-07-22 13:27:01,794 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~695.4m/729148000, currentsize=320.9m/336535920 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 15202ms, sequenceid=20111, compaction requested=true
2014-07-22 13:27:01,795 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:01,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 13:27:01,795 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 386.3m
2014-07-22 13:27:01,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 13:27:01,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:01,795 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:01,795 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:27:01,801 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:27:02,712 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:02,933 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:02,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101492 synced till here 101488
2014-07-22 13:27:03,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060821612 with entries=87, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060822933
2014-07-22 13:27:03,136 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20120, memsize=237.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/b532fd016b814c638a3c0a205080e26c
2014-07-22 13:27:03,153 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/b532fd016b814c638a3c0a205080e26c as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/b532fd016b814c638a3c0a205080e26c
2014-07-22 13:27:03,166 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/b532fd016b814c638a3c0a205080e26c, entries=865510, sequenceid=20120, filesize=61.7m
2014-07-22 13:27:03,167 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~741.0m/777031840, currentsize=359.1m/376580240 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 18662ms, sequenceid=20120, compaction requested=true
2014-07-22 13:27:03,167 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:03,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 45 store files, 0 compacting, 45 eligible, 2000 blocking
2014-07-22 13:27:03,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 45 files from compaction candidates
2014-07-22 13:27:03,168 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 785.2m
2014-07-22 13:27:03,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:03,168 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:03,168 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:27:03,206 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:27:04,159 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:04,180 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101581 synced till here 101579
2014-07-22 13:27:04,193 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:04,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060822933 with entries=89, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060824159
2014-07-22 13:27:05,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:05,522 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101669 synced till here 101664
2014-07-22 13:27:05,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060824159 with entries=88, filesize=65.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060825501
2014-07-22 13:27:06,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:06,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101759 synced till here 101756
2014-07-22 13:27:06,425 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060825501 with entries=90, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060826382
2014-07-22 13:27:07,876 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:08,464 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101854 synced till here 101852
2014-07-22 13:27:08,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060826382 with entries=95, filesize=70.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060827876
2014-07-22 13:27:09,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:09,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 101948 synced till here 101946
2014-07-22 13:27:09,671 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060827876 with entries=94, filesize=68.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060829504
2014-07-22 13:27:10,355 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20298, memsize=147.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/6613c44c69c64b53b78cab6131e6d1a3
2014-07-22 13:27:10,370 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/6613c44c69c64b53b78cab6131e6d1a3 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/6613c44c69c64b53b78cab6131e6d1a3
2014-07-22 13:27:10,398 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/6613c44c69c64b53b78cab6131e6d1a3, entries=536490, sequenceid=20298, filesize=38.2m
2014-07-22 13:27:10,399 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~386.3m/405043040, currentsize=51.8m/54340480 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 8604ms, sequenceid=20298, compaction requested=true
2014-07-22 13:27:10,401 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 26 store files, 0 compacting, 26 eligible, 2000 blocking
2014-07-22 13:27:10,401 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 26 files from compaction candidates
2014-07-22 13:27:10,401 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:10,402 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:10,402 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:27:10,402 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:10,402 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 935.7m
2014-07-22 13:27:10,915 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:11,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102045 synced till here 102043
2014-07-22 13:27:11,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060829504 with entries=97, filesize=72.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060830915
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060731164
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060732920
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060734302
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060736737
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060738639
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060740630
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060742435
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060743436
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060745140
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060746704
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060748266
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060750799
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060752998
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060755679
2014-07-22 13:27:11,092 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060757498
2014-07-22 13:27:11,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060759769
2014-07-22 13:27:11,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060761662
2014-07-22 13:27:11,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060763351
2014-07-22 13:27:11,220 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:12,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:12,396 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102136 synced till here 102130
2014-07-22 13:27:12,469 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060830915 with entries=91, filesize=66.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060832374
2014-07-22 13:27:14,368 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:14,669 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102249 synced till here 102247
2014-07-22 13:27:14,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060832374 with entries=113, filesize=81.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060834369
2014-07-22 13:27:15,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:15,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102337 synced till here 102334
2014-07-22 13:27:16,056 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060834369 with entries=88, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060835966
2014-07-22 13:27:17,393 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:17,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102426 synced till here 102423
2014-07-22 13:27:17,487 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060835966 with entries=89, filesize=65.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060837393
2014-07-22 13:27:17,746 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20356, memsize=238.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/91e3980b6628439ea3cbbcb2d8177a3b
2014-07-22 13:27:17,761 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/91e3980b6628439ea3cbbcb2d8177a3b as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/91e3980b6628439ea3cbbcb2d8177a3b
2014-07-22 13:27:17,777 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/91e3980b6628439ea3cbbcb2d8177a3b, entries=867390, sequenceid=20356, filesize=61.8m
2014-07-22 13:27:17,778 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~788.2m/826507600, currentsize=289.9m/304007520 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 14610ms, sequenceid=20356, compaction requested=true
2014-07-22 13:27:17,778 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:17,778 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 637.3m
2014-07-22 13:27:17,779 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 13:27:17,779 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 13:27:17,780 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:17,780 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:17,780 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:27:17,788 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:27:18,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:18,938 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:18,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102522 synced till here 102515
2014-07-22 13:27:19,024 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060837393 with entries=96, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060838872
2014-07-22 13:27:20,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:20,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102626 synced till here 102623
2014-07-22 13:27:20,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060838872 with entries=104, filesize=77.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060840391
2014-07-22 13:27:22,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:22,830 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102714 synced till here 102711
2014-07-22 13:27:22,869 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060840391 with entries=88, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060842815
2014-07-22 13:27:24,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:24,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102809 synced till here 102800
2014-07-22 13:27:24,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060842815 with entries=95, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060844326
2014-07-22 13:27:25,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:25,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060844326 with entries=91, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060845235
2014-07-22 13:27:26,704 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:26,724 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 102985 synced till here 102982
2014-07-22 13:27:26,748 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060845235 with entries=85, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060846704
2014-07-22 13:27:28,161 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:28,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103079 synced till here 103069
2014-07-22 13:27:28,352 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060846704 with entries=94, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060848162
2014-07-22 13:27:30,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:30,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103180 synced till here 103160
2014-07-22 13:27:30,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060848162 with entries=101, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060850151
2014-07-22 13:27:32,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:32,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103293 synced till here 103270
2014-07-22 13:27:33,591 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060850151 with entries=113, filesize=85.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060852258
2014-07-22 13:27:34,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:34,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103395 synced till here 103373
2014-07-22 13:27:35,514 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20461, memsize=382.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/daebab96618640a5b735a4639d84d01b
2014-07-22 13:27:35,544 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/daebab96618640a5b735a4639d84d01b as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/daebab96618640a5b735a4639d84d01b
2014-07-22 13:27:35,557 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/daebab96618640a5b735a4639d84d01b, entries=1394030, sequenceid=20461, filesize=99.3m
2014-07-22 13:27:35,559 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~938.8m/984362240, currentsize=453.7m/475776000 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 25157ms, sequenceid=20461, compaction requested=true
2014-07-22 13:27:35,560 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:35,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 13:27:35,560 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 933.4m
2014-07-22 13:27:35,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 13:27:35,560 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:35,561 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:35,561 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:27:35,691 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:27:35,738 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060852258 with entries=102, filesize=77.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060854263
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060764224
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060772790
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060774936
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060777097
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060778573
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060780624
2014-07-22 13:27:35,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060782622
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060784633
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060786568
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060788790
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060790891
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060792895
2014-07-22 13:27:35,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060794979
2014-07-22 13:27:36,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:37,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103506 synced till here 103479
2014-07-22 13:27:37,752 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060854263 with entries=111, filesize=87.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060856437
2014-07-22 13:27:37,801 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:39,723 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1275ms
GC pool 'ParNew' had collection(s): count=1 time=1307ms
2014-07-22 13:27:39,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:39,869 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103618 synced till here 103596
2014-07-22 13:27:40,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060856437 with entries=112, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060859850
2014-07-22 13:27:42,665 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:42,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103744 synced till here 103720
2014-07-22 13:27:43,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060859850 with entries=126, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060862666
2014-07-22 13:27:44,972 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:45,051 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103854 synced till here 103827
2014-07-22 13:27:45,137 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060862666 with entries=110, filesize=73.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060864972
2014-07-22 13:27:47,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:47,326 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 103979 synced till here 103958
2014-07-22 13:27:48,655 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1063ms
GC pool 'ParNew' had collection(s): count=1 time=1093ms
2014-07-22 13:27:48,768 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060864972 with entries=125, filesize=98.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060867282
2014-07-22 13:27:49,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:49,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104090 synced till here 104066
2014-07-22 13:27:50,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060867282 with entries=111, filesize=88.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060869760
2014-07-22 13:27:51,314 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,314 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,314 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,315 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,317 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,318 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,318 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,320 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,320 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,321 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,322 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,322 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,323 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,325 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,325 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,325 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,326 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,326 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,329 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,329 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,329 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,329 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,329 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,330 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,330 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,330 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,330 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,330 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,331 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,332 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,333 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,334 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,334 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,335 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,335 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,335 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,337 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,661 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,661 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,663 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,663 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,664 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,664 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,664 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,665 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,665 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,665 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,665 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,665 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:51,672 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:27:52,196 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20529, memsize=484.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/468048c4d81a4c859fed450e5332887e
2014-07-22 13:27:52,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/468048c4d81a4c859fed450e5332887e as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/468048c4d81a4c859fed450e5332887e
2014-07-22 13:27:52,229 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/468048c4d81a4c859fed450e5332887e, entries=1764040, sequenceid=20529, filesize=125.6m
2014-07-22 13:27:52,230 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~646.9m/678301920, currentsize=519.9m/545168000 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 34452ms, sequenceid=20529, compaction requested=true
2014-07-22 13:27:52,231 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:27:52,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 13:27:52,231 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 559ms
2014-07-22 13:27:52,231 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 13:27:52,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:27:52,231 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 815.5m
2014-07-22 13:27:52,231 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:27:52,231 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:27:52,233 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 572ms
2014-07-22 13:27:52,233 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,233 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-07-22 13:27:52,233 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,234 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-07-22 13:27:52,234 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,234 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 573ms
2014-07-22 13:27:52,234 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,234 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 573ms
2014-07-22 13:27:52,234 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,249 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 585ms
2014-07-22 13:27:52,249 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,249 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 585ms
2014-07-22 13:27:52,249 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,254 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 590ms
2014-07-22 13:27:52,254 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,257 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 594ms
2014-07-22 13:27:52,257 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,257 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 594ms
2014-07-22 13:27:52,257 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,258 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 597ms
2014-07-22 13:27:52,258 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,263 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 602ms
2014-07-22 13:27:52,263 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,263 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 926ms
2014-07-22 13:27:52,263 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,263 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 928ms
2014-07-22 13:27:52,263 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,265 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 930ms
2014-07-22 13:27:52,265 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,266 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 931ms
2014-07-22 13:27:52,266 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,266 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 932ms
2014-07-22 13:27:52,266 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,266 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 932ms
2014-07-22 13:27:52,266 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,266 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 933ms
2014-07-22 13:27:52,266 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,266 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 934ms
2014-07-22 13:27:52,266 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,267 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 936ms
2014-07-22 13:27:52,267 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,267 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 937ms
2014-07-22 13:27:52,267 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,268 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 939ms
2014-07-22 13:27:52,268 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,268 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 938ms
2014-07-22 13:27:52,268 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,268 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 943ms
2014-07-22 13:27:52,268 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,269 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 943ms
2014-07-22 13:27:52,269 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,277 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 948ms
2014-07-22 13:27:52,277 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,277 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 952ms
2014-07-22 13:27:52,277 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,285 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 956ms
2014-07-22 13:27:52,285 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,285 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 956ms
2014-07-22 13:27:52,285 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 957ms
2014-07-22 13:27:52,286 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 960ms
2014-07-22 13:27:52,286 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 960ms
2014-07-22 13:27:52,286 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 961ms
2014-07-22 13:27:52,286 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 961ms
2014-07-22 13:27:52,286 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,286 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 961ms
2014-07-22 13:27:52,287 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,293 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 970ms
2014-07-22 13:27:52,327 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,327 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-07-22 13:27:52,327 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,327 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-07-22 13:27:52,328 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,328 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1008ms
2014-07-22 13:27:52,328 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,328 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1008ms
2014-07-22 13:27:52,329 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,329 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1010ms
2014-07-22 13:27:52,329 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,329 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1011ms
2014-07-22 13:27:52,329 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,337 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1019ms
2014-07-22 13:27:52,337 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,337 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1020ms
2014-07-22 13:27:52,338 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,353 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1039ms
2014-07-22 13:27:52,353 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,354 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1039ms
2014-07-22 13:27:52,354 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,354 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1040ms
2014-07-22 13:27:52,354 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:52,354 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1040ms
2014-07-22 13:27:52,354 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:27:53,677 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:27:53,933 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:53,937 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:27:53,949 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104225 synced till here 104192
2014-07-22 13:27:54,200 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060869760 with entries=135, filesize=89.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060873934
2014-07-22 13:27:54,462 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:27:56,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:56,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104344 synced till here 104308
2014-07-22 13:27:56,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060873934 with entries=119, filesize=95.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060876031
2014-07-22 13:27:57,078 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:57,551 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104459 synced till here 104444
2014-07-22 13:27:57,660 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060876031 with entries=115, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060877079
2014-07-22 13:27:59,647 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:27:59,995 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104573 synced till here 104539
2014-07-22 13:28:00,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060877079 with entries=114, filesize=94.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060879648
2014-07-22 13:28:01,182 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,186 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,187 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,187 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,188 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,190 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,192 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,193 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,194 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,195 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,195 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,197 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,198 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,198 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,199 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,200 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,201 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,203 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,203 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,203 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,205 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,205 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,205 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,206 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,206 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,207 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,209 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,209 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,212 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,213 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,220 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,225 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,225 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,227 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,229 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,229 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,230 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,231 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,232 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,246 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,246 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,250 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,250 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,250 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,250 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,251 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,315 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,334 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,335 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:01,351 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:28:06,183 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,187 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,188 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,188 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,188 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,190 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,193 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,193 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,194 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,195 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,196 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,197 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,198 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,198 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,200 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,200 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,201 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,203 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,204 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,204 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,206 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,206 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5019ms
2014-07-22 13:28:06,206 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,206 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,207 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,207 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 13:28:06,210 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,210 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,212 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,213 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,221 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,225 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,226 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,227 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,229 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,230 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,231 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-07-22 13:28:06,231 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,233 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,246 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,247 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,250 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,250 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-07-22 13:28:06,250 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,251 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-07-22 13:28:06,252 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,315 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,335 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:06,335 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:28:06,352 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:28:11,184 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,187 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,188 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,188 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,188 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,191 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,194 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,194 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:28:11,195 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,196 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,198 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,199 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,199 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,200 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,200 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,202 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,204 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,204 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,205 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,206 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,207 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10019ms
2014-07-22 13:28:11,207 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,207 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,207 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:28:11,208 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-22 13:28:11,210 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,210 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,213 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,214 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,221 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,226 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,226 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,228 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,230 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,231 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10002ms
2014-07-22 13:28:11,231 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-22 13:28:11,232 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,233 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,247 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,247 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,250 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,250 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10004ms
2014-07-22 13:28:11,251 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,252 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10005ms
2014-07-22 13:28:11,252 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,316 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:28:11,335 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,336 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:11,352 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:28:16,185 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,187 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,188 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,188 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,189 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-22 13:28:16,191 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,194 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,195 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,195 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,196 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,197 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,198 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,199 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,200 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,200 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,201 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,202 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,204 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,204 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,205 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,206 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,207 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15020ms
2014-07-22 13:28:16,207 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,207 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,208 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,208 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-22 13:28:16,210 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,210 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,213 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,214 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,221 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,226 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,227 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,228 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,230 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,231 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,231 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15004ms
2014-07-22 13:28:16,232 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,234 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,247 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,248 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,251 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15000ms
2014-07-22 13:28:16,251 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15005ms
2014-07-22 13:28:16,251 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,252 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15006ms
2014-07-22 13:28:16,253 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15002ms
2014-07-22 13:28:16,316 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,335 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,337 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:16,352 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15001ms
2014-07-22 13:28:19,362 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20745, memsize=722.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/86d04bfbb2064b69a8deec5dea26b93c
2014-07-22 13:28:19,380 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/86d04bfbb2064b69a8deec5dea26b93c as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/86d04bfbb2064b69a8deec5dea26b93c
2014-07-22 13:28:19,395 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/86d04bfbb2064b69a8deec5dea26b93c, entries=2628670, sequenceid=20745, filesize=187.1m
2014-07-22 13:28:19,396 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~961.2m/1007853680, currentsize=387.6m/406449440 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 43836ms, sequenceid=20745, compaction requested=true
2014-07-22 13:28:19,396 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:28:19,396 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 46 store files, 0 compacting, 46 eligible, 2000 blocking
2014-07-22 13:28:19,397 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18046ms
2014-07-22 13:28:19,397 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 46 files from compaction candidates
2014-07-22 13:28:19,397 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18062ms
2014-07-22 13:28:19,397 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 843.6m
2014-07-22 13:28:19,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:28:19,397 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,397 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:28:19,397 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18063ms
2014-07-22 13:28:19,397 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,397 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:28:19,401 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18086ms
2014-07-22 13:28:19,401 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,401 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18150ms
2014-07-22 13:28:19,401 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,402 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18156ms
2014-07-22 13:28:19,402 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,402 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18152ms
2014-07-22 13:28:19,402 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,402 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18156ms
2014-07-22 13:28:19,402 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,405 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18155ms
2014-07-22 13:28:19,405 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,405 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18159ms
2014-07-22 13:28:19,406 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,406 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18160ms
2014-07-22 13:28:19,406 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,406 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18174ms
2014-07-22 13:28:19,406 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,411 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18180ms
2014-07-22 13:28:19,412 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,412 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18185ms
2014-07-22 13:28:19,412 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,413 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18184ms
2014-07-22 13:28:19,413 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,413 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18184ms
2014-07-22 13:28:19,413 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,421 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18194ms
2014-07-22 13:28:19,421 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,423 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18198ms
2014-07-22 13:28:19,423 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,423 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18198ms
2014-07-22 13:28:19,423 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,423 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18203ms
2014-07-22 13:28:19,423 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,429 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18216ms
2014-07-22 13:28:19,429 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,437 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18225ms
2014-07-22 13:28:19,437 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,437 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18228ms
2014-07-22 13:28:19,437 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,438 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18229ms
2014-07-22 13:28:19,438 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,445 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18242ms
2014-07-22 13:28:19,445 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,445 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18240ms
2014-07-22 13:28:19,445 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,449 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18243ms
2014-07-22 13:28:19,449 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,449 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18243ms
2014-07-22 13:28:19,449 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,450 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18263ms
2014-07-22 13:28:19,450 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,450 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18245ms
2014-07-22 13:28:19,450 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,452 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18249ms
2014-07-22 13:28:19,452 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,452 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18249ms
2014-07-22 13:28:19,452 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,455 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18252ms
2014-07-22 13:28:19,455 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,461 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18260ms
2014-07-22 13:28:19,461 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,461 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18261ms
2014-07-22 13:28:19,461 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:19,537 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18338ms
2014-07-22 13:28:19,538 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,538 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18340ms
2014-07-22 13:28:19,538 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,539 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22444,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877094,"queuetimems":3183,"class":"HRegionServer","responsesize":17072,"method":"Multi"}
2014-07-22 13:28:19,547 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22461,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877085,"queuetimems":3403,"class":"HRegionServer","responsesize":17230,"method":"Multi"}
2014-07-22 13:28:19,547 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877093,"queuetimems":3214,"class":"HRegionServer","responsesize":16811,"method":"Multi"}
2014-07-22 13:28:19,548 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18350ms
2014-07-22 13:28:19,549 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,549 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877085,"queuetimems":5151,"class":"HRegionServer","responsesize":16952,"method":"Multi"}
2014-07-22 13:28:19,557 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18360ms
2014-07-22 13:28:19,557 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,558 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876949,"queuetimems":5360,"class":"HRegionServer","responsesize":16894,"method":"Multi"}
2014-07-22 13:28:19,559 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22596,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876962,"queuetimems":5248,"class":"HRegionServer","responsesize":16897,"method":"Multi"}
2014-07-22 13:28:19,562 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18367ms
2014-07-22 13:28:19,562 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,563 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18368ms
2014-07-22 13:28:19,563 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,563 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18369ms
2014-07-22 13:28:19,563 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,563 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876950,"queuetimems":5268,"class":"HRegionServer","responsesize":17044,"method":"Multi"}
2014-07-22 13:28:19,563 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22588,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876975,"queuetimems":5135,"class":"HRegionServer","responsesize":17147,"method":"Multi"}
2014-07-22 13:28:19,563 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877085,"queuetimems":3433,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:28:19,565 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18373ms
2014-07-22 13:28:19,565 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,565 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18372ms
2014-07-22 13:28:19,566 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,567 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18378ms
2014-07-22 13:28:19,567 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,567 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18379ms
2014-07-22 13:28:19,567 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,568 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18380ms
2014-07-22 13:28:19,568 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,568 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18381ms
2014-07-22 13:28:19,568 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,568 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18382ms
2014-07-22 13:28:19,568 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060879648 with entries=85, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060899520
2014-07-22 13:28:19,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060797423
2014-07-22 13:28:19,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060806567
2014-07-22 13:28:19,576 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060808669
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810158
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060810971
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060812661
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060813929
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060815383
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060817116
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060819193
2014-07-22 13:28:19,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060820167
2014-07-22 13:28:19,617 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18435ms
2014-07-22 13:28:19,618 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:28:19,618 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22619,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876949,"queuetimems":5329,"class":"HRegionServer","responsesize":16649,"method":"Multi"}
2014-07-22 13:28:19,618 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877085,"queuetimems":3352,"class":"HRegionServer","responsesize":16959,"method":"Multi"}
2014-07-22 13:28:19,618 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22600,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876969,"queuetimems":5193,"class":"HRegionServer","responsesize":17354,"method":"Multi"}
2014-07-22 13:28:19,647 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22697,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876949,"queuetimems":5391,"class":"HRegionServer","responsesize":16953,"method":"Multi"}
2014-07-22 13:28:19,654 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876949,"queuetimems":5455,"class":"HRegionServer","responsesize":16926,"method":"Multi"}
2014-07-22 13:28:19,654 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876974,"queuetimems":5166,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 13:28:20,503 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876962,"queuetimems":5217,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-07-22 13:28:20,503 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060876950,"queuetimems":5299,"class":"HRegionServer","responsesize":17098,"method":"Multi"}
2014-07-22 13:28:20,568 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20883, memsize=535.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/6a1493c9e9ac4105816a361c4d9ea2f2
2014-07-22 13:28:20,580 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/6a1493c9e9ac4105816a361c4d9ea2f2 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/6a1493c9e9ac4105816a361c4d9ea2f2
2014-07-22 13:28:20,590 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/6a1493c9e9ac4105816a361c4d9ea2f2, entries=1948590, sequenceid=20883, filesize=138.8m
2014-07-22 13:28:20,590 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~815.5m/855063200, currentsize=176.1m/184630000 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 28359ms, sequenceid=20883, compaction requested=true
2014-07-22 13:28:20,591 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:28:20,591 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 13:28:20,591 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 655.0m
2014-07-22 13:28:20,591 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 13:28:20,591 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:28:20,591 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:28:20,591 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:28:20,775 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:28:21,054 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22639,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878414,"queuetimems":0,"class":"HRegionServer","responsesize":16898,"method":"Multi"}
2014-07-22 13:28:21,056 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877100,"queuetimems":3126,"class":"HRegionServer","responsesize":16883,"method":"Multi"}
2014-07-22 13:28:21,082 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23560,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877521,"queuetimems":3184,"class":"HRegionServer","responsesize":17206,"method":"Multi"}
2014-07-22 13:28:21,088 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20857,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060880230,"queuetimems":1647,"class":"HRegionServer","responsesize":16897,"method":"Multi"}
2014-07-22 13:28:21,088 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878189,"queuetimems":0,"class":"HRegionServer","responsesize":17280,"method":"Multi"}
2014-07-22 13:28:21,088 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22849,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878239,"queuetimems":0,"class":"HRegionServer","responsesize":16952,"method":"Multi"}
2014-07-22 13:28:21,091 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877082,"queuetimems":5211,"class":"HRegionServer","responsesize":16920,"method":"Multi"}
2014-07-22 13:28:21,102 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877518,"queuetimems":3448,"class":"HRegionServer","responsesize":17510,"method":"Multi"}
2014-07-22 13:28:21,102 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878317,"queuetimems":0,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-07-22 13:28:21,106 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878274,"queuetimems":0,"class":"HRegionServer","responsesize":17098,"method":"Multi"}
2014-07-22 13:28:21,167 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24080,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877086,"queuetimems":3294,"class":"HRegionServer","responsesize":16805,"method":"Multi"}
2014-07-22 13:28:21,168 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878366,"queuetimems":1,"class":"HRegionServer","responsesize":16953,"method":"Multi"}
2014-07-22 13:28:21,174 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23064,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878109,"queuetimems":0,"class":"HRegionServer","responsesize":16537,"method":"Multi"}
2014-07-22 13:28:21,174 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23131,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878042,"queuetimems":1,"class":"HRegionServer","responsesize":17118,"method":"Multi"}
2014-07-22 13:28:21,167 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878010,"queuetimems":0,"class":"HRegionServer","responsesize":16926,"method":"Multi"}
2014-07-22 13:28:21,174 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877098,"queuetimems":3152,"class":"HRegionServer","responsesize":16918,"method":"Multi"}
2014-07-22 13:28:21,186 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23651,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877534,"queuetimems":2992,"class":"HRegionServer","responsesize":16845,"method":"Multi"}
2014-07-22 13:28:21,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:21,305 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104790 synced till here 104756
2014-07-22 13:28:21,369 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:28:21,556 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23401,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878155,"queuetimems":0,"class":"HRegionServer","responsesize":17354,"method":"Multi"}
2014-07-22 13:28:21,556 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877530,"queuetimems":3028,"class":"HRegionServer","responsesize":17144,"method":"Multi"}
2014-07-22 13:28:21,576 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21640,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060879930,"queuetimems":1465,"class":"HRegionServer","responsesize":16984,"method":"Multi"}
2014-07-22 13:28:21,585 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21630,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060879946,"queuetimems":1395,"class":"HRegionServer","responsesize":16649,"method":"Multi"}
2014-07-22 13:28:21,592 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24489,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877084,"queuetimems":5181,"class":"HRegionServer","responsesize":17280,"method":"Multi"}
2014-07-22 13:28:21,601 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877522,"queuetimems":3144,"class":"HRegionServer","responsesize":17602,"method":"Multi"}
2014-07-22 13:28:21,602 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24051,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877521,"queuetimems":3484,"class":"HRegionServer","responsesize":17027,"method":"Multi"}
2014-07-22 13:28:21,602 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877523,"queuetimems":3100,"class":"HRegionServer","responsesize":16925,"method":"Multi"}
2014-07-22 13:28:21,601 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23497,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060878075,"queuetimems":1,"class":"HRegionServer","responsesize":17044,"method":"Multi"}
2014-07-22 13:28:21,602 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877093,"queuetimems":3251,"class":"HRegionServer","responsesize":17060,"method":"Multi"}
2014-07-22 13:28:21,602 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877523,"queuetimems":3064,"class":"HRegionServer","responsesize":16894,"method":"Multi"}
2014-07-22 13:28:21,623 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24469,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060877101,"queuetimems":3096,"class":"HRegionServer","responsesize":16705,"method":"Multi"}
2014-07-22 13:28:21,623 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060879938,"queuetimems":1426,"class":"HRegionServer","responsesize":16894,"method":"Multi"}
2014-07-22 13:28:21,870 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060899520 with entries=132, filesize=93.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060901283
2014-07-22 13:28:21,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:21,973 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:28:22,999 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21749,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060881248,"queuetimems":2569,"class":"HRegionServer","responsesize":16911,"method":"Multi"}
2014-07-22 13:28:23,598 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:23,601 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22404,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060881196,"queuetimems":2550,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-07-22 13:28:23,650 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060881183,"queuetimems":2568,"class":"HRegionServer","responsesize":17147,"method":"Multi"}
2014-07-22 13:28:23,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 104908 synced till here 104880
2014-07-22 13:28:23,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060901283 with entries=118, filesize=92.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060903599
2014-07-22 13:28:23,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:24,156 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:28:25,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:25,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105041 synced till here 105030
2014-07-22 13:28:25,420 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060903599 with entries=133, filesize=92.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060905149
2014-07-22 13:28:25,421 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:27,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:27,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105145 synced till here 105126
2014-07-22 13:28:27,723 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060905149 with entries=104, filesize=82.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060907473
2014-07-22 13:28:27,751 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:28,982 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:28,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105248 synced till here 105244
2014-07-22 13:28:29,025 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060907473 with entries=103, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060908982
2014-07-22 13:28:29,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:31,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:31,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060908982 with entries=85, filesize=60.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060911167
2014-07-22 13:28:31,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:33,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:33,176 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060911167 with entries=87, filesize=62.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060913130
2014-07-22 13:28:33,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:34,786 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:34,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105507 synced till here 105506
2014-07-22 13:28:34,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060913130 with entries=87, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060914787
2014-07-22 13:28:34,905 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:36,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:36,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105593 synced till here 105592
2014-07-22 13:28:36,302 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060914787 with entries=86, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060916263
2014-07-22 13:28:36,304 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:28:36,965 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=20981, memsize=246.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/55fdc31c92ba4ade8b240a0a0a17ce96
2014-07-22 13:28:36,981 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/55fdc31c92ba4ade8b240a0a0a17ce96 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/55fdc31c92ba4ade8b240a0a0a17ce96
2014-07-22 13:28:36,995 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/55fdc31c92ba4ade8b240a0a0a17ce96, entries=898250, sequenceid=20981, filesize=64.0m
2014-07-22 13:28:36,996 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~700.2m/734229920, currentsize=284.5m/298337680 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 16405ms, sequenceid=20981, compaction requested=true
2014-07-22 13:28:36,996 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:28:36,996 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 13:28:36,997 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 395.7m
2014-07-22 13:28:36,997 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 13:28:36,997 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:28:36,997 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:28:36,997 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:28:37,027 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:28:37,817 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:28:37,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:37,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060916263 with entries=93, filesize=68.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060917825
2014-07-22 13:28:39,479 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:39,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105775 synced till here 105771
2014-07-22 13:28:39,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060917825 with entries=89, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060919479
2014-07-22 13:28:41,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:41,295 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 105866 synced till here 105856
2014-07-22 13:28:41,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060919479 with entries=91, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060921268
2014-07-22 13:28:42,347 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:42,380 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060921268 with entries=90, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060922347
2014-07-22 13:28:43,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:43,823 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106041 synced till here 106036
2014-07-22 13:28:44,011 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060922347 with entries=85, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060923806
2014-07-22 13:28:45,538 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:45,604 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21005, memsize=385.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/260ddb0d53a043da95c99db9f417b7d0
2014-07-22 13:28:45,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106133 synced till here 106130
2014-07-22 13:28:45,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060923806 with entries=92, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060925539
2014-07-22 13:28:45,683 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/260ddb0d53a043da95c99db9f417b7d0 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/260ddb0d53a043da95c99db9f417b7d0
2014-07-22 13:28:45,692 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/260ddb0d53a043da95c99db9f417b7d0, entries=1403760, sequenceid=21005, filesize=100.0m
2014-07-22 13:28:45,693 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~843.6m/884557520, currentsize=442.6m/464059440 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 26296ms, sequenceid=21005, compaction requested=true
2014-07-22 13:28:45,693 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:28:45,693 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 13:28:45,693 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 828.0m
2014-07-22 13:28:45,694 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 13:28:45,694 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:28:45,694 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:28:45,694 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:28:45,719 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:28:47,027 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:28:47,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:47,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106248 synced till here 106245
2014-07-22 13:28:47,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060925539 with entries=115, filesize=85.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060927228
2014-07-22 13:28:47,962 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21145, memsize=200.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/db56ec6a56c94060adb045010ecec55c
2014-07-22 13:28:47,977 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/db56ec6a56c94060adb045010ecec55c as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/db56ec6a56c94060adb045010ecec55c
2014-07-22 13:28:48,304 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/db56ec6a56c94060adb045010ecec55c, entries=731180, sequenceid=21145, filesize=52.1m
2014-07-22 13:28:48,305 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~395.7m/414920640, currentsize=59.0m/61873360 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 11308ms, sequenceid=21145, compaction requested=true
2014-07-22 13:28:48,305 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:28:48,306 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 27 store files, 0 compacting, 27 eligible, 2000 blocking
2014-07-22 13:28:48,306 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 27 files from compaction candidates
2014-07-22 13:28:48,306 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 659.6m
2014-07-22 13:28:48,306 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:28:48,306 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:28:48,306 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:28:48,858 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:48,866 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:28:49,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106353 synced till here 106352
2014-07-22 13:28:49,284 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060927228 with entries=105, filesize=76.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060928858
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060821612
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060822933
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060824159
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060825501
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060826382
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060827876
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060829504
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060830915
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060832374
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060834369
2014-07-22 13:28:49,285 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060835966
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060837393
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060838872
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060840391
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060842815
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060844326
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060845235
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060846704
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060848162
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060850151
2014-07-22 13:28:49,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060852258
2014-07-22 13:28:50,458 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:50,476 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106440 synced till here 106436
2014-07-22 13:28:50,555 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060928858 with entries=87, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060930459
2014-07-22 13:28:51,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:52,045 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106534 synced till here 106531
2014-07-22 13:28:52,126 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060930459 with entries=94, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060931997
2014-07-22 13:28:53,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:53,836 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106640 synced till here 106621
2014-07-22 13:28:53,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060931997 with entries=106, filesize=80.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060933682
2014-07-22 13:28:55,260 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:55,293 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106729 synced till here 106727
2014-07-22 13:28:55,313 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060933682 with entries=89, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060935260
2014-07-22 13:28:56,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:56,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106816 synced till here 106812
2014-07-22 13:28:56,733 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060935260 with entries=87, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060936664
2014-07-22 13:28:57,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:57,580 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060936664 with entries=84, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060937552
2014-07-22 13:28:58,651 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:58,670 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 106987 synced till here 106986
2014-07-22 13:28:58,681 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060937552 with entries=87, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060938652
2014-07-22 13:28:59,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:28:59,938 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107074 synced till here 107070
2014-07-22 13:28:59,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060938652 with entries=87, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060939915
2014-07-22 13:29:01,498 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:01,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107174 synced till here 107171
2014-07-22 13:29:01,697 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060939915 with entries=100, filesize=72.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060941499
2014-07-22 13:29:03,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:03,094 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107262 synced till here 107256
2014-07-22 13:29:03,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060941499 with entries=88, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060943068
2014-07-22 13:29:03,704 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21283, memsize=272.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5fabb241e5ae46b8b5403b06ebb2c7b7
2014-07-22 13:29:03,923 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5fabb241e5ae46b8b5403b06ebb2c7b7 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5fabb241e5ae46b8b5403b06ebb2c7b7
2014-07-22 13:29:03,943 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5fabb241e5ae46b8b5403b06ebb2c7b7, entries=992340, sequenceid=21283, filesize=70.7m
2014-07-22 13:29:03,944 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~829.4m/869673120, currentsize=363.5m/381196560 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 18251ms, sequenceid=21283, compaction requested=true
2014-07-22 13:29:03,944 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:29:03,944 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 47 store files, 0 compacting, 47 eligible, 2000 blocking
2014-07-22 13:29:03,945 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 798.4m
2014-07-22 13:29:03,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 47 files from compaction candidates
2014-07-22 13:29:03,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:29:03,945 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:29:03,945 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:29:03,981 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:29:04,587 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:04,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107387 synced till here 107386
2014-07-22 13:29:05,066 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:29:05,779 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060943068 with entries=125, filesize=88.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060944588
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060854263
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060856437
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060859850
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060862666
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060864972
2014-07-22 13:29:05,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060867282
2014-07-22 13:29:05,850 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21310, memsize=315.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/931f390fcc3d4cae854e6aa68deffcae
2014-07-22 13:29:05,868 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/931f390fcc3d4cae854e6aa68deffcae as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/931f390fcc3d4cae854e6aa68deffcae
2014-07-22 13:29:05,883 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/931f390fcc3d4cae854e6aa68deffcae, entries=1147690, sequenceid=21310, filesize=81.8m
2014-07-22 13:29:05,883 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~662.7m/694849520, currentsize=341.6m/358221200 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 17577ms, sequenceid=21310, compaction requested=true
2014-07-22 13:29:05,884 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:29:05,884 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 13:29:05,884 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 829.7m
2014-07-22 13:29:05,884 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 13:29:05,884 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:29:05,884 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:29:05,884 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:29:06,147 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:29:07,277 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:07,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107484 synced till here 107467
2014-07-22 13:29:07,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060944588 with entries=97, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060947277
2014-07-22 13:29:07,417 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060869760
2014-07-22 13:29:07,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060873934
2014-07-22 13:29:07,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060876031
2014-07-22 13:29:07,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060877079
2014-07-22 13:29:07,466 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:29:08,532 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:08,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107575 synced till here 107571
2014-07-22 13:29:08,600 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060947277 with entries=91, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060948532
2014-07-22 13:29:09,596 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:09,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107659 synced till here 107658
2014-07-22 13:29:09,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060948532 with entries=84, filesize=62.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060949596
2014-07-22 13:29:11,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:11,837 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060949596 with entries=114, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060951086
2014-07-22 13:29:14,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:14,343 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107859 synced till here 107856
2014-07-22 13:29:14,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060951086 with entries=86, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060954326
2014-07-22 13:29:15,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:16,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 107955 synced till here 107951
2014-07-22 13:29:16,206 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060954326 with entries=96, filesize=71.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060955286
2014-07-22 13:29:17,153 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:17,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108057 synced till here 108048
2014-07-22 13:29:18,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060955286 with entries=102, filesize=74.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060957153
2014-07-22 13:29:18,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:18,835 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108147 synced till here 108144
2014-07-22 13:29:18,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060957153 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060958807
2014-07-22 13:29:19,726 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=2, accesses=9341, hits=2761, hitRatio=29.55%, , cachingAccesses=2768, cachingHits=2761, cachingHitsRatio=99.74%, evictions=0, evicted=5, evictedPerRun=Infinity
2014-07-22 13:29:20,730 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:20,748 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108231 synced till here 108229
2014-07-22 13:29:20,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060958807 with entries=84, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060960730
2014-07-22 13:29:22,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:22,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108317 synced till here 108315
2014-07-22 13:29:22,510 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060960730 with entries=86, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060962451
2014-07-22 13:29:24,122 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:24,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108409 synced till here 108401
2014-07-22 13:29:24,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060962451 with entries=92, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060964123
2014-07-22 13:29:24,235 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:29:25,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:25,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108503 synced till here 108494
2014-07-22 13:29:25,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060964123 with entries=94, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060965552
2014-07-22 13:29:25,962 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:25,963 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:25,964 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:25,964 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:25,964 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:25,992 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,005 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,006 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,007 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,008 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,022 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,023 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,035 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,076 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,108 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,657 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,779 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,831 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,876 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:26,941 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:27,224 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:28,181 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:28,215 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:28,246 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,002 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,015 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,030 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,063 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,094 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,125 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,160 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,190 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,222 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:29,252 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,174 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,205 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,237 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,269 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,302 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,333 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,366 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,397 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:30,963 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:30,964 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:30,964 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-07-22 13:29:30,964 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:30,965 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:30,993 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:31,005 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,007 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,007 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,008 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:31,022 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,023 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,035 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,076 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:31,108 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,658 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:31,779 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,831 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,876 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:31,941 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:32,225 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:32,271 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,303 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,335 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,367 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,398 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,429 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,461 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,493 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:32,856 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21503, memsize=598.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c1a432b3622648ef867cedf449aa9019
2014-07-22 13:29:32,874 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/c1a432b3622648ef867cedf449aa9019 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c1a432b3622648ef867cedf449aa9019
2014-07-22 13:29:32,890 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/c1a432b3622648ef867cedf449aa9019, entries=2179740, sequenceid=21503, filesize=155.2m
2014-07-22 13:29:32,890 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~805.2m/844314080, currentsize=369.7m/387649680 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 28946ms, sequenceid=21503, compaction requested=true
2014-07-22 13:29:32,891 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:29:32,891 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 13:29:32,891 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 398ms
2014-07-22 13:29:32,891 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,891 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 13:29:32,891 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 430ms
2014-07-22 13:29:32,891 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:29:32,891 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,891 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 743.4m
2014-07-22 13:29:32,891 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 462ms
2014-07-22 13:29:32,891 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,891 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:29:32,891 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 493ms
2014-07-22 13:29:32,892 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,892 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:29:32,893 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 526ms
2014-07-22 13:29:32,893 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,893 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 558ms
2014-07-22 13:29:32,893 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,894 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 591ms
2014-07-22 13:29:32,894 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,894 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 623ms
2014-07-22 13:29:32,894 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,894 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5670ms
2014-07-22 13:29:32,894 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,894 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5953ms
2014-07-22 13:29:32,894 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,894 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6018ms
2014-07-22 13:29:32,894 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,905 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6074ms
2014-07-22 13:29:32,905 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,911 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6132ms
2014-07-22 13:29:32,911 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,911 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6254ms
2014-07-22 13:29:32,911 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,912 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6804ms
2014-07-22 13:29:32,912 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,919 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6844ms
2014-07-22 13:29:32,919 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,919 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6884ms
2014-07-22 13:29:32,919 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,927 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6904ms
2014-07-22 13:29:32,927 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,927 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6905ms
2014-07-22 13:29:32,927 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,927 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6920ms
2014-07-22 13:29:32,927 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,929 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6922ms
2014-07-22 13:29:32,929 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,929 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6923ms
2014-07-22 13:29:32,930 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,930 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6925ms
2014-07-22 13:29:32,930 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,945 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6953ms
2014-07-22 13:29:32,945 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,953 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6989ms
2014-07-22 13:29:32,953 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,957 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6994ms
2014-07-22 13:29:32,957 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,958 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6996ms
2014-07-22 13:29:32,958 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,959 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6996ms
2014-07-22 13:29:32,959 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,959 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6997ms
2014-07-22 13:29:32,959 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,960 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2563ms
2014-07-22 13:29:32,960 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,962 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2596ms
2014-07-22 13:29:32,962 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,971 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2638ms
2014-07-22 13:29:32,971 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,972 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2670ms
2014-07-22 13:29:32,972 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,972 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2703ms
2014-07-22 13:29:32,972 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,972 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2735ms
2014-07-22 13:29:32,972 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,972 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2767ms
2014-07-22 13:29:32,972 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,977 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2803ms
2014-07-22 13:29:32,977 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,977 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3725ms
2014-07-22 13:29:32,977 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,978 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3756ms
2014-07-22 13:29:32,978 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,978 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3788ms
2014-07-22 13:29:32,978 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,978 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3818ms
2014-07-22 13:29:32,978 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,978 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3853ms
2014-07-22 13:29:32,978 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,985 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3891ms
2014-07-22 13:29:32,985 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,986 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3923ms
2014-07-22 13:29:32,986 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,993 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3963ms
2014-07-22 13:29:32,993 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,993 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3978ms
2014-07-22 13:29:32,993 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,994 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3992ms
2014-07-22 13:29:32,994 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,994 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4748ms
2014-07-22 13:29:32,994 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,995 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4779ms
2014-07-22 13:29:32,995 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:32,995 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4814ms
2014-07-22 13:29:32,995 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:29:34,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:34,284 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:29:34,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108596 synced till here 108585
2014-07-22 13:29:34,453 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060965552 with entries=93, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060974284
2014-07-22 13:29:34,494 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:29:36,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:36,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108721 synced till here 108698
2014-07-22 13:29:36,330 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060974284 with entries=125, filesize=96.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060976134
2014-07-22 13:29:36,796 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966033,"queuetimems":0,"class":"HRegionServer","responsesize":16545,"method":"Multi"}
2014-07-22 13:29:36,891 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966874,"queuetimems":0,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-07-22 13:29:36,894 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966829,"queuetimems":0,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-07-22 13:29:36,903 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10126,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966776,"queuetimems":1,"class":"HRegionServer","responsesize":17335,"method":"Multi"}
2014-07-22 13:29:36,906 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966074,"queuetimems":1,"class":"HRegionServer","responsesize":16600,"method":"Multi"}
2014-07-22 13:29:37,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:37,695 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966656,"queuetimems":0,"class":"HRegionServer","responsesize":17094,"method":"Multi"}
2014-07-22 13:29:37,707 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11600,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060966106,"queuetimems":1,"class":"HRegionServer","responsesize":16541,"method":"Multi"}
2014-07-22 13:29:37,707 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10474,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060967222,"queuetimems":0,"class":"HRegionServer","responsesize":16948,"method":"Multi"}
2014-07-22 13:29:37,711 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060965990,"queuetimems":0,"class":"HRegionServer","responsesize":16971,"method":"Multi"}
2014-07-22 13:29:37,752 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108848 synced till here 108821
2014-07-22 13:29:37,759 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21554, memsize=642.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/203b3b24a2ef428291cb40ba84543b27
2014-07-22 13:29:37,770 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/203b3b24a2ef428291cb40ba84543b27 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/203b3b24a2ef428291cb40ba84543b27
2014-07-22 13:29:37,784 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/203b3b24a2ef428291cb40ba84543b27, entries=2339860, sequenceid=21554, filesize=166.6m
2014-07-22 13:29:37,785 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~848.5m/889714240, currentsize=423.3m/443837920 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 31901ms, sequenceid=21554, compaction requested=true
2014-07-22 13:29:37,785 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:29:37,785 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 13:29:37,786 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 13:29:37,786 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:29:37,786 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:29:37,786 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 783.7m
2014-07-22 13:29:37,786 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:29:37,796 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:29:38,139 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060976134 with entries=127, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060977694
2014-07-22 13:29:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060879648
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060899520
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060901283
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060903599
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060905149
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060907473
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060908982
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060911167
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060913130
2014-07-22 13:29:38,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060914787
2014-07-22 13:29:38,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:38,617 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:29:38,716 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:39,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 108957 synced till here 108933
2014-07-22 13:29:39,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060977694 with entries=109, filesize=87.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060978716
2014-07-22 13:29:39,796 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:40,790 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:40,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060978716 with entries=101, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060980791
2014-07-22 13:29:40,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:41,925 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:42,060 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060980791 with entries=95, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060981926
2014-07-22 13:29:42,060 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:43,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:43,458 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109249 synced till here 109239
2014-07-22 13:29:43,499 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060981926 with entries=96, filesize=71.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060983219
2014-07-22 13:29:43,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:44,721 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:44,747 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109337 synced till here 109334
2014-07-22 13:29:44,791 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060983219 with entries=88, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060984721
2014-07-22 13:29:44,792 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:45,969 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:45,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109424 synced till here 109422
2014-07-22 13:29:46,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060984721 with entries=87, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060985969
2014-07-22 13:29:46,023 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:47,312 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:47,327 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109510 synced till here 109508
2014-07-22 13:29:47,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060985969 with entries=86, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060987312
2014-07-22 13:29:47,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:48,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:48,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109597 synced till here 109595
2014-07-22 13:29:48,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060987312 with entries=87, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060988636
2014-07-22 13:29:48,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:49,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:49,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060988636 with entries=86, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060989799
2014-07-22 13:29:49,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:51,156 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:29:51,346 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,347 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,363 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109783 synced till here 109780
2014-07-22 13:29:51,390 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,394 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,395 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,396 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060989799 with entries=100, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060991157
2014-07-22 13:29:51,399 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:29:51,422 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,449 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,506 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:51,998 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,011 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,372 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,568 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,619 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,668 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,721 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,769 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:52,812 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,178 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,446 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,477 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,508 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,539 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,571 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,604 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,637 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,670 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,701 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:53,733 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,459 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,492 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,524 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,555 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,587 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,618 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,650 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,682 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,714 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:55,746 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:56,346 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:56,347 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,364 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:56,391 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,395 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,396 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,396 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:56,423 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,449 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:56,506 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:56,998 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:57,012 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:57,372 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:57,569 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:57,600 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:57,619 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:57,635 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:57,669 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:57,672 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,510 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:58,510 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5789ms
2014-07-22 13:29:58,511 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5741ms
2014-07-22 13:29:58,511 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5699ms
2014-07-22 13:29:58,511 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5333ms
2014-07-22 13:29:58,511 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5065ms
2014-07-22 13:29:58,512 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5034ms
2014-07-22 13:29:58,522 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,532 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,540 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:58,555 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,571 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:58,587 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,604 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:58,618 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,637 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:29:58,653 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,670 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:58,686 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:29:58,702 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:29:58,734 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:00,460 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:00,493 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:00,524 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:00,556 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:00,588 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:00,619 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:00,650 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:00,682 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:00,714 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:00,746 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:01,347 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,347 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,364 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,392 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,396 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,396 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,397 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,423 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,449 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-07-22 13:30:01,506 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:01,999 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:02,012 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:02,267 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21766, memsize=695.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e6fe76a5df8945ca985bbb44460d801b
2014-07-22 13:30:02,372 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-07-22 13:30:02,463 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/e6fe76a5df8945ca985bbb44460d801b as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e6fe76a5df8945ca985bbb44460d801b
2014-07-22 13:30:02,477 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/e6fe76a5df8945ca985bbb44460d801b, entries=2531190, sequenceid=21766, filesize=180.1m
2014-07-22 13:30:02,478 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~743.4m/779508800, currentsize=381.4m/399958320 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 29587ms, sequenceid=21766, compaction requested=true
2014-07-22 13:30:02,478 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:30:02,478 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 48 store files, 0 compacting, 48 eligible, 2000 blocking
2014-07-22 13:30:02,479 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 48 files from compaction candidates
2014-07-22 13:30:02,479 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10108ms
2014-07-22 13:30:02,479 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,479 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 383.8m
2014-07-22 13:30:02,479 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:30:02,479 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10468ms
2014-07-22 13:30:02,479 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,479 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:30:02,479 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10481ms
2014-07-22 13:30:02,479 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,479 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:30:02,481 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10976ms
2014-07-22 13:30:02,481 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,493 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11044ms
2014-07-22 13:30:02,493 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,493 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11071ms
2014-07-22 13:30:02,493 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,494 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11097ms
2014-07-22 13:30:02,494 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,494 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11099ms
2014-07-22 13:30:02,494 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,494 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11100ms
2014-07-22 13:30:02,494 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,505 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11115ms
2014-07-22 13:30:02,505 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,505 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-07-22 13:30:02,505 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,506 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11160ms
2014-07-22 13:30:02,506 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,506 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11160ms
2014-07-22 13:30:02,506 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,506 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6761ms
2014-07-22 13:30:02,507 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,507 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6793ms
2014-07-22 13:30:02,507 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,507 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6825ms
2014-07-22 13:30:02,507 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,507 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6857ms
2014-07-22 13:30:02,507 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,507 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6889ms
2014-07-22 13:30:02,507 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,520 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6933ms
2014-07-22 13:30:02,520 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,520 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6965ms
2014-07-22 13:30:02,521 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,521 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6997ms
2014-07-22 13:30:02,521 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,522 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991056,"queuetimems":0,"class":"HRegionServer","responsesize":16625,"method":"Multi"}
2014-07-22 13:30:02,525 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7033ms
2014-07-22 13:30:02,525 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,525 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7066ms
2014-07-22 13:30:02,525 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,525 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8792ms
2014-07-22 13:30:02,525 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,525 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8824ms
2014-07-22 13:30:02,525 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,526 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991004,"queuetimems":0,"class":"HRegionServer","responsesize":17206,"method":"Multi"}
2014-07-22 13:30:02,530 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3843ms
2014-07-22 13:30:02,530 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,530 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8861ms
2014-07-22 13:30:02,530 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,531 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3877ms
2014-07-22 13:30:02,532 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,532 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8895ms
2014-07-22 13:30:02,532 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,533 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3914ms
2014-07-22 13:30:02,533 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,541 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8937ms
2014-07-22 13:30:02,541 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,543 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3955ms
2014-07-22 13:30:02,543 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,543 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8972ms
2014-07-22 13:30:02,543 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,544 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3988ms
2014-07-22 13:30:02,544 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,544 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9005ms
2014-07-22 13:30:02,544 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,545 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4013ms
2014-07-22 13:30:02,545 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,545 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4023ms
2014-07-22 13:30:02,545 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,547 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9070ms
2014-07-22 13:30:02,547 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,547 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9101ms
2014-07-22 13:30:02,547 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,550 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9371ms
2014-07-22 13:30:02,550 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,550 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9738ms
2014-07-22 13:30:02,550 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,550 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9781ms
2014-07-22 13:30:02,550 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,551 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9830ms
2014-07-22 13:30:02,551 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,551 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9043ms
2014-07-22 13:30:02,552 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,552 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4880ms
2014-07-22 13:30:02,552 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,552 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9884ms
2014-07-22 13:30:02,552 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,552 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4917ms
2014-07-22 13:30:02,552 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,553 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9933ms
2014-07-22 13:30:02,554 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,554 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4954ms
2014-07-22 13:30:02,554 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,554 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9986ms
2014-07-22 13:30:02,554 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:02,610 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:30:02,670 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991113,"queuetimems":0,"class":"HRegionServer","responsesize":17126,"method":"Multi"}
2014-07-22 13:30:03,846 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12625,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991220,"queuetimems":0,"class":"HRegionServer","responsesize":16756,"method":"Multi"}
2014-07-22 13:30:03,883 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:30:04,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:04,269 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991169,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:04,269 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12935,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991333,"queuetimems":0,"class":"HRegionServer","responsesize":17042,"method":"Multi"}
2014-07-22 13:30:04,569 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991273,"queuetimems":1,"class":"HRegionServer","responsesize":16663,"method":"Multi"}
2014-07-22 13:30:04,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 109944 synced till here 109900
2014-07-22 13:30:05,949 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13954,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991995,"queuetimems":439,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-07-22 13:30:05,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060991157 with entries=161, filesize=124.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061004266
2014-07-22 13:30:06,136 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991503,"queuetimems":0,"class":"HRegionServer","responsesize":16951,"method":"Multi"}
2014-07-22 13:30:06,136 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13768,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992368,"queuetimems":0,"class":"HRegionServer","responsesize":17160,"method":"Multi"}
2014-07-22 13:30:06,136 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991446,"queuetimems":0,"class":"HRegionServer","responsesize":16847,"method":"Multi"}
2014-07-22 13:30:06,136 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14127,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992009,"queuetimems":0,"class":"HRegionServer","responsesize":16848,"method":"Multi"}
2014-07-22 13:30:06,586 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10938,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995648,"queuetimems":0,"class":"HRegionServer","responsesize":17000,"method":"Multi"}
2014-07-22 13:30:06,592 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993475,"queuetimems":0,"class":"HRegionServer","responsesize":16663,"method":"Multi"}
2014-07-22 13:30:06,594 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995457,"queuetimems":0,"class":"HRegionServer","responsesize":16714,"method":"Multi"}
2014-07-22 13:30:06,594 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12862,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993732,"queuetimems":0,"class":"HRegionServer","responsesize":16756,"method":"Multi"}
2014-07-22 13:30:06,596 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995712,"queuetimems":0,"class":"HRegionServer","responsesize":16988,"method":"Multi"}
2014-07-22 13:30:06,597 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060991387,"queuetimems":0,"class":"HRegionServer","responsesize":16487,"method":"Multi"}
2014-07-22 13:30:06,599 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993700,"queuetimems":0,"class":"HRegionServer","responsesize":17126,"method":"Multi"}
2014-07-22 13:30:06,602 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10985,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995616,"queuetimems":0,"class":"HRegionServer","responsesize":16727,"method":"Multi"}
2014-07-22 13:30:06,611 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993444,"queuetimems":1,"class":"HRegionServer","responsesize":16625,"method":"Multi"}
2014-07-22 13:30:06,611 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995554,"queuetimems":1,"class":"HRegionServer","responsesize":16848,"method":"Multi"}
2014-07-22 13:30:06,611 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993602,"queuetimems":0,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-07-22 13:30:06,611 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995522,"queuetimems":1,"class":"HRegionServer","responsesize":17066,"method":"Multi"}
2014-07-22 13:30:06,612 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995585,"queuetimems":0,"class":"HRegionServer","responsesize":16960,"method":"Multi"}
2014-07-22 13:30:06,612 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993538,"queuetimems":0,"class":"HRegionServer","responsesize":17042,"method":"Multi"}
2014-07-22 13:30:06,612 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995744,"queuetimems":0,"class":"HRegionServer","responsesize":16878,"method":"Multi"}
2014-07-22 13:30:06,612 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12976,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993635,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:06,687 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21826, memsize=671.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b91c0466f5c6480fbeeaaa6dbf669882
2014-07-22 13:30:06,715 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/b91c0466f5c6480fbeeaaa6dbf669882 as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b91c0466f5c6480fbeeaaa6dbf669882
2014-07-22 13:30:06,738 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993176,"queuetimems":0,"class":"HRegionServer","responsesize":16727,"method":"Multi"}
2014-07-22 13:30:06,741 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13074,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993667,"queuetimems":0,"class":"HRegionServer","responsesize":17206,"method":"Multi"}
2014-07-22 13:30:06,758 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14191,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992566,"queuetimems":0,"class":"HRegionServer","responsesize":16988,"method":"Multi"}
2014-07-22 13:30:06,744 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/b91c0466f5c6480fbeeaaa6dbf669882, entries=2445280, sequenceid=21826, filesize=174.0m
2014-07-22 13:30:06,762 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~788.3m/826557840, currentsize=372.6m/390680640 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 28975ms, sequenceid=21826, compaction requested=true
2014-07-22 13:30:06,762 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 13:30:06,762 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 13:30:06,762 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:30:06,763 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:30:06,763 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:30:06,763 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:30:06,763 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 810.4m
2014-07-22 13:30:06,911 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:06,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110090 synced till here 110045
2014-07-22 13:30:07,961 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15293,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992667,"queuetimems":0,"class":"HRegionServer","responsesize":16714,"method":"Multi"}
2014-07-22 13:30:07,962 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15243,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992719,"queuetimems":0,"class":"HRegionServer","responsesize":17000,"method":"Multi"}
2014-07-22 13:30:07,963 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993570,"queuetimems":1,"class":"HRegionServer","responsesize":16847,"method":"Multi"}
2014-07-22 13:30:07,964 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992617,"queuetimems":1,"class":"HRegionServer","responsesize":16960,"method":"Multi"}
2014-07-22 13:30:07,964 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12474,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995490,"queuetimems":1,"class":"HRegionServer","responsesize":16951,"method":"Multi"}
2014-07-22 13:30:07,965 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992810,"queuetimems":0,"class":"HRegionServer","responsesize":17066,"method":"Multi"}
2014-07-22 13:30:07,965 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060997596,"queuetimems":1,"class":"HRegionServer","responsesize":16663,"method":"Multi"}
2014-07-22 13:30:07,966 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060992767,"queuetimems":0,"class":"HRegionServer","responsesize":16878,"method":"Multi"}
2014-07-22 13:30:07,966 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060995680,"queuetimems":0,"class":"HRegionServer","responsesize":17160,"method":"Multi"}
2014-07-22 13:30:07,967 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060997632,"queuetimems":0,"class":"HRegionServer","responsesize":16756,"method":"Multi"}
2014-07-22 13:30:08,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061004266 with entries=146, filesize=96.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061006911
2014-07-22 13:30:08,072 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14566,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060993506,"queuetimems":0,"class":"HRegionServer","responsesize":16487,"method":"Multi"}
2014-07-22 13:30:08,090 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406060997668,"queuetimems":0,"class":"HRegionServer","responsesize":16847,"method":"Multi"}
2014-07-22 13:30:08,174 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:30:08,666 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:30:08,791 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:09,682 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110222 synced till here 110199
2014-07-22 13:30:09,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061006911 with entries=132, filesize=101.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061008791
2014-07-22 13:30:10,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:10,642 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110327 synced till here 110324
2014-07-22 13:30:10,691 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061008791 with entries=105, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061010629
2014-07-22 13:30:12,694 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:12,719 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110418 synced till here 110414
2014-07-22 13:30:12,778 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061010629 with entries=91, filesize=65.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061012695
2014-07-22 13:30:14,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:14,523 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110538 synced till here 110536
2014-07-22 13:30:14,544 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061012695 with entries=120, filesize=88.4m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061014085
2014-07-22 13:30:15,918 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:15,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110625 synced till here 110623
2014-07-22 13:30:15,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061014085 with entries=87, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061015919
2014-07-22 13:30:17,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:17,268 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110711 synced till here 110709
2014-07-22 13:30:17,291 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061015919 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061017249
2014-07-22 13:30:18,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:19,340 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=21972, memsize=314.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/bb17035330f547d9ac461c270ffd18bc
2014-07-22 13:30:19,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110853 synced till here 110852
2014-07-22 13:30:19,366 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061017249 with entries=142, filesize=105.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061018715
2014-07-22 13:30:19,408 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/bb17035330f547d9ac461c270ffd18bc as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/bb17035330f547d9ac461c270ffd18bc
2014-07-22 13:30:19,419 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/bb17035330f547d9ac461c270ffd18bc, entries=1144220, sequenceid=21972, filesize=81.4m
2014-07-22 13:30:19,420 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~383.8m/402420000, currentsize=98.5m/103331360 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 16941ms, sequenceid=21972, compaction requested=true
2014-07-22 13:30:19,421 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:30:19,421 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 28 store files, 0 compacting, 28 eligible, 2000 blocking
2014-07-22 13:30:19,421 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 1.0g
2014-07-22 13:30:19,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 28 files from compaction candidates
2014-07-22 13:30:19,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:30:19,422 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:30:19,422 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:30:20,808 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:30:20,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:20,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 110939 synced till here 110937
2014-07-22 13:30:20,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061018715 with entries=86, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061020872
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060916263
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060917825
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060919479
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060921268
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060922347
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060923806
2014-07-22 13:30:20,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060925539
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060927228
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060928858
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060930459
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060931997
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060933682
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060935260
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060936664
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060937552
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060938652
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060939915
2014-07-22 13:30:20,967 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060941499
2014-07-22 13:30:22,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:22,267 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111027 synced till here 111020
2014-07-22 13:30:22,668 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061020872 with entries=88, filesize=65.7m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061022248
2014-07-22 13:30:23,977 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:24,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111119 synced till here 111106
2014-07-22 13:30:24,094 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061022248 with entries=92, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061023978
2014-07-22 13:30:25,262 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:25,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111209 synced till here 111206
2014-07-22 13:30:25,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061023978 with entries=90, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061025263
2014-07-22 13:30:25,588 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,593 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,594 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,637 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,652 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,715 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,775 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,834 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,896 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:25,959 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,011 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,062 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,116 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,162 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,205 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,236 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,319 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,384 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,551 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:26,850 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,022 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,252 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,493 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,749 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,783 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,815 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,848 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:27,888 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,648 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,658 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,677 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,709 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,742 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,773 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,804 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,835 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:28,866 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,700 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,736 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,768 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,801 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,832 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,864 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,896 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,927 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,957 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:29,987 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:30,589 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:30,593 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,594 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,637 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,652 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,715 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:30,776 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,834 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,896 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:30,959 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,011 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,063 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:31,116 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,162 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,205 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,237 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:31,320 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:31,385 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:31,551 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:31,731 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:31,815 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:31,846 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:31,850 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:32,023 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:32,252 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:32,493 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:32,750 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:32,784 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:32,815 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:32,849 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:32,888 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,649 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:33,659 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:33,677 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,709 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,742 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,773 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,804 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,835 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:33,866 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:34,891 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5026ms
2014-07-22 13:30:34,891 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5191ms
2014-07-22 13:30:34,891 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5155ms
2014-07-22 13:30:34,891 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5123ms
2014-07-22 13:30:34,891 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5091ms
2014-07-22 13:30:34,892 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5060ms
2014-07-22 13:30:34,897 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:34,927 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:34,958 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:34,988 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:35,122 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22047, memsize=612.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a94fa747089143e58d855120058322d2
2014-07-22 13:30:35,147 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/a94fa747089143e58d855120058322d2 as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a94fa747089143e58d855120058322d2
2014-07-22 13:30:35,168 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/a94fa747089143e58d855120058322d2, entries=2230290, sequenceid=22047, filesize=158.8m
2014-07-22 13:30:35,168 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~836.5m/877120000, currentsize=365.9m/383708800 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 28405ms, sequenceid=22047, compaction requested=true
2014-07-22 13:30:35,169 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:30:35,169 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 13:30:35,169 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5182ms
2014-07-22 13:30:35,169 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,169 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5212ms
2014-07-22 13:30:35,169 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 827.2m
2014-07-22 13:30:35,169 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 13:30:35,169 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,169 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:30:35,169 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5242ms
2014-07-22 13:30:35,170 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,169 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:30:35,170 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5274ms
2014-07-22 13:30:35,170 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,170 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:30:35,170 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5338ms
2014-07-22 13:30:35,170 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,170 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5370ms
2014-07-22 13:30:35,170 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,185 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5417ms
2014-07-22 13:30:35,185 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,185 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5449ms
2014-07-22 13:30:35,185 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,186 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5486ms
2014-07-22 13:30:35,186 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,186 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5322ms
2014-07-22 13:30:35,186 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,186 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6320ms
2014-07-22 13:30:35,187 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,189 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6354ms
2014-07-22 13:30:35,189 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,189 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6385ms
2014-07-22 13:30:35,190 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,192 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6418ms
2014-07-22 13:30:35,192 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,193 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6450ms
2014-07-22 13:30:35,193 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,194 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6485ms
2014-07-22 13:30:35,194 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,194 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6517ms
2014-07-22 13:30:35,194 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,196 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6538ms
2014-07-22 13:30:35,196 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,197 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6549ms
2014-07-22 13:30:35,198 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,198 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7310ms
2014-07-22 13:30:35,198 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,198 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7350ms
2014-07-22 13:30:35,198 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,198 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7383ms
2014-07-22 13:30:35,198 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,199 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7416ms
2014-07-22 13:30:35,199 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,201 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7452ms
2014-07-22 13:30:35,201 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,202 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7709ms
2014-07-22 13:30:35,202 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,202 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7950ms
2014-07-22 13:30:35,202 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,203 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8180ms
2014-07-22 13:30:35,203 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,204 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8354ms
2014-07-22 13:30:35,204 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,205 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3359ms
2014-07-22 13:30:35,205 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,205 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-07-22 13:30:35,205 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,205 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3475ms
2014-07-22 13:30:35,205 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,205 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8655ms
2014-07-22 13:30:35,205 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,206 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8822ms
2014-07-22 13:30:35,206 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,206 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8887ms
2014-07-22 13:30:35,206 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,206 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8970ms
2014-07-22 13:30:35,206 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,206 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9001ms
2014-07-22 13:30:35,207 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,207 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9045ms
2014-07-22 13:30:35,207 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,207 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9091ms
2014-07-22 13:30:35,207 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,207 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9145ms
2014-07-22 13:30:35,207 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,207 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9196ms
2014-07-22 13:30:35,208 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,208 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9249ms
2014-07-22 13:30:35,208 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,208 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9312ms
2014-07-22 13:30:35,208 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,208 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9374ms
2014-07-22 13:30:35,208 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,208 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9433ms
2014-07-22 13:30:35,209 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,209 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9495ms
2014-07-22 13:30:35,209 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,209 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9557ms
2014-07-22 13:30:35,209 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,209 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9572ms
2014-07-22 13:30:35,209 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,210 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9615ms
2014-07-22 13:30:35,210 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,210 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9617ms
2014-07-22 13:30:35,210 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,222 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9633ms
2014-07-22 13:30:35,222 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:35,552 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:30:35,824 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:35,875 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111314 synced till here 111298
2014-07-22 13:30:35,937 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:30:36,003 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10532,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025470,"queuetimems":1,"class":"HRegionServer","responsesize":16814,"method":"Multi"}
2014-07-22 13:30:36,286 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061025263 with entries=105, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061035825
2014-07-22 13:30:36,286 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060943068
2014-07-22 13:30:37,431 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1039ms
GC pool 'ParNew' had collection(s): count=1 time=1055ms
2014-07-22 13:30:37,735 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025526,"queuetimems":1,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:30:38,261 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:38,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111440 synced till here 111423
2014-07-22 13:30:38,436 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061035825 with entries=126, filesize=91.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061038261
2014-07-22 13:30:39,364 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027781,"queuetimems":0,"class":"HRegionServer","responsesize":17015,"method":"Multi"}
2014-07-22 13:30:39,364 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028674,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:39,370 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028834,"queuetimems":1,"class":"HRegionServer","responsesize":16875,"method":"Multi"}
2014-07-22 13:30:39,374 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12524,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026849,"queuetimems":0,"class":"HRegionServer","responsesize":6156,"method":"Multi"}
2014-07-22 13:30:39,370 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10662,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028707,"queuetimems":1,"class":"HRegionServer","responsesize":17417,"method":"Multi"}
2014-07-22 13:30:39,378 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027813,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:30:39,380 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11632,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027748,"queuetimems":1,"class":"HRegionServer","responsesize":16693,"method":"Multi"}
2014-07-22 13:30:39,386 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10584,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028802,"queuetimems":0,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 13:30:39,386 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028646,"queuetimems":1,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 13:30:39,387 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10616,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028771,"queuetimems":0,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:30:39,388 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11506,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027882,"queuetimems":0,"class":"HRegionServer","responsesize":16702,"method":"Multi"}
2014-07-22 13:30:39,388 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11543,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027845,"queuetimems":0,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:30:39,388 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10523,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028865,"queuetimems":1,"class":"HRegionServer","responsesize":16711,"method":"Multi"}
2014-07-22 13:30:39,514 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13353,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026160,"queuetimems":1,"class":"HRegionServer","responsesize":16913,"method":"Multi"}
2014-07-22 13:30:39,590 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026382,"queuetimems":0,"class":"HRegionServer","responsesize":17068,"method":"Multi"}
2014-07-22 13:30:39,596 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13047,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026549,"queuetimems":1,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 13:30:39,597 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026060,"queuetimems":0,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 13:30:39,598 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10940,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061028657,"queuetimems":0,"class":"HRegionServer","responsesize":16814,"method":"Multi"}
2014-07-22 13:30:39,598 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025831,"queuetimems":0,"class":"HRegionServer","responsesize":17417,"method":"Multi"}
2014-07-22 13:30:39,603 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027491,"queuetimems":0,"class":"HRegionServer","responsesize":16656,"method":"Multi"}
2014-07-22 13:30:39,727 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14014,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025712,"queuetimems":1,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:30:39,872 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:39,874 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13638,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026235,"queuetimems":1,"class":"HRegionServer","responsesize":16557,"method":"Multi"}
2014-07-22 13:30:39,892 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027250,"queuetimems":0,"class":"HRegionServer","responsesize":16942,"method":"Multi"}
2014-07-22 13:30:39,896 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14304,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025591,"queuetimems":0,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 13:30:39,900 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14001,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025894,"queuetimems":1,"class":"HRegionServer","responsesize":16693,"method":"Multi"}
2014-07-22 13:30:39,906 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13693,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026203,"queuetimems":0,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:30:39,908 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026009,"queuetimems":0,"class":"HRegionServer","responsesize":16702,"method":"Multi"}
2014-07-22 13:30:39,907 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025650,"queuetimems":0,"class":"HRegionServer","responsesize":16875,"method":"Multi"}
2014-07-22 13:30:39,906 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12886,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061027020,"queuetimems":0,"class":"HRegionServer","responsesize":17063,"method":"Multi"}
2014-07-22 13:30:39,909 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025957,"queuetimems":1,"class":"HRegionServer","responsesize":17015,"method":"Multi"}
2014-07-22 13:30:39,906 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14133,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061025772,"queuetimems":0,"class":"HRegionServer","responsesize":16810,"method":"Multi"}
2014-07-22 13:30:39,908 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026317,"queuetimems":0,"class":"HRegionServer","responsesize":16686,"method":"Multi"}
2014-07-22 13:30:40,053 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13939,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061026114,"queuetimems":1,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:40,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111561 synced till here 111550
2014-07-22 13:30:40,214 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061038261 with entries=121, filesize=81.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061039873
2014-07-22 13:30:41,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:41,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111667 synced till here 111647
2014-07-22 13:30:42,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061039873 with entries=106, filesize=82.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061041901
2014-07-22 13:30:43,768 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:43,910 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111797 synced till here 111747
2014-07-22 13:30:44,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061041901 with entries=130, filesize=85.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061043768
2014-07-22 13:30:45,439 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,441 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,441 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,443 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,445 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,447 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,451 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,451 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,451 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,467 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:45,469 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,470 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,471 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,471 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 111893 synced till here 111884
2014-07-22 13:30:45,505 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,505 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,526 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,528 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,528 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,528 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,529 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,529 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,532 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,532 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061043768 with entries=96, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061045467
2014-07-22 13:30:45,569 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,570 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,910 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,941 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:45,974 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,006 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,038 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,069 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,100 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,132 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,164 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,196 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,228 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,259 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,290 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,533 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,563 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,595 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,627 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,637 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,668 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,702 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,734 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,765 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:46,796 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:49,448 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:50,957 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5016ms
2014-07-22 13:30:50,957 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5428ms
2014-07-22 13:30:50,958 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5454ms
2014-07-22 13:30:50,959 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5429ms
2014-07-22 13:30:50,959 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5427ms
2014-07-22 13:30:50,959 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5389ms
2014-07-22 13:30:50,959 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5390ms
2014-07-22 13:30:50,960 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-07-22 13:30:50,960 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5519ms
2014-07-22 13:30:50,960 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5517ms
2014-07-22 13:30:50,960 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5515ms
2014-07-22 13:30:50,960 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5522ms
2014-07-22 13:30:50,961 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5515ms
2014-07-22 13:30:50,961 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5520ms
2014-07-22 13:30:50,961 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5511ms
2014-07-22 13:30:50,961 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:30:50,962 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5510ms
2014-07-22 13:30:50,962 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5493ms
2014-07-22 13:30:50,962 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5492ms
2014-07-22 13:30:50,962 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5491ms
2014-07-22 13:30:50,963 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5493ms
2014-07-22 13:30:50,963 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5458ms
2014-07-22 13:30:50,963 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5437ms
2014-07-22 13:30:50,964 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5436ms
2014-07-22 13:30:50,964 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5436ms
2014-07-22 13:30:50,964 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5436ms
2014-07-22 13:30:50,974 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,006 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,039 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,069 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,100 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,132 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,164 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,197 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:51,228 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,259 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,290 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,533 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,564 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:51,596 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,627 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,637 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,669 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:51,702 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,734 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,765 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:51,797 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-22 13:30:53,441 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:53,470 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1406057959640: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-22 13:30:54,395 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22247, memsize=718.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bdb69a6204ec49d8913ceed0a6bddea5
2014-07-22 13:30:54,411 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/bdb69a6204ec49d8913ceed0a6bddea5 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bdb69a6204ec49d8913ceed0a6bddea5
2014-07-22 13:30:54,428 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/bdb69a6204ec49d8913ceed0a6bddea5, entries=2617490, sequenceid=22247, filesize=186.4m
2014-07-22 13:30:54,428 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~1.0g/1097777680, currentsize=324.7m/340516800 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 35007ms, sequenceid=22247, compaction requested=true
2014-07-22 13:30:54,429 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:30:54,429 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 13:30:54,429 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 13:30:54,429 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:30:54,429 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:30:54,429 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 959ms
2014-07-22 13:30:54,429 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 933.5m
2014-07-22 13:30:54,430 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,429 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:30:54,430 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 989ms
2014-07-22 13:30:54,430 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,430 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7634ms
2014-07-22 13:30:54,430 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,430 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7665ms
2014-07-22 13:30:54,430 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,441 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7707ms
2014-07-22 13:30:54,441 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,442 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7740ms
2014-07-22 13:30:54,442 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,443 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7774ms
2014-07-22 13:30:54,443 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,444 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7806ms
2014-07-22 13:30:54,444 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,444 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7817ms
2014-07-22 13:30:54,444 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,445 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7849ms
2014-07-22 13:30:54,445 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,445 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7882ms
2014-07-22 13:30:54,445 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,445 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7912ms
2014-07-22 13:30:54,445 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,445 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8155ms
2014-07-22 13:30:54,445 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,449 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-22 13:30:54,449 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,453 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8194ms
2014-07-22 13:30:54,453 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,455 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8226ms
2014-07-22 13:30:54,455 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,455 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8259ms
2014-07-22 13:30:54,455 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,456 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8291ms
2014-07-22 13:30:54,456 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,463 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8331ms
2014-07-22 13:30:54,463 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,464 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8363ms
2014-07-22 13:30:54,464 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,465 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8396ms
2014-07-22 13:30:54,465 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,469 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8431ms
2014-07-22 13:30:54,469 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,469 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8463ms
2014-07-22 13:30:54,469 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,472 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8498ms
2014-07-22 13:30:54,472 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,472 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8944ms
2014-07-22 13:30:54,472 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,474 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8945ms
2014-07-22 13:30:54,474 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,474 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8946ms
2014-07-22 13:30:54,474 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,474 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8948ms
2014-07-22 13:30:54,474 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,475 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8969ms
2014-07-22 13:30:54,476 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,485 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9015ms
2014-07-22 13:30:54,485 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,486 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9015ms
2014-07-22 13:30:54,487 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,488 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9017ms
2014-07-22 13:30:54,488 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,489 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9019ms
2014-07-22 13:30:54,489 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,489 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9038ms
2014-07-22 13:30:54,489 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,490 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9038ms
2014-07-22 13:30:54,490 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,491 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9040ms
2014-07-22 13:30:54,491 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,492 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9051ms
2014-07-22 13:30:54,493 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,494 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9047ms
2014-07-22 13:30:54,494 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,497 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9059ms
2014-07-22 13:30:54,497 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,498 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9052ms
2014-07-22 13:30:54,498 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,499 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9055ms
2014-07-22 13:30:54,499 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,499 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9058ms
2014-07-22 13:30:54,499 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,499 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8589ms
2014-07-22 13:30:54,499 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,499 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8930ms
2014-07-22 13:30:54,499 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,500 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8930ms
2014-07-22 13:30:54,500 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,501 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8968ms
2014-07-22 13:30:54,501 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,505 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8976ms
2014-07-22 13:30:54,505 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,506 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9002ms
2014-07-22 13:30:54,506 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,508 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8978ms
2014-07-22 13:30:54,508 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,508 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8567ms
2014-07-22 13:30:54,508 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1406057959640
2014-07-22 13:30:54,637 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10866,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043770,"queuetimems":2404,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:30:54,910 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10903,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044006,"queuetimems":534,"class":"HRegionServer","responsesize":16913,"method":"Multi"}
2014-07-22 13:30:54,914 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10937,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043976,"queuetimems":2343,"class":"HRegionServer","responsesize":17015,"method":"Multi"}
2014-07-22 13:30:54,924 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10896,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044027,"queuetimems":351,"class":"HRegionServer","responsesize":6156,"method":"Multi"}
2014-07-22 13:30:55,076 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:30:55,090 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044002,"queuetimems":635,"class":"HRegionServer","responsesize":16656,"method":"Multi"}
2014-07-22 13:30:55,090 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044002,"queuetimems":605,"class":"HRegionServer","responsesize":16557,"method":"Multi"}
2014-07-22 13:30:56,407 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043990,"queuetimems":2051,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:30:56,414 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12411,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044002,"queuetimems":573,"class":"HRegionServer","responsesize":16686,"method":"Multi"}
2014-07-22 13:30:56,418 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043976,"queuetimems":2289,"class":"HRegionServer","responsesize":16814,"method":"Multi"}
2014-07-22 13:30:56,418 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12442,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043975,"queuetimems":2399,"class":"HRegionServer","responsesize":16810,"method":"Multi"}
2014-07-22 13:30:56,418 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044026,"queuetimems":430,"class":"HRegionServer","responsesize":16942,"method":"Multi"}
2014-07-22 13:30:56,429 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12452,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043976,"queuetimems":2206,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:30:56,434 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043974,"queuetimems":2470,"class":"HRegionServer","responsesize":16702,"method":"Multi"}
2014-07-22 13:30:56,434 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12463,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043971,"queuetimems":2533,"class":"HRegionServer","responsesize":16693,"method":"Multi"}
2014-07-22 13:30:56,442 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12464,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043977,"queuetimems":2141,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 13:30:56,434 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12406,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044027,"queuetimems":380,"class":"HRegionServer","responsesize":17068,"method":"Multi"}
2014-07-22 13:30:56,475 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043976,"queuetimems":2173,"class":"HRegionServer","responsesize":16875,"method":"Multi"}
2014-07-22 13:30:56,476 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12497,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043978,"queuetimems":2078,"class":"HRegionServer","responsesize":17417,"method":"Multi"}
2014-07-22 13:30:56,477 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043978,"queuetimems":2111,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 13:30:56,475 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12460,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044014,"queuetimems":457,"class":"HRegionServer","responsesize":17063,"method":"Multi"}
2014-07-22 13:30:56,482 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12468,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061044013,"queuetimems":496,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 13:30:56,486 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043976,"queuetimems":2238,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:56,490 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061043998,"queuetimems":651,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:30:56,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:56,631 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061045268,"queuetimems":0,"class":"HRegionServer","responsesize":16711,"method":"Multi"}
2014-07-22 13:30:56,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112029 synced till here 111997
2014-07-22 13:30:56,861 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:30:56,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061045467 with entries=136, filesize=97.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061056630
2014-07-22 13:30:56,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060944588
2014-07-22 13:30:56,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060947277
2014-07-22 13:30:56,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060948532
2014-07-22 13:30:56,932 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060949596
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060951086
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060954326
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060955286
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060957153
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060958807
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060960730
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060962451
2014-07-22 13:30:56,933 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060964123
2014-07-22 13:30:58,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:30:58,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112143 synced till here 112120
2014-07-22 13:30:58,762 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046625,"queuetimems":0,"class":"HRegionServer","responsesize":16942,"method":"Multi"}
2014-07-22 13:30:58,763 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046795,"queuetimems":0,"class":"HRegionServer","responsesize":16656,"method":"Multi"}
2014-07-22 13:30:58,762 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046130,"queuetimems":0,"class":"HRegionServer","responsesize":16814,"method":"Multi"}
2014-07-22 13:30:58,770 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12176,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046593,"queuetimems":0,"class":"HRegionServer","responsesize":16686,"method":"Multi"}
2014-07-22 13:30:58,771 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046195,"queuetimems":0,"class":"HRegionServer","responsesize":16702,"method":"Multi"}
2014-07-22 13:30:58,771 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046698,"queuetimems":0,"class":"HRegionServer","responsesize":16557,"method":"Multi"}
2014-07-22 13:30:58,771 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12672,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046099,"queuetimems":1,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-07-22 13:30:58,772 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12137,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046635,"queuetimems":0,"class":"HRegionServer","responsesize":6156,"method":"Multi"}
2014-07-22 13:30:58,798 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061056630 with entries=114, filesize=86.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061058587
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046763,"queuetimems":1,"class":"HRegionServer","responsesize":17063,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061045971,"queuetimems":0,"class":"HRegionServer","responsesize":17417,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046288,"queuetimems":0,"class":"HRegionServer","responsesize":17276,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046067,"queuetimems":0,"class":"HRegionServer","responsesize":16693,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12432,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046562,"queuetimems":1,"class":"HRegionServer","responsesize":16999,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046162,"queuetimems":0,"class":"HRegionServer","responsesize":16863,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13055,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061045939,"queuetimems":0,"class":"HRegionServer","responsesize":16875,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046226,"queuetimems":0,"class":"HRegionServer","responsesize":16759,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12958,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046036,"queuetimems":0,"class":"HRegionServer","responsesize":16995,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046731,"queuetimems":0,"class":"HRegionServer","responsesize":16938,"method":"Multi"}
2014-07-22 13:30:58,995 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046530,"queuetimems":0,"class":"HRegionServer","responsesize":17068,"method":"Multi"}
2014-07-22 13:30:58,997 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046257,"queuetimems":0,"class":"HRegionServer","responsesize":16810,"method":"Multi"}
2014-07-22 13:30:58,997 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12993,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046004,"queuetimems":0,"class":"HRegionServer","responsesize":17015,"method":"Multi"}
2014-07-22 13:30:58,998 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12331,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061046666,"queuetimems":0,"class":"HRegionServer","responsesize":16913,"method":"Multi"}
2014-07-22 13:30:59,031 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.53:54760","starttimems":1406061045908,"queuetimems":0,"class":"HRegionServer","responsesize":17011,"method":"Multi"}
2014-07-22 13:31:02,258 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22302, memsize=509.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/85d904de840749f8a0f500285dee1c38
2014-07-22 13:31:02,271 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/85d904de840749f8a0f500285dee1c38 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/85d904de840749f8a0f500285dee1c38
2014-07-22 13:31:02,308 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/85d904de840749f8a0f500285dee1c38, entries=1854800, sequenceid=22302, filesize=132.2m
2014-07-22 13:31:02,308 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~827.2m/867372800, currentsize=287.5m/301429120 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 27139ms, sequenceid=22302, compaction requested=true
2014-07-22 13:31:02,309 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:02,309 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 49 store files, 0 compacting, 49 eligible, 2000 blocking
2014-07-22 13:31:02,309 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 654.4m
2014-07-22 13:31:02,309 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 49 files from compaction candidates
2014-07-22 13:31:02,309 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:02,310 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:02,310 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:31:02,768 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:06,909 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27.
2014-07-22 13:31:07,293 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:07,323 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112250 synced till here 112246
2014-07-22 13:31:07,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061058587 with entries=107, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061067293
2014-07-22 13:31:07,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060965552
2014-07-22 13:31:07,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060974284
2014-07-22 13:31:07,386 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060976134
2014-07-22 13:31:08,486 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:08,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112341 synced till here 112338
2014-07-22 13:31:08,545 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061067293 with entries=91, filesize=63.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061068486
2014-07-22 13:31:09,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:09,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112428 synced till here 112426
2014-07-22 13:31:09,990 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061068486 with entries=87, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061069747
2014-07-22 13:31:10,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:11,193 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112516 synced till here 112514
2014-07-22 13:31:11,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061069747 with entries=88, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061070835
2014-07-22 13:31:12,189 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0.
2014-07-22 13:31:12,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:12,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112618 synced till here 112617
2014-07-22 13:31:12,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061070835 with entries=102, filesize=76.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061072211
2014-07-22 13:31:13,634 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:13,656 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112705 synced till here 112702
2014-07-22 13:31:13,677 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061072211 with entries=87, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061073635
2014-07-22 13:31:15,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:15,934 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112805 synced till here 112802
2014-07-22 13:31:15,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061073635 with entries=100, filesize=73.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061075714
2014-07-22 13:31:16,113 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22477, memsize=306.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5f65f29334864b0ab0069743f51d19db
2014-07-22 13:31:16,125 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/5f65f29334864b0ab0069743f51d19db as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5f65f29334864b0ab0069743f51d19db
2014-07-22 13:31:16,135 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/5f65f29334864b0ab0069743f51d19db, entries=1114680, sequenceid=22477, filesize=79.4m
2014-07-22 13:31:16,136 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~654.4m/686163280, currentsize=187.9m/196978080 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 13827ms, sequenceid=22477, compaction requested=true
2014-07-22 13:31:16,136 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:16,136 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 13:31:16,137 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 592.5m
2014-07-22 13:31:16,137 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 13:31:16,137 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:16,137 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:16,137 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:31:16,946 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:17,693 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:17,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061075714 with entries=83, filesize=61.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061077693
2014-07-22 13:31:19,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:19,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 112975 synced till here 112974
2014-07-22 13:31:19,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061077693 with entries=87, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061079413
2014-07-22 13:31:20,686 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349.
2014-07-22 13:31:20,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:20,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113066 synced till here 113062
2014-07-22 13:31:20,910 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22438, memsize=541.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/7260b500600348798374a9810799ce0d
2014-07-22 13:31:20,922 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/7260b500600348798374a9810799ce0d as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/7260b500600348798374a9810799ce0d
2014-07-22 13:31:20,931 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/7260b500600348798374a9810799ce0d, entries=1973090, sequenceid=22438, filesize=140.6m
2014-07-22 13:31:20,932 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~933.5m/978834000, currentsize=351.9m/369040320 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 26503ms, sequenceid=22438, compaction requested=true
2014-07-22 13:31:20,932 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:20,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 13:31:20,932 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27., current region memstore size 557.3m
2014-07-22 13:31:20,932 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 13:31:20,933 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:20,933 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:20,933 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:31:20,935 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb.
2014-07-22 13:31:20,945 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061079413 with entries=91, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061080842
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060977694
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060978716
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060980791
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060981926
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060983219
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060984721
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060985969
2014-07-22 13:31:20,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060987312
2014-07-22 13:31:20,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060988636
2014-07-22 13:31:20,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406060989799
2014-07-22 13:31:21,405 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:22,452 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:22,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113153 synced till here 113151
2014-07-22 13:31:22,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061080842 with entries=87, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061082453
2014-07-22 13:31:24,515 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:24,547 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061082453 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061084516
2014-07-22 13:31:24,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:31:25,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:25,899 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113325 synced till here 113322
2014-07-22 13:31:25,932 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061084516 with entries=86, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061085873
2014-07-22 13:31:25,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:31:28,432 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:28,486 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061085873 with entries=86, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061088432
2014-07-22 13:31:28,487 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:31:29,793 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:29,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113500 synced till here 113499
2014-07-22 13:31:29,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061088432 with entries=89, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061089794
2014-07-22 13:31:29,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:31:31,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:32,314 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22637, memsize=296.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5051786c9b764acd8b5841e971447f9f
2014-07-22 13:31:32,328 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/5051786c9b764acd8b5841e971447f9f as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5051786c9b764acd8b5841e971447f9f
2014-07-22 13:31:32,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 113603 synced till here 113602
2014-07-22 13:31:32,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061089794 with entries=103, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061091621
2014-07-22 13:31:32,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 25e1297b777ae6ae4c4565d60879d0b0
2014-07-22 13:31:32,637 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/5051786c9b764acd8b5841e971447f9f, entries=1079880, sequenceid=22637, filesize=76.9m
2014-07-22 13:31:32,638 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~594.0m/622804480, currentsize=236.1m/247580800 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 16502ms, sequenceid=22637, compaction requested=true
2014-07-22 13:31:32,639 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:32,639 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 13:31:32,639 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 13:31:32,639 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0., current region memstore size 349.4m
2014-07-22 13:31:32,639 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:32,639 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:32,639 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
2014-07-22 13:31:32,937 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:34,569 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7.
2014-07-22 13:31:34,871 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22673, memsize=271.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5ce865ea27ba4794b13c5224b0f25349
2014-07-22 13:31:34,889 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/.tmp/5ce865ea27ba4794b13c5224b0f25349 as hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5ce865ea27ba4794b13c5224b0f25349
2014-07-22 13:31:34,907 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1723c12a1a2e0533afeaf8fb8dd82e27/family/5ce865ea27ba4794b13c5224b0f25349, entries=987890, sequenceid=22673, filesize=70.3m
2014-07-22 13:31:34,908 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~558.8m/585933520, currentsize=184.8m/193747040 for region usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. in 13976ms, sequenceid=22673, compaction requested=true
2014-07-22 13:31:34,908 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:34,908 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 50 store files, 0 compacting, 50 eligible, 2000 blocking
2014-07-22 13:31:34,909 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349., current region memstore size 454.8m
2014-07-22 13:31:34,909 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 50 files from compaction candidates
2014-07-22 13:31:34,909 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:34,909 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:34,909 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user3,1406058339792.1723c12a1a2e0533afeaf8fb8dd82e27. because compaction request was cancelled
2014-07-22 13:31:34,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-22 13:31:35,010 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061091621 with entries=84, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1406057959640/slave1%2C60020%2C1406057959640.1406061094964
2014-07-22 13:31:35,474 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:40,508 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22736, memsize=211.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/a804a78f661f486b9ed367fcb53cb5e0
2014-07-22 13:31:40,523 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/.tmp/a804a78f661f486b9ed367fcb53cb5e0 as hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/a804a78f661f486b9ed367fcb53cb5e0
2014-07-22 13:31:40,535 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/25e1297b777ae6ae4c4565d60879d0b0/family/a804a78f661f486b9ed367fcb53cb5e0, entries=769250, sequenceid=22736, filesize=54.8m
2014-07-22 13:31:40,535 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~350.4m/367385280, currentsize=12.6m/13214880 for region usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. in 7896ms, sequenceid=22736, compaction requested=true
2014-07-22 13:31:40,535 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:40,536 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 29 store files, 0 compacting, 29 eligible, 2000 blocking
2014-07-22 13:31:40,536 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 29 files from compaction candidates
2014-07-22 13:31:40,536 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:40,536 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:40,536 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb., current region memstore size 558.9m
2014-07-22 13:31:40,536 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user9,1406058339792.25e1297b777ae6ae4c4565d60879d0b0. because compaction request was cancelled
2014-07-22 13:31:40,959 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:49,021 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22776, memsize=456.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/8288ace51e4644719fb5e0a5d3479ebf
2014-07-22 13:31:49,038 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/.tmp/8288ace51e4644719fb5e0a5d3479ebf as hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/8288ace51e4644719fb5e0a5d3479ebf
2014-07-22 13:31:49,053 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ba046f8ec13a73ed30320b2e3edb3349/family/8288ace51e4644719fb5e0a5d3479ebf, entries=1661880, sequenceid=22776, filesize=118.3m
2014-07-22 13:31:49,054 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~456.4m/478608240, currentsize=17.3m/18192560 for region usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. in 14145ms, sequenceid=22776, compaction requested=true
2014-07-22 13:31:49,055 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:49,055 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 51 store files, 0 compacting, 51 eligible, 2000 blocking
2014-07-22 13:31:49,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 51 files from compaction candidates
2014-07-22 13:31:49,056 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7., current region memstore size 276.5m
2014-07-22 13:31:49,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:49,056 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:49,056 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user6,1406058339792.ba046f8ec13a73ed30320b2e3edb3349. because compaction request was cancelled
2014-07-22 13:31:49,241 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-22 13:31:55,373 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22807, memsize=473.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e57edae1260445668d60125e605173da
2014-07-22 13:31:55,392 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/.tmp/e57edae1260445668d60125e605173da as hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e57edae1260445668d60125e605173da
2014-07-22 13:31:55,407 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/cdf12d16f55023d641440ce7cf8436fb/family/e57edae1260445668d60125e605173da, entries=1723140, sequenceid=22807, filesize=122.7m
2014-07-22 13:31:55,407 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~558.9m/586093840, currentsize=0.0/0 for region usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. in 14871ms, sequenceid=22807, compaction requested=true
2014-07-22 13:31:55,408 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:55,408 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 13:31:55,408 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 13:31:55,408 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:55,408 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:55,408 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user4,1406058339792.cdf12d16f55023d641440ce7cf8436fb. because compaction request was cancelled
2014-07-22 13:31:57,188 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=22821, memsize=276.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ad2f6249335e4d63ab08e9e6c2607cb2
2014-07-22 13:31:57,202 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/.tmp/ad2f6249335e4d63ab08e9e6c2607cb2 as hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ad2f6249335e4d63ab08e9e6c2607cb2
2014-07-22 13:31:57,214 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/847263c284c07b6767aa3cc0c6497be7/family/ad2f6249335e4d63ab08e9e6c2607cb2, entries=1006830, sequenceid=22821, filesize=71.7m
2014-07-22 13:31:57,214 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~276.5m/289959280, currentsize=0.0/0 for region usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. in 8159ms, sequenceid=22821, compaction requested=true
2014-07-22 13:31:57,214 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-22 13:31:57,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Selecting compaction from 52 store files, 0 compacting, 52 eligible, 2000 blocking
2014-07-22 13:31:57,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 52 files from compaction candidates
2014-07-22 13:31:57,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-22 13:31:57,215 DEBUG [regionserver60020-smallCompactions-1406058001377] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-22 13:31:57,215 DEBUG [regionserver60020-smallCompactions-1406058001377] regionserver.CompactSplitThread: Not compacting usertable,user2,1406058339792.847263c284c07b6767aa3cc0c6497be7. because compaction request was cancelled
