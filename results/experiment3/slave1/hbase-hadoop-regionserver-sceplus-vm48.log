Fri Jun 27 01:04:14 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-06-27 01:04:15,421 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-06-27 01:04:15,422 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-06-27 01:04:15,422 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 56532 22
2014-06-27 01:04:15,650 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-06-27 01:04:15,651 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 56532 9.1.143.58 22
2014-06-27 01:04:15,652 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-06-27 01:04:15,652 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-06-27 01:04:15,652 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-06-27 01:04:15,654 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=189
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-06-27 01:04:15,655 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-06-27 01:04:15,657 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-06-27 01:04:15,658 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx1000m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-06-27 01:04:15,878 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-06-27 01:04:16,225 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-06-27 01:04:16,305 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-06-27 01:04:16,317 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-06-27 01:04:16,380 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-06-27 01:04:16,381 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-06-27 01:04:16,381 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-06-27 01:04:16,386 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-06-27 01:04:16,396 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-06-27 01:04:16,478 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-06-27 01:04:16,478 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-06-27 01:04:16,482 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-06-27 01:04:16,485 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7m
2014-06-27 01:04:16,550 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-06-27 01:04:16,605 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-06-27 01:04:16,613 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-06-27 01:04:16,615 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-06-27 01:04:16,615 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-06-27 01:04:16,615 INFO  [main] mortbay.log: jetty-6.1.26
2014-06-27 01:04:16,926 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-06-27 01:04:16,970 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-27 01:04:16,973 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-27 01:04:16,974 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 01:04:16,997 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 01:04:17,003 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 01:04:17,010 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-27 01:04:17,033 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146dc5a5d110000, negotiated timeout = 90000
2014-06-27 01:04:36,696 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x178954a4, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 01:04:36,697 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x178954a4 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 01:04:36,698 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 01:04:36,698 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 01:04:36,701 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46dc5a73e10002, negotiated timeout = 90000
2014-06-27 01:04:36,946 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@256c949f
2014-06-27 01:04:36,951 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-06-27 01:04:36,956 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-06-27 01:04:36,970 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-06-27 01:04:36,999 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-06-27 01:04:37,004 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7m, globalMemStoreLimitLowMark=367.3m, maxHeap=966.7m
2014-06-27 01:04:37,008 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-06-27 01:04:37,023 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1403856254970 with port=60020, startcode=1403856256407
2014-06-27 01:04:37,399 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-06-27 01:04:37,400 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-06-27 01:04:37,439 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-06-27 01:04:37,447 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:04:37,486 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-06-27 01:04:37,498 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-06-27 01:04:37,611 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856277513
2014-06-27 01:04:37,629 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-06-27 01:04:37,634 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-06-27 01:04:37,638 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-06-27 01:04:37,642 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-06-27 01:04:37,645 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 01:04:37,645 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 01:04:37,645 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 01:04:37,645 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 01:04:37,646 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-06-27 01:04:37,656 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1403856250521, sceplus-vm48.almaden.ibm.com,60020,1403856256407] other RSs: [slave1,60020,1403856250521, sceplus-vm48.almaden.ibm.com,60020,1403856256407]
2014-06-27 01:04:37,682 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-06-27 01:04:37,684 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x6ad31e96, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 01:04:37,685 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6ad31e96 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 01:04:37,685 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 01:04:37,686 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-27 01:04:37,690 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146dc5a5d110004, negotiated timeout = 90000
2014-06-27 01:04:37,696 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-06-27 01:04:37,696 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-06-27 01:04:37,739 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1403856256407, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x146dc5a5d110000
2014-06-27 01:04:37,739 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403856256407] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1403856256407 starting
2014-06-27 01:04:37,740 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-06-27 01:04:37,740 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:04:37,740 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1403856256407'
2014-06-27 01:04:37,740 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-06-27 01:04:37,742 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-06-27 01:04:37,743 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-06-27 01:04:41,336 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 01:04:45,712 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 01:04:46,957 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 01:04:46,971 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403856256407] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403856256407 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1403853966473.1403855056776
2014-06-27 01:04:47,037 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776, length=0
2014-06-27 01:04:47,037 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 01:04:47,049 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: File hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776 might be still open, length is 0
2014-06-27 01:04:47,053 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776
2014-06-27 01:04:47,055 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=false, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776 after 2ms
2014-06-27 01:04:47,622 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403856256407] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1403856256407 has 1 tasks in progress and can't take more.
2014-06-27 01:04:47,692 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 01:04:47,696 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403856256407] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1403856256407 has 1 tasks in progress and can't take more.
2014-06-27 01:04:51,057 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=1 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776 after 4004ms
2014-06-27 01:04:51,118 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-06-27 01:04:51,118 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-06-27 01:04:51,120 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-06-27 01:04:51,325 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/02726f5710f32df4aca9849b2b408c37. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,365 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/e2bae867a9343b96341d942c702b53d9. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,383 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/81091be720e0dfb195c5a52021d51a4d. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,399 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/0dcdafdf47324776f5cf6e339b64582c. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,450 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/b6e439b4bcf7a1008d3eb7ca7f2ab1ae. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,453 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/cba3e8ffee084845d984d32046dc2e1a. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,459 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/380f152a02463b980ea8cb7680482f1f. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,466 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/6d948393ae6a7f6b47d170e1af7eaaf5. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,472 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/43f9df38876c18561de40fcc46f2bf94. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,473 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/1f43dedea29e9a73499376590e751158. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,476 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/9e6317863a1b63de04fc466687124912. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,478 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/c4985d75d74919417fe09c495ce34387. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,489 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/772f8d9f5dc6205c7a80438be3ab5c3c. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,490 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/4b25d929f16a7651fb2695e50fae6c9b. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,493 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/22db54cb3fbf86a8b2d5bd55ce925f01. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,495 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/b04c51d8b3fc6c80bd4933984ee72081. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,498 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/ba000949ab4ae5c426c353929cd3b6fd. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,504 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/58b99b5e787547f93a28b9e3ba99537a. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,507 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f8f191b595f093eafda7610980c5b657. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,510 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f24fcbb7b739d691ccbb10ea3abd15ca. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,512 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/82ac0bbe952e21af5ac206d8cf4df047. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,514 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/e538fda3fdcf525241c33fe3a961d06d. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,525 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/a563ded59249cf3ef5ed7aa462e7de58. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,527 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/1605b64e79734ac18921408a753f896d. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,529 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/192d3136f33cfb420acdecfaaad7e4a0. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,532 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/e280be7b99cec9a8198a16bd7a8d38b5. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,534 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/45a8bdd1a78cbc016009043affc460de. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,546 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/d57641afe8e3180da44801a09db66196. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,548 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/48ac8e719641b79ded3f951a765bacc9. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,550 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/a9c576340ecbe8857ae242307f25567a. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,552 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/4a6020a322256eb509120aeddb5d1569. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,563 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/c2be85eb7666054ec87575d202004b03. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,565 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/e7b05c71a61426a9209bc4a87d81fdb5. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,568 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/f0b18db342cb93ed9711230cde64f013. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,570 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/40d68e9df6dd6c4e17b4e697a9ee1818. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,572 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: This region's directory doesn't exist: hdfs://master:54310/hbase/data/default/usertable/7999f46ad69e395ffbda7a810cc305d5. It is very likely that it was already split so it's safe to discard those edits.
2014-06-27 01:04:51,602 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000002.temp region=825c3414185cc70d87bf4957812364c3
2014-06-27 01:04:51,602 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000002.temp region=849dbaad8b6230a7865d62cb21b31311
2014-06-27 01:04:51,608 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000002.temp region=c0268f38777d68e8f680cf94f4d99d57
2014-06-27 01:04:51,611 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 01:04:51,611 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 01:04:51,626 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000002.temp region=93dba25edf6488e7cd0ccf5c0b5e15c4
2014-06-27 01:04:51,663 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000002.temp region=31e1e70c5802b642d7a16a39db6915a5
2014-06-27 01:04:51,669 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-06-27 01:04:51,671 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,672 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,672 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,672 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,673 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,673 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,673 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,673 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,714 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000002.temp (wrote 32 edits in 22ms)
2014-06-27 01:04:51,715 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000002.temp (wrote 26 edits in 57ms)
2014-06-27 01:04:51,715 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000002.temp (wrote 29 edits in 25ms)
2014-06-27 01:04:51,718 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/849dbaad8b6230a7865d62cb21b31311/recovered.edits/0000000000000000033
2014-06-27 01:04:51,718 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,719 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/31e1e70c5802b642d7a16a39db6915a5/recovered.edits/0000000000000000027
2014-06-27 01:04:51,719 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000002.temp
2014-06-27 01:04:51,720 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/825c3414185cc70d87bf4957812364c3/recovered.edits/0000000000000000030
2014-06-27 01:04:51,750 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000002.temp (wrote 27 edits in 23ms)
2014-06-27 01:04:51,757 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000002.temp (wrote 18 edits in 19ms)
2014-06-27 01:04:51,759 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/93dba25edf6488e7cd0ccf5c0b5e15c4/recovered.edits/0000000000000000028
2014-06-27 01:04:51,803 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/c0268f38777d68e8f680cf94f4d99d57/recovered.edits/0000000000000000019
2014-06-27 01:04:51,804 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 641 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403853966473-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473.1403855056776 is corrupted = false progress failed = false
2014-06-27 01:04:51,813 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1403853966473.1403855056776 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:04:51,813 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403856256407 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1403853966473-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1403853966473.1403855056776 in 4831ms
2014-06-27 01:04:51,842 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 01:04:51,883 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 01:04:51,999 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:04:52,017 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:04:52,018 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-06-27 01:04:52,037 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-06-27 01:04:52,038 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-06-27 01:04:52,038 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 01:04:52,099 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
2014-06-27 01:04:52,129 INFO  [StoreFileOpenerThread-info-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-06-27 01:04:52,185 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-06-27 01:04:52,214 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-06-27 01:04:52,218 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-06-27 01:04:52,218 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-06-27 01:04:52,221 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 01:04:52,366 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:04:52,366 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 01:04:52,366 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:04:52,373 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:04:52,373 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:04:52,373 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:09:16,493 DEBUG [LruStats #0] hfile.LruBlockCache: Total=408.52 KB, free=386.28 MB, max=386.68 MB, blocks=2, accesses=5, hits=3, hitRatio=60.00%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-06-27 01:12:40,277 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:12:40,290 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:12:40,291 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 242b1f4a8cce314fe04bd5b85c8b17d8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,292 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958.
2014-06-27 01:12:40,293 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 63f84494fd2ae9b2e453f741a89285b4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,294 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning af61ffa3abad280ddfdb5f6300c06958 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,294 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:12:40,295 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Open usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:12:40,303 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 242b1f4a8cce314fe04bd5b85c8b17d8 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,304 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 63f84494fd2ae9b2e453f741a89285b4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,304 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 242b1f4a8cce314fe04bd5b85c8b17d8, NAME => 'usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-06-27 01:12:40,305 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node af61ffa3abad280ddfdb5f6300c06958 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,305 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 63f84494fd2ae9b2e453f741a89285b4, NAME => 'usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-06-27 01:12:40,305 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => af61ffa3abad280ddfdb5f6300c06958, NAME => 'usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958.', STARTKEY => 'user9', ENDKEY => ''}
2014-06-27 01:12:40,307 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 242b1f4a8cce314fe04bd5b85c8b17d8
2014-06-27 01:12:40,307 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 63f84494fd2ae9b2e453f741a89285b4
2014-06-27 01:12:40,307 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable af61ffa3abad280ddfdb5f6300c06958
2014-06-27 01:12:40,308 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:12:40,308 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:12:40,308 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958.
2014-06-27 01:12:40,321 INFO  [StoreOpener-242b1f4a8cce314fe04bd5b85c8b17d8-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 01:12:40,321 INFO  [StoreOpener-63f84494fd2ae9b2e453f741a89285b4-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 01:12:40,324 INFO  [StoreOpener-af61ffa3abad280ddfdb5f6300c06958-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 01:12:40,328 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4
2014-06-27 01:12:40,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8
2014-06-27 01:12:40,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/af61ffa3abad280ddfdb5f6300c06958
2014-06-27 01:12:40,332 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 63f84494fd2ae9b2e453f741a89285b4; next sequenceid=1
2014-06-27 01:12:40,332 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 63f84494fd2ae9b2e453f741a89285b4
2014-06-27 01:12:40,334 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 242b1f4a8cce314fe04bd5b85c8b17d8; next sequenceid=1
2014-06-27 01:12:40,334 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 242b1f4a8cce314fe04bd5b85c8b17d8
2014-06-27 01:12:40,334 INFO  [PostOpenDeployTasks:63f84494fd2ae9b2e453f741a89285b4] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:12:40,341 INFO  [PostOpenDeployTasks:242b1f4a8cce314fe04bd5b85c8b17d8] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:12:40,351 INFO  [PostOpenDeployTasks:63f84494fd2ae9b2e453f741a89285b4] catalog.MetaEditor: Updated row usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,352 INFO  [PostOpenDeployTasks:63f84494fd2ae9b2e453f741a89285b4] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:12:40,352 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 63f84494fd2ae9b2e453f741a89285b4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,353 INFO  [PostOpenDeployTasks:242b1f4a8cce314fe04bd5b85c8b17d8] catalog.MetaEditor: Updated row usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,353 INFO  [PostOpenDeployTasks:242b1f4a8cce314fe04bd5b85c8b17d8] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:12:40,354 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 242b1f4a8cce314fe04bd5b85c8b17d8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,357 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 63f84494fd2ae9b2e453f741a89285b4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,357 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 63f84494fd2ae9b2e453f741a89285b4 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,357 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,358 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 8c24dc8da28390998e501bd2fe2885c0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,358 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 242b1f4a8cce314fe04bd5b85c8b17d8 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,358 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 242b1f4a8cce314fe04bd5b85c8b17d8 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,358 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,358 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 71d4fc84a5c8f58ae058bda4626c3461 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,362 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 8c24dc8da28390998e501bd2fe2885c0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,363 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 8c24dc8da28390998e501bd2fe2885c0, NAME => 'usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-06-27 01:12:40,363 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 71d4fc84a5c8f58ae058bda4626c3461 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 01:12:40,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 71d4fc84a5c8f58ae058bda4626c3461, NAME => 'usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-06-27 01:12:40,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 8c24dc8da28390998e501bd2fe2885c0
2014-06-27 01:12:40,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 71d4fc84a5c8f58ae058bda4626c3461
2014-06-27 01:12:40,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:12:40,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:12:40,366 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined af61ffa3abad280ddfdb5f6300c06958; next sequenceid=1
2014-06-27 01:12:40,366 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node af61ffa3abad280ddfdb5f6300c06958
2014-06-27 01:12:40,368 INFO  [PostOpenDeployTasks:af61ffa3abad280ddfdb5f6300c06958] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958.
2014-06-27 01:12:40,374 INFO  [StoreOpener-71d4fc84a5c8f58ae058bda4626c3461-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 01:12:40,376 INFO  [PostOpenDeployTasks:af61ffa3abad280ddfdb5f6300c06958] catalog.MetaEditor: Updated row usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,376 INFO  [PostOpenDeployTasks:af61ffa3abad280ddfdb5f6300c06958] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958.
2014-06-27 01:12:40,377 INFO  [StoreOpener-8c24dc8da28390998e501bd2fe2885c0-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 01:12:40,377 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning af61ffa3abad280ddfdb5f6300c06958 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,381 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461
2014-06-27 01:12:40,382 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0
2014-06-27 01:12:40,382 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node af61ffa3abad280ddfdb5f6300c06958 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,382 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned af61ffa3abad280ddfdb5f6300c06958 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,382 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,383 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 71d4fc84a5c8f58ae058bda4626c3461; next sequenceid=1
2014-06-27 01:12:40,383 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 71d4fc84a5c8f58ae058bda4626c3461
2014-06-27 01:12:40,385 INFO  [PostOpenDeployTasks:71d4fc84a5c8f58ae058bda4626c3461] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:12:40,391 INFO  [PostOpenDeployTasks:71d4fc84a5c8f58ae058bda4626c3461] catalog.MetaEditor: Updated row usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,391 INFO  [PostOpenDeployTasks:71d4fc84a5c8f58ae058bda4626c3461] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:12:40,392 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 71d4fc84a5c8f58ae058bda4626c3461 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,395 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 71d4fc84a5c8f58ae058bda4626c3461 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,395 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 71d4fc84a5c8f58ae058bda4626c3461 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,395 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,422 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 8c24dc8da28390998e501bd2fe2885c0; next sequenceid=1
2014-06-27 01:12:40,422 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 8c24dc8da28390998e501bd2fe2885c0
2014-06-27 01:12:40,424 INFO  [PostOpenDeployTasks:8c24dc8da28390998e501bd2fe2885c0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:12:40,432 INFO  [PostOpenDeployTasks:8c24dc8da28390998e501bd2fe2885c0] catalog.MetaEditor: Updated row usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. with server=sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,433 INFO  [PostOpenDeployTasks:8c24dc8da28390998e501bd2fe2885c0] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:12:40,434 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 8c24dc8da28390998e501bd2fe2885c0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,438 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146dc5a5d110000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 8c24dc8da28390998e501bd2fe2885c0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 01:12:40,438 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 8c24dc8da28390998e501bd2fe2885c0 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:12:40,438 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. on sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:00,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:02,420 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 121 synced till here 85
2014-06-27 01:13:02,429 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1305ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1783ms
2014-06-27 01:13:06,239 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3309ms
GC pool 'ParNew' had collection(s): count=1 time=1360ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2143ms
2014-06-27 01:13:06,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856277513 with entries=121, filesize=95.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856780472
2014-06-27 01:13:06,830 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776419,"queuetimems":1,"class":"HRegionServer","responsesize":17141,"method":"Multi"}
2014-06-27 01:13:06,830 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10765,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856775929,"queuetimems":1,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-06-27 01:13:06,831 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856775948,"queuetimems":1,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-06-27 01:13:06,832 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776200,"queuetimems":0,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-27 01:13:06,842 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 18 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:06,844 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:06,844 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 48 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:06,844 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:06,844 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 49 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:06,844 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:06,845 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 31 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:06,845 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,572 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2327ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2602ms
2014-06-27 01:13:09,616 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13220,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776395,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:13:09,617 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 25 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,617 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:09,936 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13553,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776383,"queuetimems":0,"class":"HRegionServer","responsesize":16789,"method":"Multi"}
2014-06-27 01:13:09,936 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13722,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776214,"queuetimems":1,"class":"HRegionServer","responsesize":17342,"method":"Multi"}
2014-06-27 01:13:09,936 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856775993,"queuetimems":0,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13531,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776405,"queuetimems":0,"class":"HRegionServer","responsesize":16649,"method":"Multi"}
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 26 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 41 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 23 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 29 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776175,"queuetimems":3,"class":"HRegionServer","responsesize":17207,"method":"Multi"}
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 38 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11626,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856778310,"queuetimems":1,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856775965,"queuetimems":0,"class":"HRegionServer","responsesize":17538,"method":"Multi"}
2014-06-27 01:13:09,938 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12598,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777339,"queuetimems":1,"class":"HRegionServer","responsesize":17141,"method":"Multi"}
2014-06-27 01:13:09,937 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777304,"queuetimems":5,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 62 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 56 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777520,"queuetimems":1,"class":"HRegionServer","responsesize":16649,"method":"Multi"}
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 52 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,939 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 43 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,940 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,940 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 55 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,940 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,940 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13786,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776153,"queuetimems":147,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-06-27 01:13:09,940 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13348,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776591,"queuetimems":0,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-06-27 01:13:09,941 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 39 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,941 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,941 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 22 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,941 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,942 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13753,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776187,"queuetimems":0,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:13:09,942 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12613,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777328,"queuetimems":1,"class":"HRegionServer","responsesize":16890,"method":"Multi"}
2014-06-27 01:13:09,942 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11658,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856778283,"queuetimems":1,"class":"HRegionServer","responsesize":16535,"method":"Multi"}
2014-06-27 01:13:09,942 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13335,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776603,"queuetimems":0,"class":"HRegionServer","responsesize":16890,"method":"Multi"}
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 36 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,942 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856775980,"queuetimems":0,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776371,"queuetimems":0,"class":"HRegionServer","responsesize":17125,"method":"Multi"}
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776625,"queuetimems":0,"class":"HRegionServer","responsesize":17393,"method":"Multi"}
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 60 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13325,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856776614,"queuetimems":0,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 19 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 27 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 42 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,943 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12405,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777533,"queuetimems":1,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 21 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,944 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 53 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 57 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 16 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:09,945 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,604 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1532ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1658ms
2014-06-27 01:13:11,611 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 232 synced till here 211
2014-06-27 01:13:11,739 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12758,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856778980,"queuetimems":1,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-06-27 01:13:11,739 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 71 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,739 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,742 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856778783,"queuetimems":2,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-06-27 01:13:11,742 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14425,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777317,"queuetimems":1,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-06-27 01:13:11,742 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 64 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 58 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856777350,"queuetimems":0,"class":"HRegionServer","responsesize":17393,"method":"Multi"}
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12802,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856778940,"queuetimems":1,"class":"HRegionServer","responsesize":17007,"method":"Multi"}
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 54 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 66 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,743 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,762 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856780472 with entries=111, filesize=76.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856789935
2014-06-27 01:13:11,792 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12671,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856779121,"queuetimems":0,"class":"HRegionServer","responsesize":16872,"method":"Multi"}
2014-06-27 01:13:11,793 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 72 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,793 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:11,793 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12694,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856779099,"queuetimems":0,"class":"HRegionServer","responsesize":17175,"method":"Multi"}
2014-06-27 01:13:11,794 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 74 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:11,794 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,039 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2287ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2579ms
2014-06-27 01:13:15,227 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:15,248 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.7m is >= than blocking 386.7m size
2014-06-27 01:13:15,260 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.7m is >= than blocking 386.7m size
2014-06-27 01:13:15,263 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. due to global heap pressure
2014-06-27 01:13:15,263 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0., current region memstore size 116.3m
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15242,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780119,"queuetimems":1,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12944,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856782417,"queuetimems":0,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12697,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856782664,"queuetimems":186,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15074,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780287,"queuetimems":0,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780130,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:13:15,363 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780156,"queuetimems":1,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-06-27 01:13:15,363 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14744,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780619,"queuetimems":0,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:15,364 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856779351,"queuetimems":0,"class":"HRegionServer","responsesize":17538,"method":"Multi"}
2014-06-27 01:13:15,362 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 84 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 77 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 91 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16255,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856779110,"queuetimems":0,"class":"HRegionServer","responsesize":17128,"method":"Multi"}
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 82 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 83 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 80 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 87 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 90 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856782459,"queuetimems":0,"class":"HRegionServer","responsesize":17207,"method":"Multi"}
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,365 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 88 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,366 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,370 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 405.2m is >= than blocking 386.7m size
2014-06-27 01:13:15,370 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,370 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15061,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780309,"queuetimems":0,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 79 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14763,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780607,"queuetimems":0,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12924,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856782447,"queuetimems":18,"class":"HRegionServer","responsesize":16789,"method":"Multi"}
2014-06-27 01:13:15,371 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.3m is >= than blocking 386.7m size
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14937,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780433,"queuetimems":1,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 96 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780470,"queuetimems":0,"class":"HRegionServer","responsesize":16837,"method":"Multi"}
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780445,"queuetimems":0,"class":"HRegionServer","responsesize":17550,"method":"Multi"}
2014-06-27 01:13:15,371 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56353","starttimems":1403856780457,"queuetimems":1,"class":"HRegionServer","responsesize":17145,"method":"Multi"}
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 78 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 89 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 98 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,372 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 99 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,373 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,373 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 97 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56353: output error
2014-06-27 01:13:15,373 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:15,383 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,403 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,411 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,412 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,413 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,414 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,415 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 411.8m is >= than blocking 386.7m size
2014-06-27 01:13:15,477 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:15,545 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:15,586 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:15,593 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:15,600 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,148 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2108ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2540ms
2014-06-27 01:13:18,149 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,150 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,150 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,151 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,151 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,151 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,152 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,152 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,152 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,153 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,153 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,172 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,172 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856789935 with entries=96, filesize=70.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856795555
2014-06-27 01:13:18,172 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,172 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,185 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,852 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,865 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,961 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:18,988 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,009 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,032 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,049 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,066 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,088 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,112 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,136 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:19,159 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,268 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:21,268 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1619ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2083ms
2014-06-27 01:13:21,269 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5625ms
2014-06-27 01:13:21,269 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. due to global heap pressure
2014-06-27 01:13:21,270 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6021ms
2014-06-27 01:13:21,270 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6021ms
2014-06-27 01:13:21,270 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4., current region memstore size 113.7m
2014-06-27 01:13:21,271 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5900ms
2014-06-27 01:13:21,271 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5900ms
2014-06-27 01:13:21,271 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:13:21,271 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5871ms
2014-06-27 01:13:21,271 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5861ms
2014-06-27 01:13:21,272 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5861ms
2014-06-27 01:13:21,272 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5860ms
2014-06-27 01:13:21,272 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5858ms
2014-06-27 01:13:21,272 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5857ms
2014-06-27 01:13:21,272 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5687ms
2014-06-27 01:13:21,273 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5680ms
2014-06-27 01:13:21,274 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,285 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,295 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,306 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,316 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,325 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,344 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,374 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:21,377 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 439.9m is >= than blocking 386.7m size
2014-06-27 01:13:21,488 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=89, memsize=51.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/0c4bbf09f4c441eaa651544b0cb6107b
2014-06-27 01:13:21,520 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/0c4bbf09f4c441eaa651544b0cb6107b as hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/0c4bbf09f4c441eaa651544b0cb6107b
2014-06-27 01:13:21,533 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/0c4bbf09f4c441eaa651544b0cb6107b, entries=186210, sequenceid=89, filesize=27.8m
2014-06-27 01:13:21,534 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~124.2m/130219920, currentsize=0.0/0 for region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. in 6271ms, sequenceid=89, compaction requested=false
2014-06-27 01:13:21,536 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 159ms
2014-06-27 01:13:21,536 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,536 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 192ms
2014-06-27 01:13:21,536 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,536 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 211ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,537 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 221ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,537 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 231ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,537 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 242ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,537 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 252ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,537 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 263ms
2014-06-27 01:13:21,537 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,538 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5945ms
2014-06-27 01:13:21,538 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,540 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5955ms
2014-06-27 01:13:21,540 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,540 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6125ms
2014-06-27 01:13:21,540 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,543 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6129ms
2014-06-27 01:13:21,543 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,546 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6134ms
2014-06-27 01:13:21,546 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,547 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6136ms
2014-06-27 01:13:21,547 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,547 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6137ms
2014-06-27 01:13:21,547 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,547 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6147ms
2014-06-27 01:13:21,547 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,549 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6166ms
2014-06-27 01:13:21,549 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,549 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6178ms
2014-06-27 01:13:21,550 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,551 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6181ms
2014-06-27 01:13:21,551 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,551 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6302ms
2014-06-27 01:13:21,551 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,551 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6303ms
2014-06-27 01:13:21,551 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,551 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5952ms
2014-06-27 01:13:21,551 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,555 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2395ms
2014-06-27 01:13:21,555 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,556 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2419ms
2014-06-27 01:13:21,556 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,562 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2450ms
2014-06-27 01:13:21,562 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,569 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2481ms
2014-06-27 01:13:21,569 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,583 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2516ms
2014-06-27 01:13:21,583 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,591 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2541ms
2014-06-27 01:13:21,591 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,591 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2560ms
2014-06-27 01:13:21,591 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,591 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2582ms
2014-06-27 01:13:21,591 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,597 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2608ms
2014-06-27 01:13:21,597 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,597 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2636ms
2014-06-27 01:13:21,598 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,599 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2733ms
2014-06-27 01:13:21,599 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,600 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2747ms
2014-06-27 01:13:21,600 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,602 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3417ms
2014-06-27 01:13:21,602 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,602 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3430ms
2014-06-27 01:13:21,603 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,603 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3431ms
2014-06-27 01:13:21,603 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,604 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3431ms
2014-06-27 01:13:21,604 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,604 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12011,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56359","starttimems":1403856789593,"queuetimems":1,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-06-27 01:13:21,604 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56359","starttimems":1403856789578,"queuetimems":1,"class":"HRegionServer","responsesize":16535,"method":"Multi"}
2014-06-27 01:13:21,605 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 277 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56359: output error
2014-06-27 01:13:21,605 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:21,605 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56359","starttimems":1403856789622,"queuetimems":2,"class":"HRegionServer","responsesize":17125,"method":"Multi"}
2014-06-27 01:13:21,607 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 266 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56359: output error
2014-06-27 01:13:21,608 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:21,608 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 275 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56359: output error
2014-06-27 01:13:21,608 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:21,626 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3453ms
2014-06-27 01:13:21,630 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,630 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3477ms
2014-06-27 01:13:21,631 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,631 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3479ms
2014-06-27 01:13:21,634 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,635 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3483ms
2014-06-27 01:13:21,636 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,638 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3484ms
2014-06-27 01:13:21,638 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,638 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3488ms
2014-06-27 01:13:21,639 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3489ms
2014-06-27 01:13:21,639 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,640 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3490ms
2014-06-27 01:13:21,641 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,641 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3490ms
2014-06-27 01:13:21,641 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,647 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3495ms
2014-06-27 01:13:21,647 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,658 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3503ms
2014-06-27 01:13:21,667 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:21,670 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3519ms
2014-06-27 01:13:21,670 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:25,069 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2799ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3243ms
2014-06-27 01:13:27,893 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2323ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2444ms
2014-06-27 01:13:28,109 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18503,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56359","starttimems":1403856789606,"queuetimems":1,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:28,110 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 276 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56359: output error
2014-06-27 01:13:28,110 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:28,168 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,168 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:28,172 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. due to global heap pressure
2014-06-27 01:13:28,180 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461., current region memstore size 113.4m
2014-06-27 01:13:28,282 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,293 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,311 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,316 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,317 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:28,319 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,672 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2277ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2349ms
2014-06-27 01:13:30,675 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,675 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,675 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,676 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,676 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,676 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,677 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,677 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,678 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,678 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,678 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,679 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,679 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,679 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,679 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,680 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,680 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,682 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,804 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,804 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,819 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,821 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,822 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,823 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,823 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,823 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,824 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,824 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,824 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,828 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-27 01:13:30,873 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19256,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791616,"queuetimems":0,"class":"HRegionServer","responsesize":16535,"method":"Multi"}
2014-06-27 01:13:30,873 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 301 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:30,873 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:30,876 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 401.8m is >= than blocking 386.7m size
2014-06-27 01:13:30,877 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,878 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,879 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,880 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,880 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,881 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,882 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,883 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,896 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19195,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791701,"queuetimems":0,"class":"HRegionServer","responsesize":16837,"method":"Multi"}
2014-06-27 01:13:30,897 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 413.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,898 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 321 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:30,898 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:30,901 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 413.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,901 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 413.1m is >= than blocking 386.7m size
2014-06-27 01:13:30,901 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 413.1m is >= than blocking 386.7m size
2014-06-27 01:13:31,002 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:31,026 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=76, memsize=51.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/b4c7e0cbab9843c4915d6486bda81f8c
2014-06-27 01:13:31,044 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/b4c7e0cbab9843c4915d6486bda81f8c as hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/b4c7e0cbab9843c4915d6486bda81f8c
2014-06-27 01:13:31,060 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/b4c7e0cbab9843c4915d6486bda81f8c, entries=186900, sequenceid=76, filesize=27.9m
2014-06-27 01:13:31,061 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~113.7m/119228320, currentsize=12.3m/12849920 for region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. in 9791ms, sequenceid=76, compaction requested=false
2014-06-27 01:13:31,061 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 160ms
2014-06-27 01:13:31,061 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,061 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 160ms
2014-06-27 01:13:31,061 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,061 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 160ms
2014-06-27 01:13:31,062 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,062 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 165ms
2014-06-27 01:13:31,062 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,062 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 179ms
2014-06-27 01:13:31,062 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,063 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 182ms
2014-06-27 01:13:31,063 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,063 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 182ms
2014-06-27 01:13:31,063 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,063 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 183ms
2014-06-27 01:13:31,063 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,064 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 183ms
2014-06-27 01:13:31,064 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,064 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-06-27 01:13:31,064 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,064 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-06-27 01:13:31,064 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,064 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 187ms
2014-06-27 01:13:31,064 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,065 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 189ms
2014-06-27 01:13:31,065 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,065 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 237ms
2014-06-27 01:13:31,065 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,065 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 241ms
2014-06-27 01:13:31,065 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,065 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 242ms
2014-06-27 01:13:31,065 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,065 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 242ms
2014-06-27 01:13:31,066 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,066 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 243ms
2014-06-27 01:13:31,066 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,066 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 243ms
2014-06-27 01:13:31,066 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,066 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 244ms
2014-06-27 01:13:31,066 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,067 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 244ms
2014-06-27 01:13:31,067 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,067 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 246ms
2014-06-27 01:13:31,067 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,067 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 249ms
2014-06-27 01:13:31,067 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,067 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 263ms
2014-06-27 01:13:31,067 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,067 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 263ms
2014-06-27 01:13:31,067 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,068 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 386ms
2014-06-27 01:13:31,068 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,068 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 389ms
2014-06-27 01:13:31,068 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,068 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 389ms
2014-06-27 01:13:31,068 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,068 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 389ms
2014-06-27 01:13:31,068 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,069 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 390ms
2014-06-27 01:13:31,069 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,069 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-06-27 01:13:31,069 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,069 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 390ms
2014-06-27 01:13:31,069 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,069 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-06-27 01:13:31,069 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-06-27 01:13:31,070 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 392ms
2014-06-27 01:13:31,070 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 393ms
2014-06-27 01:13:31,070 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 394ms
2014-06-27 01:13:31,070 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 394ms
2014-06-27 01:13:31,070 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,070 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 394ms
2014-06-27 01:13:31,071 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,071 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 396ms
2014-06-27 01:13:31,071 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,071 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 396ms
2014-06-27 01:13:31,071 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,071 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 396ms
2014-06-27 01:13:31,071 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,071 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 397ms
2014-06-27 01:13:31,071 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,071 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2752ms
2014-06-27 01:13:31,072 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,072 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2758ms
2014-06-27 01:13:31,072 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,072 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2766ms
2014-06-27 01:13:31,072 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,078 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2770ms
2014-06-27 01:13:31,078 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,079 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2789ms
2014-06-27 01:13:31,081 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,081 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2812ms
2014-06-27 01:13:31,081 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:31,089 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2913ms
2014-06-27 01:13:31,089 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:34,872 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3199ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3485ms
2014-06-27 01:13:35,106 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791783,"queuetimems":1,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-27 01:13:35,107 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23346,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791761,"queuetimems":0,"class":"HRegionServer","responsesize":17342,"method":"Multi"}
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23380,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791727,"queuetimems":0,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 318 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 319 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23431,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791677,"queuetimems":0,"class":"HRegionServer","responsesize":17145,"method":"Multi"}
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 320 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791628,"queuetimems":2,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 322 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 302 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,109 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791648,"queuetimems":0,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-06-27 01:13:35,108 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:35,110 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 323 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:35,110 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,279 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2906ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3013ms
2014-06-27 01:13:38,325 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:38,342 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23260,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856795082,"queuetimems":1,"class":"HRegionServer","responsesize":17125,"method":"Multi"}
2014-06-27 01:13:38,343 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 326 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,343 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,397 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 461 synced till here 429
2014-06-27 01:13:38,532 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:38,533 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. due to global heap pressure
2014-06-27 01:13:38,533 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8., current region memstore size 123.3m
2014-06-27 01:13:38,562 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856795071,"queuetimems":0,"class":"HRegionServer","responsesize":17128,"method":"Multi"}
2014-06-27 01:13:38,562 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26173,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792389,"queuetimems":0,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-06-27 01:13:38,562 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26146,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792416,"queuetimems":1,"class":"HRegionServer","responsesize":16789,"method":"Multi"}
2014-06-27 01:13:38,562 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792270,"queuetimems":1,"class":"HRegionServer","responsesize":17007,"method":"Multi"}
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 303 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 306 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 307 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,563 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 314 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,564 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,570 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26313,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792257,"queuetimems":0,"class":"HRegionServer","responsesize":17538,"method":"Multi"}
2014-06-27 01:13:38,571 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792317,"queuetimems":0,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-06-27 01:13:38,571 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 315 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,571 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,571 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 310 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:38,571 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:38,572 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,573 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,576 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,576 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,576 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:13:38,576 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,577 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,579 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.3m is >= than blocking 386.7m size
2014-06-27 01:13:38,585 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856795555 with entries=133, filesize=102.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856818326
2014-06-27 01:13:38,650 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 388.8m is >= than blocking 386.7m size
2014-06-27 01:13:38,732 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:41,510 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2230ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2689ms
2014-06-27 01:13:41,520 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.3m is >= than blocking 386.7m size
2014-06-27 01:13:41,522 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.3m is >= than blocking 386.7m size
2014-06-27 01:13:41,531 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.3m is >= than blocking 386.7m size
2014-06-27 01:13:41,542 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.3m is >= than blocking 386.7m size
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791805,"queuetimems":0,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856791830,"queuetimems":0,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29233,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792366,"queuetimems":3,"class":"HRegionServer","responsesize":17175,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26538,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856795060,"queuetimems":1,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 317 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792306,"queuetimems":0,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 308 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29304,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792294,"queuetimems":1,"class":"HRegionServer","responsesize":17207,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29264,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792334,"queuetimems":1,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 304 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792439,"queuetimems":0,"class":"HRegionServer","responsesize":16872,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856792282,"queuetimems":1,"class":"HRegionServer","responsesize":17550,"method":"Multi"}
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56362","starttimems":1403856795104,"queuetimems":0,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 309 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 316 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 325 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,599 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,601 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 396.8m is >= than blocking 386.7m size
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 313 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,600 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 305 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 312 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,601 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,602 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 311 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56362: output error
2014-06-27 01:13:41,602 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,709 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=83, memsize=51.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/fb9530e069564fc0b5068c2a5a6735ef
2014-06-27 01:13:41,717 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 401.5m is >= than blocking 386.7m size
2014-06-27 01:13:41,717 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 401.5m is >= than blocking 386.7m size
2014-06-27 01:13:41,720 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 410.7m is >= than blocking 386.7m size
2014-06-27 01:13:41,720 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 410.7m is >= than blocking 386.7m size
2014-06-27 01:13:41,720 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-27 01:13:41,721 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-27 01:13:41,723 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 422.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,723 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 422.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,723 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 422.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,723 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 422.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,723 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 425.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,725 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 425.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,725 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 425.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,726 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/fb9530e069564fc0b5068c2a5a6735ef as hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/fb9530e069564fc0b5068c2a5a6735ef
2014-06-27 01:13:41,726 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 425.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,726 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,726 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,726 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,727 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,728 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,729 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 428.0m is >= than blocking 386.7m size
2014-06-27 01:13:41,741 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/fb9530e069564fc0b5068c2a5a6735ef, entries=186610, sequenceid=83, filesize=27.8m
2014-06-27 01:13:41,741 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~124.2m/130184880, currentsize=38.9m/40745440 for region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. in 13567ms, sequenceid=83, compaction requested=false
2014-06-27 01:13:41,741 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-06-27 01:13:41,741 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,741 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-06-27 01:13:41,741 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,741 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461., current region memstore size 38.9m
2014-06-27 01:13:41,741 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15ms
2014-06-27 01:13:41,742 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,742 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16ms
2014-06-27 01:13:41,742 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,742 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16ms
2014-06-27 01:13:41,742 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,742 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-06-27 01:13:41,742 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,742 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-06-27 01:13:41,742 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,743 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-06-27 01:13:41,743 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,743 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-06-27 01:13:41,743 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,746 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-27 01:13:41,746 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,746 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-27 01:13:41,746 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,748 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28ms
2014-06-27 01:13:41,748 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,748 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25ms
2014-06-27 01:13:41,748 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,750 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-27 01:13:41,750 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,750 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30ms
2014-06-27 01:13:41,750 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,754 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34ms
2014-06-27 01:13:41,754 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,754 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 37ms
2014-06-27 01:13:41,754 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,754 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34ms
2014-06-27 01:13:41,754 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,758 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 42ms
2014-06-27 01:13:41,758 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,759 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 43ms
2014-06-27 01:13:41,759 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,759 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 158ms
2014-06-27 01:13:41,759 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,759 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 217ms
2014-06-27 01:13:41,759 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,759 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 229ms
2014-06-27 01:13:41,759 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,763 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 240ms
2014-06-27 01:13:41,763 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,763 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 243ms
2014-06-27 01:13:41,763 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,764 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3117ms
2014-06-27 01:13:41,764 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,764 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3185ms
2014-06-27 01:13:41,764 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,764 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3187ms
2014-06-27 01:13:41,764 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,765 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3188ms
2014-06-27 01:13:41,765 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,774 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3201ms
2014-06-27 01:13:41,774 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,774 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3199ms
2014-06-27 01:13:41,774 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,774 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3201ms
2014-06-27 01:13:41,774 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,775 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3203ms
2014-06-27 01:13:41,775 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:41,821 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801314,"queuetimems":1,"class":"HRegionServer","responsesize":17550,"method":"Multi"}
2014-06-27 01:13:41,821 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856798986,"queuetimems":1,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-06-27 01:13:41,822 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 358 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:41,824 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,824 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 348 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:41,831 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:41,831 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:44,473 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1955ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2361ms
2014-06-27 01:13:46,632 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1658ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1946ms
2014-06-27 01:13:46,635 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:13:46,826 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=109, memsize=38.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/5b7d81ede4874514a9afbddef2451d6b
2014-06-27 01:13:46,862 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:46,863 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/5b7d81ede4874514a9afbddef2451d6b as hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/5b7d81ede4874514a9afbddef2451d6b
2014-06-27 01:13:49,197 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2065ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2345ms
2014-06-27 01:13:49,310 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/5b7d81ede4874514a9afbddef2451d6b, entries=141480, sequenceid=109, filesize=21.1m
2014-06-27 01:13:49,318 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~38.9m/40745440, currentsize=7.9m/8259840 for region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. in 7576ms, sequenceid=109, compaction requested=false
2014-06-27 01:13:49,318 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8., flushing=true, writesEnabled=true
2014-06-27 01:13:49,328 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=84, memsize=50.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/b451f4bf6567478798592429c39b6a5a
2014-06-27 01:13:49,342 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 571 synced till here 542
2014-06-27 01:13:49,345 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/b451f4bf6567478798592429c39b6a5a as hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b451f4bf6567478798592429c39b6a5a
2014-06-27 01:13:49,358 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b451f4bf6567478798592429c39b6a5a, entries=185440, sequenceid=84, filesize=27.6m
2014-06-27 01:13:49,359 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~126.3m/132433680, currentsize=38.6m/40463040 for region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. in 10826ms, sequenceid=84, compaction requested=false
2014-06-27 01:13:49,365 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799086,"queuetimems":1,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:13:49,365 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 343 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,365 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,369 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801293,"queuetimems":0,"class":"HRegionServer","responsesize":16872,"method":"Multi"}
2014-06-27 01:13:49,370 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 360 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,370 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,392 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30363,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799029,"queuetimems":0,"class":"HRegionServer","responsesize":16789,"method":"Multi"}
2014-06-27 01:13:49,392 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 346 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,393 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,447 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27839,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801608,"queuetimems":175,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-06-27 01:13:49,447 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30400,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799047,"queuetimems":1,"class":"HRegionServer","responsesize":17128,"method":"Multi"}
2014-06-27 01:13:49,447 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28124,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801323,"queuetimems":0,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 352 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 356 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 345 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,448 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,453 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856810874,"queuetimems":9393,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-06-27 01:13:49,453 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801605,"queuetimems":200,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:49,453 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 349 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856798959,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 353 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 340 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,454 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856818326 with entries=110, filesize=77.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856826863
2014-06-27 01:13:49,498 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30635,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856798863,"queuetimems":2,"class":"HRegionServer","responsesize":17342,"method":"Multi"}
2014-06-27 01:13:49,498 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30388,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799110,"queuetimems":1,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-27 01:13:49,498 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801608,"queuetimems":164,"class":"HRegionServer","responsesize":17145,"method":"Multi"}
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 341 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 351 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,498 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30491,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799007,"queuetimems":1,"class":"HRegionServer","responsesize":16837,"method":"Multi"}
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856798183,"queuetimems":2,"class":"HRegionServer","responsesize":16535,"method":"Multi"}
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 342 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801272,"queuetimems":1,"class":"HRegionServer","responsesize":17007,"method":"Multi"}
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 347 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,500 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,500 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 362 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,500 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28126,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801372,"queuetimems":1,"class":"HRegionServer","responsesize":17207,"method":"Multi"}
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,500 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,499 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801304,"queuetimems":1,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-06-27 01:13:49,500 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 338 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,501 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,501 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799064,"queuetimems":1,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-06-27 01:13:49,501 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 354 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,501 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,501 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 359 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,502 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:49,502 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 344 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:49,502 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,249 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2051ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2523ms
2014-06-27 01:13:52,327 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856798780,"queuetimems":2,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-06-27 01:13:52,327 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 339 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,327 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,356 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24245,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856808110,"queuetimems":6652,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-06-27 01:13:52,356 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799158,"queuetimems":1,"class":"HRegionServer","responsesize":17125,"method":"Multi"}
2014-06-27 01:13:52,356 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801342,"queuetimems":0,"class":"HRegionServer","responsesize":16653,"method":"Multi"}
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 350 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 355 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 363 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,357 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,362 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13996,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856818366,"queuetimems":1,"class":"HRegionServer","responsesize":16649,"method":"Multi"}
2014-06-27 01:13:52,363 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856818356,"queuetimems":5,"class":"HRegionServer","responsesize":16865,"method":"Multi"}
2014-06-27 01:13:52,363 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 408 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,363 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,365 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 409 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,365 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14056,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856818309,"queuetimems":1,"class":"HRegionServer","responsesize":17393,"method":"Multi"}
2014-06-27 01:13:52,365 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856801283,"queuetimems":1,"class":"HRegionServer","responsesize":17175,"method":"Multi"}
2014-06-27 01:13:52,365 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856799134,"queuetimems":1,"class":"HRegionServer","responsesize":17538,"method":"Multi"}
2014-06-27 01:13:52,370 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 361 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,370 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,370 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 364 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,370 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,366 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,367 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 407 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,379 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,426 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21524,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56371","starttimems":1403856810899,"queuetimems":5816,"class":"HRegionServer","responsesize":17393,"method":"Multi"}
2014-06-27 01:13:52,427 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 368 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56371: output error
2014-06-27 01:13:52,427 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856821518,"queuetimems":0,"class":"HRegionServer","responsesize":16193,"method":"Multi"}
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10951,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856821528,"queuetimems":0,"class":"HRegionServer","responsesize":17141,"method":"Multi"}
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 410 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 413 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,480 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:52,481 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10941,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56387","starttimems":1403856821540,"queuetimems":1,"class":"HRegionServer","responsesize":16890,"method":"Multi"}
2014-06-27 01:13:52,481 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 415 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56387: output error
2014-06-27 01:13:52,481 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,274 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1524ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1719ms
2014-06-27 01:13:54,430 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:54,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 668 synced till here 655
2014-06-27 01:13:54,509 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824484,"queuetimems":0,"class":"HRegionServer","responsesize":17018,"method":"Multi"}
2014-06-27 01:13:54,509 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 421 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,509 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,524 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 424 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,524 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856826863 with entries=97, filesize=72.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856834431
2014-06-27 01:13:54,679 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10043,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824635,"queuetimems":0,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:13:54,679 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824504,"queuetimems":1,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:13:54,680 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:54,680 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 425 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,680 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,680 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. due to global heap pressure
2014-06-27 01:13:54,680 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 444 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,680 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,680 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4., current region memstore size 104.1m
2014-06-27 01:13:54,688 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 437 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,689 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,689 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 440 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,689 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,689 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 435 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,689 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,693 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.5m is >= than blocking 386.7m size
2014-06-27 01:13:54,693 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.5m is >= than blocking 386.7m size
2014-06-27 01:13:54,694 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.5m is >= than blocking 386.7m size
2014-06-27 01:13:54,694 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,695 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,696 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,696 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.6m is >= than blocking 386.7m size
2014-06-27 01:13:54,713 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 441 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,713 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,713 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 438 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,713 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,713 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 439 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,714 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,714 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 432 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,714 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,714 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 442 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,714 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10106,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824608,"queuetimems":0,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10132,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824582,"queuetimems":0,"class":"HRegionServer","responsesize":17342,"method":"Multi"}
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 445 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 422 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:54,715 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:54,784 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:55,139 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 401.4m is >= than blocking 386.7m size
2014-06-27 01:13:55,173 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 401.4m is >= than blocking 386.7m size
2014-06-27 01:13:55,572 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=144, memsize=51.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/188d516eca9b42cf976a679b6deed3c3
2014-06-27 01:13:55,585 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/188d516eca9b42cf976a679b6deed3c3 as hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/188d516eca9b42cf976a679b6deed3c3
2014-06-27 01:13:55,595 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/188d516eca9b42cf976a679b6deed3c3, entries=186900, sequenceid=144, filesize=27.9m
2014-06-27 01:13:55,595 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~104.1m/109122080, currentsize=0.0/0 for region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. in 915ms, sequenceid=144, compaction requested=false
2014-06-27 01:13:55,596 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 423ms
2014-06-27 01:13:55,596 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,596 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 457ms
2014-06-27 01:13:55,596 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,596 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-27 01:13:55,596 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,596 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-27 01:13:55,596 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,596 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-27 01:13:55,597 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,597 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 903ms
2014-06-27 01:13:55,597 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,597 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-27 01:13:55,597 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,597 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-27 01:13:55,597 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,610 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 917ms
2014-06-27 01:13:55,610 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,610 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 917ms
2014-06-27 01:13:55,610 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,613 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 919ms
2014-06-27 01:13:55,613 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,613 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 919ms
2014-06-27 01:13:55,613 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,613 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 920ms
2014-06-27 01:13:55,613 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,614 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 921ms
2014-06-27 01:13:55,614 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:13:55,627 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 431 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,628 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,631 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824659,"queuetimems":0,"class":"HRegionServer","responsesize":17143,"method":"Multi"}
2014-06-27 01:13:55,631 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 436 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 443 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 429 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 433 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,632 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,639 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 428 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,639 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,643 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11090,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56398","starttimems":1403856824553,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:13:55,643 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 434 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,643 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,643 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 430 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,644 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:55,644 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 423 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56398: output error
2014-06-27 01:13:55,644 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:13:56,912 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:13:56,913 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. due to global heap pressure
2014-06-27 01:13:56,913 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0., current region memstore size 108.4m
2014-06-27 01:13:57,058 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:13:57,107 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:13:57,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 765 synced till here 764
2014-06-27 01:13:57,145 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856834431 with entries=97, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856837107
2014-06-27 01:13:57,417 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:13:57,417 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:13:57,430 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.7m is >= than blocking 386.7m size
2014-06-27 01:13:57,431 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 392.8m is >= than blocking 386.7m size
2014-06-27 01:13:57,433 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 392.8m is >= than blocking 386.7m size
2014-06-27 01:13:57,444 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.4m is >= than blocking 386.7m size
2014-06-27 01:13:57,455 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,461 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,493 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,515 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,538 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,563 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:57,587 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,715 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1853ms
GC pool 'ParNew' had collection(s): count=1 time=687ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1424ms
2014-06-27 01:13:59,730 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,741 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,751 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,761 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,771 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,788 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,805 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,831 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,857 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,892 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,924 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,952 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,976 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:13:59,999 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:14:00,026 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:14:00,052 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:14:00,073 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 395.9m is >= than blocking 386.7m size
2014-06-27 01:14:00,083 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=160, memsize=63.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/b5b343cd860b45f3bb858f56aa14843a
2014-06-27 01:14:00,096 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/b5b343cd860b45f3bb858f56aa14843a as hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/b5b343cd860b45f3bb858f56aa14843a
2014-06-27 01:14:00,107 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/b5b343cd860b45f3bb858f56aa14843a, entries=231730, sequenceid=160, filesize=34.5m
2014-06-27 01:14:00,107 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~108.4m/113665760, currentsize=12.4m/13028720 for region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. in 3194ms, sequenceid=160, compaction requested=false
2014-06-27 01:14:00,108 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 35ms
2014-06-27 01:14:00,108 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,108 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 56ms
2014-06-27 01:14:00,108 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,108 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 82ms
2014-06-27 01:14:00,108 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,108 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 109ms
2014-06-27 01:14:00,108 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,108 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 132ms
2014-06-27 01:14:00,109 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,109 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 157ms
2014-06-27 01:14:00,109 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,110 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 186ms
2014-06-27 01:14:00,110 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,113 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 221ms
2014-06-27 01:14:00,113 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,113 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 256ms
2014-06-27 01:14:00,113 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,114 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 283ms
2014-06-27 01:14:00,114 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,114 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 309ms
2014-06-27 01:14:00,114 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,114 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 326ms
2014-06-27 01:14:00,114 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,114 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 343ms
2014-06-27 01:14:00,115 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,117 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 356ms
2014-06-27 01:14:00,118 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,118 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 367ms
2014-06-27 01:14:00,118 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,118 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 377ms
2014-06-27 01:14:00,118 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,118 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 388ms
2014-06-27 01:14:00,118 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,586 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2531ms
2014-06-27 01:14:00,586 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,587 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3024ms
2014-06-27 01:14:00,587 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,588 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3050ms
2014-06-27 01:14:00,588 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,588 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3073ms
2014-06-27 01:14:00,588 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,590 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3097ms
2014-06-27 01:14:00,590 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,593 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3129ms
2014-06-27 01:14:00,593 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,594 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3139ms
2014-06-27 01:14:00,594 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,594 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3150ms
2014-06-27 01:14:00,594 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,594 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3161ms
2014-06-27 01:14:00,594 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,594 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3163ms
2014-06-27 01:14:00,595 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,595 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3166ms
2014-06-27 01:14:00,596 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,601 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3184ms
2014-06-27 01:14:00,601 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:00,601 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3184ms
2014-06-27 01:14:00,601 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:02,904 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1838ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1958ms
2014-06-27 01:14:03,692 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:03,699 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:03,700 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. due to global heap pressure
2014-06-27 01:14:03,700 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8., current region memstore size 120.5m
2014-06-27 01:14:05,826 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1657ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2118ms
2014-06-27 01:14:05,828 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,829 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,833 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,833 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,833 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,834 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,835 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,835 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,835 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,835 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,836 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,837 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.1m is >= than blocking 386.7m size
2014-06-27 01:14:05,843 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 403.3m is >= than blocking 386.7m size
2014-06-27 01:14:05,843 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 403.3m is >= than blocking 386.7m size
2014-06-27 01:14:05,844 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 403.3m is >= than blocking 386.7m size
2014-06-27 01:14:05,844 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 406.4m is >= than blocking 386.7m size
2014-06-27 01:14:05,845 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 406.4m is >= than blocking 386.7m size
2014-06-27 01:14:05,847 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 406.4m is >= than blocking 386.7m size
2014-06-27 01:14:05,847 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 406.4m is >= than blocking 386.7m size
2014-06-27 01:14:05,852 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 868 synced till here 856
2014-06-27 01:14:05,858 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 406.4m is >= than blocking 386.7m size
2014-06-27 01:14:05,869 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,870 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,870 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,870 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,872 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,882 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,894 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:05,895 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:06,060 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856837107 with entries=103, filesize=82.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856843692
2014-06-27 01:14:06,063 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:06,093 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-27 01:14:06,093 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-27 01:14:06,094 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 424.7m is >= than blocking 386.7m size
2014-06-27 01:14:06,096 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 424.7m is >= than blocking 386.7m size
2014-06-27 01:14:06,096 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 424.7m is >= than blocking 386.7m size
2014-06-27 01:14:06,104 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 427.9m is >= than blocking 386.7m size
2014-06-27 01:14:06,106 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 427.9m is >= than blocking 386.7m size
2014-06-27 01:14:06,169 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:09,192 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1780ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2182ms
2014-06-27 01:14:09,204 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 427.9m is >= than blocking 386.7m size
2014-06-27 01:14:09,214 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 427.9m is >= than blocking 386.7m size
2014-06-27 01:14:09,235 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=165, memsize=69.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/b0c72a537332497f93666ac60f78029c
2014-06-27 01:14:09,250 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/b0c72a537332497f93666ac60f78029c as hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b0c72a537332497f93666ac60f78029c
2014-06-27 01:14:09,262 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b0c72a537332497f93666ac60f78029c, entries=253480, sequenceid=165, filesize=37.8m
2014-06-27 01:14:09,263 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~123.6m/129588480, currentsize=1.6m/1719280 for region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. in 5563ms, sequenceid=165, compaction requested=false
2014-06-27 01:14:09,263 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 49ms
2014-06-27 01:14:09,263 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,266 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 62ms
2014-06-27 01:14:09,266 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,269 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3163ms
2014-06-27 01:14:09,270 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,270 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3166ms
2014-06-27 01:14:09,270 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,271 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3174ms
2014-06-27 01:14:09,271 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,271 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3175ms
2014-06-27 01:14:09,271 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,271 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3177ms
2014-06-27 01:14:09,271 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,271 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3178ms
2014-06-27 01:14:09,271 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,273 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3181ms
2014-06-27 01:14:09,273 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,273 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3210ms
2014-06-27 01:14:09,273 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,274 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3379ms
2014-06-27 01:14:09,274 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,275 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3380ms
2014-06-27 01:14:09,275 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,275 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3393ms
2014-06-27 01:14:09,275 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,276 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3404ms
2014-06-27 01:14:09,276 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,276 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3406ms
2014-06-27 01:14:09,276 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,296 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3427ms
2014-06-27 01:14:09,296 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,297 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3427ms
2014-06-27 01:14:09,297 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,302 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3433ms
2014-06-27 01:14:09,302 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,304 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3446ms
2014-06-27 01:14:09,304 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,304 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3457ms
2014-06-27 01:14:09,304 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,305 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3457ms
2014-06-27 01:14:09,305 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,306 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3460ms
2014-06-27 01:14:09,306 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,306 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3462ms
2014-06-27 01:14:09,306 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,315 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3471ms
2014-06-27 01:14:09,315 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,321 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3478ms
2014-06-27 01:14:09,321 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,323 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3480ms
2014-06-27 01:14:09,323 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,324 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3486ms
2014-06-27 01:14:09,324 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,324 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3491ms
2014-06-27 01:14:09,324 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,326 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3495ms
2014-06-27 01:14:09,326 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,327 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3491ms
2014-06-27 01:14:09,327 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,328 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3496ms
2014-06-27 01:14:09,328 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,330 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3495ms
2014-06-27 01:14:09,330 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,330 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3498ms
2014-06-27 01:14:09,330 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,330 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3497ms
2014-06-27 01:14:09,331 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,331 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3498ms
2014-06-27 01:14:09,331 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,334 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3504ms
2014-06-27 01:14:09,334 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,335 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3506ms
2014-06-27 01:14:09,335 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:09,337 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3509ms
2014-06-27 01:14:09,337 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:12,367 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2175ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2552ms
2014-06-27 01:14:12,459 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:14:12,460 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461., current region memstore size 129.1m
2014-06-27 01:14:12,461 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:12,461 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. due to global heap pressure
2014-06-27 01:14:12,462 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4., current region memstore size 87.1m
2014-06-27 01:14:12,580 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 386.8m is >= than blocking 386.7m size
2014-06-27 01:14:12,583 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 388.4m is >= than blocking 386.7m size
2014-06-27 01:14:12,586 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-27 01:14:12,593 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.8m is >= than blocking 386.7m size
2014-06-27 01:14:12,593 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.8m is >= than blocking 386.7m size
2014-06-27 01:14:12,593 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 394.8m is >= than blocking 386.7m size
2014-06-27 01:14:12,651 WARN  [RpcServer.reader=0,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-06-27 01:14:12,821 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 397.7m is >= than blocking 386.7m size
2014-06-27 01:14:12,821 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 524 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:12,823 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:12,850 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 526 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:12,850 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:12,861 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:15,038 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1670ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2152ms
2014-06-27 01:14:15,052 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:15,054 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 402.9m is >= than blocking 386.7m size
2014-06-27 01:14:15,055 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 402.9m is >= than blocking 386.7m size
2014-06-27 01:14:15,056 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 402.9m is >= than blocking 386.7m size
2014-06-27 01:14:15,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 969 synced till here 949
2014-06-27 01:14:15,226 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:15,255 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 410.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,256 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 410.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,256 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.1m is >= than blocking 386.7m size
2014-06-27 01:14:15,256 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 412.1m is >= than blocking 386.7m size
2014-06-27 01:14:15,257 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 416.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,257 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 416.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,257 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 416.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,258 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-27 01:14:15,259 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 426.1m is >= than blocking 386.7m size
2014-06-27 01:14:15,259 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 429.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,260 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 429.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,260 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 429.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,260 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-27 01:14:15,261 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-27 01:14:15,261 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-27 01:14:15,261 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-27 01:14:15,262 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 432.3m is >= than blocking 386.7m size
2014-06-27 01:14:15,262 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 435.4m is >= than blocking 386.7m size
2014-06-27 01:14:15,263 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,263 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,263 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,263 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,263 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,264 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.2m is >= than blocking 386.7m size
2014-06-27 01:14:15,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856843692 with entries=101, filesize=80.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856855053
2014-06-27 01:14:15,310 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 441.7m is >= than blocking 386.7m size
2014-06-27 01:14:15,655 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=203, memsize=71.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/6e115906702242aaac966f2ae382cfbc
2014-06-27 01:14:15,667 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/6e115906702242aaac966f2ae382cfbc as hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/6e115906702242aaac966f2ae382cfbc
2014-06-27 01:14:15,678 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/6e115906702242aaac966f2ae382cfbc, entries=260400, sequenceid=203, filesize=38.8m
2014-06-27 01:14:15,678 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~90.2m/94596000, currentsize=1.6m/1678960 for region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. in 3217ms, sequenceid=203, compaction requested=true
2014-06-27 01:14:15,680 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-27 01:14:15,681 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 371ms
2014-06-27 01:14:15,681 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,681 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-27 01:14:15,681 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 418ms
2014-06-27 01:14:15,681 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,682 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 419ms
2014-06-27 01:14:15,682 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,683 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 99131774 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-27 01:14:15,683 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 420ms
2014-06-27 01:14:15,683 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,683 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 420ms
2014-06-27 01:14:15,684 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,684 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 421ms
2014-06-27 01:14:15,684 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,684 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 421ms
2014-06-27 01:14:15,684 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,684 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 423ms
2014-06-27 01:14:15,684 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,686 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 427ms
2014-06-27 01:14:15,686 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,686 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 426ms
2014-06-27 01:14:15,686 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,686 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 425ms
2014-06-27 01:14:15,686 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,687 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 427ms
2014-06-27 01:14:15,687 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,694 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 434ms
2014-06-27 01:14:15,694 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,694 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 437ms
2014-06-27 01:14:15,694 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,695 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 435ms
2014-06-27 01:14:15,695 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,695 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 436ms
2014-06-27 01:14:15,695 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,697 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 437ms
2014-06-27 01:14:15,697 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,697 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 439ms
2014-06-27 01:14:15,697 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,698 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 441ms
2014-06-27 01:14:15,698 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,698 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 441ms
2014-06-27 01:14:15,698 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,698 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 441ms
2014-06-27 01:14:15,699 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,699 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 443ms
2014-06-27 01:14:15,699 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,699 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 443ms
2014-06-27 01:14:15,699 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,701 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 446ms
2014-06-27 01:14:15,701 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,701 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 446ms
2014-06-27 01:14:15,701 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,701 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 645ms
2014-06-27 01:14:15,701 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,701 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 646ms
2014-06-27 01:14:15,701 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,701 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 647ms
2014-06-27 01:14:15,702 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,702 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2881ms
2014-06-27 01:14:15,702 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,702 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3109ms
2014-06-27 01:14:15,702 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,702 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3109ms
2014-06-27 01:14:15,703 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,705 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3112ms
2014-06-27 01:14:15,706 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,707 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856843676,"queuetimems":1,"class":"HRegionServer","responsesize":17053,"method":"Multi"}
2014-06-27 01:14:15,707 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 533 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:15,707 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:15,710 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3124ms
2014-06-27 01:14:15,710 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,710 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3127ms
2014-06-27 01:14:15,710 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,714 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: 63f84494fd2ae9b2e453f741a89285b4 - family: Initiating major compaction
2014-06-27 01:14:15,715 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HRegion: Starting compaction on family in region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:14:15,715 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp, totalSize=94.5m
2014-06-27 01:14:15,716 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3132ms
2014-06-27 01:14:15,730 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:15,764 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/b4c7e0cbab9843c4915d6486bda81f8c, keycount=18690, bloomtype=ROW, size=27.9m, encoding=NONE, seqNum=76, earliestPutTs=1403856778620
2014-06-27 01:14:15,764 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/188d516eca9b42cf976a679b6deed3c3, keycount=18690, bloomtype=ROW, size=27.9m, encoding=NONE, seqNum=144, earliestPutTs=1403856811180
2014-06-27 01:14:15,764 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/6e115906702242aaac966f2ae382cfbc, keycount=26040, bloomtype=ROW, size=38.8m, encoding=NONE, seqNum=203, earliestPutTs=1403856835662
2014-06-27 01:14:15,826 DEBUG [regionserver60020-smallCompactions-1403856855680] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:17,976 DEBUG [LruStats #0] hfile.LruBlockCache: Total=408.52 KB, free=386.28 MB, max=386.68 MB, blocks=2, accesses=16, hits=3, hitRatio=18.75%, , cachingAccesses=5, cachingHits=3, cachingHitsRatio=60.00%, evictions=0, evicted=0, evictedPerRun=NaN
2014-06-27 01:14:17,978 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1939ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2037ms
2014-06-27 01:14:17,995 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18165,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839829,"queuetimems":1,"class":"HRegionServer","responsesize":16724,"method":"Multi"}
2014-06-27 01:14:17,995 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837459,"queuetimems":0,"class":"HRegionServer","responsesize":17538,"method":"Multi"}
2014-06-27 01:14:17,995 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856843684,"queuetimems":0,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-27 01:14:17,995 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 495 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 487 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 535 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:17,996 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18258,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839738,"queuetimems":0,"class":"HRegionServer","responsesize":17207,"method":"Multi"}
2014-06-27 01:14:17,997 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 502 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:17,997 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15068,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856842965,"queuetimems":1,"class":"HRegionServer","responsesize":16905,"method":"Multi"}
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 529 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839889,"queuetimems":0,"class":"HRegionServer","responsesize":16837,"method":"Multi"}
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 493 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837429,"queuetimems":1,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:14:18,034 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18179,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839855,"queuetimems":1,"class":"HRegionServer","responsesize":17097,"method":"Multi"}
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 488 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18256,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839779,"queuetimems":1,"class":"HRegionServer","responsesize":17213,"method":"Multi"}
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 494 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 498 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839749,"queuetimems":0,"class":"HRegionServer","responsesize":16770,"method":"Multi"}
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18061,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839975,"queuetimems":1,"class":"HRegionServer","responsesize":17550,"method":"Multi"}
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 501 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20521,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837514,"queuetimems":1,"class":"HRegionServer","responsesize":16984,"method":"Multi"}
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 517 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,037 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,037 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 508 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,037 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,035 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18308,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839728,"queuetimems":1,"class":"HRegionServer","responsesize":17145,"method":"Multi"}
2014-06-27 01:14:18,036 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837537,"queuetimems":1,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-06-27 01:14:18,038 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 503 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,038 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,038 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 506 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,038 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20567,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837561,"queuetimems":0,"class":"HRegionServer","responsesize":17076,"method":"Multi"}
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20637,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837491,"queuetimems":0,"class":"HRegionServer","responsesize":17053,"method":"Multi"}
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 505 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,130 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,130 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 509 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,130 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856843181,"queuetimems":1,"class":"HRegionServer","responsesize":16984,"method":"Multi"}
2014-06-27 01:14:18,130 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 534 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,130 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,131 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:18,131 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958. due to global heap pressure
2014-06-27 01:14:18,131 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958., current region memstore size 89.6m
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18324,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839804,"queuetimems":1,"class":"HRegionServer","responsesize":17125,"method":"Multi"}
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18359,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856839769,"queuetimems":0,"class":"HRegionServer","responsesize":16577,"method":"Multi"}
2014-06-27 01:14:18,132 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 497 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,129 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20544,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856837585,"queuetimems":0,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-27 01:14:18,135 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,236 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 499 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,236 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,239 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 504 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,239 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,274 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856845857,"queuetimems":0,"class":"HRegionServer","responsesize":16905,"method":"Multi"}
2014-06-27 01:14:18,274 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 539 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,274 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,276 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 391.4m is >= than blocking 386.7m size
2014-06-27 01:14:18,276 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 391.4m is >= than blocking 386.7m size
2014-06-27 01:14:18,277 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 391.4m is >= than blocking 386.7m size
2014-06-27 01:14:18,277 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 391.4m is >= than blocking 386.7m size
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 548 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12225,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856846060,"queuetimems":1,"class":"HRegionServer","responsesize":17053,"method":"Multi"}
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12415,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856845870,"queuetimems":1,"class":"HRegionServer","responsesize":16535,"method":"Multi"}
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 552 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,286 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 540 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856846089,"queuetimems":1,"class":"HRegionServer","responsesize":16984,"method":"Multi"}
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856845879,"queuetimems":0,"class":"HRegionServer","responsesize":17141,"method":"Multi"}
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 549 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 544 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,287 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 541 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,288 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,288 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 545 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:18,288 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:18,353 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 402.6m is >= than blocking 386.7m size
2014-06-27 01:14:18,355 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 404.1m is >= than blocking 386.7m size
2014-06-27 01:14:18,355 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 404.1m is >= than blocking 386.7m size
2014-06-27 01:14:18,355 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 404.1m is >= than blocking 386.7m size
2014-06-27 01:14:18,356 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 404.1m is >= than blocking 386.7m size
2014-06-27 01:14:18,357 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 404.1m is >= than blocking 386.7m size
2014-06-27 01:14:18,385 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:19,850 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1371ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1511ms
2014-06-27 01:14:19,893 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=198, memsize=73.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/d947626fc7d64e38a45dc7a0d28fa433
2014-06-27 01:14:19,910 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/d947626fc7d64e38a45dc7a0d28fa433 as hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/d947626fc7d64e38a45dc7a0d28fa433
2014-06-27 01:14:19,920 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/d947626fc7d64e38a45dc7a0d28fa433, entries=266160, sequenceid=198, filesize=39.7m
2014-06-27 01:14:19,920 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~135.2m/141774480, currentsize=18.7m/19580800 for region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. in 7460ms, sequenceid=198, compaction requested=true
2014-06-27 01:14:19,921 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-27 01:14:19,921 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1564ms
2014-06-27 01:14:19,921 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,921 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1565ms
2014-06-27 01:14:19,921 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,921 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1566ms
2014-06-27 01:14:19,921 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,922 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1566ms
2014-06-27 01:14:19,922 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,922 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1568ms
2014-06-27 01:14:19,922 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,922 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1570ms
2014-06-27 01:14:19,922 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,922 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1645ms
2014-06-27 01:14:19,922 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,923 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1647ms
2014-06-27 01:14:19,923 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,929 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1653ms
2014-06-27 01:14:19,929 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,930 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1653ms
2014-06-27 01:14:19,930 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:19,935 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14092,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856845843,"queuetimems":1,"class":"HRegionServer","responsesize":16975,"method":"Multi"}
2014-06-27 01:14:19,935 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 538 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:19,936 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,057 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 565 service: ClientService methodName: Multi size: 3.1m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:20,058 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,058 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 564 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:20,058 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,058 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 563 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:20,058 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,059 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10293,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56411","starttimems":1403856849766,"queuetimems":2,"class":"HRegionServer","responsesize":16685,"method":"Multi"}
2014-06-27 01:14:20,059 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 562 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:20,060 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,060 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 566 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56411: output error
2014-06-27 01:14:20,060 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:20,147 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=193, memsize=22.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/af61ffa3abad280ddfdb5f6300c06958/.tmp/9ab0cd54eb6b4ca19415e158c7505571
2014-06-27 01:14:20,160 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/af61ffa3abad280ddfdb5f6300c06958/.tmp/9ab0cd54eb6b4ca19415e158c7505571 as hdfs://master:54310/hbase/data/default/usertable/af61ffa3abad280ddfdb5f6300c06958/family/9ab0cd54eb6b4ca19415e158c7505571
2014-06-27 01:14:20,171 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/af61ffa3abad280ddfdb5f6300c06958/family/9ab0cd54eb6b4ca19415e158c7505571, entries=80470, sequenceid=193, filesize=12.0m
2014-06-27 01:14:20,171 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~90.1m/94433600, currentsize=8.0m/8437200 for region usertable,user9,1403856759966.af61ffa3abad280ddfdb5f6300c06958. in 2040ms, sequenceid=193, compaction requested=false
2014-06-27 01:14:21,445 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/.tmp/3c1a630688394f0c8fb58d68660c7670 as hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/3c1a630688394f0c8fb58d68660c7670
2014-06-27 01:14:21,463 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Removing store files after compaction...
2014-06-27 01:14:21,474 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/b4c7e0cbab9843c4915d6486bda81f8c, to hdfs://master:54310/hbase/archive/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/b4c7e0cbab9843c4915d6486bda81f8c
2014-06-27 01:14:21,477 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/188d516eca9b42cf976a679b6deed3c3, to hdfs://master:54310/hbase/archive/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/188d516eca9b42cf976a679b6deed3c3
2014-06-27 01:14:21,479 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/6e115906702242aaac966f2ae382cfbc, to hdfs://master:54310/hbase/archive/data/default/usertable/63f84494fd2ae9b2e453f741a89285b4/family/6e115906702242aaac966f2ae382cfbc
2014-06-27 01:14:21,479 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. into 3c1a630688394f0c8fb58d68660c7670(size=39.4m), total size for store is 39.4m. This selection was in queue for 0sec, and took 5sec to execute.
2014-06-27 01:14:21,480 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4., storeName=family, fileCount=3, fileSize=94.5m, priority=7, time=2611340068223703; duration=5sec
2014-06-27 01:14:21,481 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-27 01:14:21,481 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-27 01:14:21,481 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 92895271 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-27 01:14:21,481 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: 71d4fc84a5c8f58ae058bda4626c3461 - family: Initiating major compaction
2014-06-27 01:14:21,482 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HRegion: Starting compaction on family in region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461.
2014-06-27 01:14:21,482 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp, totalSize=88.6m
2014-06-27 01:14:21,482 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/fb9530e069564fc0b5068c2a5a6735ef, keycount=18661, bloomtype=ROW, size=27.8m, encoding=NONE, seqNum=83, earliestPutTs=1403856779893
2014-06-27 01:14:21,482 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/5b7d81ede4874514a9afbddef2451d6b, keycount=14148, bloomtype=ROW, size=21.1m, encoding=NONE, seqNum=109, earliestPutTs=1403856811078
2014-06-27 01:14:21,482 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/d947626fc7d64e38a45dc7a0d28fa433, keycount=26616, bloomtype=ROW, size=39.7m, encoding=NONE, seqNum=198, earliestPutTs=1403856834278
2014-06-27 01:14:21,492 DEBUG [regionserver60020-smallCompactions-1403856855680] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:23,277 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/f74891fd46c04bdcbfbd1678f7cffae0 as hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/f74891fd46c04bdcbfbd1678f7cffae0
2014-06-27 01:14:23,293 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Removing store files after compaction...
2014-06-27 01:14:23,303 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/fb9530e069564fc0b5068c2a5a6735ef, to hdfs://master:54310/hbase/archive/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/fb9530e069564fc0b5068c2a5a6735ef
2014-06-27 01:14:23,305 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/5b7d81ede4874514a9afbddef2451d6b, to hdfs://master:54310/hbase/archive/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/5b7d81ede4874514a9afbddef2451d6b
2014-06-27 01:14:23,307 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/d947626fc7d64e38a45dc7a0d28fa433, to hdfs://master:54310/hbase/archive/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/d947626fc7d64e38a45dc7a0d28fa433
2014-06-27 01:14:23,307 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. into f74891fd46c04bdcbfbd1678f7cffae0(size=39.4m), total size for store is 39.4m. This selection was in queue for 0sec, and took 1sec to execute.
2014-06-27 01:14:23,308 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461., storeName=family, fileCount=3, fileSize=88.6m, priority=7, time=2611345835557102; duration=1sec
2014-06-27 01:14:23,308 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-27 01:14:23,333 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:23,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856855053 with entries=100, filesize=60.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856863333
2014-06-27 01:14:23,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856277513
2014-06-27 01:14:23,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856780472
2014-06-27 01:14:23,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856789935
2014-06-27 01:14:23,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856795555
2014-06-27 01:14:23,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856818326
2014-06-27 01:14:23,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856826863
2014-06-27 01:14:24,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:24,918 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1156 synced till here 1155
2014-06-27 01:14:24,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856863333 with entries=87, filesize=62.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856864729
2014-06-27 01:14:25,031 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:14:25,032 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0., current region memstore size 128.7m
2014-06-27 01:14:25,144 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:25,144 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. due to global heap pressure
2014-06-27 01:14:25,145 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8., current region memstore size 105.7m
2014-06-27 01:14:25,173 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:25,271 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:25,386 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 388.2m is >= than blocking 386.7m size
2014-06-27 01:14:25,393 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-27 01:14:25,846 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-27 01:14:26,071 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=244, memsize=82.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/33dc164bee28464e93021e1443666dc5
2014-06-27 01:14:26,083 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/33dc164bee28464e93021e1443666dc5 as hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/33dc164bee28464e93021e1443666dc5
2014-06-27 01:14:26,084 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=236, memsize=68.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/9f849a43d49148dbbc161379fe7c0306
2014-06-27 01:14:26,097 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/33dc164bee28464e93021e1443666dc5, entries=298840, sequenceid=244, filesize=44.6m
2014-06-27 01:14:26,098 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~128.7m/134995600, currentsize=4.7m/4913040 for region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. in 1066ms, sequenceid=244, compaction requested=true
2014-06-27 01:14:26,098 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-27 01:14:26,098 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-27 01:14:26,098 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 252ms
2014-06-27 01:14:26,098 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 112061419 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-27 01:14:26,098 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:26,098 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: 8c24dc8da28390998e501bd2fe2885c0 - family: Initiating major compaction
2014-06-27 01:14:26,099 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 705ms
2014-06-27 01:14:26,099 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/9f849a43d49148dbbc161379fe7c0306 as hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/9f849a43d49148dbbc161379fe7c0306
2014-06-27 01:14:26,099 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:26,099 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HRegion: Starting compaction on family in region usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:14:26,099 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp, totalSize=106.9m
2014-06-27 01:14:26,099 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/0c4bbf09f4c441eaa651544b0cb6107b, keycount=18621, bloomtype=ROW, size=27.8m, encoding=NONE, seqNum=89, earliestPutTs=1403856778073
2014-06-27 01:14:26,099 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/b5b343cd860b45f3bb858f56aa14843a, keycount=23173, bloomtype=ROW, size=34.5m, encoding=NONE, seqNum=160, earliestPutTs=1403856801541
2014-06-27 01:14:26,099 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/33dc164bee28464e93021e1443666dc5, keycount=29884, bloomtype=ROW, size=44.6m, encoding=NONE, seqNum=244, earliestPutTs=1403856837025
2014-06-27 01:14:26,100 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 714ms
2014-06-27 01:14:26,101 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:26,139 DEBUG [regionserver60020-smallCompactions-1403856855680] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:26,364 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/9f849a43d49148dbbc161379fe7c0306, entries=248030, sequenceid=236, filesize=37.0m
2014-06-27 01:14:26,365 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~107.3m/112490480, currentsize=9.4m/9812000 for region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. in 1219ms, sequenceid=236, compaction requested=true
2014-06-27 01:14:26,366 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-27 01:14:27,266 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:27,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856864729 with entries=84, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856867266
2014-06-27 01:14:27,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856834431
2014-06-27 01:14:27,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856837107
2014-06-27 01:14:29,566 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/.tmp/00bc0815aa9d48b8b4704c58a2677148 as hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/00bc0815aa9d48b8b4704c58a2677148
2014-06-27 01:14:29,580 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Removing store files after compaction...
2014-06-27 01:14:29,586 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/0c4bbf09f4c441eaa651544b0cb6107b, to hdfs://master:54310/hbase/archive/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/0c4bbf09f4c441eaa651544b0cb6107b
2014-06-27 01:14:29,589 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/b5b343cd860b45f3bb858f56aa14843a, to hdfs://master:54310/hbase/archive/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/b5b343cd860b45f3bb858f56aa14843a
2014-06-27 01:14:29,591 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/33dc164bee28464e93021e1443666dc5, to hdfs://master:54310/hbase/archive/data/default/usertable/8c24dc8da28390998e501bd2fe2885c0/family/33dc164bee28464e93021e1443666dc5
2014-06-27 01:14:29,591 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0. into 00bc0815aa9d48b8b4704c58a2677148(size=53.5m), total size for store is 53.5m. This selection was in queue for 0sec, and took 3sec to execute.
2014-06-27 01:14:29,591 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0., storeName=family, fileCount=3, fileSize=106.9m, priority=7, time=2611350452585712; duration=3sec
2014-06-27 01:14:29,592 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-27 01:14:29,592 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-27 01:14:29,592 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 107369927 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-27 01:14:29,592 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: 242b1f4a8cce314fe04bd5b85c8b17d8 - family: Initiating major compaction
2014-06-27 01:14:29,592 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HRegion: Starting compaction on family in region usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8.
2014-06-27 01:14:29,593 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp, totalSize=102.4m
2014-06-27 01:14:29,593 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b451f4bf6567478798592429c39b6a5a, keycount=18544, bloomtype=ROW, size=27.6m, encoding=NONE, seqNum=84, earliestPutTs=1403856786317
2014-06-27 01:14:29,593 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b0c72a537332497f93666ac60f78029c, keycount=25348, bloomtype=ROW, size=37.8m, encoding=NONE, seqNum=165, earliestPutTs=1403856826878
2014-06-27 01:14:29,593 DEBUG [regionserver60020-smallCompactions-1403856855680] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/9f849a43d49148dbbc161379fe7c0306, keycount=24803, bloomtype=ROW, size=37.0m, encoding=NONE, seqNum=236, earliestPutTs=1403856849272
2014-06-27 01:14:29,605 DEBUG [regionserver60020-smallCompactions-1403856855680] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:30,060 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:30,092 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856867266 with entries=85, filesize=61.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856870061
2014-06-27 01:14:30,701 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:30,701 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. due to global heap pressure
2014-06-27 01:14:30,702 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461., current region memstore size 116.2m
2014-06-27 01:14:30,832 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:30,910 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.5m is >= than blocking 386.7m size
2014-06-27 01:14:30,927 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:31,976 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:31,996 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,032 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,062 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,074 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,216 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,227 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,237 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,247 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,275 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,301 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,327 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,346 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/.tmp/f9e6883200ae4499acc6f7c2437b39b5 as hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/f9e6883200ae4499acc6f7c2437b39b5
2014-06-27 01:14:32,361 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,366 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Removing store files after compaction...
2014-06-27 01:14:32,377 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b451f4bf6567478798592429c39b6a5a, to hdfs://master:54310/hbase/archive/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b451f4bf6567478798592429c39b6a5a
2014-06-27 01:14:32,381 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b0c72a537332497f93666ac60f78029c, to hdfs://master:54310/hbase/archive/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/b0c72a537332497f93666ac60f78029c
2014-06-27 01:14:32,384 DEBUG [regionserver60020-smallCompactions-1403856855680] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/9f849a43d49148dbbc161379fe7c0306, to hdfs://master:54310/hbase/archive/data/default/usertable/242b1f4a8cce314fe04bd5b85c8b17d8/family/9f849a43d49148dbbc161379fe7c0306
2014-06-27 01:14:32,384 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8. into f9e6883200ae4499acc6f7c2437b39b5(size=51.7m), total size for store is 51.7m. This selection was in queue for 0sec, and took 2sec to execute.
2014-06-27 01:14:32,385 INFO  [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1403856759966.242b1f4a8cce314fe04bd5b85c8b17d8., storeName=family, fileCount=3, fileSize=102.4m, priority=7, time=2611353946112626; duration=2sec
2014-06-27 01:14:32,385 DEBUG [regionserver60020-smallCompactions-1403856855680] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-27 01:14:32,412 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,445 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,524 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,533 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,548 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,695 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,805 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,830 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:32,861 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:33,375 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=276, memsize=102.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/f7cc1ce7053a43ee8e6e55f144dbf5bb
2014-06-27 01:14:33,376 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:33,386 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:33,388 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/.tmp/f7cc1ce7053a43ee8e6e55f144dbf5bb as hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/f7cc1ce7053a43ee8e6e55f144dbf5bb
2014-06-27 01:14:33,395 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:33,405 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:34,771 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:34,798 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:34,820 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:34,869 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.1m is >= than blocking 386.7m size
2014-06-27 01:14:34,934 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/71d4fc84a5c8f58ae058bda4626c3461/family/f7cc1ce7053a43ee8e6e55f144dbf5bb, entries=372060, sequenceid=276, filesize=55.5m
2014-06-27 01:14:34,935 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~117.8m/123555840, currentsize=3.0m/3162240 for region usertable,user6,1403856759966.71d4fc84a5c8f58ae058bda4626c3461. in 4233ms, sequenceid=276, compaction requested=false
2014-06-27 01:14:34,935 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 66ms
2014-06-27 01:14:34,935 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,935 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 115ms
2014-06-27 01:14:34,936 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,936 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 138ms
2014-06-27 01:14:34,936 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,936 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 165ms
2014-06-27 01:14:34,936 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,943 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1538ms
2014-06-27 01:14:34,943 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,943 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1548ms
2014-06-27 01:14:34,944 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,944 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1558ms
2014-06-27 01:14:34,944 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,945 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1568ms
2014-06-27 01:14:34,945 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,945 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2084ms
2014-06-27 01:14:34,946 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,946 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2116ms
2014-06-27 01:14:34,946 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,946 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2142ms
2014-06-27 01:14:34,946 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,947 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2252ms
2014-06-27 01:14:34,948 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,948 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2400ms
2014-06-27 01:14:34,948 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,948 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2415ms
2014-06-27 01:14:34,948 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,948 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2424ms
2014-06-27 01:14:34,948 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,949 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2504ms
2014-06-27 01:14:34,950 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,951 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2538ms
2014-06-27 01:14:34,951 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,952 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2592ms
2014-06-27 01:14:34,953 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,954 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2626ms
2014-06-27 01:14:34,954 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,954 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2653ms
2014-06-27 01:14:34,955 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,956 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2680ms
2014-06-27 01:14:34,956 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,956 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2709ms
2014-06-27 01:14:34,956 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,956 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2719ms
2014-06-27 01:14:34,956 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,956 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2729ms
2014-06-27 01:14:34,957 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,957 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2741ms
2014-06-27 01:14:34,957 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,957 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2883ms
2014-06-27 01:14:34,957 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,958 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2895ms
2014-06-27 01:14:34,958 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,959 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2926ms
2014-06-27 01:14:34,959 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,960 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2964ms
2014-06-27 01:14:34,960 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,961 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2985ms
2014-06-27 01:14:34,961 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,961 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4035ms
2014-06-27 01:14:34,961 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:34,962 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4053ms
2014-06-27 01:14:34,962 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403856256407
2014-06-27 01:14:37,568 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2193ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2446ms
2014-06-27 01:14:37,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 01:14:37,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1413 synced till here 1412
2014-06-27 01:14:38,112 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856870061 with entries=88, filesize=69.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403856256407/sceplus-vm48.almaden.ibm.com%2C60020%2C1403856256407.1403856877871
2014-06-27 01:14:41,024 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2419ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2880ms
2014-06-27 01:14:45,098 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3073ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3522ms
2014-06-27 01:14:45,225 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10358,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56459","starttimems":1403856874867,"queuetimems":1,"class":"HRegionServer","responsesize":17072,"method":"Multi"}
2014-06-27 01:14:45,226 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56459","starttimems":1403856874769,"queuetimems":1,"class":"HRegionServer","responsesize":17105,"method":"Multi"}
2014-06-27 01:14:45,226 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10407,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:56459","starttimems":1403856874818,"queuetimems":0,"class":"HRegionServer","responsesize":16595,"method":"Multi"}
2014-06-27 01:14:45,226 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 750 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56459: output error
2014-06-27 01:14:45,226 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:45,226 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 746 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:56459: output error
2014-06-27 01:14:45,226 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0.
2014-06-27 01:14:45,375 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 01:14:45,373 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1403856759966.8c24dc8da28390998e501bd2fe2885c0., current region memstore size 130.2m
2014-06-27 01:14:45,227 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:45,382 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4. due to global heap pressure
2014-06-27 01:14:45,383 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4., current region memstore size 125.1m
2014-06-27 01:14:45,386 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1403856759966.63f84494fd2ae9b2e453f741a89285b4.
2014-06-27 01:14:45,392 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 745 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:56459: output error
2014-06-27 01:14:45,392 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-27 01:14:45,396 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.7m is >= than blocking 386.7m size
2014-06-27 01:14:45,397 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 387.7m is >= than blocking 386.7m size
2014-06-27 01:14:45,397 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.2m is >= than blocking 386.7m size
2014-06-27 01:14:45,399 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.2m is >= than blocking 386.7m size
2014-06-27 01:14:45,401 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.2m is >= than blocking 386.7m size
2014-06-27 01:14:45,401 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 389.2m is >= than blocking 386.7m size
2014-06-27 01:14:45,533 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:45,562 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:45,568 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,179 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,179 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,179 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2580ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2737ms
2014-06-27 01:14:48,180 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,181 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,181 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,181 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,181 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,182 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,182 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,183 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,184 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,184 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,184 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,184 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,187 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,188 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,188 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,189 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,196 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,206 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 418.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,373 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,374 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,374 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,374 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,375 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,376 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,376 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,377 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 431.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,447 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 437.1m is >= than blocking 386.7m size
2014-06-27 01:14:48,447 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 438.6m is >= than blocking 386.7m size
2014-06-27 01:14:48,448 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,449 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,450 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 440.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,462 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 443.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,468 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 443.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,469 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 443.3m is >= than blocking 386.7m size
2014-06-27 01:14:48,526 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 451.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,527 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 451.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,527 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 451.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,527 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 451.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,531 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403856256407: the global memstore size 451.2m is >= than blocking 386.7m size
2014-06-27 01:14:48,556 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:48,611 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 01:14:51,267 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5699ms
2014-06-27 01:14:51,268 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5706ms
2014-06-27 01:14:51,269 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5867ms
2014-06-27 01:14:51,269 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5868ms
2014-06-27 01:14:51,269 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5873ms
2014-06-27 01:14:51,270 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2589ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2742ms
2014-06-27 01:14:51,270 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5873ms
2014-06-27 01:14:51,270 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5873ms
2014-06-27 01:14:51,270 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5871ms
2014-06-27 01:14:51,270 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5737ms
2014-06-27 01:14:54,066 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5535ms
2014-06-27 01:14:54,066 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5619ms
2014-06-27 01:14:54,066 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5692ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5694ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5871ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5880ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5540ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5605ms
2014-06-27 01:14:54,067 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5886ms
2014-06-27 01:14:54,068 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5600ms
2014-06-27 01:14:54,068 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5599ms
2014-06-27 01:14:54,068 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2298ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2502ms
2014-06-27 01:14:54,069 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5542ms
2014-06-27 01:14:54,069 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5542ms
2014-06-27 01:14:54,069 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5542ms
2014-06-27 01:14:54,070 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5891ms
2014-06-27 01:14:54,070 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5891ms
2014-06-27 01:14:54,070 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-06-27 01:14:54,070 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5889ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5890ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5889ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5889ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:14:54,071 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5888ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5884ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5884ms
2014-06-27 01:14:54,072 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5883ms
2014-06-27 01:14:54,073 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5867ms
2014-06-27 01:14:54,073 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5700ms
2014-06-27 01:14:54,073 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5699ms
2014-06-27 01:14:54,073 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5699ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5699ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5698ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5698ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5627ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5627ms
2014-06-27 01:14:54,074 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5626ms
2014-06-27 01:14:54,075 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5626ms
2014-06-27 01:14:56,522 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1953ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2251ms
2014-06-27 01:14:56,523 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10813ms
2014-06-27 01:14:56,523 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10955ms
2014-06-27 01:14:56,523 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10961ms
2014-06-27 01:14:56,523 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11122ms
2014-06-27 01:14:56,523 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11122ms
2014-06-27 01:14:56,524 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11128ms
2014-06-27 01:14:56,524 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11127ms
2014-06-27 01:14:56,524 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11127ms
2014-06-27 01:14:56,524 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11125ms
2014-06-27 01:14:59,309 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10860ms
2014-06-27 01:14:59,310 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11122ms
2014-06-27 01:14:59,310 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11121ms
2014-06-27 01:14:59,310 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10937ms
2014-06-27 01:14:59,311 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11105ms
2014-06-27 01:14:59,311 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10938ms
2014-06-27 01:14:59,311 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2288ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2643ms
2014-06-27 01:14:59,311 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10937ms
2014-06-27 01:14:59,312 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10937ms
2014-06-27 01:14:59,312 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10937ms
2014-06-27 01:14:59,313 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10936ms
2014-06-27 01:14:59,314 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10937ms
2014-06-27 01:14:59,314 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10867ms
2014-06-27 01:14:59,315 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10867ms
2014-06-27 01:14:59,315 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10867ms
2014-06-27 01:14:59,315 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10784ms
2014-06-27 01:14:59,316 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10869ms
2014-06-27 01:14:59,316 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10942ms
2014-06-27 01:14:59,316 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11120ms
2014-06-27 01:14:59,316 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11129ms
2014-06-27 01:14:59,317 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10790ms
2014-06-27 01:14:59,317 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10855ms
2014-06-27 01:14:59,318 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11136ms
2014-06-27 01:14:59,318 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10850ms
2014-06-27 01:14:59,319 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10849ms
2014-06-27 01:14:59,319 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10793ms
2014-06-27 01:14:59,319 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10792ms
2014-06-27 01:14:59,320 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10793ms
2014-06-27 01:14:59,320 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-06-27 01:14:59,321 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-06-27 01:14:59,321 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-06-27 01:14:59,322 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,323 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-06-27 01:14:59,324 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,324 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,325 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,325 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,325 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,326 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11141ms
2014-06-27 01:14:59,326 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,326 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11142ms
2014-06-27 01:14:59,326 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11138ms
2014-06-27 01:15:02,061 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2249ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2593ms
2014-06-27 01:15:02,061 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16461ms
2014-06-27 01:15:02,062 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16661ms
2014-06-27 01:15:02,062 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16529ms
2014-06-27 01:15:02,062 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16494ms
2014-06-27 01:15:02,063 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16500ms
2014-06-27 01:15:02,063 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16662ms
2014-06-27 01:15:02,064 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16668ms
2014-06-27 01:15:02,065 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16668ms
2014-06-27 01:15:02,065 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16668ms
2014-06-27 01:15:04,794 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16606ms
2014-06-27 01:15:04,794 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16614ms
2014-06-27 01:15:04,795 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16614ms
2014-06-27 01:15:04,796 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16611ms
2014-06-27 01:15:04,796 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16615ms
2014-06-27 01:15:04,796 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2234ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2602ms
2014-06-27 01:15:04,797 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16615ms
2014-06-27 01:15:04,798 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16615ms
2014-06-27 01:15:04,798 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16615ms
2014-06-27 01:15:04,801 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16616ms
2014-06-27 01:15:04,801 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16617ms
2014-06-27 01:15:04,802 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16617ms
2014-06-27 01:15:04,803 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16354ms
2014-06-27 01:15:04,803 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16614ms
2014-06-27 01:15:04,803 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16615ms
2014-06-27 01:15:04,804 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16430ms
2014-06-27 01:15:04,804 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16598ms
2014-06-27 01:15:04,804 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16431ms
2014-06-27 01:15:04,804 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16430ms
2014-06-27 01:15:04,804 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16430ms
2014-06-27 01:15:04,805 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16429ms
2014-06-27 01:15:04,805 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16429ms
2014-06-27 01:15:04,805 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16429ms
2014-06-27 01:15:04,805 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16358ms
2014-06-27 01:15:04,805 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16358ms
2014-06-27 01:15:04,806 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16358ms
2014-06-27 01:15:04,806 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16275ms
2014-06-27 01:15:04,807 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16360ms
2014-06-27 01:15:04,807 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16433ms
2014-06-27 01:15:04,808 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16612ms
2014-06-27 01:15:04,808 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16621ms
2014-06-27 01:15:04,809 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16281ms
2014-06-27 01:15:04,809 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16347ms
2014-06-27 01:15:04,810 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16628ms
2014-06-27 01:15:04,810 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16342ms
2014-06-27 01:15:04,811 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16341ms
2014-06-27 01:15:04,811 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16285ms
2014-06-27 01:15:04,811 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16284ms
2014-06-27 01:15:04,812 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16285ms
2014-06-27 01:15:04,812 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16633ms
2014-06-27 01:15:04,812 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16633ms
2014-06-27 01:15:04,813 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16633ms
2014-06-27 01:15:07,538 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2241ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2655ms
2014-06-27 01:15:07,538 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21919ms
2014-06-27 01:15:07,538 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22139ms
2014-06-27 01:15:07,541 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22138ms
2014-06-27 01:15:07,542 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22008ms
2014-06-27 01:15:07,542 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21974ms
2014-06-27 01:15:07,542 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21980ms
2014-06-27 01:15:07,543 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22142ms
2014-06-27 01:15:07,543 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22147ms
2014-06-27 01:15:07,543 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22146ms
2014-06-27 01:15:10,279 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22098ms
2014-06-27 01:15:10,279 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21832ms
2014-06-27 01:15:10,280 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21833ms
2014-06-27 01:15:10,281 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22101ms
2014-06-27 01:15:10,282 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21833ms
2014-06-27 01:15:10,282 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21751ms
2014-06-27 01:15:10,283 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21836ms
2014-06-27 01:15:10,283 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2243ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2673ms
2014-06-27 01:15:10,283 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21909ms
2014-06-27 01:15:10,283 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22087ms
2014-06-27 01:15:10,283 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22096ms
2014-06-27 01:15:10,284 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21757ms
2014-06-27 01:15:10,284 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21822ms
2014-06-27 01:15:10,284 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22103ms
2014-06-27 01:15:10,284 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21816ms
2014-06-27 01:15:10,285 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21816ms
2014-06-27 01:15:10,285 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21759ms
2014-06-27 01:15:10,286 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21759ms
2014-06-27 01:15:10,286 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21759ms
2014-06-27 01:15:10,286 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22107ms
2014-06-27 01:15:10,287 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22108ms
2014-06-27 01:15:10,287 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22099ms
2014-06-27 01:15:10,287 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22106ms
2014-06-27 01:15:10,287 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22103ms
2014-06-27 01:15:10,288 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22106ms
2014-06-27 01:15:10,288 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22106ms
2014-06-27 01:15:10,288 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22106ms
2014-06-27 01:15:10,288 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22105ms
2014-06-27 01:15:10,288 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22105ms
2014-06-27 01:15:10,289 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22104ms
2014-06-27 01:15:10,289 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22105ms
2014-06-27 01:15:10,290 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21841ms
2014-06-27 01:15:10,290 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22101ms
2014-06-27 01:15:10,290 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22102ms
2014-06-27 01:15:10,290 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21917ms
2014-06-27 01:15:10,290 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 22084ms
2014-06-27 01:15:10,291 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21918ms
2014-06-27 01:15:10,291 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21917ms
2014-06-27 01:15:10,291 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21917ms
2014-06-27 01:15:10,291 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21916ms
2014-06-27 01:15:10,291 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21915ms
2014-06-27 01:15:10,292 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21916ms
2014-06-27 01:15:12,765 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27367ms
2014-06-27 01:15:12,765 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27368ms
2014-06-27 01:15:12,765 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1981ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2423ms
2014-06-27 01:15:12,990 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27366ms
2014-06-27 01:15:12,991 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27589ms
2014-06-27 01:15:12,991 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27458ms
2014-06-27 01:15:12,991 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27423ms
2014-06-27 01:15:12,991 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27429ms
2014-06-27 01:15:12,992 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27591ms
2014-06-27 01:15:12,993 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27597ms
2014-06-27 01:15:15,768 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27392ms
2014-06-27 01:15:15,768 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27586ms
2014-06-27 01:15:15,769 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27587ms
2014-06-27 01:15:15,769 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27586ms
2014-06-27 01:15:15,769 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27582ms
2014-06-27 01:15:15,770 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27323ms
2014-06-27 01:15:15,770 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27587ms
2014-06-27 01:15:15,771 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27587ms
2014-06-27 01:15:15,771 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27587ms
2014-06-27 01:15:15,772 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27323ms
2014-06-27 01:15:15,772 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27583ms
2014-06-27 01:15:15,772 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2280ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2973ms
2014-06-27 01:15:15,773 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27584ms
2014-06-27 01:15:15,773 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27400ms
2014-06-27 01:15:15,774 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27568ms
2014-06-27 01:15:15,776 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27403ms
2014-06-27 01:15:16,008 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27403ms
2014-06-27 01:15:16,008 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27634ms
2014-06-27 01:15:16,009 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27634ms
2014-06-27 01:15:16,011 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27635ms
2014-06-27 01:15:16,011 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27831ms
2014-06-27 01:15:16,011 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27564ms
2014-06-27 01:15:16,012 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27831ms
2014-06-27 01:15:16,012 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27564ms
2014-06-27 01:15:16,012 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27481ms
2014-06-27 01:15:16,012 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27565ms
2014-06-27 01:15:16,013 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27639ms
2014-06-27 01:15:16,014 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27817ms
2014-06-27 01:15:16,014 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27487ms
2014-06-27 01:15:16,015 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27552ms
2014-06-27 01:15:16,015 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27834ms
2014-06-27 01:15:16,016 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27548ms
2014-06-27 01:15:16,016 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27547ms
2014-06-27 01:15:16,017 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27491ms
2014-06-27 01:15:16,017 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27490ms
2014-06-27 01:15:16,017 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27490ms
2014-06-27 01:15:16,018 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27838ms
2014-06-27 01:15:16,018 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27839ms
2014-06-27 01:15:16,018 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27830ms
2014-06-27 01:15:16,018 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27837ms
2014-06-27 01:15:16,019 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27835ms
2014-06-27 01:15:16,019 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27838ms
2014-06-27 01:15:19,054 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2781ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3251ms
2014-06-27 01:15:19,054 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33655ms
2014-06-27 01:15:19,056 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33659ms
2014-06-27 01:15:19,056 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33659ms
2014-06-27 01:15:19,057 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33658ms
2014-06-27 01:15:19,057 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33656ms
2014-06-27 01:15:19,058 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33525ms
2014-06-27 01:15:19,058 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33490ms
2014-06-27 01:15:19,059 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33496ms
2014-06-27 01:15:19,061 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33659ms
2014-06-27 01:15:21,591 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33410ms
2014-06-27 01:15:21,830 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33299ms
2014-06-27 01:15:21,830 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33383ms
2014-06-27 01:15:21,831 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33456ms
2014-06-27 01:15:21,831 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33642ms
2014-06-27 01:15:21,831 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33457ms
2014-06-27 01:15:21,831 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33649ms
2014-06-27 01:15:21,832 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33636ms
2014-06-27 01:15:21,832 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2277ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2755ms
2014-06-27 01:15:21,832 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33305ms
2014-06-27 01:15:21,832 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33370ms
2014-06-27 01:15:21,833 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33652ms
2014-06-27 01:15:21,833 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33364ms
2014-06-27 01:15:21,833 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33307ms
2014-06-27 01:15:21,834 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33306ms
2014-06-27 01:15:21,835 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33308ms
2014-06-27 01:15:21,835 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33656ms
2014-06-27 01:15:21,835 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33656ms
2014-06-27 01:15:21,835 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33647ms
2014-06-27 01:15:21,839 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33658ms
2014-06-27 01:15:21,840 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33655ms
2014-06-27 01:15:21,840 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33372ms
2014-06-27 01:15:21,840 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33464ms
2014-06-27 01:15:21,840 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33658ms
2014-06-27 01:15:21,840 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33657ms
2014-06-27 01:15:21,841 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33654ms
2014-06-27 01:15:21,841 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33394ms
2014-06-27 01:15:21,842 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33659ms
2014-06-27 01:15:21,842 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33658ms
2014-06-27 01:15:21,843 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33659ms
2014-06-27 01:15:24,751 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2419ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2905ms
2014-06-27 01:15:24,752 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33394ms
2014-06-27 01:15:30,867 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 01:15:30,868 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-27 01:15:30,869 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-27 01:15:30,870 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@2959ee1d
2014-06-27 01:15:30,923 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 01:15:30,932 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-27 01:15:30,942 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146dc5a5d11000c, negotiated timeout = 30000
Fri Jun 27 17:54:29 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-06-27 17:54:29,939 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-06-27 17:54:29,939 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-06-27 17:54:29,939 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-06-27 17:54:30,173 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 34009 22
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 17:54:30,174 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-06-27 17:54:30,175 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 34009 9.1.143.58 22
2014-06-27 17:54:30,175 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-06-27 17:54:30,175 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-06-27 17:54:30,175 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-06-27 17:54:30,177 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 17:54:30,177 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=194
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-06-27 17:54:30,178 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-06-27 17:54:30,179 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-06-27 17:54:30,179 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-06-27 17:54:30,181 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-06-27 17:54:30,181 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx1000m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-06-27 17:54:30,404 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-06-27 17:54:30,750 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-06-27 17:54:30,828 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-06-27 17:54:30,842 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-06-27 17:54:30,907 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-06-27 17:54:30,909 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-06-27 17:54:30,909 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-06-27 17:54:30,914 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-06-27 17:54:30,918 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-06-27 17:54:31,001 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-06-27 17:54:31,001 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-06-27 17:54:31,006 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-06-27 17:54:31,008 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7m
2014-06-27 17:54:31,077 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-06-27 17:54:31,131 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-06-27 17:54:31,140 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-06-27 17:54:31,142 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-06-27 17:54:31,142 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-06-27 17:54:31,142 INFO  [main] mortbay.log: jetty-6.1.26
2014-06-27 17:54:31,474 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-27 17:54:31,524 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-27 17:54:31,524 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-27 17:54:31,525 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-27 17:54:31,526 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 17:54:31,547 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 17:54:31,550 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 17:54:31,555 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 17:54:31,573 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46dff75d470000, negotiated timeout = 90000
2014-06-27 17:54:31,627 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x7acd5aa1, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 17:54:31,628 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7acd5aa1 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 17:54:31,629 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 17:54:31,629 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 17:54:31,633 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46dff75d470001, negotiated timeout = 90000
2014-06-27 17:54:31,873 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@711ae512
2014-06-27 17:54:31,877 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-06-27 17:54:31,882 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-06-27 17:54:31,906 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-06-27 17:54:31,934 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-06-27 17:54:31,939 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7m, globalMemStoreLimitLowMark=367.3m, maxHeap=966.7m
2014-06-27 17:54:31,943 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-06-27 17:54:31,962 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1403916869483 with port=60020, startcode=1403916870929
2014-06-27 17:54:32,348 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-06-27 17:54:32,348 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-06-27 17:54:32,383 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-06-27 17:54:32,390 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:32,425 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-06-27 17:54:32,438 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-06-27 17:54:32,525 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916872442
2014-06-27 17:54:32,540 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-06-27 17:54:32,545 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-06-27 17:54:32,549 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-06-27 17:54:32,553 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-06-27 17:54:32,556 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 17:54:32,556 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 17:54:32,556 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 17:54:32,556 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 17:54:32,557 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-06-27 17:54:32,566 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1403856250521, sceplus-vm48.almaden.ibm.com,60020,1403916870929, slave1,60020,1403916865318] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1403916870929, slave1,60020,1403916865318]
2014-06-27 17:54:32,596 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-06-27 17:54:32,601 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x7f85e8ac, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 17:54:32,602 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x7f85e8ac connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 17:54:32,602 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 17:54:32,603 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 17:54:32,609 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46dff75d470002, negotiated timeout = 90000
2014-06-27 17:54:32,616 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-06-27 17:54:32,616 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-06-27 17:54:32,660 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1403916870929, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x46dff75d470000
2014-06-27 17:54:32,660 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1403916870929 starting
2014-06-27 17:54:32,660 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-06-27 17:54:32,660 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:32,661 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1403916870929'
2014-06-27 17:54:32,661 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-06-27 17:54:32,661 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-06-27 17:54:32,662 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-06-27 17:54:32,696 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899545752.meta
2014-06-27 17:54:32,739 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899545752.meta, length=9
2014-06-27 17:54:32,739 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:32,746 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899545752.meta
2014-06-27 17:54:32,747 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899545752.meta after 1ms
2014-06-27 17:54:32,771 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-06-27 17:54:32,771 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-06-27 17:54:32,772 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-06-27 17:54:32,783 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:32,784 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:32,784 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:32,785 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 0 edits across 0 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899545752.meta is corrupted = false progress failed = false
2014-06-27 17:54:32,791 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899545752.meta to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:32,791 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899545752.meta in 86ms
2014-06-27 17:54:32,811 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 17:54:36,445 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving slave1,60020,1403856250521's hlogs to my queue
2014-06-27 17:54:36,456 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/slave1,60020,1403856250521/lock
2014-06-27 17:54:36,876 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 17:54:36,877 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 17:54:36,883 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900601929
2014-06-27 17:54:36,908 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900601929, length=65892273
2014-06-27 17:54:36,908 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:36,912 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900601929
2014-06-27 17:54:36,914 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900601929 after 2ms
2014-06-27 17:54:36,934 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-06-27 17:54:36,934 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-06-27 17:54:36,938 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-06-27 17:54:37,091 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002210.temp region=9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:37,147 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002208.temp region=0851869dffc7f464037f6a44f5d8fd79
2014-06-27 17:54:37,197 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002212.temp region=44feafaf7e55d7380d25c5e765424332
2014-06-27 17:54:37,240 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002206.temp region=6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:37,253 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002211.temp region=6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:37,272 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002207.temp region=96d73fc1bf678b50fe84f5cfb743c6b8
2014-06-27 17:54:37,284 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002214.temp region=b69aa9295265857524ac75e5169f1b1b
2014-06-27 17:54:37,298 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002207.temp region=ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:37,307 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002177.temp region=32f11db1bbbc522b79c77a6e6d7887b2
2014-06-27 17:54:37,604 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899445716
2014-06-27 17:54:37,629 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899445716, length=65117745
2014-06-27 17:54:37,629 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:37,634 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899445716
2014-06-27 17:54:37,635 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899445716 after 1ms
2014-06-27 17:54:37,648 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-06-27 17:54:37,649 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-06-27 17:54:37,649 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-06-27 17:54:37,663 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002189.temp region=6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:37,666 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002196.temp region=b69aa9295265857524ac75e5169f1b1b
2014-06-27 17:54:37,673 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002160.temp region=32f11db1bbbc522b79c77a6e6d7887b2
2014-06-27 17:54:37,678 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002195.temp region=44feafaf7e55d7380d25c5e765424332
2014-06-27 17:54:37,709 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002194.temp region=6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:37,717 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002194.temp region=9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:37,723 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002192.temp region=0851869dffc7f464037f6a44f5d8fd79
2014-06-27 17:54:37,760 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002191.temp region=96d73fc1bf678b50fe84f5cfb743c6b8
2014-06-27 17:54:37,764 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002191.temp region=ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:37,901 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:37,901 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:37,909 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:37,909 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002208.temp
2014-06-27 17:54:37,911 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002177.temp
2014-06-27 17:54:37,911 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002212.temp
2014-06-27 17:54:37,911 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002208.temp
2014-06-27 17:54:37,911 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002177.temp
2014-06-27 17:54:37,911 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002211.temp
2014-06-27 17:54:37,912 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002212.temp
2014-06-27 17:54:37,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002206.temp
2014-06-27 17:54:37,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002207.temp
2014-06-27 17:54:37,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002210.temp
2014-06-27 17:54:37,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002207.temp
2014-06-27 17:54:37,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002214.temp
2014-06-27 17:54:37,919 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002177.temp (wrote 9 edits in 54ms)
2014-06-27 17:54:37,919 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002208.temp (wrote 11 edits in 139ms)
2014-06-27 17:54:37,920 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002212.temp (wrote 12 edits in 194ms)
2014-06-27 17:54:37,922 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002177.temp to hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002185
2014-06-27 17:54:37,922 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002211.temp
2014-06-27 17:54:37,923 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002208.temp to hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002218
2014-06-27 17:54:37,924 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002206.temp
2014-06-27 17:54:37,927 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002212.temp to hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002223
2014-06-27 17:54:37,927 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002207.temp
2014-06-27 17:54:37,931 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002211.temp (wrote 11 edits in 121ms)
2014-06-27 17:54:37,932 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002206.temp (wrote 10 edits in 128ms)
2014-06-27 17:54:37,935 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002207.temp (wrote 10 edits in 106ms)
2014-06-27 17:54:37,936 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002211.temp to hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002221
2014-06-27 17:54:37,936 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002210.temp
2014-06-27 17:54:37,936 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002206.temp to hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002215
2014-06-27 17:54:37,936 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002207.temp
2014-06-27 17:54:37,941 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002210.temp (wrote 11 edits in 207ms)
2014-06-27 17:54:37,941 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002207.temp to hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002216
2014-06-27 17:54:37,941 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002214.temp
2014-06-27 17:54:37,942 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002207.temp (wrote 9 edits in 94ms)
2014-06-27 17:54:37,945 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002210.temp to hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002220
2014-06-27 17:54:37,946 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002214.temp (wrote 11 edits in 115ms)
2014-06-27 17:54:37,946 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002207.temp to hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002215
2014-06-27 17:54:37,988 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002214.temp to hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002224
2014-06-27 17:54:37,988 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 94 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900601929 is corrupted = false progress failed = false
2014-06-27 17:54:37,995 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900601929 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:37,995 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900601929 in 1111ms
2014-06-27 17:54:38,134 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:38,134 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:38,138 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:38,138 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002192.temp
2014-06-27 17:54:38,138 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002160.temp
2014-06-27 17:54:38,138 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002192.temp
2014-06-27 17:54:38,138 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002195.temp
2014-06-27 17:54:38,139 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002160.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002194.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002189.temp
2014-06-27 17:54:38,139 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002195.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002191.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002194.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002191.temp
2014-06-27 17:54:38,139 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002196.temp
2014-06-27 17:54:38,145 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002160.temp (wrote 17 edits in 48ms)
2014-06-27 17:54:38,146 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002195.temp (wrote 17 edits in 109ms)
2014-06-27 17:54:38,146 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002192.temp (wrote 16 edits in 108ms)
2014-06-27 17:54:38,148 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002160.temp to hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002176
2014-06-27 17:54:38,148 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002194.temp
2014-06-27 17:54:38,148 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002195.temp to hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002211
2014-06-27 17:54:38,148 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002189.temp
2014-06-27 17:54:38,153 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002192.temp to hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002207
2014-06-27 17:54:38,153 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002191.temp
2014-06-27 17:54:38,153 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002194.temp (wrote 17 edits in 130ms)
2014-06-27 17:54:38,154 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002189.temp (wrote 17 edits in 99ms)
2014-06-27 17:54:38,157 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002191.temp (wrote 16 edits in 130ms)
2014-06-27 17:54:38,157 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002194.temp to hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002210
2014-06-27 17:54:38,157 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002194.temp
2014-06-27 17:54:38,158 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002189.temp to hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002205
2014-06-27 17:54:38,158 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002191.temp
2014-06-27 17:54:38,161 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002194.temp (wrote 16 edits in 92ms)
2014-06-27 17:54:38,162 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002191.temp (wrote 16 edits in 163ms)
2014-06-27 17:54:38,162 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002191.temp to hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002206
2014-06-27 17:54:38,162 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002196.temp
2014-06-27 17:54:38,164 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002194.temp to hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002209
2014-06-27 17:54:38,166 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002191.temp to hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002206
2014-06-27 17:54:38,202 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002196.temp (wrote 18 edits in 121ms)
2014-06-27 17:54:38,205 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002196.temp to hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002213
2014-06-27 17:54:38,205 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 150 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403899445716 is corrupted = false progress failed = false
2014-06-27 17:54:38,210 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899445716 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:38,210 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403899445716 in 605ms
2014-06-27 17:54:38,283 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900639594
2014-06-27 17:54:38,301 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900639594, length=66135287
2014-06-27 17:54:38,301 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:38,305 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900639594
2014-06-27 17:54:38,305 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900639594 after 0ms
2014-06-27 17:54:38,314 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-06-27 17:54:38,314 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-06-27 17:54:38,317 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-06-27 17:54:38,334 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002216.temp region=6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:38,339 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002222.temp region=6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:38,345 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002216.temp region=ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:38,353 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002221.temp region=9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:38,361 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002217.temp region=96d73fc1bf678b50fe84f5cfb743c6b8
2014-06-27 17:54:38,367 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002225.temp region=b69aa9295265857524ac75e5169f1b1b
2014-06-27 17:54:38,372 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002224.temp region=44feafaf7e55d7380d25c5e765424332
2014-06-27 17:54:38,376 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002186.temp region=32f11db1bbbc522b79c77a6e6d7887b2
2014-06-27 17:54:38,387 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002219.temp region=0851869dffc7f464037f6a44f5d8fd79
2014-06-27 17:54:38,718 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:38,719 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:38,725 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:38,725 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002219.temp
2014-06-27 17:54:38,726 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002186.temp
2014-06-27 17:54:38,726 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002219.temp
2014-06-27 17:54:38,726 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002224.temp
2014-06-27 17:54:38,726 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002186.temp
2014-06-27 17:54:38,726 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002222.temp
2014-06-27 17:54:38,726 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002216.temp
2014-06-27 17:54:38,726 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002217.temp
2014-06-27 17:54:38,727 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002224.temp
2014-06-27 17:54:38,727 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002221.temp
2014-06-27 17:54:38,727 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002216.temp
2014-06-27 17:54:38,727 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002225.temp
2014-06-27 17:54:38,732 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002186.temp (wrote 9 edits in 32ms)
2014-06-27 17:54:38,733 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002224.temp (wrote 8 edits in 64ms)
2014-06-27 17:54:38,734 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002219.temp (wrote 9 edits in 87ms)
2014-06-27 17:54:38,735 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002186.temp to hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002194
2014-06-27 17:54:38,736 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002222.temp
2014-06-27 17:54:38,736 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002224.temp to hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002231
2014-06-27 17:54:38,736 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002216.temp
2014-06-27 17:54:38,740 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002219.temp to hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002227
2014-06-27 17:54:38,740 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002217.temp
2014-06-27 17:54:38,740 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002222.temp (wrote 9 edits in 77ms)
2014-06-27 17:54:38,741 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002216.temp (wrote 10 edits in 85ms)
2014-06-27 17:54:38,745 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002222.temp to hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002230
2014-06-27 17:54:38,745 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002221.temp
2014-06-27 17:54:38,745 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002217.temp (wrote 9 edits in 62ms)
2014-06-27 17:54:38,746 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002216.temp to hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002225
2014-06-27 17:54:38,746 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002216.temp
2014-06-27 17:54:38,750 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002221.temp (wrote 9 edits in 67ms)
2014-06-27 17:54:38,750 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002216.temp (wrote 9 edits in 67ms)
2014-06-27 17:54:38,750 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002217.temp to hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002225
2014-06-27 17:54:38,751 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002225.temp
2014-06-27 17:54:38,752 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002221.temp to hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002229
2014-06-27 17:54:38,754 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002216.temp to hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002224
2014-06-27 17:54:38,790 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002225.temp (wrote 8 edits in 78ms)
2014-06-27 17:54:38,792 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002225.temp to hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002232
2014-06-27 17:54:38,793 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 80 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900639594 is corrupted = false progress failed = false
2014-06-27 17:54:38,799 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900639594 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:38,799 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900639594 in 515ms
2014-06-27 17:54:38,805 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900788794
2014-06-27 17:54:38,822 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-27 17:54:38,829 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900788794, length=66529410
2014-06-27 17:54:38,829 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:38,834 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900788794
2014-06-27 17:54:38,835 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900788794 after 1ms
2014-06-27 17:54:38,844 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-06-27 17:54:38,844 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-06-27 17:54:38,845 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-06-27 17:54:38,859 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002270.temp region=0851869dffc7f464037f6a44f5d8fd79
2014-06-27 17:54:38,862 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002268.temp region=6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:38,866 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002269.temp region=96d73fc1bf678b50fe84f5cfb743c6b8
2014-06-27 17:54:38,873 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002269.temp region=ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:38,878 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002274.temp region=44feafaf7e55d7380d25c5e765424332
2014-06-27 17:54:38,880 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002273.temp region=6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:38,891 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002276.temp region=b69aa9295265857524ac75e5169f1b1b
2014-06-27 17:54:38,911 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002273.temp region=9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:38,919 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002238.temp region=32f11db1bbbc522b79c77a6e6d7887b2
2014-06-27 17:54:39,233 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:39,233 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:39,239 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:39,239 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002270.temp
2014-06-27 17:54:39,240 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002238.temp
2014-06-27 17:54:39,240 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002270.temp
2014-06-27 17:54:39,240 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002274.temp
2014-06-27 17:54:39,240 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002238.temp
2014-06-27 17:54:39,240 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002273.temp
2014-06-27 17:54:39,240 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002274.temp
2014-06-27 17:54:39,241 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002268.temp
2014-06-27 17:54:39,241 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002269.temp
2014-06-27 17:54:39,241 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002273.temp
2014-06-27 17:54:39,241 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002269.temp
2014-06-27 17:54:39,241 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002276.temp
2014-06-27 17:54:39,246 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002238.temp (wrote 8 edits in 33ms)
2014-06-27 17:54:39,247 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002274.temp (wrote 11 edits in 88ms)
2014-06-27 17:54:39,248 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002270.temp (wrote 10 edits in 92ms)
2014-06-27 17:54:39,249 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002238.temp to hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002245
2014-06-27 17:54:39,249 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002273.temp
2014-06-27 17:54:39,250 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002274.temp to hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002285
2014-06-27 17:54:39,250 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002268.temp
2014-06-27 17:54:39,254 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002270.temp to hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002279
2014-06-27 17:54:39,254 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002269.temp
2014-06-27 17:54:39,255 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002273.temp (wrote 11 edits in 112ms)
2014-06-27 17:54:39,256 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002268.temp (wrote 10 edits in 71ms)
2014-06-27 17:54:39,258 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002273.temp to hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002283
2014-06-27 17:54:39,258 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002273.temp
2014-06-27 17:54:39,259 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002269.temp (wrote 9 edits in 68ms)
2014-06-27 17:54:39,260 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002268.temp to hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002277
2014-06-27 17:54:39,260 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002269.temp
2014-06-27 17:54:39,263 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002273.temp (wrote 10 edits in 88ms)
2014-06-27 17:54:39,264 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002269.temp (wrote 8 edits in 62ms)
2014-06-27 17:54:39,264 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002269.temp to hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002277
2014-06-27 17:54:39,264 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002276.temp
2014-06-27 17:54:39,266 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002273.temp to hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002282
2014-06-27 17:54:39,267 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002269.temp to hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002276
2014-06-27 17:54:39,306 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002276.temp (wrote 10 edits in 104ms)
2014-06-27 17:54:39,308 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002276.temp to hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002285
2014-06-27 17:54:39,309 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 87 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900788794 is corrupted = false progress failed = false
2014-06-27 17:54:39,317 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900788794 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:39,317 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900788794 in 511ms
2014-06-27 17:54:39,809 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403916870929] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900783056
2014-06-27 17:54:39,828 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900783056, length=64362340
2014-06-27 17:54:39,828 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-06-27 17:54:39,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900783056
2014-06-27 17:54:39,832 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900783056 after 1ms
2014-06-27 17:54:39,840 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-06-27 17:54:39,840 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-06-27 17:54:39,841 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-06-27 17:54:39,853 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002254.temp region=ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:39,860 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002257.temp region=0851869dffc7f464037f6a44f5d8fd79
2014-06-27 17:54:39,862 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002224.temp region=32f11db1bbbc522b79c77a6e6d7887b2
2014-06-27 17:54:39,868 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002263.temp region=b69aa9295265857524ac75e5169f1b1b
2014-06-27 17:54:39,873 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002255.temp region=6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:39,879 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002262.temp region=44feafaf7e55d7380d25c5e765424332
2014-06-27 17:54:39,885 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002256.temp region=96d73fc1bf678b50fe84f5cfb743c6b8
2014-06-27 17:54:39,891 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002260.temp region=9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:39,896 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002261.temp region=6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:40,218 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-06-27 17:54:40,218 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-06-27 17:54:40,223 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-06-27 17:54:40,223 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002257.temp
2014-06-27 17:54:40,223 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002224.temp
2014-06-27 17:54:40,223 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002257.temp
2014-06-27 17:54:40,223 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002262.temp
2014-06-27 17:54:40,224 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002224.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002261.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002255.temp
2014-06-27 17:54:40,224 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002262.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002256.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002260.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002254.temp
2014-06-27 17:54:40,224 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002263.temp
2014-06-27 17:54:40,229 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002224.temp (wrote 14 edits in 29ms)
2014-06-27 17:54:40,230 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002262.temp (wrote 12 edits in 59ms)
2014-06-27 17:54:40,230 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002257.temp (wrote 13 edits in 87ms)
2014-06-27 17:54:40,232 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002224.temp to hdfs://master:54310/hbase/data/default/usertable/32f11db1bbbc522b79c77a6e6d7887b2/recovered.edits/0000000000000002237
2014-06-27 17:54:40,232 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002261.temp
2014-06-27 17:54:40,232 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002262.temp to hdfs://master:54310/hbase/data/default/usertable/44feafaf7e55d7380d25c5e765424332/recovered.edits/0000000000000002273
2014-06-27 17:54:40,233 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002255.temp
2014-06-27 17:54:40,235 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002257.temp to hdfs://master:54310/hbase/data/default/usertable/0851869dffc7f464037f6a44f5d8fd79/recovered.edits/0000000000000002269
2014-06-27 17:54:40,235 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002256.temp
2014-06-27 17:54:40,235 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002261.temp (wrote 12 edits in 71ms)
2014-06-27 17:54:40,236 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002255.temp (wrote 13 edits in 99ms)
2014-06-27 17:54:40,239 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002256.temp (wrote 13 edits in 69ms)
2014-06-27 17:54:40,239 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002261.temp to hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002272
2014-06-27 17:54:40,239 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002260.temp
2014-06-27 17:54:40,240 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002255.temp to hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002267
2014-06-27 17:54:40,240 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002254.temp
2014-06-27 17:54:40,242 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002260.temp (wrote 13 edits in 70ms)
2014-06-27 17:54:40,243 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002256.temp to hdfs://master:54310/hbase/data/default/usertable/96d73fc1bf678b50fe84f5cfb743c6b8/recovered.edits/0000000000000002268
2014-06-27 17:54:40,243 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002263.temp
2014-06-27 17:54:40,243 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002254.temp (wrote 14 edits in 107ms)
2014-06-27 17:54:40,246 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002260.temp to hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002272
2014-06-27 17:54:40,246 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002254.temp to hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002267
2014-06-27 17:54:40,282 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002263.temp (wrote 13 edits in 79ms)
2014-06-27 17:54:40,285 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002263.temp to hdfs://master:54310/hbase/data/default/usertable/b69aa9295265857524ac75e5169f1b1b/recovered.edits/0000000000000002275
2014-06-27 17:54:40,286 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 117 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1403856250521-splitting/slave1%2C60020%2C1403856250521.1403900783056 is corrupted = false progress failed = false
2014-06-27 17:54:40,291 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900783056 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:40,291 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1403916870929 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1403856250521-splitting%2Fslave1%252C60020%252C1403856250521.1403900783056 in 481ms
2014-06-27 17:54:40,802 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.
2014-06-27 17:54:40,910 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6302562d3142c0f697210f458fb6fc5d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,911 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c.
2014-06-27 17:54:40,912 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585.
2014-06-27 17:54:40,913 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9b741484aa13ab2851f1f938d0b7b98c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,913 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a.
2014-06-27 17:54:40,914 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6b6070a8b8f9bff0f0254b8ea39a1585 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,915 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 17:54:40,934 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6b6070a8b8f9bff0f0254b8ea39a1585 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,935 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 6b6070a8b8f9bff0f0254b8ea39a1585, NAME => 'usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-06-27 17:54:40,937 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9b741484aa13ab2851f1f938d0b7b98c from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,937 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6302562d3142c0f697210f458fb6fc5d from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:40,937 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 9b741484aa13ab2851f1f938d0b7b98c, NAME => 'usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-06-27 17:54:40,938 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 6302562d3142c0f697210f458fb6fc5d, NAME => 'usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-06-27 17:54:40,954 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585.
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.
2014-06-27 17:54:40,955 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c.
2014-06-27 17:54:40,969 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] util.NativeCodeLoader: Loaded the native-hadoop library
2014-06-27 17:54:40,970 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-06-27 17:54:40,972 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:40,972 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:40,972 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:41,035 INFO  [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [9, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 17:54:41,035 INFO  [StoreOpener-6b6070a8b8f9bff0f0254b8ea39a1585-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [9, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 17:54:41,040 INFO  [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [9, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 17:54:41,074 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-06-27 17:54:41,103 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:41,116 DEBUG [StoreOpener-6b6070a8b8f9bff0f0254b8ea39a1585-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/family/2e80f6e4d7854703a635811592c2c54e, isReference=false, isBulkLoadResult=false, seqid=2170, majorCompaction=true
2014-06-27 17:54:41,116 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/04b381529d144f6dbbd64aa0ab747c2f, isReference=false, isBulkLoadResult=false, seqid=905, majorCompaction=false
2014-06-27 17:54:41,119 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/01b9ac9a56e247f394c45c867d64fce9, isReference=false, isBulkLoadResult=false, seqid=2087, majorCompaction=false
2014-06-27 17:54:41,124 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/0679e2895dbf4d47a7be11a73e973998, isReference=false, isBulkLoadResult=false, seqid=2193, majorCompaction=false
2014-06-27 17:54:41,129 DEBUG [StoreOpener-6b6070a8b8f9bff0f0254b8ea39a1585-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/family/a74d9f2853784c1780ddd4550662e280, isReference=false, isBulkLoadResult=false, seqid=2245, majorCompaction=false
2014-06-27 17:54:41,139 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/5b30508f81614e68b32aa7cac1b6d981, isReference=false, isBulkLoadResult=false, seqid=2183, majorCompaction=false
2014-06-27 17:54:41,141 DEBUG [StoreOpener-6b6070a8b8f9bff0f0254b8ea39a1585-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/family/b00e872c66b04fbfa918a81383256807, isReference=false, isBulkLoadResult=false, seqid=2188, majorCompaction=false
2014-06-27 17:54:41,145 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/2d305b82b01c4637b6b389148dd37ce9, isReference=false, isBulkLoadResult=false, seqid=2160, majorCompaction=false
2014-06-27 17:54:41,155 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/6509b4bd91a9447bb3be2a82fec8835e, isReference=false, isBulkLoadResult=false, seqid=812, majorCompaction=false
2014-06-27 17:54:41,161 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/463cca76d9424d7a83b8e9772ce56bde, isReference=false, isBulkLoadResult=false, seqid=2233, majorCompaction=false
2014-06-27 17:54:41,167 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/6cdd950ab095450e9f13109113ae948e, isReference=false, isBulkLoadResult=false, seqid=2245, majorCompaction=false
2014-06-27 17:54:41,171 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/46568ee2d66d4c9bb2d4a4787281f320, isReference=false, isBulkLoadResult=false, seqid=507, majorCompaction=false
2014-06-27 17:54:41,177 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/997d0a94ab9f4a48927700bedf1605a9, isReference=false, isBulkLoadResult=false, seqid=2193, majorCompaction=false
2014-06-27 17:54:41,179 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/4b1b971d96e54255b50f3e5a8def7110, isReference=false, isBulkLoadResult=false, seqid=2284, majorCompaction=false
2014-06-27 17:54:41,192 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/c5a521c363ae4e4c9a4725153e5ebe92, isReference=false, isBulkLoadResult=false, seqid=423, majorCompaction=false
2014-06-27 17:54:41,195 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/703c4db8afad4b9594bf11d2b8b00c91, isReference=false, isBulkLoadResult=false, seqid=52, majorCompaction=false
2014-06-27 17:54:41,196 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 10 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:41,203 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 2205 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002205
2014-06-27 17:54:41,207 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/abc55972c2dd415d9ca76d7c1bec3a92, isReference=false, isBulkLoadResult=false, seqid=1323, majorCompaction=false
2014-06-27 17:54:41,207 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 2215 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002215
2014-06-27 17:54:41,211 DEBUG [StoreOpener-9b741484aa13ab2851f1f938d0b7b98c-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/e11d86f8b0624a68a71cbc68e6ddbb45, isReference=false, isBulkLoadResult=false, seqid=2130, majorCompaction=false
2014-06-27 17:54:41,212 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 2225 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002225
2014-06-27 17:54:41,214 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 2233 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002233
2014-06-27 17:54:41,216 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 2244 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002244
2014-06-27 17:54:41,217 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002254
2014-06-27 17:54:41,237 DEBUG [StoreOpener-6302562d3142c0f697210f458fb6fc5d-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/dfe9527226804785a2adacbd1858c29f, isReference=false, isBulkLoadResult=false, seqid=104, majorCompaction=false
2014-06-27 17:54:41,239 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 9 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:41,241 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 2209 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002209
2014-06-27 17:54:41,243 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 2220 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002220
2014-06-27 17:54:41,244 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 2229 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002229
2014-06-27 17:54:41,245 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 2238 and minimum sequenceid for the region is 2245, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002238
2014-06-27 17:54:41,247 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002250
2014-06-27 17:54:41,247 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 10 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:41,286 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2210 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002210
2014-06-27 17:54:41,287 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2221 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002221
2014-06-27 17:54:41,288 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2230 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002230
2014-06-27 17:54:41,289 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2240 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002240
2014-06-27 17:54:41,291 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2251 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002251
2014-06-27 17:54:41,292 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2260 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002260
2014-06-27 17:54:41,293 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2272 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002272
2014-06-27 17:54:41,294 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2283 and minimum sequenceid for the region is 2284, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002283
2014-06-27 17:54:41,295 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002295
2014-06-27 17:54:41,784 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:41,787 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 28950, skipped 34600, firstSequenceidInLog=2239, maxSequenceidInLog=2250, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002250
2014-06-27 17:54:41,789 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002259
2014-06-27 17:54:41,839 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:41,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 44370, skipped 0, firstSequenceidInLog=2246, maxSequenceidInLog=2254, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002254
2014-06-27 17:54:41,842 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002267
2014-06-27 17:54:41,940 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:41,942 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 61520, skipped 0, firstSequenceidInLog=2285, maxSequenceidInLog=2295, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002295
2014-06-27 17:54:41,944 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002296
2014-06-27 17:54:42,031 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:42,034 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 41450, skipped 0, firstSequenceidInLog=2251, maxSequenceidInLog=2259, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002259
2014-06-27 17:54:42,035 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002272
2014-06-27 17:54:42,043 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:42,046 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 5230, skipped 0, firstSequenceidInLog=2296, maxSequenceidInLog=2296, path=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002296
2014-06-27 17:54:42,046 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Started memstore flush for usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d., current region memstore size 18.3m; wal is null, using passed sequenceid=2296
2014-06-27 17:54:42,074 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:54:42,149 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:42,152 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 49570, skipped 0, firstSequenceidInLog=2255, maxSequenceidInLog=2267, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002267
2014-06-27 17:54:42,153 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002277
2014-06-27 17:54:42,270 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:42,272 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 48590, skipped 0, firstSequenceidInLog=2260, maxSequenceidInLog=2272, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002272
2014-06-27 17:54:42,274 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002282
2014-06-27 17:54:42,485 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:42,488 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 50670, skipped 0, firstSequenceidInLog=2268, maxSequenceidInLog=2277, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002277
2014-06-27 17:54:42,490 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002286
2014-06-27 17:54:42,621 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:42,625 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 51520, skipped 0, firstSequenceidInLog=2273, maxSequenceidInLog=2282, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002282
2014-06-27 17:54:42,627 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002294
2014-06-27 17:54:42,866 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:42,869 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 50840, skipped 0, firstSequenceidInLog=2278, maxSequenceidInLog=2286, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002286
2014-06-27 17:54:42,871 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002290
2014-06-27 17:54:43,001 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:43,003 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 21870, skipped 0, firstSequenceidInLog=2287, maxSequenceidInLog=2290, path=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002290
2014-06-27 17:54:43,003 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Started memstore flush for usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585., current region memstore size 59.7m; wal is null, using passed sequenceid=2290
2014-06-27 17:54:43,025 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:54:43,028 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:43,043 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2296, memsize=16.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/.tmp/298b9b6d9b15410aa2916beaf774534d
2014-06-27 17:54:43,068 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/.tmp/298b9b6d9b15410aa2916beaf774534d as hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/298b9b6d9b15410aa2916beaf774534d
2014-06-27 17:54:43,075 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/298b9b6d9b15410aa2916beaf774534d, entries=61520, sequenceid=2296, filesize=4.4m
2014-06-27 17:54:43,076 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Finished memstore flush of ~18.3m/19223920, currentsize=0.0/0 for region usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. in 1030ms, sequenceid=2296, compaction requested=true; wal=null
2014-06-27 17:54:43,079 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002210
2014-06-27 17:54:43,080 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002221
2014-06-27 17:54:43,081 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002230
2014-06-27 17:54:43,082 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002240
2014-06-27 17:54:43,084 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002251
2014-06-27 17:54:43,085 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002260
2014-06-27 17:54:43,086 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002272
2014-06-27 17:54:43,086 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002283
2014-06-27 17:54:43,087 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002295
2014-06-27 17:54:43,088 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/recovered.edits/0000000000000002296
2014-06-27 17:54:43,091 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:43,093 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 6302562d3142c0f697210f458fb6fc5d; next sequenceid=2297
2014-06-27 17:54:43,093 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6302562d3142c0f697210f458fb6fc5d
2014-06-27 17:54:43,094 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 68610, skipped 0, firstSequenceidInLog=2283, maxSequenceidInLog=2294, path=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002294
2014-06-27 17:54:43,094 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Started memstore flush for usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c., current region memstore size 65.7m; wal is null, using passed sequenceid=2294
2014-06-27 17:54:43,096 INFO  [PostOpenDeployTasks:6302562d3142c0f697210f458fb6fc5d] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.
2014-06-27 17:54:43,100 DEBUG [PostOpenDeployTasks:6302562d3142c0f697210f458fb6fc5d] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-27 17:54:43,103 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 10 blocking
2014-06-27 17:54:43,104 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 10 files of size 931372909 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-06-27 17:54:43,106 DEBUG [regionserver60020-smallCompactions-1403916883100] regionserver.HStore: 6302562d3142c0f697210f458fb6fc5d - family: Initiating major compaction
2014-06-27 17:54:43,106 INFO  [regionserver60020-smallCompactions-1403916883100] regionserver.HRegion: Starting compaction on family in region usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.
2014-06-27 17:54:43,106 INFO  [regionserver60020-smallCompactions-1403916883100] regionserver.HStore: Starting compaction of 10 file(s) in family of usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/.tmp, totalSize=888.2m
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/703c4db8afad4b9594bf11d2b8b00c91, keycount=28226, bloomtype=ROW, size=20.1m, encoding=NONE, seqNum=52, earliestPutTs=1403893610279
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/dfe9527226804785a2adacbd1858c29f, keycount=28949, bloomtype=ROW, size=20.6m, encoding=NONE, seqNum=104, earliestPutTs=1403893629343
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/46568ee2d66d4c9bb2d4a4787281f320, keycount=222937, bloomtype=ROW, size=158.9m, encoding=NONE, seqNum=507, earliestPutTs=1403893679615
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/04b381529d144f6dbbd64aa0ab747c2f, keycount=219463, bloomtype=ROW, size=156.4m, encoding=NONE, seqNum=905, earliestPutTs=1403894027607
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/abc55972c2dd415d9ca76d7c1bec3a92, keycount=230719, bloomtype=ROW, size=164.5m, encoding=NONE, seqNum=1323, earliestPutTs=1403894478181
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/2d305b82b01c4637b6b389148dd37ce9, keycount=459895, bloomtype=ROW, size=327.8m, encoding=NONE, seqNum=2160, earliestPutTs=1403894929417
2014-06-27 17:54:43,108 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/0679e2895dbf4d47a7be11a73e973998, keycount=14529, bloomtype=ROW, size=10.4m, encoding=NONE, seqNum=2193, earliestPutTs=1403895829885
2014-06-27 17:54:43,109 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/463cca76d9424d7a83b8e9772ce56bde, keycount=11056, bloomtype=ROW, size=7.9m, encoding=NONE, seqNum=2233, earliestPutTs=1403900599936
2014-06-27 17:54:43,109 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/4b1b971d96e54255b50f3e5a8def7110, keycount=24165, bloomtype=ROW, size=17.2m, encoding=NONE, seqNum=2284, earliestPutTs=1403900646819
2014-06-27 17:54:43,109 DEBUG [regionserver60020-smallCompactions-1403916883100] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/298b9b6d9b15410aa2916beaf774534d, keycount=6152, bloomtype=ROW, size=4.4m, encoding=NONE, seqNum=2296, earliestPutTs=1403900856017
2014-06-27 17:54:43,116 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:54:43,252 DEBUG [regionserver60020-smallCompactions-1403916883100] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:54:43,254 INFO  [regionserver60020-smallCompactions-1403916883100] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:43,254 INFO  [regionserver60020-smallCompactions-1403916883100] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:43,358 INFO  [PostOpenDeployTasks:6302562d3142c0f697210f458fb6fc5d] catalog.MetaEditor: Updated row usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. with server=sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:43,360 INFO  [PostOpenDeployTasks:6302562d3142c0f697210f458fb6fc5d] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d.
2014-06-27 17:54:43,360 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6302562d3142c0f697210f458fb6fc5d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:43,370 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6302562d3142c0f697210f458fb6fc5d from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:43,370 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 6302562d3142c0f697210f458fb6fc5d to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:43,370 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:43,371 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ada638120bf588c7dab6abbb5e43934a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:43,376 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ada638120bf588c7dab6abbb5e43934a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:43,376 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => ada638120bf588c7dab6abbb5e43934a, NAME => 'usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-06-27 17:54:43,377 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:43,377 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a.
2014-06-27 17:54:43,382 INFO  [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [9, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-27 17:54:43,400 DEBUG [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/9d5b29a086ce4ca89dd21e92e54056b4, isReference=false, isBulkLoadResult=false, seqid=465, majorCompaction=false
2014-06-27 17:54:43,407 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:43,407 DEBUG [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/9fefd1f1781e4003a06dcac7bf4752a0, isReference=false, isBulkLoadResult=false, seqid=65, majorCompaction=false
2014-06-27 17:54:43,415 DEBUG [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/bb8233d9196643ebaa389a2d47d5c024, isReference=false, isBulkLoadResult=false, seqid=2128, majorCompaction=false
2014-06-27 17:54:43,422 DEBUG [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/bdd8e1b4af49448882e2d827a6f078ab, isReference=false, isBulkLoadResult=false, seqid=2190, majorCompaction=false
2014-06-27 17:54:43,428 DEBUG [StoreOpener-ada638120bf588c7dab6abbb5e43934a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/ffe50328800743e59825aa9b61fba99e, isReference=false, isBulkLoadResult=false, seqid=2268, majorCompaction=false
2014-06-27 17:54:43,439 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 10 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:43,440 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2206 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002206
2014-06-27 17:54:43,441 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2215 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002215
2014-06-27 17:54:43,443 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2224 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002224
2014-06-27 17:54:43,444 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2233 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002233
2014-06-27 17:54:43,445 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2237 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002237
2014-06-27 17:54:43,447 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2253 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002253
2014-06-27 17:54:43,448 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 2267 and minimum sequenceid for the region is 2268, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002267
2014-06-27 17:54:43,449 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002276
2014-06-27 17:54:43,656 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:43,658 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 39560, skipped 0, firstSequenceidInLog=2269, maxSequenceidInLog=2276, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002276
2014-06-27 17:54:43,660 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002280
2014-06-27 17:54:43,770 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:43,772 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 22150, skipped 0, firstSequenceidInLog=2277, maxSequenceidInLog=2280, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002280
2014-06-27 17:54:43,774 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002282
2014-06-27 17:54:43,826 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:43,829 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 11030, skipped 0, firstSequenceidInLog=2281, maxSequenceidInLog=2282, path=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002282
2014-06-27 17:54:43,829 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Started memstore flush for usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a., current region memstore size 20.0m; wal is null, using passed sequenceid=2282
2014-06-27 17:54:43,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:54:43,850 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:43,851 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] compress.CodecPool: Got brand-new compressor
2014-06-27 17:54:44,440 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2282, memsize=20.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/.tmp/612427a3d1c041198eedad755a08c20d
2014-06-27 17:54:44,449 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/.tmp/612427a3d1c041198eedad755a08c20d as hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/612427a3d1c041198eedad755a08c20d
2014-06-27 17:54:44,456 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/family/612427a3d1c041198eedad755a08c20d, entries=72740, sequenceid=2282, filesize=5.2m
2014-06-27 17:54:44,456 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Finished memstore flush of ~20.0m/20948960, currentsize=0.0/0 for region usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a. in 627ms, sequenceid=2282, compaction requested=false; wal=null
2014-06-27 17:54:44,457 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002206
2014-06-27 17:54:44,458 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002215
2014-06-27 17:54:44,459 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002224
2014-06-27 17:54:44,460 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002233
2014-06-27 17:54:44,461 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002237
2014-06-27 17:54:44,463 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002253
2014-06-27 17:54:44,463 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002267
2014-06-27 17:54:44,464 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002276
2014-06-27 17:54:44,465 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002280
2014-06-27 17:54:44,466 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/ada638120bf588c7dab6abbb5e43934a/recovered.edits/0000000000000002282
2014-06-27 17:54:44,468 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined ada638120bf588c7dab6abbb5e43934a; next sequenceid=2283
2014-06-27 17:54:44,468 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node ada638120bf588c7dab6abbb5e43934a
2014-06-27 17:54:44,472 INFO  [PostOpenDeployTasks:ada638120bf588c7dab6abbb5e43934a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a.
2014-06-27 17:54:44,480 INFO  [PostOpenDeployTasks:ada638120bf588c7dab6abbb5e43934a] catalog.MetaEditor: Updated row usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a. with server=sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,480 INFO  [PostOpenDeployTasks:ada638120bf588c7dab6abbb5e43934a] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a.
2014-06-27 17:54:44,481 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning ada638120bf588c7dab6abbb5e43934a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,488 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node ada638120bf588c7dab6abbb5e43934a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,488 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned ada638120bf588c7dab6abbb5e43934a to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,488 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user8,1403893600646.ada638120bf588c7dab6abbb5e43934a. on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,488 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:44,494 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-27 17:54:44,494 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => e5ee55a21ff19d69490518939b0887e0, NAME => 'hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.', STARTKEY => '', ENDKEY => ''}
2014-06-27 17:54:44,495 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table namespace e5ee55a21ff19d69490518939b0887e0
2014-06-27 17:54:44,495 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 17:54:44,500 INFO  [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 604800000, major jitter 0.500000
2014-06-27 17:54:44,559 DEBUG [StoreOpener-e5ee55a21ff19d69490518939b0887e0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0/info/5b0102065d284f308d4c0a8d64d9fab5, isReference=false, isBulkLoadResult=false, seqid=4, majorCompaction=false
2014-06-27 17:54:44,562 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/hbase/namespace/e5ee55a21ff19d69490518939b0887e0
2014-06-27 17:54:44,563 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined e5ee55a21ff19d69490518939b0887e0; next sequenceid=5
2014-06-27 17:54:44,564 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e5ee55a21ff19d69490518939b0887e0
2014-06-27 17:54:44,567 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Post open deploy tasks for region=hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 17:54:44,575 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] catalog.MetaEditor: Updated row hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. with server=sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,575 INFO  [PostOpenDeployTasks:e5ee55a21ff19d69490518939b0887e0] regionserver.HRegionServer: Finished post open deploy task for hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0.
2014-06-27 17:54:44,576 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,585 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e5ee55a21ff19d69490518939b0887e0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,585 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned e5ee55a21ff19d69490518939b0887e0 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,585 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened hbase:namespace,,1402645258293.e5ee55a21ff19d69490518939b0887e0. on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,897 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2290, memsize=59.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/.tmp/9a2e2f98a2754755b6894687a4f6ed34
2014-06-27 17:54:44,907 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/.tmp/9a2e2f98a2754755b6894687a4f6ed34 as hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/family/9a2e2f98a2754755b6894687a4f6ed34
2014-06-27 17:54:44,915 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/family/9a2e2f98a2754755b6894687a4f6ed34, entries=216990, sequenceid=2290, filesize=15.5m
2014-06-27 17:54:44,915 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Finished memstore flush of ~59.7m/62586960, currentsize=0.0/0 for region usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585. in 1912ms, sequenceid=2290, compaction requested=false; wal=null
2014-06-27 17:54:44,916 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002205
2014-06-27 17:54:44,917 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002215
2014-06-27 17:54:44,918 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002225
2014-06-27 17:54:44,919 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002233
2014-06-27 17:54:44,920 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002244
2014-06-27 17:54:44,921 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002254
2014-06-27 17:54:44,922 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002267
2014-06-27 17:54:44,923 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002277
2014-06-27 17:54:44,924 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002286
2014-06-27 17:54:44,925 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/6b6070a8b8f9bff0f0254b8ea39a1585/recovered.edits/0000000000000002290
2014-06-27 17:54:44,927 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 6b6070a8b8f9bff0f0254b8ea39a1585; next sequenceid=2291
2014-06-27 17:54:44,927 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 6b6070a8b8f9bff0f0254b8ea39a1585
2014-06-27 17:54:44,933 INFO  [PostOpenDeployTasks:6b6070a8b8f9bff0f0254b8ea39a1585] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585.
2014-06-27 17:54:44,941 INFO  [PostOpenDeployTasks:6b6070a8b8f9bff0f0254b8ea39a1585] catalog.MetaEditor: Updated row usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585. with server=sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,941 INFO  [PostOpenDeployTasks:6b6070a8b8f9bff0f0254b8ea39a1585] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585.
2014-06-27 17:54:44,942 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 6b6070a8b8f9bff0f0254b8ea39a1585 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,949 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 6b6070a8b8f9bff0f0254b8ea39a1585 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:44,949 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 6b6070a8b8f9bff0f0254b8ea39a1585 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:44,949 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585. on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:45,087 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2294, memsize=65.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/.tmp/47b1a85e011d4afa954b1c111b28b209
2014-06-27 17:54:45,096 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/.tmp/47b1a85e011d4afa954b1c111b28b209 as hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/47b1a85e011d4afa954b1c111b28b209
2014-06-27 17:54:45,103 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/family/47b1a85e011d4afa954b1c111b28b209, entries=238440, sequenceid=2294, filesize=17.0m
2014-06-27 17:54:45,104 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Finished memstore flush of ~65.7m/68864560, currentsize=0.0/0 for region usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c. in 2010ms, sequenceid=2294, compaction requested=false; wal=null
2014-06-27 17:54:45,105 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002209
2014-06-27 17:54:45,106 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002220
2014-06-27 17:54:45,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002229
2014-06-27 17:54:45,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002238
2014-06-27 17:54:45,109 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002250
2014-06-27 17:54:45,110 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002259
2014-06-27 17:54:45,110 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002272
2014-06-27 17:54:45,111 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002282
2014-06-27 17:54:45,112 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9b741484aa13ab2851f1f938d0b7b98c/recovered.edits/0000000000000002294
2014-06-27 17:54:45,114 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 9b741484aa13ab2851f1f938d0b7b98c; next sequenceid=2295
2014-06-27 17:54:45,114 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9b741484aa13ab2851f1f938d0b7b98c
2014-06-27 17:54:45,118 INFO  [PostOpenDeployTasks:9b741484aa13ab2851f1f938d0b7b98c] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c.
2014-06-27 17:54:45,127 INFO  [PostOpenDeployTasks:9b741484aa13ab2851f1f938d0b7b98c] catalog.MetaEditor: Updated row usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c. with server=sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:45,127 INFO  [PostOpenDeployTasks:9b741484aa13ab2851f1f938d0b7b98c] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c.
2014-06-27 17:54:45,128 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9b741484aa13ab2851f1f938d0b7b98c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:45,135 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46dff75d470000, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9b741484aa13ab2851f1f938d0b7b98c from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-27 17:54:45,135 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 9b741484aa13ab2851f1f938d0b7b98c to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:45,135 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c. on sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:54:55,669 INFO  [regionserver60020-smallCompactions-1403916883100] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:55,704 INFO  [regionserver60020-smallCompactions-1403916883100] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:55,916 INFO  [RpcServer.handler=26,port=60020] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:57,196 INFO  [regionserver60020-smallCompactions-1403916883100] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:57,338 INFO  [RpcServer.handler=29,port=60020] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:57,571 INFO  [RpcServer.handler=26,port=60020] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:57,571 INFO  [RpcServer.handler=36,port=60020] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:57,571 INFO  [RpcServer.handler=42,port=60020] compress.CodecPool: Got brand-new decompressor
2014-06-27 17:54:58,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 17:54:58,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916872442 with entries=73, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916898720
2014-06-27 17:55:06,054 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 17:55:06,097 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 148 synced till here 146
2014-06-27 17:55:06,128 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916898720 with entries=75, filesize=64.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916906054
2014-06-27 17:55:14,987 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 17:55:15,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916906054 with entries=71, filesize=61.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916914988
2014-06-27 17:55:17,996 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 17:55:17,997 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. due to global heap pressure
2014-06-27 17:55:17,997 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d., current region memstore size 93.1m
2014-06-27 17:55:18,044 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:55:19,957 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:20,247 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:20,363 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:20,380 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:20,533 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:20,863 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,108 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,112 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1147ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1220ms
2014-06-27 17:55:22,192 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,351 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,466 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,492 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,511 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:22,560 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 17:55:22,560 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c. due to global heap pressure
2014-06-27 17:55:22,561 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1403893600646.9b741484aa13ab2851f1f938d0b7b98c., current region memstore size 98.1m
2014-06-27 17:55:22,736 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-27 17:55:22,742 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,116 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,129 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,284 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,308 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,412 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1403916870929: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-27 17:55:24,789 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2358, memsize=93.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/.tmp/de6ef206c2fa49e187c1b65405981d64
2014-06-27 17:55:24,801 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/.tmp/de6ef206c2fa49e187c1b65405981d64 as hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/de6ef206c2fa49e187c1b65405981d64
2014-06-27 17:55:24,811 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/6302562d3142c0f697210f458fb6fc5d/family/de6ef206c2fa49e187c1b65405981d64, entries=339000, sequenceid=2358, filesize=24.2m
2014-06-27 17:55:24,811 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~93.1m/97629600, currentsize=4.6m/4783280 for region usertable,user2,1403893600646.6302562d3142c0f697210f458fb6fc5d. in 6814ms, sequenceid=2358, compaction requested=false
2014-06-27 17:55:24,814 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 402ms
2014-06-27 17:55:24,814 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,814 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 506ms
2014-06-27 17:55:24,814 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,814 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 530ms
2014-06-27 17:55:24,815 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,815 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 686ms
2014-06-27 17:55:24,815 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,815 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 700ms
2014-06-27 17:55:24,815 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,816 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2074ms
2014-06-27 17:55:24,816 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,816 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2305ms
2014-06-27 17:55:24,817 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,817 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2325ms
2014-06-27 17:55:24,819 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,819 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2353ms
2014-06-27 17:55:24,819 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,820 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2468ms
2014-06-27 17:55:24,820 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,821 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2628ms
2014-06-27 17:55:24,821 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,821 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2713ms
2014-06-27 17:55:24,821 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,821 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3959ms
2014-06-27 17:55:24,821 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,821 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4288ms
2014-06-27 17:55:24,822 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,822 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4442ms
2014-06-27 17:55:24,822 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,823 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4459ms
2014-06-27 17:55:24,823 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,823 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4576ms
2014-06-27 17:55:24,825 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:24,826 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4869ms
2014-06-27 17:55:24,826 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1403916870929
2014-06-27 17:55:28,170 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1472ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1585ms
2014-06-27 17:55:29,867 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1196ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1424ms
2014-06-27 17:55:31,998 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1630ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1902ms
2014-06-27 17:55:32,129 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11388,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33981","starttimems":1403916918791,"queuetimems":1,"class":"HRegionServer","responsesize":16134,"method":"Multi"}
2014-06-27 17:55:33,739 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1240ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1510ms
2014-06-27 17:55:35,889 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1649ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2000ms
2014-06-27 17:55:35,983 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-27 17:55:36,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 302 synced till here 298
2014-06-27 17:55:37,536 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1146ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1467ms
2014-06-27 17:55:39,208 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1171ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1550ms
2014-06-27 17:55:39,242 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916914988 with entries=83, filesize=67.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403916870929/sceplus-vm48.almaden.ibm.com%2C60020%2C1403916870929.1403916935984
2014-06-27 17:55:39,243 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-27 17:55:39,243 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585. due to global heap pressure
2014-06-27 17:55:39,245 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1403893600646.6b6070a8b8f9bff0f0254b8ea39a1585., current region memstore size 99.0m
2014-06-27 17:55:40,749 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1041ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1394ms
2014-06-27 17:55:42,859 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1609ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2058ms
2014-06-27 17:55:44,473 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1113ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1600ms
2014-06-27 17:55:46,421 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1448ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1940ms
2014-06-27 17:56:07,576 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1640ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2133ms
2014-06-27 17:56:12,064 WARN  [RpcServer.handler=31,port=60020] regionserver.HRegion: Failed getting lock in batch put, row=user602519981969147017
java.io.IOException: Timed out waiting for lock for row: user602519981969147017
	at org.apache.hadoop.hbase.regionserver.HRegion.getRowLock(HRegion.java:3508)
	at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutation(HRegion.java:2381)
	at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2248)
	at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2200)
	at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:2204)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.doBatchOp(HRegionServer.java:4263)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.doNonAtomicRegionMutation(HRegionServer.java:3479)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:3369)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29503)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:168)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:39)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:111)
	at java.lang.Thread.run(Thread.java:701)
2014-06-27 17:56:26,842 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-27 17:56:26,843 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-27 17:56:26,844 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-27 17:56:26,844 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-27 17:56:26,844 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-27 17:56:26,844 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-27 17:56:26,844 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-27 17:56:26,845 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@2959ee1d
2014-06-27 17:56:26,891 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 17:56:26,900 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 17:56:26,948 INFO  [main-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46dff75d470004, negotiated timeout = 30000
Fri Jun 27 21:35:39 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-06-27 21:35:39,637 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-06-27 21:35:39,638 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-06-27 21:35:39,638 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-06-27 21:35:39,873 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-06-27 21:35:39,873 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-06-27 21:35:39,873 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 47025 22
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-06-27 21:35:39,874 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 47025 9.1.143.58 22
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-06-27 21:35:39,875 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-06-27 21:35:39,877 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=197
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-06-27 21:35:39,878 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-06-27 21:35:39,879 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-06-27 21:35:39,879 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-06-27 21:35:39,879 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-06-27 21:35:39,881 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-06-27 21:35:39,881 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx1000m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-06-27 21:35:40,101 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-06-27 21:35:40,450 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-06-27 21:35:40,531 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-06-27 21:35:40,543 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-06-27 21:35:40,606 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-06-27 21:35:40,607 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-06-27 21:35:40,607 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-06-27 21:35:40,612 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-06-27 21:35:40,617 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-06-27 21:35:40,695 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-06-27 21:35:40,695 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-06-27 21:35:40,699 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-06-27 21:35:40,701 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7m
2014-06-27 21:35:40,766 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-06-27 21:35:40,818 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-06-27 21:35:40,827 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-06-27 21:35:40,829 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-06-27 21:35:40,829 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-06-27 21:35:40,829 INFO  [main] mortbay.log: jetty-6.1.26
2014-06-27 21:35:41,135 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-27 21:35:41,178 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-27 21:35:41,180 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-06-27 21:35:41,180 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 21:35:41,202 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 21:35:41,205 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 21:35:41,209 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 21:35:41,228 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46e0c1d7210000, negotiated timeout = 90000
2014-06-27 21:35:41,279 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x6566aa35, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 21:35:41,280 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6566aa35 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 21:35:41,281 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 21:35:41,282 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-27 21:35:41,286 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146e0c1c0500003, negotiated timeout = 90000
2014-06-27 21:35:41,534 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@25ea22df
2014-06-27 21:35:41,537 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-06-27 21:35:41,542 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-06-27 21:35:41,565 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-06-27 21:35:41,594 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-06-27 21:35:41,599 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7m, globalMemStoreLimitLowMark=367.3m, maxHeap=966.7m
2014-06-27 21:35:41,603 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-06-27 21:35:41,616 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1403930139160 with port=60020, startcode=1403930140626
2014-06-27 21:35:41,998 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-06-27 21:35:41,998 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-06-27 21:35:42,027 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-06-27 21:35:42,035 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403930140626
2014-06-27 21:35:42,078 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-06-27 21:35:42,091 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-06-27 21:35:42,161 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403930140626/sceplus-vm48.almaden.ibm.com%2C60020%2C1403930140626.1403930142097
2014-06-27 21:35:42,172 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-06-27 21:35:42,176 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-06-27 21:35:42,180 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-06-27 21:35:42,184 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-06-27 21:35:42,187 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 21:35:42,187 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 21:35:42,187 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-27 21:35:42,187 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-27 21:35:42,188 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-06-27 21:35:42,194 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1403930134835, sceplus-vm48.almaden.ibm.com,60020,1403930140626] other RSs: [slave1,60020,1403930134835, sceplus-vm48.almaden.ibm.com,60020,1403930140626]
2014-06-27 21:35:42,216 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-06-27 21:35:42,219 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x27e2a446, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-27 21:35:42,220 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x27e2a446 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-27 21:35:42,220 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-27 21:35:42,221 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-27 21:35:42,226 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46e0c1d7210001, negotiated timeout = 90000
2014-06-27 21:35:42,236 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-06-27 21:35:42,236 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-06-27 21:35:42,283 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1403930140626, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x46e0c1d7210000
2014-06-27 21:35:42,283 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403930140626] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1403930140626 starting
2014-06-27 21:35:42,283 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-06-27 21:35:42,283 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1403930140626
2014-06-27 21:35:42,283 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1403930140626'
2014-06-27 21:35:42,283 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-06-27 21:35:42,284 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-06-27 21:35:42,285 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-06-27 21:36:39,404 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: Exiting; cluster shutdown set and not carrying any regions
2014-06-27 21:36:39,408 INFO  [regionserver60020] ipc.RpcServer: Stopping server on 60020
2014-06-27 21:36:39,410 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: stopping
2014-06-27 21:36:39,413 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2014-06-27 21:36:39,414 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2014-06-27 21:36:39,423 INFO  [regionserver60020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2014-06-27 21:36:39,424 INFO  [regionserver60020] regionserver.HRegionServer: Stopping infoServer
2014-06-27 21:36:39,426 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403930140626] regionserver.SplitLogWorker: SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2014-06-27 21:36:39,427 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1403930140626] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1403930140626 exiting
2014-06-27 21:36:39,430 INFO  [regionserver60020] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:60030
2014-06-27 21:36:39,557 INFO  [regionserver60020.logRoller] regionserver.LogRoller: LogRoller exiting.
2014-06-27 21:36:39,557 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: MemStoreFlusher.0 exiting
2014-06-27 21:36:39,557 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: MemStoreFlusher.1 exiting
2014-06-27 21:36:39,557 INFO  [regionserver60020.compactionChecker] regionserver.HRegionServer$CompactionChecker: regionserver60020.compactionChecker exiting
2014-06-27 21:36:39,557 INFO  [regionserver60020.nonceCleaner] regionserver.ServerNonceManager$1: regionserver60020.nonceCleaner exiting
2014-06-27 21:36:39,557 INFO  [regionserver60020] snapshot.RegionServerSnapshotManager: Stopping RegionServerSnapshotManager gracefully.
2014-06-27 21:36:39,559 INFO  [regionserver60020] regionserver.HRegionServer: stopping server sceplus-vm48.almaden.ibm.com,60020,1403930140626
2014-06-27 21:36:39,559 DEBUG [regionserver60020] catalog.CatalogTracker: Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@25ea22df
2014-06-27 21:36:39,559 INFO  [regionserver60020] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x146e0c1c0500003
2014-06-27 21:36:39,564 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x146e0c1c0500003 closed
2014-06-27 21:36:39,564 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-06-27 21:36:39,571 INFO  [regionserver60020] regionserver.HRegionServer: stopping server sceplus-vm48.almaden.ibm.com,60020,1403930140626; all regions closed.
2014-06-27 21:36:39,571 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-06-27 21:36:39,571 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-06-27 21:36:39,572 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-06-27 21:36:39,572 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-06-27 21:36:39,572 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-06-27 21:36:39,572 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-06-27 21:36:39,573 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-06-27 21:36:39,573 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-06-27 21:36:39,573 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-06-27 21:36:39,573 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-06-27 21:36:39,573 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-06-27 21:36:39,573 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-06-27 21:36:39,574 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-06-27 21:36:39,574 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-06-27 21:36:39,574 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1403930140626
2014-06-27 21:36:39,592 DEBUG [regionserver60020] wal.FSHLog: Moved 1 WAL file(s) to /hbase/oldWALs
2014-06-27 21:36:39,694 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-06-27 21:36:39,694 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-06-27 21:36:42,190 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-06-27 21:36:42,191 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-06-27 21:36:42,191 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-06-27 21:36:42,191 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-06-27 21:36:42,192 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-06-27 21:36:42,192 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-06-27 21:36:42,192 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-06-27 21:36:42,203 INFO  [regionserver60020] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x46e0c1d7210001
2014-06-27 21:36:42,208 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x46e0c1d7210001 closed
2014-06-27 21:36:42,208 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-06-27 21:36:42,221 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x46e0c1d7210000 closed
2014-06-27 21:36:42,221 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-06-27 21:36:42,221 INFO  [regionserver60020] regionserver.HRegionServer: stopping server sceplus-vm48.almaden.ibm.com,60020,1403930140626; zookeeper connection closed.
2014-06-27 21:36:42,221 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-06-27 21:36:42,225 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=Thread[Thread-9,5,main]
2014-06-27 21:36:42,225 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-06-27 21:36:42,227 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook finished.
